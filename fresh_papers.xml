<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 30 Mar 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling</title><link>http://arxiv.org/abs/2403.19655v1</link><description>3D Gaussian Splatting (GS) have achieved considerable improvement over NeuralRadiance Fields in terms of 3D fitting fidelity and rendering speed. However,this unstructured representation with scattered Gaussians poses a significantchallenge for generative modeling. To address the problem, we introduceGaussianCube, a structured GS representation that is both powerful andefficient for generative modeling. We achieve this by first proposing amodified densification-constrained GS fitting algorithm which can yieldhigh-quality fitting results using a fixed number of free Gaussians, and thenre-arranging the Gaussians into a predefined voxel grid via Optimal Transport.The structured grid representation allows us to use standard 3D U-Net as ourbackbone in diffusion generative modeling without elaborate designs. Extensiveexperiments conducted on ShapeNet and OmniObject3D show that our model achievesstate-of-the-art generation results both qualitatively and quantitatively,underscoring the potential of GaussianCube as a powerful and versatile 3Drepresentation.</description><author>Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</author><pubDate>Thu, 28 Mar 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19655v1</guid></item><item><title>RSMamba: Remote Sensing Image Classification with State Space Model</title><link>http://arxiv.org/abs/2403.19654v1</link><description>Remote sensing image classification forms the foundation of variousunderstanding tasks, serving a crucial function in remote sensing imageinterpretation. The recent advancements of Convolutional Neural Networks (CNNs)and Transformers have markedly enhanced classification accuracy. Nonetheless,remote sensing scene classification remains a significant challenge, especiallygiven the complexity and diversity of remote sensing scenarios and thevariability of spatiotemporal resolutions. The capacity for whole-imageunderstanding can provide more precise semantic cues for scene discrimination.In this paper, we introduce RSMamba, a novel architecture for remote sensingimage classification. RSMamba is based on the State Space Model (SSM) andincorporates an efficient, hardware-aware design known as the Mamba. Itintegrates the advantages of both a global receptive field and linear modelingcomplexity. To overcome the limitation of the vanilla Mamba, which can onlymodel causal sequences and is not adaptable to two-dimensional image data, wepropose a dynamic multi-path activation mechanism to augment Mamba's capacityto model non-causal data. Notably, RSMamba maintains the inherent modelingmechanism of the vanilla Mamba, yet exhibits superior performance acrossmultiple remote sensing image classification datasets. This indicates thatRSMamba holds significant potential to function as the backbone of futurevisual foundation models. The code will be available at\url{https://github.com/KyanChen/RSMamba}.</description><author>Keyan Chen, Bowen Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, Zhenwei Shi</author><pubDate>Thu, 28 Mar 2024 18:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19654v1</guid></item><item><title>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond</title><link>http://arxiv.org/abs/2403.19653v1</link><description>Modern text-to-image (T2I) diffusion models can generate images withremarkable realism and creativity. These advancements have sparked research infake image detection and attribution, yet prior studies have not fully exploredthe practical and scientific dimensions of this task. In addition toattributing images to 12 state-of-the-art T2I generators, we provide extensiveanalyses on what inference stage hyperparameters and image modifications arediscernible. Our experiments reveal that initialization seeds are highlydetectable, along with other subtle variations in the image generation processto some extent. We further investigate what visual traces are leveraged inimage attribution by perturbing high-frequency details and employing mid-levelrepresentations of image style and structure. Notably, altering high-frequencyinformation causes only slight reductions in accuracy, and training anattributor on style representations outperforms training on RGB images. Ouranalyses underscore that fake images are detectable and attributable at variouslevels of visual granularity than previously explored.</description><author>Katherine Xu, Lingzhi Zhang, Jianbo Shi</author><pubDate>Thu, 28 Mar 2024 18:59:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19653v1</guid></item><item><title>InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction</title><link>http://arxiv.org/abs/2403.19652v1</link><description>Text-conditioned human motion generation has experienced significantadvancements with diffusion models trained on extensive motion capture data andcorresponding textual annotations. However, extending such success to 3Ddynamic human-object interaction (HOI) generation faces notable challenges,primarily due to the lack of large-scale interaction data and comprehensivedescriptions that align with these interactions. This paper takes theinitiative and showcases the potential of generating human-object interactionswithout direct training on text-interaction pair data. Our key insight inachieving this is that interaction semantics and dynamics can be decoupled.Being unable to learn interaction semantics through supervised training, weinstead leverage pre-trained large models, synergizing knowledge from a largelanguage model and a text-to-motion model. While such knowledge offershigh-level control over interaction semantics, it cannot grasp the intricaciesof low-level interaction dynamics. To overcome this issue, we further introducea world model designed to comprehend simple physics, modeling how human actionsinfluence object motion. By integrating these components, our novel framework,InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shotmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and ourcomprehensive experimental analysis demonstrates its capability to generaterealistic and coherent interaction sequences that seamlessly align with thetext directives.</description><author>Sirui Xu, Ziyin Wang, Yu-Xiong Wang, Liang-Yan Gui</author><pubDate>Thu, 28 Mar 2024 18:59:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19652v1</guid></item><item><title>MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</title><link>http://arxiv.org/abs/2403.19651v1</link><description>Image retrieval, i.e., finding desired images given a reference image,inherently encompasses rich, multi-faceted search intents that are difficult tocapture solely using image-based measures. Recent work leverages textinstructions to allow users to more freely express their search intents.However, existing work primarily focuses on image pairs that are visuallysimilar and/or can be characterized by a small set of pre-defined relations.The core thesis of this paper is that text instructions can enable retrievingimages with richer relations beyond visual similarity. To show this, weintroduce MagicLens, a series of self-supervised image retrieval models thatsupport open-ended instructions. MagicLens is built on a key novel insight:image pairs that naturally occur on the same web pages contain a wide range ofimplicit relations (e.g., inside view of), and we can bring those implicitrelations explicit by synthesizing instructions via large multimodal models(LMMs) and large language models (LLMs). Trained on 36.7M (query image,instruction, target image) triplets with rich semantic relations mined from theweb, MagicLens achieves comparable or better results on eight benchmarks ofvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.Remarkably, it outperforms previous SOTA but with a 50X smaller model size onmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpusfurther demonstrate the diversity of search intents supported by MagicLens.</description><author>Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, Ming-Wei Chang</author><pubDate>Thu, 28 Mar 2024 18:59:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19651v1</guid></item><item><title>ACT-Diffusion: Efficient Adversarial Consistency Training for One-step Diffusion Models</title><link>http://arxiv.org/abs/2311.14097v3</link><description>Though diffusion models excel in image generation, their step-by-stepdenoising leads to slow generation speeds. Consistency training addresses thisissue with single-step sampling but often produces lower-quality generationsand requires high training costs. In this paper, we show that optimizingconsistency training loss minimizes the Wasserstein distance between target andgenerated distributions. As timestep increases, the upper bound accumulatesprevious consistency training losses. Therefore, larger batch sizes are neededto reduce both current and accumulated losses. We propose AdversarialConsistency Training (ACT), which directly minimizes the Jensen-Shannon (JS)divergence between distributions at each timestep using a discriminator.Theoretically, ACT enhances generation quality, and convergence. Byincorporating a discriminator into the consistency training framework, ourmethod achieves improved FID scores on CIFAR10 and ImageNet 64$\times$64 andLSUN Cat 256$\times$256 datasets, retains zero-shot image inpaintingcapabilities, and uses less than $1/6$ of the original batch size and fewerthan $1/2$ of the model parameters and training steps compared to the baselinemethod, this leads to a substantial reduction in resource consumption. Our codeis available:https://github.com/kong13661/ACT</description><author>Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Xu, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu</author><pubDate>Thu, 28 Mar 2024 18:59:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14097v3</guid></item><item><title>GraspXL: Generating Grasping Motions for Diverse Objects at Scale</title><link>http://arxiv.org/abs/2403.19649v1</link><description>Human hands possess the dexterity to interact with diverse objects such asgrasping specific parts of the objects and/or approaching them from desireddirections. More importantly, humans can grasp objects of any shape withoutobject-specific skills. Recent works synthesize grasping motions followingsingle objectives such as a desired approach heading direction or a graspingarea. Moreover, they usually rely on expensive 3D hand-object data duringtraining and inference, which limits their capability to synthesize graspingmotions for unseen objects at scale. In this paper, we unify the generation ofhand-object grasping motions across multiple motion objectives, diverse objectshapes and dexterous hand morphologies in a policy learning framework GraspXL.The objectives are composed of the graspable area, heading direction duringapproach, wrist rotation, and hand position. Without requiring any 3Dhand-object interaction data, our policy trained with 58 objects can robustlysynthesize diverse grasping motions for more than 500k unseen objects with asuccess rate of 82.2%. At the same time, the policy adheres to objectives,which enables the generation of diverse grasps per object. Moreover, we showthat our framework can be deployed to different dexterous hands and work withreconstructed or generated objects. We quantitatively and qualitativelyevaluate our method to show the efficacy of our approach. Our model and codewill be available.</description><author>Hui Zhang, Sammy Christen, Zicong Fan, Otmar Hilliges, Jie Song</author><pubDate>Thu, 28 Mar 2024 18:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19649v1</guid></item><item><title>Sharp bounds for max-sliced Wasserstein distances</title><link>http://arxiv.org/abs/2403.00666v3</link><description>We obtain essentially matching upper and lower bounds for the expectedmax-sliced 1-Wasserstein distance between a probability measure on a separableHilbert space and its empirical distribution from $n$ samples. By proving aBanach space version of this result, we also obtain an upper bound, that issharp up to a log factor, for the expected max-sliced 2-Wasserstein distancebetween a symmetric probability measure $\mu$ on a Euclidean space and itssymmetrized empirical distribution in terms of the operator norm of thecovariance matrix of $\mu$ and the diameter of the support of $\mu$.</description><author>March T. Boedihardjo</author><pubDate>Thu, 28 Mar 2024 18:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00666v3</guid></item><item><title>Human-compatible driving partners through data-regularized self-play reinforcement learning</title><link>http://arxiv.org/abs/2403.19648v1</link><description>A central challenge for autonomous vehicles is coordinating with humans.Therefore, incorporating realistic human agents is essential for scalabletraining and evaluation of autonomous driving systems in simulation. Simulationagents are typically developed by imitating large-scale, high-quality datasetsof human driving. However, pure imitation learning agents empirically have highcollision rates when executed in a multi-agent closed-loop setting. To buildagents that are realistic and effective in closed-loop settings, we proposeHuman-Regularized PPO (HR-PPO), a multi-agent algorithm where agents aretrained through self-play with a small penalty for deviating from a humanreference policy. In contrast to prior work, our approach is RL-first and onlyuses 30 minutes of imperfect human demonstrations. We evaluate agents in alarge set of multi-agent traffic scenes. Results show our HR-PPO agents arehighly effective in achieving goals, with a success rate of 93%, an off-roadrate of 3.5%, and a collision rate of 3%. At the same time, the agents drive ina human-like manner, as measured by their similarity to existing human drivinglogs. We also find that HR-PPO agents show considerable improvements on proxymeasures for coordination with human driving, particularly in highlyinteractive scenarios. We open-source our code and trained agents athttps://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agentbehaviors at https://sites.google.com/view/driving-partners.</description><author>Daphne Cornelisse, Eugene Vinitsky</author><pubDate>Thu, 28 Mar 2024 18:56:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19648v1</guid></item><item><title>RouterBench: A Benchmark for Multi-LLM Routing System</title><link>http://arxiv.org/abs/2403.12031v2</link><description>As the range of applications for Large Language Models (LLMs) continues togrow, the demand for effective serving solutions becomes increasingly critical.Despite the versatility of LLMs, no single model can optimally address alltasks and applications, particularly when balancing performance with cost. Thislimitation has led to the development of LLM routing systems, which combine thestrengths of various models to overcome the constraints of individual LLMs.Yet, the absence of a standardized benchmark for evaluating the performance ofLLM routers hinders progress in this area. To bridge this gap, we presentRouterBench, a novel evaluation framework designed to systematically assess theefficacy of LLM routing systems, along with a comprehensive dataset comprisingover 405k inference outcomes from representative LLMs to support thedevelopment of routing strategies. We further propose a theoretical frameworkfor LLM routing, and deliver a comparative analysis of various routingapproaches through RouterBench, highlighting their potentials and limitationswithin our evaluation framework. This work not only formalizes and advances thedevelopment of LLM routing systems but also sets a standard for theirassessment, paving the way for more accessible and economically viable LLMdeployments. The code and data are available athttps://github.com/withmartian/routerbench.</description><author>Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, Shriyash Kaustubh Upadhyay</author><pubDate>Thu, 28 Mar 2024 18:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12031v2</guid></item><item><title>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</title><link>http://arxiv.org/abs/2403.19647v1</link><description>We introduce methods for discovering and applying sparse feature circuits.These are causally implicated subnetworks of human-interpretable features forexplaining language model behaviors. Circuits identified in prior work consistof polysemantic and difficult-to-interpret units like attention heads orneurons, rendering them unsuitable for many downstream applications. Incontrast, sparse feature circuits enable detailed understanding ofunanticipated mechanisms. Because they are based on fine-grained units, sparsefeature circuits are useful for downstream tasks: We introduce SHIFT, where weimprove the generalization of a classifier by ablating features that a humanjudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervisedand scalable interpretability pipeline by discovering thousands of sparsefeature circuits for automatically discovered model behaviors.</description><author>Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller</author><pubDate>Thu, 28 Mar 2024 18:56:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19647v1</guid></item><item><title>Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates</title><link>http://arxiv.org/abs/2403.11687v2</link><description>We study the problem of efficiently computing the derivative of thefixed-point of a parametric nondifferentiable contraction map. This problem haswide applications in machine learning, including hyperparameter optimization,meta-learning and data poisoning attacks. We analyze two popular approaches:iterative differentiation (ITD) and approximate implicit differentiation (AID).A key challenge behind the nonsmooth setting is that the chain rule does nothold anymore. Building upon the recent work by Bolte et al. (2022), who provedlinear convergence of nondifferentiable ITD, we provide an improved linear ratefor ITD and a slightly better rate for AID, both in the deterministic case. Wefurther introduce NSID, a new stochastic method to compute the implicitderivative when the fixed point is defined as the composition of an outer mapand an inner map which is accessible only through a stochastic unbiasedestimator. We establish rates for the convergence of NSID, encompassing thebest available rates in the smooth setting. We present illustrative experimentsconfirming our analysis.</description><author>Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo</author><pubDate>Thu, 28 Mar 2024 18:56:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11687v2</guid></item><item><title>Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning</title><link>http://arxiv.org/abs/2403.19646v1</link><description>Monitoring changes in the Earth's surface is crucial for understandingnatural processes and human impacts, necessitating precise and comprehensiveinterpretation methodologies. Remote sensing satellite imagery offers a uniqueperspective for monitoring these changes, leading to the emergence of remotesensing image change interpretation (RSICI) as a significant research focus.Current RSICI technology encompasses change detection and change captioning,each with its limitations in providing comprehensive interpretation. To addressthis, we propose an interactive Change-Agent which integrates a multi-levelchange interpretation (MCI) model as eyes and a large language model (LLM) asthe brain. Our Change-Agent can follow user instructions to achievecomprehensive change interpretation and insightful analysis according to userinstructions, such as change detection and change captioning, change objectcounting, change cause analysis, etc. Our proposed MCI model contains twobranches of pixel-level change detection and semantic-level change captioning,in which multiple BI-temporal Iterative Interaction (BI3) layers utilize LocalPerception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA)modules to enhance the model's discriminative feature representationcapabilities. To train the MCI model, we build the LEVIR-MCI dataset withchange masks and captions of bi-temporal images. Extensive experimentsdemonstrate the effectiveness of the proposed change interpretation model andhighlight the promising potential of our Change-Agent in facilitatingcomprehensive and intelligent interpretation of surface changes. We will makeour dataset and codebase of the change interpretation model and Change-Agentpublicly available to facilitate future research athttps://github.com/Chen-Yang-Liu/Change-Agent</description><author>Chenyang Liu, Keyan Chen, Haotian Zhang, Zipeng Qi, Zhengxia Zou, Zhenwei Shi</author><pubDate>Thu, 28 Mar 2024 18:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19646v1</guid></item><item><title>Direct Superpoints Matching for Robust Point Cloud Registration</title><link>http://arxiv.org/abs/2307.01362v3</link><description>Deep neural networks endow the downsampled superpoints with highlydiscriminative feature representations. Previous dominant point cloudregistration approaches match these feature representations as the first step,e.g., using the Sinkhorn algorithm. A RANSAC-like method is then usuallyadopted as a post-processing refinement to filter the outliers. Other dominantmethod is to directly predict the superpoint matchings using learned MLPlayers. Both of them have drawbacks: RANSAC-based methods are computationallyintensive and prediction-based methods suffer from outputing non-existingpoints in the point cloud. In this paper, we propose a straightforward andeffective baseline to find correspondences of superpoints in a global matchingmanner. We employ the normalized matching scores as weights for eachcorrespondence, allowing us to reject the outliers and further weigh the restinliers when fitting the transformation matrix without relying on thecumbersome RANSAC. Moreover, the entire model can be trained in an end-to-endfashion, leading to better accuracy. Our simple yet effective baseline showscomparable or even better results than state-of-the-art methods on threedatasets including ModelNet, 3DMatch, and KITTI. We do not advocate ourapproach to be \emph{the} solution for point cloud registration but use theresults to emphasize the role of matching strategy for point cloudregistration. The code and models are available athttps://github.com/neu-vi/Superpoints_Registration.</description><author>Aniket Gupta, Yiming Xie, Hanumant Singh, Huaizu Jiang</author><pubDate>Thu, 28 Mar 2024 18:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01362v3</guid></item><item><title>GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models</title><link>http://arxiv.org/abs/2403.19645v1</link><description>The rapid advancement in image generation models has predominantly beendriven by diffusion models, which have demonstrated unparalleled success ingenerating high-fidelity, diverse images from textual prompts. Despite theirsuccess, diffusion models encounter substantial challenges in the domain ofimage editing, particularly in executing disentangled edits-changes that targetspecific attributes of an image while leaving irrelevant parts untouched. Incontrast, Generative Adversarial Networks (GANs) have been recognized for theirsuccess in disentangled edits through their interpretable latent spaces. Weintroduce GANTASTIC, a novel framework that takes existing directions frompre-trained GAN models-representative of specific, controllable attributes-andtransfers these directions into diffusion-based models. This novel approach notonly maintains the generative quality and diversity that diffusion models areknown for but also significantly enhances their capability to perform precise,targeted image edits, thereby leveraging the best of both worlds.</description><author>Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag</author><pubDate>Thu, 28 Mar 2024 18:55:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19645v1</guid></item><item><title>Learnable Earth Parser: Discovering 3D Prototypes in Aerial Scans</title><link>http://arxiv.org/abs/2304.09704v2</link><description>We propose an unsupervised method for parsing large 3D scans of real-worldscenes with easily-interpretable shapes. This work aims to provide a practicaltool for analyzing 3D scenes in the context of aerial surveying and mapping,without the need for user annotations. Our approach is based on a probabilisticreconstruction model that decomposes an input 3D point cloud into a small setof learned prototypical 3D shapes. The resulting reconstruction is visuallyinterpretable and can be used to perform unsupervised instance and low-shotsemantic segmentation of complex scenes. We demonstrate the usefulness of ourmodel on a novel dataset of seven large aerial LiDAR scans from diversereal-world scenarios. Our approach outperforms state-of-the-art unsupervisedmethods in terms of decomposition accuracy while remaining visuallyinterpretable. Our code and dataset are available athttps://romainloiseau.fr/learnable-earth-parser/</description><author>Romain Loiseau, Elliot Vincent, Mathieu Aubry, Loic Landrieu</author><pubDate>Thu, 28 Mar 2024 18:53:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09704v2</guid></item><item><title>Siamese Vision Transformers are Scalable Audio-visual Learners</title><link>http://arxiv.org/abs/2403.19638v1</link><description>Traditional audio-visual methods rely on independent audio and visualbackbones, which is costly and not scalable. In this work, we investigate usingan audio-visual siamese network (AVSiam) for efficient and scalableaudio-visual pretraining. Our framework uses a single shared vision transformerbackbone to process audio and visual inputs, improving its parameterefficiency, reducing the GPU memory footprint, and allowing us to scale ourmethod to larger datasets and model sizes. We pretrain our model using acontrastive audio-visual matching objective with a multi-ratio random maskingscheme, which enables our model to process larger audio-visual instancebatches, helpful for contrastive learning. Unlike prior audio-visual methods,our method can robustly handle audio, visual, and audio-visual inputs with asingle shared ViT backbone. Furthermore, despite using the shared backbone forboth modalities, AVSiam achieves competitive or even better results than priormethods on AudioSet and VGGSound for audio-visual classification and retrieval.Our code is available at https://github.com/GenjiB/AVSiam</description><author>Yan-Bo Lin, Gedas Bertasius</author><pubDate>Thu, 28 Mar 2024 18:52:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19638v1</guid></item><item><title>Asymmetric and trial-dependent modeling: the contribution of LIA to SdSV Challenge Task 2</title><link>http://arxiv.org/abs/2403.19634v1</link><description>The SdSv challenge Task 2 provided an opportunity to assess efficiency androbustness of modern text-independent speaker verification systems. But it alsomade it possible to test new approaches, capable of taking into account themain issues of this challenge (duration, language, ...). This paper describesthe contributions of our laboratory to the speaker recognition field. Thesecontributions highlight two other challenges in addition to short-duration andlanguage: the mismatch between enrollment and test data and the one betweensubsets of the evaluation trial dataset. The proposed approaches experimentallyshow their relevance and efficiency on the SdSv evaluation, and could be ofinterest in many real-life applications.</description><author>Pierre-Michel Bousquet, Mickael Rouvier</author><pubDate>Thu, 28 Mar 2024 18:49:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19634v1</guid></item><item><title>GauStudio: A Modular Framework for 3D Gaussian Splatting and Beyond</title><link>http://arxiv.org/abs/2403.19632v1</link><description>We present GauStudio, a novel modular framework for modeling 3D GaussianSplatting (3DGS) to provide standardized, plug-and-play components for users toeasily customize and implement a 3DGS pipeline. Supported by our framework, wepropose a hybrid Gaussian representation with foreground and skyball backgroundmodels. Experiments demonstrate this representation reduces artifacts inunbounded outdoor scenes and improves novel view synthesis. Finally, we proposeGaussian Splatting Surface Reconstruction (GauS), a novel render-then-fuseapproach for high-fidelity mesh reconstruction from 3DGS inputs withoutfine-tuning. Overall, our GauStudio framework, hybrid representation, and GauSapproach enhance 3DGS modeling and rendering capabilities, enablinghigher-quality novel view synthesis and surface reconstruction.</description><author>Chongjie Ye, Yinyu Nie, Jiahao Chang, Yuantao Chen, Yihao Zhi, Xiaoguang Han</author><pubDate>Thu, 28 Mar 2024 18:47:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19632v1</guid></item><item><title>Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models</title><link>http://arxiv.org/abs/2403.19631v1</link><description>Large Language Models (LLMs) have shown proficiency in question-answeringtasks but often struggle to integrate real-time knowledge updates, leading topotentially outdated or inaccurate responses. This problem becomes even morechallenging when dealing with multi-hop questions since they require LLMs toupdate and integrate multiple knowledge pieces relevant to the questions. Totackle the problem, we propose the Retrieval-Augmented model Editing (RAE)framework tailored for multi-hop question answering. RAE first retrieves editedfacts and then refines the language model through in-context learning.Specifically, our retrieval approach, based on mutual information maximization,leverages the reasoning abilities of LLMs to identify chain facts that na\"ivesimilarity-based searches might miss. Additionally, our framework incorporatesa pruning strategy to eliminate redundant information from the retrieved facts,which enhances the editing accuracy and mitigates the hallucination problem.Our framework is supported by theoretical justification for its fact retrievalefficacy. Finally, comprehensive evaluation across various LLMs validates RAE'sability in providing accurate answers with updated knowledge.</description><author>Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu</author><pubDate>Thu, 28 Mar 2024 18:47:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19631v1</guid></item><item><title>Metric Learning from Limited Pairwise Preference Comparisons</title><link>http://arxiv.org/abs/2403.19629v1</link><description>We study metric learning from preference comparisons under the ideal pointmodel, in which a user prefers an item over another if it is closer to theirlatent ideal item. These items are embedded into $\mathbb{R}^d$ equipped withan unknown Mahalanobis distance shared across users. While recent work showsthat it is possible to simultaneously recover the metric and ideal items given$\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have alimited budget of $o(d)$ comparisons. We study whether the metric can still berecovered, even though it is known that learning individual ideal items is nowno longer possible. We show that in general, $o(d)$ comparisons reveals noinformation about the metric, even with infinitely many users. However, whencomparisons are made over items that exhibit low-dimensional structure, eachuser can contribute to learning the metric restricted to a low-dimensionalsubspace so that the metric can be jointly identified. We present adivide-and-conquer approach that achieves this, and provide theoreticalrecovery guarantees and empirical validation.</description><author>Zhi Wang, Geelon So, Ramya Korlakai Vinayak</author><pubDate>Thu, 28 Mar 2024 18:46:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19629v1</guid></item><item><title>Top-$k$ Classification and Cardinality-Aware Prediction</title><link>http://arxiv.org/abs/2403.19625v1</link><description>We present a detailed study of top-$k$ classification, the task of predictingthe $k$ most probable classes for an input, extending beyond single-classprediction. We demonstrate that several prevalent surrogate loss functions inmulti-class classification, such as comp-sum and constrained losses, aresupported by $H$-consistency bounds with respect to the top-$k$ loss. Thesebounds guarantee consistency in relation to the hypothesis set $H$, providingstronger guarantees than Bayes-consistency due to their non-asymptotic andhypothesis-set specific nature. To address the trade-off between accuracy andcardinality $k$, we further introduce cardinality-aware loss functions throughinstance-dependent cost-sensitive learning. For these functions, we derivecost-sensitive comp-sum and constrained surrogate losses, establishing their$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads tonew cardinality-aware algorithms for top-$k$ classification. We report theresults of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHNdatasets demonstrating the effectiveness and benefit of these algorithms.</description><author>Anqi Mao, Mehryar Mohri, Yutao Zhong</author><pubDate>Thu, 28 Mar 2024 18:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19625v1</guid></item><item><title>RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents</title><link>http://arxiv.org/abs/2403.19622v1</link><description>The ultimate goals of robotic learning is to acquire a comprehensive andgeneralizable robotic system capable of performing both seen skills within thetraining distribution and unseen skills in novel environments. Recent progressin utilizing language models as high-level planners has demonstrated that thecomplexity of tasks can be reduced through decomposing them intoprimitive-level plans, making it possible to generalize on novel robotic tasksin a composable manner. Despite the promising future, the community is not yetadequately prepared for composable generalization agents, particularly due tothe lack of primitive-level real-world robotic datasets. In this paper, wepropose a primitive-level robotic dataset, namely RH20T-P, which contains about33000 video clips covering 44 diverse and complicated robotic tasks. Each clipis manually annotated according to a set of meticulously designed primitiveskills, facilitating the future development of composable generalizationagents. To validate the effectiveness of RH20T-P, we also construct a potentialand scalable agent based on RH20T-P, called RA-P. Equipped with two plannersspecialized in task decomposition and motion planning, RA-P can adapt to novelphysical skills through composable generalization. Our website and videos canbe found at https://sites.google.com/view/rh20t-primitive/main. Dataset andcode will be made available soon.</description><author>Zeren Chen, Zhelun Shi, Xiaoya Lu, Lehan He, Sucheng Qian, Hao Shu Fang, Zhenfei Yin, Wanli Ouyang, Jing Shao, Yu Qiao, Cewu Lu, Lu Sheng</author><pubDate>Thu, 28 Mar 2024 18:42:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19622v1</guid></item><item><title>Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models</title><link>http://arxiv.org/abs/2403.19620v1</link><description>Generative Adversarial Networks (GANs) have shown great success in generatinghigh quality images and are thus used as one of the main approaches to generateart images. However, usually the image generation process involves samplingfrom the latent space of the learned art representations, allowing littlecontrol over the output. In this work, we first employ GANs that are trained toproduce creative images using an architecture known as Creative AdversarialNetworks (CANs), then, we employ an evolutionary approach to navigate withinthe latent space of the models to discover images. We use automatic aestheticand collaborative interactive human evaluation metrics to assess the generatedimages. In the human interactive evaluation case, we propose a collaborativeevaluation based on the assessments of several participants. Furthermore, wealso experiment with an intelligent mutation operator that aims to improve thequality of the images through local search based on an aesthetic measure. Weevaluate the effectiveness of this approach by comparing the results producedby the automatic and collaborative interactive evolution. The results show thatthe proposed approach can generate highly attractive art images when theevolution is guided by collaborative human feedback.</description><author>Ole Hall, Anil Yaman</author><pubDate>Thu, 28 Mar 2024 18:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19620v1</guid></item><item><title>Quantum machine learning for image classification</title><link>http://arxiv.org/abs/2304.09224v2</link><description>Image classification, a pivotal task in multiple industries, facescomputational challenges due to the burgeoning volume of visual data. Thisresearch addresses these challenges by introducing two quantum machine learningmodels that leverage the principles of quantum mechanics for effectivecomputations. Our first model, a hybrid quantum neural network with parallelquantum circuits, enables the execution of computations even in the noisyintermediate-scale quantum era, where circuits with a large number of qubitsare currently infeasible. This model demonstrated a record-breakingclassification accuracy of 99.21% on the full MNIST dataset, surpassing theperformance of known quantum-classical models, while having eight times fewerparameters than its classical counterpart. Also, the results of testing thishybrid model on a Medical MNIST (classification accuracy over 99%), and onCIFAR-10 (classification accuracy over 82%), can serve as evidence of thegeneralizability of the model and highlights the efficiency of quantum layersin distinguishing common features of input data. Our second model introduces ahybrid quantum neural network with a Quanvolutional layer, reducing imageresolution via a convolution process. The model matches the performance of itsclassical counterpart, having four times fewer trainable parameters, andoutperforms a classical model with equal weight parameters. These modelsrepresent advancements in quantum machine learning research and illuminate thepath towards more accurate image classification systems.</description><author>Arsenii Senokosov, Alexandr Sedykh, Asel Sagingalieva, Basil Kyriacou, Alexey Melnikov</author><pubDate>Thu, 28 Mar 2024 18:36:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.09224v2</guid></item><item><title>Boosting Latent Diffusion with Flow Matching</title><link>http://arxiv.org/abs/2312.07360v2</link><description>Recently, there has been tremendous progress in visual synthesis and theunderlying generative models. Here, diffusion models (DMs) stand outparticularly, but lately, flow matching (FM) has also garnered considerableinterest. While DMs excel in providing diverse images, they suffer from longtraining and slow generation. With latent diffusion, these issues are onlypartially alleviated. Conversely, FM offers faster training and inference butexhibits less diversity in synthesis. We demonstrate that introducing FMbetween the Diffusion model and the convolutional decoder offershigh-resolution image synthesis with reduced computational cost and model size.Diffusion can then efficiently provide the necessary generation diversity. FMcompensates for the lower resolution, mapping the small latent space to ahigh-dimensional one. Subsequently, the convolutional decoder of the LDM mapsthese latents to high-resolution images. By combining the diversity of DMs, theefficiency of FMs, and the effectiveness of convolutional decoders, we achievestate-of-the-art high-resolution image synthesis at $1024^2$ with minimalcomputational cost. Importantly, our approach is orthogonal to recentapproximation and speed-up strategies for the underlying DMs, making it easilyintegrable into various DM frameworks.</description><author>Johannes S. Fischer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan A. Baumann, Bj√∂rn Ommer</author><pubDate>Thu, 28 Mar 2024 18:35:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07360v2</guid></item><item><title>SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing</title><link>http://arxiv.org/abs/2403.19615v1</link><description>In this paper, we present a Scale-adaptive method for Anti-aliasing GaussianSplatting (SA-GS). While the state-of-the-art method Mip-Splatting needsmodifying the training procedure of Gaussian splatting, our method functions attest-time and is training-free. Specifically, SA-GS can be applied to anypretrained Gaussian splatting field as a plugin to significantly improve thefield's anti-alising performance. The core technique is to apply 2Dscale-adaptive filters to each Gaussian during test time. As pointed out byMip-Splatting, observing Gaussians at different frequencies leads to mismatchesbetween the Gaussian scales during training and testing. Mip-Splatting resolvesthis issue using 3D smoothing and 2D Mip filters, which are unfortunately notaware of testing frequency. In this work, we show that a 2D scale-adaptivefilter that is informed of testing frequency can effectively match the Gaussianscale, thus making the Gaussian primitive distribution remain consistent acrossdifferent testing frequencies. When scale inconsistency is eliminated, samplingrates smaller than the scene frequency result in conventional jaggedness, andwe propose to integrate the projected 2D Gaussian within each pixel duringtesting. This integration is actually a limiting case of super-sampling, whichsignificantly improves anti-aliasing performance over vanilla GaussianSplatting. Through extensive experiments using various settings and bothbounded and unbounded scenes, we show SA-GS performs comparably with or betterthan Mip-Splatting. Note that super-sampling and integration are only effectivewhen our scale-adaptive filtering is activated. Our codes, data and models areavailable at https://github.com/zsy1987/SA-GS.</description><author>Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao</author><pubDate>Thu, 28 Mar 2024 18:32:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19615v1</guid></item><item><title>ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D</title><link>http://arxiv.org/abs/2403.19612v1</link><description>Effective recognition of spatial patterns and learning their hierarchy iscrucial in modern spatial data analysis. Volumetric data applications seektechniques ensuring invariance not only to shifts but also to patternrotations. While traditional methods can readily achieve translationalinvariance, rotational invariance possesses multiple challenges and remains anactive area of research. Here, we present ILPO-Net (Invariant to Local PatternsOrientation Network), a novel approach that handles arbitrarily shaped patternswith the convolutional operation inherently invariant to local spatial patternorientations using the Wigner matrix expansions. Our architecture seamlesslyintegrates the new convolution operator and, when benchmarked on diversevolumetric datasets such as MedMNIST and CATH, demonstrates superiorperformance over the baselines with significantly reduced parameter counts - upto 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,ILPO-Net's rotational invariance paves the way for other applications acrossmultiple disciplines. Our code is publicly available athttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.</description><author>Dmitrii Zhemchuzhnikov, Sergei Grudinin</author><pubDate>Thu, 28 Mar 2024 18:32:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19612v1</guid></item><item><title>Nearest Neighbor Classication for Classical Image Upsampling</title><link>http://arxiv.org/abs/2403.19611v1</link><description>Given a set of ordered pixel data in the form of an image, our goal is toperform upsampling on the data such that: the resulting resolution is improvedby some factor, the final result passes the human test, having added new,believable, and realistic information and detail to the image, the timecomplexity for upscaling is relatively close to that of lossy upscalingimplementations.</description><author>Evan Matthews, Nicolas Prate</author><pubDate>Thu, 28 Mar 2024 18:31:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19611v1</guid></item><item><title>SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects</title><link>http://arxiv.org/abs/2403.19607v1</link><description>Acquiring accurate depth information of transparent objects usingoff-the-shelf RGB-D cameras is a well-known challenge in Computer Vision andRobotics. Depth estimation/completion methods are typically employed andtrained on datasets with quality depth labels acquired from either simulation,additional sensors or specialized data collection setups and known 3d models.However, acquiring reliable depth information for datasets at scale is notstraightforward, limiting training scalability and generalization. NeuralRadiance Fields (NeRFs) are learning-free approaches and have demonstrated widesuccess in novel view synthesis and shape recovery. However, heuristics andcontrolled environments (lights, backgrounds, etc) are often required toaccurately capture specular surfaces. In this paper, we propose using VisualFoundation Models (VFMs) for segmentation in a zero-shot, label-free way toguide the NeRF reconstruction process for these objects via the simultaneousreconstruction of semantic fields and extensions to increase robustness. Ourproposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significantperformance on depth completion datasets for transparent objects and roboticgrasping.</description><author>Avinash Ummadisingu, Jongkeum Choi, Koki Yamane, Shimpei Masuda, Naoki Fukaya, Kuniyuki Takahashi</author><pubDate>Thu, 28 Mar 2024 18:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19607v1</guid></item><item><title>Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free Prediction</title><link>http://arxiv.org/abs/2403.19605v1</link><description>Decision-making pipelines are generally characterized by tradeoffs amongvarious risk functions. It is often desirable to manage such tradeoffs in adata-adaptive manner. As we demonstrate, if this is done naively, state-of-theart uncertainty quantification methods can lead to significant violations ofputative risk guarantees. To address this issue, we develop methods that permit valid control of riskwhen threshold and tradeoff parameters are chosen adaptively. Our methodologysupports monotone and nearly-monotone risks, but otherwise makes nodistributional assumptions. To illustrate the benefits of our approach, we carry out numericalexperiments on synthetic data and the large-scale vision dataset MS-COCO.</description><author>Drew T. Nguyen, Reese Pathak, Anastasios N. Angelopoulos, Stephen Bates, Michael I. Jordan</author><pubDate>Thu, 28 Mar 2024 18:28:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19605v1</guid></item><item><title>Semantic Map-based Generation of Navigation Instructions</title><link>http://arxiv.org/abs/2403.19603v1</link><description>We are interested in the generation of navigation instructions, either intheir own right or as training material for robotic navigation task. In thispaper, we propose a new approach to navigation instruction generation byframing the problem as an image captioning task using semantic maps as visualinput. Conventional approaches employ a sequence of panorama images to generatenavigation instructions. Semantic maps abstract away from visual details andfuse the information in multiple panorama images into a single top-downrepresentation, thereby reducing computational complexity to process the input.We present a benchmark dataset for instruction generation using semantic maps,propose an initial model and ask human subjects to manually assess the qualityof generated instructions. Our initial investigations show promise in usingsemantic maps for instruction generation instead of a sequence of panoramaimages, but there is vast scope for improvement. We release the code for datapreparation and model training at https://github.com/chengzu-li/VLGen.</description><author>Chengzu Li, Chao Zhang, Simone Teufel, Rama Sanand Doddipatla, Svetlana Stoyanchev</author><pubDate>Thu, 28 Mar 2024 18:27:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19603v1</guid></item><item><title>Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection</title><link>http://arxiv.org/abs/2311.11278v2</link><description>Deepfake detection faces a critical generalization hurdle, with performancedeteriorating when there is a mismatch between the distributions of trainingand testing data. A broadly received explanation is the tendency of thesedetectors to be overfitted to forgery-specific artifacts, rather than learningfeatures that are widely applicable across various forgeries. To address thisissue, we propose a simple yet effective detector called LSDA(\underline{L}atent \underline{S}pace \underline{D}ata\underline{A}ugmentation), which is based on a heuristic idea: representationswith a wider variety of forgeries should be able to learn a more generalizabledecision boundary, thereby mitigating the overfitting of method-specificfeatures (see Fig.~\ref{fig:toy}). Following this idea, we propose to enlargethe forgery space by constructing and simulating variations within and acrossforgery features in the latent space. This approach encompasses the acquisitionof enriched, domain-specific features and the facilitation of smoothertransitions between different forgery types, effectively bridging domain gaps.Our approach culminates in refining a binary classifier that leverages thedistilled knowledge from the enhanced features, striving for a generalizabledeepfake detector. Comprehensive experiments show that our proposed method issurprisingly effective and transcends state-of-the-art detectors across severalwidely used benchmarks.</description><author>Zhiyuan Yan, Yuhao Luo, Siwei Lyu, Qingshan Liu, Baoyuan Wu</author><pubDate>Thu, 28 Mar 2024 18:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11278v2</guid></item><item><title>Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model</title><link>http://arxiv.org/abs/2403.19600v1</link><description>Text-to-image (T2I) generative models have recently emerged as a powerfultool, enabling the creation of photo-realistic images and giving rise to amultitude of applications. However, the effective integration of T2I modelsinto fundamental image classification tasks remains an open question. Aprevalent strategy to bolster image classification performance is throughaugmenting the training set with synthetic images generated by T2I models. Inthis study, we scrutinize the shortcomings of both current generative andconventional data augmentation techniques. Our analysis reveals that thesemethods struggle to produce images that are both faithful (in terms offoreground objects) and diverse (in terms of background contexts) fordomain-specific concepts. To tackle this challenge, we introduce an innovativeinter-class data augmentation method known as Diff-Mix(https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset byperforming image translations between classes. Our empirical resultsdemonstrate that Diff-Mix achieves a better balance between faithfulness anddiversity, leading to a marked improvement in performance across diverse imageclassification scenarios, including few-shot, conventional, and long-tailclassifications for domain-specific datasets.</description><author>Zhicai Wang, Longhui Wei, Tan Wang, Heyu Chen, Yanbin Hao, Xiang Wang, Xiangnan He, Qi Tian</author><pubDate>Thu, 28 Mar 2024 18:23:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19600v1</guid></item><item><title>Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions</title><link>http://arxiv.org/abs/2311.17048v2</link><description>Zero-shot referring expression comprehension aims at localizing boundingboxes in an image corresponding to provided textual prompts, which requires:(i) a fine-grained disentanglement of complex visual scene and textual context,and (ii) a capacity to understand relationships among disentangled entities.Unfortunately, existing large vision-language alignment (VLA) models, e.g.,CLIP, struggle with both aspects so cannot be directly used for this task. Tomitigate this gap, we leverage large foundation models to disentangle bothimages and texts into triplets in the format of (subject, predicate, object).After that, grounding is accomplished by calculating the structural similaritymatrix between visual and textual triplets with a VLA model, and subsequentlypropagate it to an instance-level similarity matrix. Furthermore, to equip VLAmodels with the ability of relationship understanding, we design atriplet-matching objective to fine-tune the VLA models on a collection ofcurated dataset containing abundant entity relationships. Experimentsdemonstrate that our visual grounding performance increase of up to 19.5% overthe SOTA zero-shot model on RefCOCO/+/g. On the more challenging Who's Waldodataset, our zero-shot approach achieves comparable accuracy to the fullysupervised model. Code is available athttps://github.com/Show-han/Zeroshot_REC.</description><author>Zeyu Han, Fangrui Zhu, Qianru Lao, Huaizu Jiang</author><pubDate>Thu, 28 Mar 2024 18:23:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17048v2</guid></item><item><title>LocCa: Visual Pretraining with Location-aware Captioners</title><link>http://arxiv.org/abs/2403.19596v1</link><description>Image captioning has been shown as an effective pretraining method similar tocontrastive pretraining. However, the incorporation of location-awareinformation into visual pretraining remains an area with limited research. Inthis paper, we propose a simple visual pretraining method with location-awarecaptioners (LocCa). LocCa uses a simple image captioner task interface, toteach a model to read out rich information, i.e. bounding box coordinates, andcaptions, conditioned on the image pixel input. Thanks to the multitaskcapabilities of an encoder-decoder architecture, we show that an imagecaptioner can easily handle multiple tasks during pretraining. Our experimentsdemonstrate that LocCa outperforms standard captioners significantly onlocalization downstream tasks while maintaining comparable performance onholistic tasks.</description><author>Bo Wan, Michael Tschannen, Yongqin Xian, Filip Pavetic, Ibrahim Alabdulmohsin, Xiao Wang, Andr√© Susano Pinto, Andreas Steiner, Lucas Beyer, Xiaohua Zhai</author><pubDate>Thu, 28 Mar 2024 18:20:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19596v1</guid></item><item><title>Situation Awareness for Driver-Centric Driving Style Adaptation</title><link>http://arxiv.org/abs/2403.19595v1</link><description>There is evidence that the driving style of an autonomous vehicle isimportant to increase the acceptance and trust of the passengers. The drivingsituation has been found to have a significant influence on human drivingbehavior. However, current driving style models only partially incorporatedriving environment information, limiting the alignment between an agent andthe given situation. Therefore, we propose a situation-aware driving stylemodel based on different visual feature encoders pretrained on fleet data, aswell as driving behavior predictors, which are adapted to the driving style ofa specific driver. Our experiments show that the proposed method outperformsstatic driving styles significantly and forms plausible situation clusters.Furthermore, we found that feature encoders pretrained on our dataset lead tomore precise driving behavior modeling. In contrast, feature encoderspretrained supervised and unsupervised on different data sources lead to morespecific situation clusters, which can be utilized to constrain and control thedriving style adaptation for specific situations. Moreover, in a real-worldsetting, where driving style adaptation is happening iteratively, we found theMLP-based behavior predictors achieve good performance initially but sufferfrom catastrophic forgetting. In contrast, behavior predictors based onsituationdependent statistics can learn iteratively from continuous datastreams by design. Overall, our experiments show that important information fordriving behavior prediction is contained within the visual feature encoder. Thedataset is publicly available athuggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.</description><author>Johann Haselberger, Bonifaz Stuhr, Bernhard Schick, Steffen M√ºller</author><pubDate>Thu, 28 Mar 2024 18:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19595v1</guid></item><item><title>Promptly Predicting Structures: The Return of Inference</title><link>http://arxiv.org/abs/2401.06877v2</link><description>Prompt-based methods have been used extensively across NLP to build zero- andfew-shot label predictors. Many NLP tasks are naturally structured: that is,their outputs consist of multiple labels which constrain each other. Annotatingdata for such tasks can be cumbersome. Can the promise of the prompt-basedparadigm be extended to such structured outputs? In this paper, we present aframework for constructing zero- and few-shot linguistic structure predictors.Our key insight is that we can use structural constraints -- and combinatorialinference derived from them -- to filter out inconsistent structures predictedby large language models. We instantiated this framework on two structuredprediction tasks, and five datasets. Across all cases, our results show thatenforcing consistency not only constructs structurally valid outputs, but alsoimproves performance over the unconstrained variants.</description><author>Maitrey Mehta, Valentina Pyatkin, Vivek Srikumar</author><pubDate>Thu, 28 Mar 2024 18:17:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06877v2</guid></item><item><title>Frame by Familiar Frame: Understanding Replication in Video Diffusion Models</title><link>http://arxiv.org/abs/2403.19593v1</link><description>Building on the momentum of image generation diffusion models, there is anincreasing interest in video-based diffusion models. However, video generationposes greater challenges due to its higher-dimensional nature, the scarcity oftraining data, and the complex spatiotemporal relationships involved. Imagegeneration models, due to their extensive data requirements, have alreadystrained computational resources to their limits. There have been instances ofthese models reproducing elements from the training samples, leading toconcerns and even legal disputes over sample replication. Video diffusionmodels, which operate with even more constrained datasets and are tasked withgenerating both spatial and temporal content, may be more prone to replicatingsamples from their training sets. Compounding the issue, these models are oftenevaluated using metrics that inadvertently reward replication. In our paper, wepresent a systematic investigation into the phenomenon of sample replication invideo diffusion models. We scrutinize various recent diffusion models for videosynthesis, assessing their tendency to replicate spatial and temporal contentin both unconditional and conditional generation scenarios. Our studyidentifies strategies that are less likely to lead to replication. Furthermore,we propose new evaluation strategies that take replication into account,offering a more accurate measure of a model's ability to generate the originalcontent.</description><author>Aimon Rahman, Malsha V. Perera, Vishal M. Patel</author><pubDate>Thu, 28 Mar 2024 18:15:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19593v1</guid></item><item><title>Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach</title><link>http://arxiv.org/abs/2403.05950v2</link><description>Accurate classification of objects in 3D point clouds is a significantproblem in several applications, such as autonomous navigation andaugmented/virtual reality scenarios, which has become a research hot spot. Inthis paper, we presented a deep learning strategy for 3D object classificationin augmented reality. The proposed approach is a combination of the GRU andLSTM. LSTM networks learn longer dependencies well, but due to the number ofgates, it takes longer to train; on the other hand, GRU networks have a weakerperformance than LSTM, but their training speed is much higher than GRU, whichis The speed is due to its fewer gates. The proposed approach used thecombination of speed and accuracy of these two networks. The proposed approachachieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includeseight classes (unlabeled, man-made terrain, natural terrain, high vegetation,low vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, thetraditional machine learning approaches could achieve a maximum accuracy of0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,Hybrid Model, GRULSTM, GRU, LSTM</description><author>Ramin Mousa, Mitra Khezli, Mohamadreza Azadi, Vahid Nikoofard, Saba Hesaraki</author><pubDate>Thu, 28 Mar 2024 18:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05950v2</guid></item><item><title>Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers</title><link>http://arxiv.org/abs/2403.19591v1</link><description>Non-linear functions are prevalent in Transformers and their lightweightvariants, incurring substantial and frequently underestimated hardware costs.Previous state-of-the-art works optimize these operations by piece-wise linearapproximation and store the parameters in look-up tables (LUT), but most ofthem require unfriendly high-precision arithmetics such as FP/INT 32 and lackconsideration of integer-only INT quantization. This paper proposed a geneticLUT-Approximation algorithm namely GQA-LUT that can automatically determine theparameters with quantization awareness. The results demonstrate that GQA-LUTachieves negligible degradation on the challenging semantic segmentation taskfor both vanilla and linear Transformer models. Besides, proposed GQA-LUTenables the employment of INT8-based LUT-Approximation that achieves an areasavings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to thehigh-precision FP/INT 32 alternatives. Code is available at https://github.com/PingchengDong/GQA-LUT.</description><author>Pingcheng Dong, Yonghao Tan, Dong Zhang, Tianwei Ni, Xuejiao Liu, Yu Liu, Peng Luo, Luhong Liang, Shih-Yang Liu, Xijie Huang, Huaiyu Zhu, Yun Pan, Fengwei An, Kwang-Ting Cheng</author><pubDate>Thu, 28 Mar 2024 18:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19591v1</guid></item><item><title>TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes</title><link>http://arxiv.org/abs/2403.19589v1</link><description>3D dense captioning stands as a cornerstone in achieving a comprehensiveunderstanding of 3D scenes through natural language. It has recently witnessedremarkable achievements, particularly in indoor settings. However, theexploration of 3D dense captioning in outdoor scenes is hindered by two majorchallenges: 1) the \textbf{domain gap} between indoor and outdoor scenes, suchas dynamics and sparse visual inputs, makes it difficult to directly adaptexisting indoor methods; 2) the \textbf{lack of data} with comprehensivebox-caption pair annotations specifically tailored for outdoor scenes. To thisend, we introduce the new task of outdoor 3D dense captioning. As input, weassume a LiDAR point cloud and a set of RGB images captured by the panoramiccamera rig. The expected output is a set of object boxes with captions. Totackle this task, we propose the TOD3Cap network, which leverages the BEVrepresentation to generate object box proposals and integrates RelationQ-Former with LLaMA-Adapter to generate rich captions for these objects. Wealso introduce the TOD3Cap dataset, the largest one to our knowledge for 3Ddense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3Koutdoor objects from 850 scenes. Notably, our TOD3Cap network can effectivelylocalize and caption 3D objects in outdoor scenes, which outperforms baselinemethods by a significant margin (+9.6 CiDEr@0.5IoU). Code, data, and models arepublicly available at https://github.com/jxbbb/TOD3Cap.</description><author>Bu Jin, Yupeng Zheng, Pengfei Li, Weize Li, Yuhang Zheng, Sujie Hu, Xinyu Liu, Jinwei Zhu, Zhijie Yan, Haiyang Sun, Kun Zhan, Peng Jia, Xiaoxiao Long, Yilun Chen, Hao Zhao</author><pubDate>Thu, 28 Mar 2024 18:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19589v1</guid></item><item><title>DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs</title><link>http://arxiv.org/abs/2403.19588v1</link><description>This paper revives Densely Connected Convolutional Networks (DenseNets) andreveals the underrated effectiveness over predominant ResNet-stylearchitectures. We believe DenseNets' potential was overlooked due to untouchedtraining methods and traditional design elements not fully revealing theircapabilities. Our pilot study shows dense connections through concatenation arestrong, demonstrating that DenseNets can be revitalized to compete with modernarchitectures. We methodically refine suboptimal components - architecturaladjustments, block redesign, and improved training recipes towards wideningDenseNets and boosting memory efficiency while keeping concatenation shortcuts.Our models, employing simple architectural elements, ultimately surpass SwinTransformer, ConvNeXt, and DeiT-III - key architectures in the residuallearning lineage. Furthermore, our models exhibit near state-of-the-artperformance on ImageNet-1K, competing with the very recent models anddownstream tasks, ADE20k semantic segmentation, and COCO objectdetection/instance segmentation. Finally, we provide empirical analyses thatuncover the merits of the concatenation over additive shortcuts, steering arenewed preference towards DenseNet-style designs. Our code is available athttps://github.com/naver-ai/rdnet.</description><author>Donghyun Kim, Byeongho Heo, Dongyoon Han</author><pubDate>Thu, 28 Mar 2024 18:12:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19588v1</guid></item><item><title>Taming the Interactive Particle Langevin Algorithm -- the superlinear case</title><link>http://arxiv.org/abs/2403.19587v1</link><description>Recent advances in stochastic optimization have yielded the interactiveparticle Langevin algorithm (IPLA), which leverages the notion of interactingparticle systems (IPS) to efficiently sample from approximate posteriordensities. This becomes particularly crucial within the framework ofExpectation-Maximization (EM), where the E-step is computationally challengingor even intractable. Although prior research has focused on scenarios involvingconvex cases with gradients of log densities that grow at most linearly, ourwork extends this framework to include polynomial growth. Taming techniques areemployed to produce an explicit discretization scheme that yields a new classof stable, under such non-linearities, algorithms which are called tamedinteractive particle Langevin algorithms (tIPLA). We obtain non-asymptoticconvergence error estimates in Wasserstein-2 distance for the new class underan optimal rate.</description><author>Tim Johnston, Nikolaos Makras, Sotirios Sabanis</author><pubDate>Thu, 28 Mar 2024 18:11:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19587v1</guid></item><item><title>Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective</title><link>http://arxiv.org/abs/2403.18346v2</link><description>Recent advancements in Large Language Models (LLMs) have facilitated thedevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,MLLMs often suffer from an over-reliance on unimodal biases (e.g., languagebias and vision bias), leading to incorrect answers in complex multimodaltasks. To investigate this issue, we propose a causal framework to interpretthe biases in Visual Question Answering (VQA) problems. Within our framework,we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,and assess the causal effect of biases through an in-depth causal analysis.Motivated by the causal graph, we introduce a novel MORE dataset, consisting of12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,necessitating multi-hop reasoning and the surmounting of unimodal biases.Furthermore, we propose two strategies to mitigate unimodal biases and enhanceMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)framework for limited-access MLLMs and the refinement of open-source MLLMsthrough fine-tuning. Extensive quantitative and qualitative experiments offervaluable insights for future research. Our project page is athttps://opencausalab.github.io/MORE.</description><author>Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu</author><pubDate>Thu, 28 Mar 2024 18:09:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18346v2</guid></item><item><title>TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering</title><link>http://arxiv.org/abs/2403.19586v1</link><description>Four-dimensional Digital Subtraction Angiography (4D DSA) is a medicalimaging technique that provides a series of 2D images captured at differentstages and angles during the process of contrast agent filling blood vessels.It plays a significant role in the diagnosis of cerebrovascular diseases.Improving the rendering quality and speed under sparse sampling is importantfor observing the status and location of lesions. The current methods exhibitinadequate rendering quality in sparse views and suffer from slow renderingspeed. To overcome these limitations, we propose TOGS, a Gaussian splattingmethod with opacity offset over time, which can effectively improve therendering quality and speed of 4D DSA. We introduce an opacity offset table foreach Gaussian to model the temporal variations in the radiance of the contrastagent. By interpolating the opacity offset table, the opacity variation of theGaussian at different time points can be determined. This enables us to renderthe 2D DSA image at that specific moment. Additionally, we introduced a Smoothloss term in the loss function to mitigate overfitting issues that may arise inthe model when dealing with sparse view scenarios. During the training phase,we randomly prune Gaussians, thereby reducing the storage overhead of themodel. The experimental results demonstrate that compared to previous methods,this model achieves state-of-the-art reconstruction quality under the samenumber of training views. Additionally, it enables real-time rendering whilemaintaining low storage overhead. The code will be publicly available.</description><author>Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu</author><pubDate>Thu, 28 Mar 2024 18:08:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19586v1</guid></item><item><title>Learned representation-guided diffusion models for large-image generation</title><link>http://arxiv.org/abs/2312.07330v2</link><description>To synthesize high-fidelity samples, diffusion models typically requireauxiliary data to guide the generation process. However, it is impractical toprocure the painstaking patch-level annotation effort required in specializeddomains like histopathology and satellite imagery; it is often performed bydomain experts and involves hundreds of millions of patches. Modern-dayself-supervised learning (SSL) representations encode rich semantic and visualinformation. In this paper, we posit that such representations are expressiveenough to act as proxies to fine-grained human labels. We introduce a novelapproach that trains diffusion models conditioned on embeddings from SSL. Ourdiffusion models successfully project these features back to high-qualityhistopathology and remote sensing images. In addition, we construct largerimages by assembling spatially consistent patches inferred from SSL embeddings,preserving long-range dependencies. Augmenting real data by generatingvariations of real images improves downstream classifier accuracy forpatch-level and larger, image-scale classification tasks. Our models areeffective even on datasets not encountered during training, demonstrating theirrobustness and generalizability. Generating images from learned embeddings isagnostic to the source of the embeddings. The SSL embeddings used to generate alarge image can either be extracted from a reference image, or sampled from anauxiliary model conditioned on any related modality (e.g. class labels, text,genomic data). As proof of concept, we introduce the text-to-large imagesynthesis paradigm where we successfully synthesize large pathology andsatellite images out of text descriptions.</description><author>Alexandros Graikos, Srikar Yellapragada, Minh-Quan Le, Saarthak Kapse, Prateek Prasanna, Joel Saltz, Dimitris Samaras</author><pubDate>Thu, 28 Mar 2024 18:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07330v2</guid></item><item><title>Human Gaussian Splatting: Real-time Rendering of Animatable Avatars</title><link>http://arxiv.org/abs/2311.17113v2</link><description>This work addresses the problem of real-time rendering of photorealistichuman body avatars learned from multi-view videos. While the classicalapproaches to model and render virtual humans generally use a textured mesh,recent research has developed neural body representations that achieveimpressive visual quality. However, these models are difficult to render inreal-time and their quality degrades when the character is animated with bodyposes different than the training observations. We propose an animatable humanmodel based on 3D Gaussian Splatting, that has recently emerged as a veryefficient alternative to neural radiance fields. The body is represented by aset of gaussian primitives in a canonical space which is deformed with a coarseto fine approach that combines forward skinning and local non-rigid refinement.We describe how to learn our Human Gaussian Splatting (HuGS) model in anend-to-end fashion from multi-view observations, and evaluate it against thestate-of-the-art approaches for novel pose synthesis of clothed body. Ourmethod achieves 1.5 dB PSNR improvement over the state-of-the-art on THuman4dataset while being able to render in real-time (80 fps for 512x512resolution).</description><author>Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, Eduardo P√©rez-Pellitero</author><pubDate>Thu, 28 Mar 2024 18:07:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17113v2</guid></item><item><title>Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2403.19584v1</link><description>Geolocating precise locations from images presents a challenging problem incomputer vision and information retrieval.Traditional methods typically employeither classification, which dividing the Earth surface into grid cells andclassifying images accordingly, or retrieval, which identifying locations bymatching images with a database of image-location pairs. However,classification-based approaches are limited by the cell size and cannot yieldprecise predictions, while retrieval-based systems usually suffer from poorsearch quality and inadequate coverage of the global landscape at varied scaleand aggregation levels. To overcome these drawbacks, we present Img2Loc, anovel system that redefines image geolocalization as a text generation task.This is achieved using cutting-edge large multi-modality models like GPT4V orLLaVA with retrieval augmented generation. Img2Loc first employs CLIP-basedrepresentations to generate an image-based coordinate query database. It thenuniquely combines query results with images itself, forming elaborate promptscustomized for LMMs. When tested on benchmark datasets such as Im2GPS3k andYFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-artmodels but does so without any model training.</description><author>Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai</author><pubDate>Thu, 28 Mar 2024 18:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19584v1</guid></item><item><title>Predicting Species Occurrence Patterns from Partial Observations</title><link>http://arxiv.org/abs/2403.18028v2</link><description>To address the interlinked biodiversity and climate crises, we need anunderstanding of where species occur and how these patterns are changing.However, observational data on most species remains very limited, and theamount of data available varies greatly between taxonomic groups. We introducethe problem of predicting species occurrence patterns given (a) satelliteimagery, and (b) known information on the occurrence of other species. Toevaluate algorithms on this task, we introduce SatButterfly, a dataset ofsatellite images, environmental data and observational data for butterflies,which is designed to pair with the existing SatBird dataset of birdobservational data. To address this task, we propose a general model, R-Tran,for predicting species occurrence patterns that enables the use of partialobservational data wherever found. We find that R-Tran outperforms othermethods in predicting species encounter rates with partial information bothwithin a taxon (birds) and across taxa (birds and butterflies). Our approachopens new perspectives to leveraging insights from species with abundant datato other species with scarce data, by modelling the ecosystems in which theyco-occur.</description><author>Hager Radi Abdelwahed, M√©lisande Teng, David Rolnick</author><pubDate>Thu, 28 Mar 2024 18:06:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18028v2</guid></item><item><title>OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation</title><link>http://arxiv.org/abs/2403.19580v1</link><description>In the current state of 3D object detection research, the severe scarcity ofannotated 3D data, substantial disparities across different data modalities,and the absence of a unified architecture, have impeded the progress towardsthe goal of universality. In this paper, we propose \textbf{OV-Uni3DETR}, aunified open-vocabulary 3D detector via cycle-modality propagation. Comparedwith existing 3D detectors, OV-Uni3DETR offers distinct advantages: 1)Open-vocabulary 3D detection: During training, it leverages various accessibledata, especially extensive 2D detection images, to boost training diversity.During inference, it can detect both seen and unseen classes. 2) Modalityunifying: It seamlessly accommodates input data from any given modality,effectively addressing scenarios involving disparate modalities or missingsensor information, thereby supporting test-time modality switching. 3) Sceneunifying: It provides a unified multi-modal model architecture for diversescenes collected by distinct sensors. Specifically, we propose thecycle-modality propagation, aimed at propagating knowledge bridging 2D and 3Dmodalities, to support the aforementioned functionalities. 2D semanticknowledge from large-vocabulary learning guides novel class discovery in the 3Ddomain, and 3D geometric knowledge provides localization supervision for 2Ddetection images. OV-Uni3DETR achieves the state-of-the-art performance onvarious scenarios, surpassing existing methods by more than 6\% on average. Itsperformance using only RGB images is on par with or even surpasses that ofprevious point cloud based methods. Code and pre-trained models will bereleased later.</description><author>Zhenyu Wang, Yali Li, Taichi Liu, Hengshuang Zhao, Shengjin Wang</author><pubDate>Thu, 28 Mar 2024 18:05:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19580v1</guid></item><item><title>The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation</title><link>http://arxiv.org/abs/2403.19579v1</link><description>The pursuit of learning robust representations without human supervision is alongstanding challenge. The recent advancements in self-supervised contrastivelearning approaches have demonstrated high performance across variousrepresentation learning challenges. However, current methods depend on therandom transformation of training examples, resulting in some cases ofunrepresentative positive pairs that can have a large impact on learning. Thislimitation not only impedes the convergence of the learning process but therobustness of the learnt representation as well as requiring larger batch sizesto improve robustness to such bad batches. This paper attempts to alleviate theinfluence of false positive and false negative pairs by employing pairwisesimilarity calculations through the Fr\'echet ResNet Distance (FRD), therebyobtaining robust representations from unlabelled data. The effectiveness of theproposed method is substantiated by empirical results, where a linearclassifier trained on self-supervised contrastive representations achieved animpressive 87.74\% top-1 accuracy on STL10 and 99.31\% on the Flower102dataset. These results emphasize the potential of the proposed approach inpushing the boundaries of the state-of-the-art in self-supervised contrastivelearning, particularly for image classification tasks.</description><author>Ozgu Goksu, Nicolas Pugeault</author><pubDate>Thu, 28 Mar 2024 18:04:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19579v1</guid></item><item><title>Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics</title><link>http://arxiv.org/abs/2403.19578v1</link><description>We show that off-the-shelf text-based Transformers, with no additionaltraining, can perform few-shot in-context visual imitation learning, mappingvisual observations to action sequences that emulate the demonstrator'sbehaviour. We achieve this by transforming visual observations (inputs) andtrajectories of actions (outputs) into sequences of tokens that atext-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via aframework we call Keypoint Action Tokens (KAT). Despite being trained only onlanguage, we show that these Transformers excel at translating tokenised visualkeypoint observations into action trajectories, performing on par or betterthan state-of-the-art imitation learning (diffusion policies) in the low-dataregime on a suite of real-world, everyday tasks. Rather than operating in thelanguage domain as is typical, KAT leverages text-based Transformers to operatein the vision and action domains to learn general patterns in demonstrationdata for highly efficient imitation learning, indicating promising new avenuesfor repurposing natural language models for embodied tasks. Videos areavailable at https://www.robot-learning.uk/keypoint-action-tokens.</description><author>Norman Di Palo, Edward Johns</author><pubDate>Thu, 28 Mar 2024 18:04:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19578v1</guid></item><item><title>Evaluating gesture generation in a large-scale open challenge: The GENEA Challenge 2022</title><link>http://arxiv.org/abs/2303.08737v2</link><description>This paper reports on the second GENEA Challenge to benchmark data-drivenautomatic co-speech gesture generation. Participating teams used the samespeech and motion dataset to build gesture-generation systems. Motion generatedby all these systems was rendered to video using a standardised visualisationpipeline and evaluated in several large, crowdsourced user studies. Unlike whencomparing different research papers, differences in results are here only dueto differences between methods, enabling direct comparison between systems. Thedataset was based on 18 hours of full-body motion capture, including fingers,of different persons engaging in a dyadic conversation. Ten teams participatedin the challenge across two tiers: full-body and upper-body gesticulation. Foreach tier, we evaluated both the human-likeness of the gesture motion and itsappropriateness for the specific speech signal. Our evaluations decouplehuman-likeness from gesture appropriateness, which has been a difficult problemin the field. The evaluation results show some synthetic gesture conditions being rated assignificantly more human-like than 3D human motion capture. To the best of ourknowledge, this has not been demonstrated before. On the other hand, allsynthetic motion is found to be vastly less appropriate for the speech than theoriginal motion-capture recordings. We also find that conventional objectivemetrics do not correlate well with subjective human-likeness ratings in thislarge evaluation. The one exception is the Fr\'echet gesture distance (FGD),which achieves a Kendall's tau rank correlation of around $-0.5$. Based on thechallenge results we formulate numerous recommendations for system building andevaluation.</description><author>Taras Kucherenko, Pieter Wolfert, Youngwoo Yoon, Carla Viegas, Teodor Nikolov, Mihail Tsakov, Gustav Eje Henter</author><pubDate>Thu, 28 Mar 2024 17:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08737v2</guid></item><item><title>Deep Reinforcement Learning: A Convex Optimization Approach</title><link>http://arxiv.org/abs/2402.19212v4</link><description>In this paper, we consider reinforcement learning of nonlinear systems withcontinuous state and action spaces. We present an episodic learning algorithm,where we for each episode use convex optimization to find a two-layer neuralnetwork approximation of the optimal $Q$-function. The convex optimizationapproach guarantees that the weights calculated at each episode are optimal,with respect to the given sampled states and actions of the current episode.For stable nonlinear systems, we show that the algorithm converges and that theconverging parameters of the trained neural network can be made arbitrarilyclose to the optimal neural network parameters. In particular, if theregularization parameter is $\rho$ and the time horizon is $T$, then theparameters of the trained neural network converge to $w$, where the distancebetween $w$ from the optimal parameters $w^\star$ is bounded by$\mathcal{O}(\rho T^{-1})$. That is, when the number of episodes goes toinfinity, there exists a constant $C$ such that \[\|w-w^\star\| \leC\cdot\frac{\rho}{T}.\] In particular, our algorithm converges arbitrarilyclose to the optimal neural network parameters as the time horizon increases oras the regularization parameter decreases.</description><author>Ather Gattami</author><pubDate>Thu, 28 Mar 2024 17:59:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19212v4</guid></item><item><title>Swarm Characteristics Classification Using Neural Networks</title><link>http://arxiv.org/abs/2403.19572v1</link><description>Understanding the characteristics of swarming autonomous agents is criticalfor defense and security applications. This article presents a study on usingsupervised neural network time series classification (NN TSC) to predict keyattributes and tactics of swarming autonomous agents for military contexts.Specifically, NN TSC is applied to infer two binary attributes - communicationand proportional navigation - which combine to define four mutually exclusiveswarm tactics. We identify a gap in literature on using NNs for swarmclassification and demonstrate the effectiveness of NN TSC in rapidly deducingintelligence about attacking swarms to inform counter-maneuvers. Throughsimulated swarm-vs-swarm engagements, we evaluate NN TSC performance in termsof observation window requirements, noise robustness, and scalability to swarmsize. Key findings show NNs can predict swarm behaviors with 97% accuracy usingshort observation windows of 20 time steps, while also demonstrating gracefuldegradation down to 80% accuracy under 50% noise, as well as excellentscalability to swarm sizes from 10 to 100 agents. These capabilities arepromising for real-time decision-making support in defense scenarios by rapidlyinferring insights about swarm behavior.</description><author>Donald W. Peltier III, Isaac Kaminer, Abram Clark, Marko Orescanin</author><pubDate>Thu, 28 Mar 2024 17:56:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19572v1</guid></item><item><title>Differentially Private Distributed Estimation and Learning</title><link>http://arxiv.org/abs/2306.15865v5</link><description>We study distributed estimation and learning problems in a networkedenvironment where agents exchange information to estimate unknown statisticalproperties of random variables from their privately observed samples. Theagents can collectively estimate the unknown quantities by exchanginginformation about their private observations, but they also face privacy risks.Our novel algorithms extend the existing distributed estimation literature andenable the participating agents to estimate a complete sufficient statisticfrom private signals acquired offline or online over time and to preserve theprivacy of their signals and network neighborhoods. This is achieved throughlinear aggregation schemes with adjusted randomization schemes that add noiseto the exchanged estimates subject to differential privacy (DP) constraints,both in an offline and online manner. We provide convergence rate analysis andtight finite-time convergence bounds. We show that the noise that minimizes theconvergence time to the best estimates is the Laplace noise, with parameterscorresponding to each agent's sensitivity to their signal and networkcharacteristics. Our algorithms are amenable to dynamic topologies andbalancing privacy and accuracy trade-offs. Finally, to supplement and validateour theoretical results, we run experiments on real-world data from the USPower Grid Network and electric consumption data from German Households toestimate the average power consumption of power stations and households underall privacy regimes and show that our method outperforms existing first-order,privacy-aware, distributed optimization methods.</description><author>Marios Papachristou, M. Amin Rahimian</author><pubDate>Thu, 28 Mar 2024 17:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15865v5</guid></item><item><title>GrINd: Grid Interpolation Network for Scattered Observations</title><link>http://arxiv.org/abs/2403.19570v1</link><description>Predicting the evolution of spatiotemporal physical systems from sparse andscattered observational data poses a significant challenge in variousscientific domains. Traditional methods rely on dense grid-structured data,limiting their applicability in scenarios with sparse observations. To addressthis challenge, we introduce GrINd (Grid Interpolation Network for ScatteredObservations), a novel network architecture that leverages the high-performanceof grid-based models by mapping scattered observations onto a high-resolutiongrid using a Fourier Interpolation Layer. In the high-resolution space, aNeuralPDE-class model predicts the system's state at future timepoints usingdifferentiable ODE solvers and fully convolutional neural networksparametrizing the system's dynamics. We empirically evaluate GrINd on theDynaBench benchmark dataset, comprising six different physical systems observedat scattered locations, demonstrating its state-of-the-art performance comparedto existing models. GrINd offers a promising approach for forecasting physicalsystems from sparse, scattered observational data, extending the applicabilityof deep learning methods to real-world scenarios with limited dataavailability.</description><author>Andrzej Dulny, Paul Heinisch, Andreas Hotho, Anna Krause</author><pubDate>Thu, 28 Mar 2024 17:52:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19570v1</guid></item><item><title>Towards Generalizable Tumor Synthesis</title><link>http://arxiv.org/abs/2402.19470v2</link><description>Tumor synthesis enables the creation of artificial tumors in medical images,facilitating the training of AI models for tumor detection and segmentation.However, success in tumor synthesis hinges on creating visually realistictumors that are generalizable across multiple organs and, furthermore, theresulting AI models being capable of detecting real tumors in images sourcedfrom different domains (e.g., hospitals). This paper made a progressive stridetoward generalizable tumor synthesis by leveraging a critical observation:early-stage tumors (&lt; 2cm) tend to have similar imaging characteristics incomputed tomography (CT), whether they originate in the liver, pancreas, orkidneys. We have ascertained that generative AI models, e.g., Diffusion Models,can create realistic tumors generalized to a range of organs even when trainedon a limited number of tumor examples from only one organ. Moreover, we haveshown that AI models trained on these synthetic tumors can be generalized todetect and segment real tumors from CT volumes, encompassing a broad spectrumof patient demographics, imaging protocols, and healthcare facilities.</description><author>Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, Zongwei Zhou</author><pubDate>Thu, 28 Mar 2024 17:52:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19470v2</guid></item><item><title>Leveraging Variational Autoencoders for Parameterized MMSE Estimation</title><link>http://arxiv.org/abs/2307.05352v2</link><description>In this manuscript, we propose to use a variational autoencoder-basedframework for parameterizing a conditional linear minimum mean squared errorestimator. The variational autoencoder models the underlying unknown datadistribution as conditionally Gaussian, yielding the conditional first andsecond moments of the estimand, given a noisy observation. The derivedestimator is shown to approximate the minimum mean squared error estimator byutilizing the variational autoencoder as a generative prior for the estimationproblem. We propose three estimator variants that differ in their access toground-truth data during the training and estimation phases. The proposedestimator variant trained solely on noisy observations is particularlynoteworthy as it does not require access to ground-truth data during trainingor estimation. We conduct a rigorous analysis by bounding the differencebetween the proposed and the minimum mean squared error estimator, connectingthe training objective and the resulting estimation performance. Furthermore,the resulting bound reveals that the proposed estimator entails a bias-variancetradeoff, which is well-known in the estimation literature. As an exampleapplication, we portray channel estimation, allowing for a structuredcovariance matrix parameterization and low-complexity implementation.Nevertheless, the proposed framework is not limited to channel estimation butcan be applied to a broad class of estimation problems. Extensive numericalsimulations first validate the theoretical analysis of the proposed variationalautoencoder-based estimators and then demonstrate excellent estimationperformance compared to related classical and machine learning-basedstate-of-the-art estimators.</description><author>Michael Baur, Benedikt Fesl, Wolfgang Utschick</author><pubDate>Thu, 28 Mar 2024 17:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05352v2</guid></item><item><title>Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model</title><link>http://arxiv.org/abs/2311.17112v2</link><description>Parameter-efficient fine-tuning (PEFT) is an effective methodology to unleashthe potential of large foundation models in novel scenarios with limitedtraining data. In the computer vision community, PEFT has shown effectivenessin image classification, but little research has studied its ability for imagesegmentation. Fine-tuning segmentation models usually require a heavieradjustment of parameters to align the proper projection directions in theparameter space for new scenarios. This raises a challenge to existing PEFTalgorithms, as they often inject a limited number of individual parameters intoeach block, which prevents substantial adjustment of the projection directionof the parameter space due to the limitation of Hidden Markov Chain alongblocks. In this paper, we equip PEFT with a cross-block orchestration mechanismto enable the adaptation of the Segment Anything Model (SAM) to variousdownstream scenarios. We introduce a novel inter-block communication module,which integrates a learnable relation matrix to facilitate communication amongdifferent coefficient sets of each PEFT block's parameter space. Moreover, wepropose an intra-block enhancement module, which introduces a linear projectionhead whose weights are generated from a hyper-complex layer, further enhancingthe impact of the adjustment of projection directions on the entire parameterspace. Extensive experiments on diverse benchmarks demonstrate that ourproposed approach consistently improves the segmentation performancesignificantly on novel scenarios with only around 1K additional parameters.</description><author>Zelin Peng, Zhengqin Xu, Zhilin Zeng, Lingxi Xie, Qi Tian, Wei Shen</author><pubDate>Thu, 28 Mar 2024 17:51:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17112v2</guid></item><item><title>MANUS: Markerless Grasp Capture using Articulated 3D Gaussians</title><link>http://arxiv.org/abs/2312.02137v2</link><description>Understanding how we grasp objects with our hands has important applicationsin areas like robotics and mixed reality. However, this challenging problemrequires accurate modeling of the contact between hands and objects. To capturegrasps, existing methods use skeletons, meshes, or parametric models that doesnot represent hand shape accurately resulting in inaccurate contacts. Wepresent MANUS, a method for Markerless Hand-Object Grasp Capture usingArticulated 3D Gaussians. We build a novel articulated 3D Gaussiansrepresentation that extends 3D Gaussian splatting for high-fidelityrepresentation of articulating hands. Since our representation uses Gaussianprimitives, it enables us to efficiently and accurately estimate contactsbetween the hand and the object. For the most accurate results, our methodrequires tens of camera views that current datasets do not provide. Wetherefore build MANUS-Grasps, a new dataset that contains hand-object graspsviewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7Mframes. In addition to extensive qualitative results, we also show that ourmethod outperforms others on a quantitative contact evaluation method that usespaint transfer from the object to the hand.</description><author>Chandradeep Pokhariya, Ishaan N Shah, Angela Xing, Zekun Li, Kefan Chen, Avinash Sharma, Srinath Sridhar</author><pubDate>Thu, 28 Mar 2024 17:50:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02137v2</guid></item><item><title>SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution</title><link>http://arxiv.org/abs/2312.11598v3</link><description>Diffusion models have demonstrated strong potential for robotic trajectoryplanning. However, generating coherent trajectories from high-levelinstructions remains challenging, especially for long-range composition tasksrequiring multiple sequential skills. We propose SkillDiffuser, an end-to-endhierarchical planning framework integrating interpretable skill learning withconditional diffusion planning to address this problem. At the higher level,the skill abstraction module learns discrete, human-understandable skillrepresentations from visual observations and language instructions. Theselearned skill embeddings are then used to condition the diffusion model togenerate customized latent trajectories aligned with the skills. This allowsgenerating diverse state trajectories that adhere to the learnable skills. Byintegrating skill learning with conditional trajectory generation,SkillDiffuser produces coherent behavior following abstract instructions acrossdiverse tasks. Experiments on multi-task robotic manipulation benchmarks likeMeta-World and LOReL demonstrate state-of-the-art performance andhuman-interpretable skill representations from SkillDiffuser. Morevisualization results and information could be found on our website.</description><author>Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding, Ping Luo</author><pubDate>Thu, 28 Mar 2024 17:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11598v3</guid></item><item><title>Self-Improved Learning for Scalable Neural Combinatorial Optimization</title><link>http://arxiv.org/abs/2403.19561v1</link><description>The end-to-end neural combinatorial optimization (NCO) method shows promisingperformance in solving complex combinatorial optimization problems without theneed for expert design. However, existing methods struggle with large-scaleproblems, hindering their practical applicability. To overcome this limitation,this work proposes a novel Self-Improved Learning (SIL) method for betterscalability of neural combinatorial optimization. Specifically, we develop anefficient self-improved mechanism that enables direct model training onlarge-scale problem instances without any labeled data. Powered by aninnovative local reconstruction approach, this method can iteratively generatebetter solutions by itself as pseudo-labels to guide efficient model training.In addition, we design a linear complexity attention mechanism for the model toefficiently handle large-scale combinatorial problem instances with lowcomputation overhead. Comprehensive experiments on the Travelling SalesmanProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to100K nodes in both uniform and real-world distributions demonstrate thesuperior scalability of our method.</description><author>Fu Luo, Xi Lin, Zhenkun Wang, Tong Xialiang, Mingxuan Yuan, Qingfu Zhang</author><pubDate>Thu, 28 Mar 2024 17:46:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19561v1</guid></item><item><title>Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA</title><link>http://arxiv.org/abs/2403.16385v2</link><description>Understanding data visualizations like charts and plots requires reasoningabout both visual elements and numerics. Although strong in extractivequestions, current chart visual question answering (chart VQA) models suffer oncomplex reasoning questions. In this work, we address the lack of reasoningability by data augmentation. We leverage Large Language Models (LLMs), whichhave shown to have strong reasoning ability, as an automatic data annotatorthat generates question-answer annotations for chart images. The key innovationin our method lies in the Synthesize Step-by-Step strategy: our LLM-based datagenerator learns to decompose the complex question into step-by-stepsub-questions (rationales), which are then used to derive the final answerusing external tools, i.e. Python. This step-wise generation procedure istrained on synthetic data generated using a template-based QA generationpipeline. Experimental results highlight the significance of the proposedstep-by-step generation. By training with the LLM-augmented data (LAMENDA), wesignificantly enhance the chart VQA models, achieving the state-of-the-artaccuracy on the ChartQA and PlotQA datasets. In particular, our approachimproves the accuracy of the previous state-of-the-art approach from 38% to 54%on the human-written questions in the ChartQA dataset, which needs strongreasoning. We hope our work underscores the potential of synthetic data andencourages further exploration of data augmentation using LLMs forreasoning-heavy tasks.</description><author>Zhuowan Li, Bhavan Jasani, Peng Tang, Shabnam Ghadar</author><pubDate>Thu, 28 Mar 2024 17:45:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16385v2</guid></item><item><title>Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset</title><link>http://arxiv.org/abs/2403.19559v1</link><description>Hate speech detection models are only as good as the data they are trainedon. Datasets sourced from social media suffer from systematic gaps and biases,leading to unreliable models with simplistic decision boundaries. Adversarialdatasets, collected by exploiting model weaknesses, promise to fix thisproblem. However, adversarial data collection can be slow and costly, andindividual annotators have limited creativity. In this paper, we introduceGAHD, a new German Adversarial Hate speech Dataset comprising ca.\ 11kexamples. During data collection, we explore new strategies for supportingannotators, to create more diverse adversarial examples more efficiently andprovide a manual analysis of annotator disagreements for each strategy. Ourexperiments show that the resulting dataset is challenging even forstate-of-the-art hate speech detection models, and that training on GAHDclearly improves model robustness. Further, we find that mixing multiplesupport strategies is most advantageous. We make GAHD publicly available athttps://github.com/jagol/gahd.</description><author>Janis Goldzycher, Paul R√∂ttger, Gerold Schneider</author><pubDate>Thu, 28 Mar 2024 17:44:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19559v1</guid></item><item><title>WoLF: Large Language Model Framework for CXR Understanding</title><link>http://arxiv.org/abs/2403.15456v2</link><description>Significant methodological strides have been made toward Chest X-ray (CXR)understanding via modern vision-language models (VLMs), demonstratingimpressive Visual Question Answering (VQA) and CXR report generation abilities.However, existing CXR understanding frameworks still possess several proceduralcaveats. (1) Previous methods solely use CXR reports, which are insufficientfor comprehensive Visual Question Answering (VQA), especially when additionalhealth-related data like medication history and prior diagnoses are needed. (2)Previous methods use raw CXR reports, which are often arbitrarily structured.While modern language models can understand various text formats, restructuringreports for clearer, organized anatomy-based information could enhance theirusefulness. (3) Current evaluation methods for CXR-VQA primarily emphasizelinguistic correctness, lacking the capability to offer nuanced assessments ofthe generated answers. In this work, to address the aforementioned caveats, weintroduce WoLF, a Wide-scope Large Language Model Framework for CXRunderstanding. To resolve (1), we capture multi-faceted records of patients,which are utilized for accurate diagnoses in real-world clinical scenarios.Specifically, we adopt the Electronic Health Records (EHR) to generateinstruction-following data suited for CXR understanding. Regarding (2), weenhance report generation performance by decoupling knowledge in CXR reportsbased on anatomical structure even within the attention step via maskedattention. To address (3), we introduce an AI-evaluation protocol optimized forassessing the capabilities of LLM. Through extensive experimental validations,WoLF demonstrates superior performance over other models on MIMIC-CXR in theAI-evaluation arena about VQA (up to +9.47%p mean score) and by metrics aboutreport generation (+7.3%p BLEU-1).</description><author>Seil Kang, Donghyun Kim, Junhyeok Kim, Hyo Kyung Lee, Seong Jae Hwang</author><pubDate>Thu, 28 Mar 2024 17:40:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15456v2</guid></item><item><title>Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition</title><link>http://arxiv.org/abs/2403.19554v1</link><description>In video-based emotion recognition, audio and visual modalities are oftenexpected to have a complementary relationship, which is widely explored usingcross-attention. However, they may also exhibit weak complementaryrelationships, resulting in poor representations of audio-visual features, thusdegrading the performance of the system. To address this issue, we proposeDynamic Cross-Attention (DCA) that can dynamically select cross-attended orunattended features on the fly based on their strong or weak complementaryrelationship with each other, respectively. Specifically, a simple yetefficient gating layer is designed to evaluate the contribution of thecross-attention mechanism and choose cross-attended features only when theyexhibit a strong complementary relationship, otherwise unattended features. Weevaluate the performance of the proposed approach on the challenging RECOLA andAff-Wild2 datasets. We also compare the proposed approach with other variantsof cross-attention and show that the proposed model consistently improves theperformance on both datasets.</description><author>R. Gnana Praveen, Jahangir Alam</author><pubDate>Thu, 28 Mar 2024 17:38:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19554v1</guid></item><item><title>A stability theorem for bigraded persistence barcodes</title><link>http://arxiv.org/abs/2303.14694v2</link><description>We define bigraded persistent homology modules and bigraded barcodes of afinite pseudo-metric space X using the ordinary and double homology of themoment-angle complex associated with the Vietoris-Rips filtration of X. Weprove a stability theorem for the bigraded persistent double homology modulesand barcodes.</description><author>Anthony Bahri, Ivan Limonchenko, Taras Panov, Jongbaek Song, Donald Stanley</author><pubDate>Thu, 28 Mar 2024 17:37:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14694v2</guid></item><item><title>GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM</title><link>http://arxiv.org/abs/2403.19549v1</link><description>Recent advancements in RGB-only dense Simultaneous Localization and Mapping(SLAM) have predominantly utilized grid-based neural implicit encodings and/orstruggle to efficiently realize global map and pose consistency. To this end,we propose an efficient RGB-only dense SLAM system using a flexible neuralpoint cloud scene representation that adapts to keyframe poses and depthupdates, without needing costly backpropagation. Another critical challenge ofRGB-only SLAM is the lack of geometric priors. To alleviate this issue, withthe aid of a monocular depth estimator, we introduce a novel DSPO layer forbundle adjustment which optimizes the pose and depth of keyframes along withthe scale of the monocular depth. Finally, our system benefits from loopclosure and online global bundle adjustment and performs either better orcompetitive to existing dense neural RGB SLAM methods in tracking, mapping andrendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The sourcecode will be made available.</description><author>Ganlin Zhang, Erik Sandstr√∂m, Youmin Zhang, Manthan Patel, Luc Van Gool, Martin R. Oswald</author><pubDate>Thu, 28 Mar 2024 17:32:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19549v1</guid></item><item><title>Investigating the Emergent Audio Classification Ability of ASR Foundation Models</title><link>http://arxiv.org/abs/2311.09363v2</link><description>Text and vision foundation models can perform many tasks in a zero-shotsetting, a desirable property that enables these systems to be applied ingeneral and low-resource settings. There has been far less work, however, onthe zero-shot abilities of ASR foundation models, with these systems typicallyfine-tuned to specific tasks or constrained to applications that match theirtraining criterion and data annotation. In this work we investigate the abilityof Whisper and MMS, ASR foundation models trained primarily for speechrecognition, to perform zero-shot audio classification. We use simpletemplate-based text prompts at the decoder and use the resulting decodingprobabilities to generate zero-shot predictions. Without training the model onextra data or adding any new parameters, we demonstrate that Whisper showspromising zero-shot classification performance on a range of 8audio-classification datasets, outperforming the accuracy of existingstate-of-the-art zero-shot baselines by an average of 9%. One important step tounlock the emergent ability is debiasing, where a simple unsupervisedreweighting method of the class probabilities yields consistent significantperformance gains. We further show that performance increases with model size,implying that as ASR foundation models scale up, they may exhibit improvedzero-shot performance.</description><author>Rao Ma, Adian Liusie, Mark J. F. Gales, Kate M. Knill</author><pubDate>Thu, 28 Mar 2024 17:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09363v2</guid></item><item><title>Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies</title><link>http://arxiv.org/abs/2310.10500v2</link><description>Forecasting models for systematic trading strategies do not adapt quicklywhen financial market conditions rapidly change, as was seen in the advent ofthe COVID-19 pandemic in 2020, causing many forecasting models to takeloss-making positions. To deal with such situations, we propose a noveltime-series trend-following forecaster that can quickly adapt to new marketconditions, referred to as regimes. We leverage recent developments from thedeep learning community and use few-shot learning. We propose the CrossAttentive Time-Series Trend Network -- X-Trend -- which takes positionsattending over a context set of financial time-series regimes. X-Trendtransfers trends from similar patterns in the context set to make forecasts,then subsequently takes positions for a new distinct target regime. By quicklyadapting to new financial regimes, X-Trend increases Sharpe ratio by 18.9% overa neural forecaster and 10-fold over a conventional Time-series Momentumstrategy during the turbulent market period from 2018 to 2023. Our strategyrecovers twice as quickly from the COVID-19 drawdown compared to theneural-forecaster. X-Trend can also take zero-shot positions on novel unseenfinancial assets obtaining a 5-fold Sharpe ratio increase versus a neuraltime-series trend forecaster over the same period. Furthermore, thecross-attention mechanism allows us to interpret the relationship betweenforecasts and patterns in the context set.</description><author>Kieran Wood, Samuel Kessler, Stephen J. Roberts, Stefan Zohren</author><pubDate>Thu, 28 Mar 2024 17:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10500v2</guid></item><item><title>WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models</title><link>http://arxiv.org/abs/2403.19548v1</link><description>Watermarking generative-AI systems, such as LLMs, has gained considerableinterest, driven by their enhanced capabilities across a wide range of tasks.Although current approaches have demonstrated that small, context-dependentshifts in the word distributions can be used to apply and detect watermarks,there has been little work in analyzing the impact that these perturbationshave on the quality of generated texts. Balancing high detectability withminimal performance degradation is crucial in terms of selecting theappropriate watermarking setting; therefore this paper proposes a simpleanalysis framework where comparative assessment, a flexible NLG evaluationframework, is used to assess the quality degradation caused by a particularwatermark setting. We demonstrate that our framework provides easyvisualization of the quality-detection trade-off of watermark settings,enabling a simple solution to find an LLM watermark operating point thatprovides a well-balanced performance. This approach is applied to two differentsummarization systems and a translation system, enabling cross-model analysisfor a task, and cross-task analysis.</description><author>Piotr Molenda, Adian Liusie, Mark J. F. Gales</author><pubDate>Thu, 28 Mar 2024 17:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19548v1</guid></item><item><title>Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models</title><link>http://arxiv.org/abs/2403.15268v2</link><description>Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have beenproposed to enhance the knowledge required for question answering over LargeLanguage Models (LLMs). However, the former depends on external resources, andboth require incorporating the explicit documents into the context, whichresults in longer contexts that lead to more resource consumption. Recent worksindicate that LLMs have modeled rich knowledge, albeit not effectivelytriggered or activated. Inspired by this, we propose a novelknowledge-augmented framework, Imagination-Augmented-Generation (IAG), whichsimulates the human capacity to compensate for knowledge deficits whileanswering questions solely through imagination, without relying on externalresources. Guided by IAG, we propose an imagine richer context method forquestion answering (IMcQA), which obtains richer context through the followingtwo modules: explicit imagination by generating a short dummy document withlong context compress and implicit imagination with HyperNetwork for generatingadapter weights. Experimental results on three datasets demonstrate that IMcQAexhibits significant advantages in both open-domain and closed-book settings,as well as in both in-distribution performance and out-of-distributiongeneralizations. Our code will be available athttps://github.com/Xnhyacinth/IAG.</description><author>Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao</author><pubDate>Thu, 28 Mar 2024 17:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15268v2</guid></item><item><title>Croissant: A Metadata Format for ML-Ready Datasets</title><link>http://arxiv.org/abs/2403.19546v1</link><description>Data is a critical resource for Machine Learning (ML), yet working with dataremains a key friction point. This paper introduces Croissant, a metadataformat for datasets that simplifies how data is used by ML tools andframeworks. Croissant makes datasets more discoverable, portable andinteroperable, thereby addressing significant challenges in ML data managementand responsible AI. Croissant is already supported by several popular datasetrepositories, spanning hundreds of thousands of datasets, ready to be loadedinto the most popular ML frameworks.</description><author>Mubashara Akhtar, Omar Benjelloun, Costanza Conforti, Joan Giner-Miguelez, Nitisha Jain, Michael Kuchnik, Quentin Lhoest, Pierre Marcenac, Manil Maskey, Peter Mattson, Luis Oala, Pierre Ruyssen, Rajat Shinde, Elena Simperl, Goeffry Thomas, Slava Tykhonov, Joaquin Vanschoren, Steffen Vogler, Carole-Jean Wu</author><pubDate>Thu, 28 Mar 2024 17:27:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19546v1</guid></item><item><title>Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments</title><link>http://arxiv.org/abs/2403.19545v1</link><description>This study explores the integration of Lamarckian system into evolutionaryrobotics (ER), comparing it with the traditional Darwinian model across variousenvironments. By adopting Lamarckian principles, where robots inherit learnedtraits, alongside Darwinian learning without inheritance, we investigateadaptation in dynamic settings. Our research, conducted in six distinctenvironmental setups, demonstrates that Lamarckian systems outperform Darwinianones in adaptability and efficiency, particularly in challenging conditions.Our analysis highlights the critical role of the interplay between controller\&amp; morphological evolution and environment adaptation, with parent-offspringsimilarities and newborn \&amp;survivors before and after learning providinginsights into the effectiveness of trait inheritance. Our findings suggestLamarckian principles could significantly advance autonomous system design,highlighting the potential for more adaptable and robust robotic solutions incomplex, real-world applications. These theoretical insights were validatedusing real physical robots, bridging the gap between simulation and practicalapplication.</description><author>Jie Luo, Karine Miras, Carlo Longhi, Oliver Weissl, Agoston E. Eiben</author><pubDate>Thu, 28 Mar 2024 17:27:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19545v1</guid></item><item><title>A Corpus for Sentence-level Subjectivity Detection on English News Articles</title><link>http://arxiv.org/abs/2305.18034v2</link><description>We develop novel annotation guidelines for sentence-level subjectivitydetection, which are not limited to language-specific cues. We use ourguidelines to collect NewsSD-ENG, a corpus of 638 objective and 411 subjectivesentences extracted from English news articles on controversial topics. Ourcorpus paves the way for subjectivity detection in English and across otherlanguages without relying on language-specific tools, such as lexicons ormachine translation. We evaluate state-of-the-art multilingualtransformer-based models on the task in mono-, multi-, and cross-languagesettings. For this purpose, we re-annotate an existing Italian corpus. Weobserve that models trained in the multilingual setting achieve the bestperformance on the task.</description><author>Francesco Antici, Andrea Galassi, Federico Ruggeri, Katerina Korre, Arianna Muti, Alessandra Bardi, Alice Fedotova, Alberto Barr√≥n-Cede√±o</author><pubDate>Thu, 28 Mar 2024 17:27:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18034v2</guid></item><item><title>Exploring Accurate 3D Phenotyping in Greenhouse through Neural Radiance Fields</title><link>http://arxiv.org/abs/2403.15981v2</link><description>Accurate collection of plant phenotyping is critical to optimisingsustainable farming practices in precision agriculture. Traditional phenotypingin controlled laboratory environments, while valuable, falls short inunderstanding plant growth under real-world conditions. Emerging sensor anddigital technologies offer a promising approach for direct phenotyping ofplants in farm environments. This study investigates a learning-basedphenotyping method using the Neural Radiance Field to achieve accurate in-situphenotyping of pepper plants in greenhouse environments. To quantitativelyevaluate the performance of this method, traditional point cloud registrationon 3D scanning data is implemented for comparison. Experimental result showsthat NeRF(Neural Radiance Fields) achieves competitive accuracy compared to the3D scanning methods. The mean distance error between the scanner-based methodand the NeRF-based method is 0.865mm. This study shows that the learning-basedNeRF method achieves similar accuracy to 3D scanning-based methods but withimproved scalability and robustness.</description><author>Junhong Zhao, Wei Ying, Yaoqiang Pan, Zhenfeng Yi, Chao Chen, Kewei Hu, Hanwen Kang</author><pubDate>Thu, 28 Mar 2024 17:21:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15981v2</guid></item><item><title>Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones</title><link>http://arxiv.org/abs/2204.08989v3</link><description>With the increasing use of smartphones in our daily lives, these devices havebecome capable of performing many complex tasks. Concerning the need forcontinuous monitoring of vital signs, especially for the elderly or those withcertain types of diseases, the development of algorithms that can estimatevital signs using smartphones has attracted researchers worldwide. Inparticular, researchers have been exploring ways to estimate vital signs, suchas heart rate, oxygen saturation levels, and respiratory rate, using algorithmsthat can be run on smartphones. However, many of these algorithms requiremultiple pre-processing steps that might introduce some implementationoverheads or require the design of a couple of hand-crafted stages to obtain anoptimal result. To address this issue, this research proposes a novelend-to-end solution to mobile-based vital sign estimation using deep learningthat eliminates the need for pre-processing. By using a fully convolutionalarchitecture, the proposed model has much fewer parameters and lesscomputational complexity compared to the architectures that use fully-connectedlayers as the prediction heads. This also reduces the risk of overfitting.Additionally, a public dataset for vital sign estimation, which includes 62videos collected from 35 men and 27 women, is provided. Overall, the proposedend-to-end approach promises significantly improved efficiency and performancefor on-device health monitoring on readily available consumer electronics.</description><author>Taha Samavati, Mahdi Farvardin, Aboozar Ghaffari</author><pubDate>Thu, 28 Mar 2024 17:17:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2204.08989v3</guid></item><item><title>De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts</title><link>http://arxiv.org/abs/2403.19539v1</link><description>Data-Free Knowledge Distillation (DFKD) is a promising task to trainhigh-performance small models to enhance actual deployment without relying onthe original training data. Existing methods commonly avoid relying on privatedata by utilizing synthetic or sampled data. However, a long-overlooked issueis that the severe distribution shifts between their substitution and originaldata, which manifests as huge differences in the quality of images and classproportions. The harmful shifts are essentially the confounder thatsignificantly causes performance bottlenecks. To tackle the issue, this paperproposes a novel perspective with causal inference to disentangle the studentmodels from the impact of such shifts. By designing a customized causal graph,we first reveal the causalities among the variables in the DFKD task.Subsequently, we propose a Knowledge Distillation Causal Intervention (KDCI)framework based on the backdoor adjustment to de-confound the confounder. KDCIcan be flexibly combined with most existing state-of-the-art baselines.Experiments in combination with six representative DFKD methods demonstrate theeffectiveness of our KDCI, which can obviously help existing methods underalmost all settings, \textit{e.g.}, improving the baseline by up to 15.54\%accuracy on the CIFAR-100 dataset.</description><author>Yuzheng Wang, Dingkang Yang, Zhaoyu Chen, Yang Liu, Siao Liu, Wenqiang Zhang, Lihua Zhang, Lizhe Qi</author><pubDate>Thu, 28 Mar 2024 17:13:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19539v1</guid></item><item><title>Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees</title><link>http://arxiv.org/abs/2301.13375v2</link><description>Robustness and safety are critical for the trustworthy deployment of deepreinforcement learning. Real-world decision making applications requirealgorithms that can guarantee robust performance and safety in the presence ofgeneral environment disturbances, while making limited assumptions on the datacollection process during training. In order to accomplish this goal, weintroduce a safe reinforcement learning framework that incorporates robustnessthrough the use of an optimal transport cost uncertainty set. We provide anefficient implementation based on applying Optimal Transport Perturbations toconstruct worst-case virtual state transitions, which does not impact datacollection during training and does not require detailed simulator access. Inexperiments on continuous control tasks with safety constraints, our approachdemonstrates robust performance while significantly improving safety atdeployment time compared to standard safe reinforcement learning.</description><author>James Queeney, Erhan Can Ozcan, Ioannis Ch. Paschalidis, Christos G. Cassandras</author><pubDate>Thu, 28 Mar 2024 17:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.13375v2</guid></item><item><title>Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance</title><link>http://arxiv.org/abs/2403.19534v1</link><description>Prior studies have made significant progress in image inpainting guided byeither text or subject image. However, the research on editing with theircombined guidance is still in the early stages. To tackle this challenge, wepresent LAR-Gen, a novel approach for image inpainting that enables seamlessinpainting of masked scene images, incorporating both the textual prompts andspecified subjects. Our approach adopts a coarse-to-fine manner to ensuresubject identity preservation and local semantic coherence. The processinvolves (i) Locate: concatenating the noise with masked scene image to achieveprecise regional editing, (ii) Assign: employing decoupled cross-attentionmechanism to accommodate multi-modal guidance, and (iii) Refine: using a novelRefineNet to supplement subject details. Additionally, to address the issue ofscarce training data, we introduce a novel data construction pipeline. Thispipeline extracts substantial pairs of data consisting of local text promptsand corresponding visual instances from a vast image dataset, leveragingpublicly available large models. Extensive experiments and varied applicationscenarios demonstrate the superiority of LAR-Gen in terms of both identitypreservation and text semantic consistency. Project page can be found at\url{https://ali-vilab.github.io/largen-page/}.</description><author>Yulin Pan, Chaojie Mao, Zeyinzi Jiang, Zhen Han, Jingfeng Zhang</author><pubDate>Thu, 28 Mar 2024 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19534v1</guid></item><item><title>Detecting Financial Bots on the Ethereum Blockchain</title><link>http://arxiv.org/abs/2403.19530v1</link><description>The integration of bots in Distributed Ledger Technologies (DLTs) fostersefficiency and automation. However, their use is also associated with predatorytrading and market manipulation, and can pose threats to system integrity. Itis therefore essential to understand the extent of bot deployment in DLTs;despite this, current detection systems are predominantly rule-based and lackflexibility. In this study, we present a novel approach that utilizes machinelearning for the detection of financial bots on the Ethereum platform. First,we systematize existing scientific literature and collect anecdotal evidence toestablish a taxonomy for financial bots, comprising 7 categories and 24subcategories. Next, we create a ground-truth dataset consisting of 133 humanand 137 bot addresses. Third, we employ both unsupervised and supervisedmachine learning algorithms to detect bots deployed on Ethereum. Thehighest-performing clustering algorithm is a Gaussian Mixture Model with anaverage cluster purity of 82.6%, while the highest-performing model for binaryclassification is a Random Forest with an accuracy of 83%. Our machinelearning-based detection mechanism contributes to understanding the Ethereumecosystem dynamics by providing additional insights into the current botlandscape.</description><author>Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer</author><pubDate>Thu, 28 Mar 2024 17:06:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19530v1</guid></item><item><title>Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2403.19527v1</link><description>Category-level 6D object pose estimation aims to estimate the rotation,translation and size of unseen instances within specific categories. In thisarea, dense correspondence-based methods have achieved leading performance.However, they do not explicitly consider the local and global geometricinformation of different instances, resulting in poor generalization ability tounseen instances with significant shape variations. To deal with this problem,we propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learningmethod for category-level 6D object pose estimation (AG-Pose), which includestwo key designs: (1) The first design is an Instance-Adaptive KeypointDetection module, which can adaptively detect a set of sparse keypoints forvarious instances to represent their geometric structures. (2) The seconddesign is a Geometric-Aware Feature Aggregation module, which can efficientlyintegrate the local and global geometric information into keypoint features.These two modules can work together to establish robust keypoint-levelcorrespondences for unseen instances, thus enhancing the generalization abilityof the model.Experimental results on CAMERA25 and REAL275 datasets show thatthe proposed AG-Pose outperforms state-of-the-art methods by a large marginwithout category-specific shape priors.</description><author>Xiao Lin, Wenfei Yang, Yuan Gao, Tianzhu Zhang</author><pubDate>Thu, 28 Mar 2024 17:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19527v1</guid></item><item><title>DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition</title><link>http://arxiv.org/abs/2403.17645v2</link><description>End-to-end automatic speech recognition (E2E ASR) systems often suffer frommistranscription of domain-specific phrases, such as named entities, sometimesleading to catastrophic failures in downstream tasks. A family of fast andlightweight named entity correction (NEC) models for ASR have recently beenproposed, which normally build on phonetic-level edit distance algorithms andhave shown impressive NEC performance. However, as the named entity (NE) listgrows, the problems of phonetic confusion in the NE list are exacerbated; forexample, homophone ambiguities increase substantially. In view of this, weproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),which leverages entity descriptions to provide additional information tofacilitate mitigation of phonetic confusion for NEC on ASR transcription. Tothis end, an efficient entity description augmented masked language model(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM toadapt swiftly to domain-specific entities for the NEC task. A series ofexperiments conducted on the AISHELL-1 and Homophone datasets confirm theeffectiveness of our modeling approach. DANCER outperforms a strong baseline,the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate(CER) reduction of about 7% relatively on AISHELL-1 for named entities. Morenotably, when tested on Homophone that contain named entities of high phoneticconfusion, DANCER offers a more pronounced CER reduction of 46% relatively overPED-NEC for named entities.</description><author>Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin, Berlin Chen</author><pubDate>Thu, 28 Mar 2024 16:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17645v2</guid></item><item><title>Model Stock: All we need is just a few fine-tuned models</title><link>http://arxiv.org/abs/2403.19522v1</link><description>This paper introduces an efficient fine-tuning method for large pre-trainedmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)performance. Breaking away from traditional practices that need a multitude offine-tuned models for averaging, our approach employs significantly fewermodels to achieve final weights yet yield superior accuracy. Drawing from keyinsights in the weight space of fine-tuned weights, we uncover a strong linkbetween the performance and proximity to the center of weight space. Based onthis, we introduce a method that approximates a center-close weight using onlytwo fine-tuned models, applicable during or after training. Our innovativelayer-wise weight averaging technique surpasses state-of-the-art model methodssuch as Model Soup, utilizing only two fine-tuned models. This strategy can beaptly coined Model Stock, highlighting its reliance on selecting a minimalnumber of models to draw a more optimized-averaged model. We demonstrate theefficacy of Model Stock with fine-tuned models based upon pre-trained CLIParchitectures, achieving remarkable performance on both ID and OOD tasks on thestandard benchmarks, all while barely bringing extra computational demands. Ourcode and pre-trained models are available athttps://github.com/naver-ai/model-stock.</description><author>Dong-Hwan Jang, Sangdoo Yun, Dongyoon Han</author><pubDate>Thu, 28 Mar 2024 16:57:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19522v1</guid></item><item><title>A Comprehensive Study of Knowledge Editing for Large Language Models</title><link>http://arxiv.org/abs/2401.01286v4</link><description>Large Language Models (LLMs) have shown extraordinary capabilities inunderstanding and generating text that closely mirrors human communication.However, a primary limitation lies in the significant computational demandsduring training, arising from their extensive parameterization. This challengeis further intensified by the dynamic nature of the world, necessitatingfrequent updates to LLMs to correct outdated information or integrate newknowledge, thereby ensuring their continued relevance. Note that manyapplications demand continual model adjustments post-training to addressdeficiencies or undesirable behaviors. There is an increasing interest inefficient, lightweight methods for on-the-fly model modifications. To this end,recent years have seen a burgeoning in the techniques of knowledge editing forLLMs, which aim to efficiently modify LLMs' behaviors within specific domainswhile preserving overall performance across various inputs. In this paper, wefirst define the knowledge editing problem and then provide a comprehensivereview of cutting-edge approaches. Drawing inspiration from educational andcognitive research theories, we propose a unified categorization criterion thatclassifies knowledge editing methods into three groups: resorting to externalknowledge, merging knowledge into the model, and editing intrinsic knowledge.Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensiveempirical evaluation of representative knowledge editing approaches.Additionally, we provide an in-depth analysis of knowledge location, which cangive a deeper understanding of the knowledge structures inherent within LLMs.Finally, we discuss several potential applications of knowledge editing,outlining its broad and impactful implications.</description><author>Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen</author><pubDate>Thu, 28 Mar 2024 16:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01286v4</guid></item><item><title>Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models</title><link>http://arxiv.org/abs/2403.19521v1</link><description>In this paper, we deeply explore the mechanisms employed by Transformer-basedlanguage models in factual recall tasks. In zero-shot scenarios, given a promptlike "The capital of France is," task-specific attention heads extract thetopic entity, such as "France," from the context and pass it to subsequent MLPsto recall the required answer such as "Paris." We introduce a novel analysismethod aimed at decomposing the outputs of the MLP into componentsunderstandable by humans. Through this method, we quantify the function of theMLP layer following these task-specific heads. In the residual stream, iteither erases or amplifies the information originating from individual heads.Moreover, it generates a component that redirects the residual stream towardsthe direction of its expected answer. These zero-shot mechanisms are alsoemployed in few-shot scenarios. Additionally, we observed a widely existentanti-overconfidence mechanism in the final layer of models, which suppressescorrect predictions. We mitigate this suppression by leveraging ourinterpretation to improve factual recall performance. Our interpretations havebeen evaluated across various language models, from the GPT-2 families to 1.3BOPT, and across tasks covering different domains of factual knowledge.</description><author>Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, Rui Yan</author><pubDate>Thu, 28 Mar 2024 16:54:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19521v1</guid></item><item><title>Machine Learning-Powered Combinatorial Clock Auction</title><link>http://arxiv.org/abs/2308.10226v2</link><description>We study the design of iterative combinatorial auctions (ICAs). The mainchallenge in this domain is that the bundle space grows exponentially in thenumber of items. To address this, several papers have recently proposed machinelearning (ML)-based preference elicitation algorithms that aim to elicit onlythe most important information from bidders. However, from a practical point ofview, the main shortcoming of this prior work is that those designs elicitbidders' preferences via value queries (i.e., ``What is your value for thebundle $\{A,B\}$?''). In most real-world ICA domains, value queries areconsidered impractical, since they impose an unrealistically high cognitiveburden on bidders, which is why they are not used in practice. In this paper,we address this shortcoming by designing an ML-powered combinatorial clockauction that elicits information from the bidders only via demand queries(i.e., ``At prices $p$, what is your most preferred bundle of items?''). Wemake two key technical contributions: First, we present a novel method fortraining an ML model on demand queries. Second, based on those trained MLmodels, we introduce an efficient method for determining the demand query withthe highest clearing potential, for which we also provide a theoreticalfoundation. We experimentally evaluate our ML-based demand query mechanism inseveral spectrum auction domains and compare it against the most establishedreal-world ICA: the combinatorial clock auction (CCA). Our mechanismsignificantly outperforms the CCA in terms of efficiency in all domains, itachieves higher efficiency in a significantly reduced number of rounds, and,using linear prices, it exhibits vastly higher clearing potential. Thus, withthis paper we bridge the gap between research and practice and propose thefirst practical ML-powered ICA.</description><author>Ermis Soumalias, Jakob Weissteiner, Jakob Heiss, Sven Seuken</author><pubDate>Thu, 28 Mar 2024 16:53:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10226v2</guid></item><item><title>VisionKG: Unleashing the Power of Visual Datasets via Knowledge Graph</title><link>http://arxiv.org/abs/2309.13610v2</link><description>The availability of vast amounts of visual data with heterogeneous featuresis a key factor for developing, testing, and benchmarking of new computervision (CV) algorithms and architectures. Most visual datasets are created andcurated for specific tasks or with limited image data distribution for veryspecific situations, and there is no unified approach to manage and access themacross diverse sources, tasks, and taxonomies. This not only createsunnecessary overheads when building robust visual recognition systems, but alsointroduces biases into learning systems and limits the capabilities ofdata-centric AI. To address these problems, we propose the Vision KnowledgeGraph (VisionKG), a novel resource that interlinks, organizes and managesvisual datasets via knowledge graphs and Semantic Web technologies. It canserve as a unified framework facilitating simple access and querying ofstate-of-the-art visual datasets, regardless of their heterogeneous formats andtaxonomies. One of the key differences between our approach and existingmethods is that ours is knowledge-based rather than metadatabased. It enhancesthe enrichment of the semantics at both image and instance levels and offersvarious data retrieval and exploratory services via SPARQL. VisionKG currentlycontains 519 million RDF triples that describe approximately 40 millionentities, and are accessible at https://vision.semkg.org and through APIs. Withthe integration of 30 datasets and four popular CV tasks, we demonstrate itsusefulness across various scenarios when working with CV pipelines.</description><author>Jicheng Yuan, Anh Le-Tuan, Manh Nguyen-Duc, Trung-Kien Tran, Manfred Hauswirth, Danh Le-Phuoc</author><pubDate>Thu, 28 Mar 2024 16:52:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13610v2</guid></item><item><title>GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</title><link>http://arxiv.org/abs/2312.02069v2</link><description>We introduce GaussianAvatars, a new method to create photorealistic headavatars that are fully controllable in terms of expression, pose, andviewpoint. The core idea is a dynamic 3D representation based on 3D Gaussiansplats that are rigged to a parametric morphable face model. This combinationfacilitates photorealistic rendering while allowing for precise animationcontrol via the underlying parametric model, e.g., through expression transferfrom a driving sequence or by manually changing the morphable model parameters.We parameterize each splat by a local coordinate frame of a triangle andoptimize for explicit displacement offset to obtain a more accurate geometricrepresentation. During avatar reconstruction, we jointly optimize for themorphable model parameters and Gaussian splat parameters in an end-to-endfashion. We demonstrate the animation capabilities of our photorealistic avatarin several challenging scenarios. For instance, we show reenactments from adriving video, where our method outperforms existing works by a significantmargin.</description><author>Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nie√üner</author><pubDate>Thu, 28 Mar 2024 16:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02069v2</guid></item><item><title>DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion</title><link>http://arxiv.org/abs/2308.16682v2</link><description>Motion capture from a limited number of body-worn sensors, such as inertialmeasurement units (IMUs) and pressure insoles, has important applications inhealth, human performance, and entertainment. Recent work has focused onaccurately reconstructing whole-body motion from a specific sensorconfiguration using six IMUs. While a common goal across applications is to usethe minimal number of sensors to achieve required accuracy, the optimalarrangement of the sensors might differ from application to application. Wepropose a single diffusion model, DiffusionPoser, which reconstructs humanmotion in real-time from an arbitrary combination of sensors, including IMUsplaced at specified locations, and, pressure insoles. Unlike existing methods,our model grants users the flexibility to determine the number and arrangementof sensors tailored to the specific activity of interest, without the need forretraining. A novel autoregressive inferencing scheme ensures real-time motionreconstruction that closely aligns with measured sensor signals. The generativenature of DiffusionPoser ensures realistic behavior, even fordegrees-of-freedom not directly measured. Qualitative results can be found onour website: https://diffusionposer.github.io/.</description><author>Tom Van Wouwe, Seunghwan Lee, Antoine Falisse, Scott Delp, C. Karen Liu</author><pubDate>Thu, 28 Mar 2024 16:49:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16682v2</guid></item><item><title>XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold</title><link>http://arxiv.org/abs/2403.19517v1</link><description>We propose XScale-NVS for high-fidelity cross-scale novel view synthesis ofreal-world large-scale scenes. Existing representations based on explicitsurface suffer from discretization resolution or UV distortion, while implicitvolumetric representations lack scalability for large scenes due to thedispersed weight distribution and surface ambiguity. In light of the abovechallenges, we introduce hash featurized manifold, a novel hash-basedfeaturization coupled with a deferred neural rendering framework. This approachfully unlocks the expressivity of the representation by explicitlyconcentrating the hash entries on the 2D manifold, thus effectivelyrepresenting highly detailed contents independent of the discretizationresolution. We also introduce a novel dataset, namely GigaNVS, to benchmarkcross-scale, high-resolution novel view synthesis of realworld large-scalescenes. Our method significantly outperforms competing baselines on variousreal-world scenes, yielding an average LPIPS that is 40% lower than priorstate-of-the-art on the challenging GigaNVS benchmark. Please see our projectpage at: xscalenvs.github.io.</description><author>Guangyu Wang, Jinzhi Zhang, Fan Wang, Ruqi Huang, Lu Fang</author><pubDate>Thu, 28 Mar 2024 16:48:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19517v1</guid></item><item><title>Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering</title><link>http://arxiv.org/abs/2403.19516v1</link><description>This paper studies the directed graph clustering problem through the lens ofstatistics, where we formulate clustering as estimating underlying communitiesin the directed stochastic block model (DSBM). We conduct the maximumlikelihood estimation (MLE) on the DSBM and thereby ascertain the most probablecommunity assignment given the observed graph structure. In addition to thestatistical point of view, we further establish the equivalence between thisMLE formulation and a novel flow optimization heuristic, which jointlyconsiders two important directed graph statistics: edge density and edgeorientation. Building on this new formulation of directed clustering, weintroduce two efficient and interpretable directed clustering algorithms, aspectral clustering algorithm and a semidefinite programming based clusteringalgorithm. We provide a theoretical upper bound on the number of misclusteredvertices of the spectral clustering algorithm using tools from matrixperturbation theory. We compare, both quantitatively and qualitatively, ourproposed algorithms with existing directed clustering methods on both syntheticand real-world data, thus providing further ground to our theoreticalcontributions.</description><author>Mihai Cucuringu, Xiaowen Dong, Ning Zhang</author><pubDate>Thu, 28 Mar 2024 16:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19516v1</guid></item><item><title>CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network</title><link>http://arxiv.org/abs/2403.19514v1</link><description>In recent years, incomplete multi-view clustering, which studies thechallenging multi-view clustering problem on missing views, has receivedgrowing research interests. Although a series of methods have been proposed toaddress this issue, the following problems still exist: 1) Almost all of theexisting methods are based on shallow models, which is difficult to obtaindiscriminative common representations. 2) These methods are generally sensitiveto noise or outliers since the negative samples are treated equally as theimportant samples. In this paper, we propose a novel incomplete multi-viewclustering network, called Cognitive Deep Incomplete Multi-view ClusteringNetwork (CDIMC-net), to address these issues. Specifically, it captures thehigh-level features and local structure of each view by incorporating theview-specific deep encoders and graph embedding strategy into a framework.Moreover, based on the human cognition, i.e., learning from easy to hard, itintroduces a self-paced strategy to select the most confident samples for modeltraining, which can reduce the negative influence of outliers. Experimentalresults on several incomplete datasets show that CDIMC-net outperforms thestate-of-the-art incomplete multi-view clustering methods.</description><author>Jie Wen, Zheng Zhang, Yong Xu, Bob Zhang, Lunke Fei, Guo-Sen Xie</author><pubDate>Thu, 28 Mar 2024 16:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19514v1</guid></item><item><title>LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</title><link>http://arxiv.org/abs/2403.17919v2</link><description>The machine learning community has witnessed impressive advancements sincethe first appearance of large language models (LLMs), yet their huge memoryconsumption has become a major roadblock to large-scale training. ParameterEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have beenproposed to alleviate this problem, but their performance still fails to matchfull parameter training in most large-scale fine-tuning settings. Attempting tocomplement this deficiency, we investigate layerwise properties of LoRA onfine-tuning tasks and observe an uncommon skewness of weight norms acrossdifferent layers. Utilizing this key observation, a surprisingly simpletraining strategy is discovered, which outperforms both LoRA and full parametertraining in a wide range of settings with memory costs as low as LoRA. We nameit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,which applies the idea of importance sampling to different layers in LLMs andrandomly freeze most middle layers during optimization. Experimental resultsshow that with similar or less GPU memory consumption, LISA surpasses LoRA oreven full parameter tuning in downstream fine-tuning tasks, where LISAconsistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Benchscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par orbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstratingits effectiveness across different domains.</description><author>Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang</author><pubDate>Thu, 28 Mar 2024 16:44:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17919v2</guid></item><item><title>Improving Clinical NLP Performance through Language Model-Generated Synthetic Clinical Data</title><link>http://arxiv.org/abs/2403.19511v1</link><description>Generative models have been showing potential for producing data in mass.This study explores the enhancement of clinical natural language processingperformance by utilizing synthetic data generated from advanced languagemodels. Promising results show feasible applications in such a high-stakesdomain.</description><author>Shan Chen, Jack Gallifant, Marco Guevara, Yanjun Gao, Majid Afshar, Timothy Miller, Dmitriy Dligach, Danielle S. Bitterman</author><pubDate>Thu, 28 Mar 2024 16:44:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19511v1</guid></item><item><title>Phonetic Segmentation of the UCLA Phonetics Lab Archive</title><link>http://arxiv.org/abs/2403.19509v1</link><description>Research in speech technologies and comparative linguistics depends on accessto diverse and accessible speech data. The UCLA Phonetics Lab Archive is one ofthe earliest multilingual speech corpora, with long-form audio recordings andphonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently,95 of these languages were time-aligned with word-level phonetic transcriptions(Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetictranscriptions and phone-level alignments of the UCLA Phonetics Lab Archive,which uses the 95-language CMU re-release as our starting point. VoxAngelesalso includes word- and phone-level segmentations from the original UCLAcorpus, as well as phonetic measurements of word and phone durations, vowelformants, and vowel f0. This corpus enhances the usability of the originaldata, particularly for quantitative phonetic typology, as demonstrated througha case study of vowel intrinsic f0. We also discuss the utility of theVoxAngeles corpus for general research and pedagogy in crosslinguisticphonetics, as well as for low-resource and multilingual speech technologies.VoxAngeles is free to download and use under a CC-BY-NC 4.0 license.</description><author>Eleanor Chodroff, Bla≈æ Pa≈æon, Annie Baker, Steven Moran</author><pubDate>Thu, 28 Mar 2024 16:42:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19509v1</guid></item><item><title>Debiasing Cardiac Imaging with Controlled Latent Diffusion Models</title><link>http://arxiv.org/abs/2403.19508v1</link><description>The progress in deep learning solutions for disease diagnosis and prognosisbased on cardiac magnetic resonance imaging is hindered by highly imbalancedand biased training data. To address this issue, we propose a method toalleviate imbalances inherent in datasets through the generation of syntheticdata based on sensitive attributes such as sex, age, body mass index, andhealth condition. We adopt ControlNet based on a denoising diffusionprobabilistic model to condition on text assembled from patient metadata andcardiac geometry derived from segmentation masks using a large-cohort study,specifically, the UK Biobank. We assess our method by evaluating the realism ofthe generated images using established quantitative metrics. Furthermore, weconduct a downstream classification task aimed at debiasing a classifier byrectifying imbalances within underrepresented groups through syntheticallygenerated samples. Our experiments demonstrate the effectiveness of theproposed approach in mitigating dataset imbalances, such as the scarcity ofyounger patients or individuals with normal BMI level suffering from heartfailure. This work represents a major step towards the adoption of syntheticdata for the development of fair and generalizable models for medicalclassification tasks. Notably, we conduct all our experiments using a single,consumer-level GPU to highlight the feasibility of our approach withinresource-constrained environments. Our code is available athttps://github.com/faildeny/debiasing-cardiac-mri.</description><author>Grzegorz Skorupko, Richard Osuala, Zuzanna Szafranowska, Kaisar Kushibar, Nay Aung, Steffen E Petersen, Karim Lekadir, Polyxeni Gkontra</author><pubDate>Thu, 28 Mar 2024 16:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19508v1</guid></item><item><title>SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations</title><link>http://arxiv.org/abs/2403.19507v1</link><description>We consider using deep neural networks to solve time-dependent partialdifferential equations (PDEs), where multi-scale processing is crucial formodeling complex, time-evolving dynamics. While the U-Net architecture withskip connections is commonly used by prior studies to enable multi-scaleprocessing, our analysis shows that the need for features to evolve acrosslayers results in temporally misaligned features in skip connections, whichlimits the model's performance. To address this limitation, we propose SineNet,consisting of multiple sequentially connected U-shaped network blocks, referredto as waves. In SineNet, high-resolution features are evolved progressivelythrough multiple stages, thereby reducing the amount of misalignment withineach stage. We furthermore analyze the role of skip connections in enablingboth parallel and sequential processing of multi-scale information. Our methodis rigorously tested on multiple PDE datasets, including the Navier-Stokesequations and shallow water equations, showcasing the advantages of ourproposed approach over conventional U-Nets with a comparable parameter budget.We further demonstrate that increasing the number of waves in SineNet whilemaintaining the same number of parameters leads to a monotonically improvedperformance. The results highlight the effectiveness of SineNet and thepotential of our approach in advancing the state-of-the-art in neural PDEsolver design. Our code is available as part of AIRS(https://github.com/divelab/AIRS).</description><author>Xuan Zhang, Jacob Helwig, Yuchao Lin, Yaochen Xie, Cong Fu, Stephan Wojtowytsch, Shuiwang Ji</author><pubDate>Thu, 28 Mar 2024 16:41:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19507v1</guid></item></channel></rss>