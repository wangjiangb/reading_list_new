<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 02 Sep 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DriveQA: Passing the Driving Knowledge Test</title><link>http://arxiv.org/abs/2508.21824v1</link><description>If a Large Language Model (LLM) were to take a driving knowledge test today,would it pass? Beyond standard spatial and visual question-answering (QA) taskson current autonomous driving benchmarks, driving knowledge tests require acomplete understanding of all traffic rules, signage, and right-of-wayprinciples. To pass this test, human drivers must discern various edge casesthat rarely appear in real-world datasets. In this work, we present DriveQA, anextensive open-source text and vision-based benchmark that exhaustively coverstraffic regulations and scenarios. Through our experiments using DriveQA, weshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well onbasic traffic rules but exhibit significant weaknesses in numerical reasoningand complex right-of-way scenarios, traffic sign variations, and spatiallayouts, (2) fine-tuning on DriveQA improves accuracy across multiplecategories, particularly in regulatory sign recognition and intersectiondecision-making, (3) controlled variations in DriveQA-V provide insights intomodel sensitivity to environmental factors such as lighting, perspective,distance, and weather conditions, and (4) pretraining on DriveQA enhancesdownstream driving task performance, leading to improved results on real-worlddatasets such as nuScenes and BDD, while also demonstrating that models caninternalize text and synthetic traffic knowledge to generalize effectivelyacross downstream QA tasks.</description><author>Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar</author><pubDate>Fri, 29 Aug 2025 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21824v1</guid></item><item><title>Beyond Frequency: The Role of Redundancy in Large Language Model Memorization</title><link>http://arxiv.org/abs/2506.12321v2</link><description>Memorization in large language models poses critical risks for privacy andfairness as these systems scale to billions of parameters. While previousstudies established correlations between memorization and factors like tokenfrequency and repetition patterns, we revealed distinct response patterns:frequency increases minimally impact memorized samples (e.g. 0.09) whilesubstantially affecting non-memorized samples (e.g., 0.25), with consistencyobserved across model scales. Through counterfactual analysis by perturbingsample prefixes and quantifying perturbation strength through token positionalchanges, we demonstrate that redundancy correlates with memorization patterns.Our findings establish that: about 79% of memorized samples are low-redundancy,these low-redundancy samples exhibit 2-fold higher vulnerability thanhigh-redundancy ones, and consequently memorized samples drop by 0.6 underperturbation while non-memorized samples drop by only 0.01, indicating thatmore redundant content becomes both more memorable and more fragile. Thesefindings suggest potential redundancy-guided approaches for data preprocessing,thereby reducing privacy risks and mitigating bias to ensure fairness in modeldeployments.</description><author>Jie Zhang, Qinghua Zhao, Chi-ho Lin, Zhongfeng Kang, Lei Li</author><pubDate>Fri, 29 Aug 2025 12:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.12321v2</guid></item><item><title>Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</title><link>http://arxiv.org/abs/2508.21589v1</link><description>Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally relyon high-quality training data. While data selection and data synthesis are twocommon strategies to improve data quality, existing approaches often facelimitations in static dataset curation that fail to adapt to evolving modelcapabilities. In this paper, we introduce Middo, a self-evolving Model-informeddynamic data optimization framework that uses model-aware data selection andcontext-preserving data refinement. Unlike conventional one-offfiltering/synthesis methods, our framework establishes a closed-loopoptimization system: (1) A self-referential diagnostic module proactivelyidentifies suboptimal samples through tri-axial model signals - loss patterns(complexity), embedding cluster dynamics (diversity), and self-alignment scores(quality); (2) An adaptive optimization engine then transforms suboptimalsamples into pedagogically valuable training points while preserving semanticintegrity; (3) This optimization process continuously evolves with modelcapability through dynamic learning principles. Experiments on multiplebenchmarks demonstrate that our \method consistently enhances the quality ofseed data and boosts LLM's performance with improving accuracy by 7.15% onaverage while maintaining the original dataset scale. This work establishes anew paradigm for sustainable LLM training through dynamic human-AI co-evolutionof data and models. Our datasets, models, and code are coming soon.</description><author>Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu</author><pubDate>Fri, 29 Aug 2025 12:47:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21589v1</guid></item><item><title>A Survey on Current Trends and Recent Advances in Text Anonymization</title><link>http://arxiv.org/abs/2508.21587v1</link><description>The proliferation of textual data containing sensitive personal informationacross various domains requires robust anonymization techniques to protectprivacy and comply with regulations, while preserving data usability fordiverse and crucial downstream tasks. This survey provides a comprehensiveoverview of current trends and recent advances in text anonymizationtechniques. We begin by discussing foundational approaches, primarily centeredon Named Entity Recognition, before examining the transformative impact ofLarge Language Models, detailing their dual role as sophisticated anonymizersand potent de-anonymization threats. The survey further exploresdomain-specific challenges and tailored solutions in critical sectors such ashealthcare, law, finance, and education. We investigate advanced methodologiesincorporating formal privacy models and risk-aware frameworks, and address thespecialized subfield of authorship anonymization. Additionally, we reviewevaluation frameworks, comprehensive metrics, benchmarks, and practicaltoolkits for real-world deployment of anonymization solutions. This reviewconsolidates current knowledge, identifies emerging trends and persistentchallenges, including the evolving privacy-utility trade-off, the need toaddress quasi-identifiers, and the implications of LLM capabilities, and aimsto guide future research directions for both academics and practitioners inthis field.</description><author>Tobias Deußer, Lorenz Sparrenberg, Armin Berger, Max Hahnbück, Christian Bauckhage, Rafet Sifa</author><pubDate>Fri, 29 Aug 2025 12:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21587v1</guid></item><item><title>Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer</title><link>http://arxiv.org/abs/2508.21581v1</link><description>Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) isessential for guiding postoperative surveillance and treatment. The Leibovichscore remains widely used for stratifying distant recurrence risk but offerslimited patient-level resolution and excludes imaging information. This studyevaluates multimodal recurrence prediction by integrating preoperative computedtomography (CT) and postoperative histopathology whole-slide images (WSIs). Amodular deep learning framework with pretrained encoders and Cox-based survivalmodeling was tested across unimodal, late fusion, and intermediate fusionsetups. In a real-world ccRCC cohort, WSI-based models consistentlyoutperformed CT-only models, underscoring the prognostic strength of pathology.Intermediate fusion further improved performance, with the best model(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Randomtie-breaking narrowed the gap between the clinical baseline and learned models,suggesting discretization may overstate individualized performance. Usingsimple embedding concatenation, radiology added value primarily through fusion.These findings demonstrate the feasibility of foundation model-based multimodalintegration for personalized ccRCC risk prediction. Future work should exploremore expressive fusion strategies, larger multimodal datasets, andgeneral-purpose CT encoders to better match pathology modeling capacity.</description><author>Daniël Boeke, Cedrik Blommestijn, Rebecca N. Wray, Kalina Chupetlovska, Shangqi Gao, Zeyu Gao, Regina G. H. Beets-Tan, Mireia Crispin-Ortuzar, James O. Jones, Wilson Silva, Ines P. Machado</author><pubDate>Fri, 29 Aug 2025 12:34:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21581v1</guid></item><item><title>Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging</title><link>http://arxiv.org/abs/2508.21580v1</link><description>Understanding temporal dynamics in medical imaging is crucial forapplications such as disease progression modeling, treatment planning andanatomical development tracking. However, most deep learning methods eitherconsider only single temporal contexts, or focus on tasks like classificationor regression, limiting their ability for fine-grained spatial predictions.While some approaches have been explored, they are often limited to singletimepoints, specific diseases or have other technical restrictions. To addressthis fundamental gap, we introduce Temporal Flow Matching (TFM), a unifiedgenerative trajectory method that (i) aims to learn the underlying temporaldistribution, (ii) by design can fall back to a nearest image predictor, i.e.predicting the last context image (LCI), as a special case, and (iii) supports$3D$ volumes, multiple prior scans, and irregular sampling. Extensivebenchmarks on three public longitudinal datasets show that TFM consistentlysurpasses spatio-temporal methods from natural imaging, establishing a newstate-of-the-art and robust baseline for $4D$ medical image prediction.</description><author>Nico Albert Disch, Yannick Kirchhoff, Robin Peretzke, Maximilian Rokuss, Saikat Roy, Constantin Ulrich, David Zimmerer, Klaus Maier-Hein</author><pubDate>Fri, 29 Aug 2025 12:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21580v1</guid></item><item><title>Maximising Kidney Glomeruli Segmentation using Minimal Labels via Self-Supervision</title><link>http://arxiv.org/abs/2412.15389v2</link><description>Histopathology, the microscopic examination of tissue samples, is essentialfor disease diagnosis and prognosis. Accurate segmentation and identificationof key regions in histopathology images are crucial for developing automatedsolutions. However, state-of-art deep learning segmentation methods like UNetrequire extensive labels, which is both costly and time-consuming, particularlywhen dealing with multiple stainings. To mitigate this, various multi-stainsegmentation methods such as UDA-GAN have been developed, which reduce the needfor labels by requiring only one (source) stain to be labelled. Nonetheless,obtaining source stain labels can still be challenging, and segmentation modelsfail when they are unavailable. This article shows that through self-supervisedpre-training$-$including SimCLR, BYOL, and a novel approach, HR-CS-CO$-$theperformance of these segmentation methods (UNet, and UDAGAN) can be retainedeven with 95% fewer labels. Notably, with self-supervised pre-training andusing only 5% labels, the performance drops are minimal: 5.9% for UNet and 6.2%for UDAGAN, compared to their respective fully supervised counterparts (withoutpre-training, using 100% labels). Furthermore, these findings are shown togeneralise beyond their training distribution to public benchmark datasets. Im-</description><author>Zeeshan Nisar, Thomas Lampert</author><pubDate>Fri, 29 Aug 2025 12:31:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.15389v2</guid></item><item><title>Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2508.21571v1</link><description>Physics informed neural networks (PINNs) represent a very popular class ofneural solvers for partial differential equations. In practice, one oftenemploys stochastic gradient descent type algorithms to train the neuralnetwork. Therefore, the convergence guarantee of stochastic gradient descent isof fundamental importance. In this work, we establish the linear convergence ofstochastic gradient descent / flow in training over-parameterized two layerPINNs for a general class of activation functions in the sense of highprobability. These results extend the existing result [18] in which gradientdescent was analyzed. The challenge of the analysis lies in handling thedynamic randomness introduced by stochastic optimization methods. The key ofthe analysis lies in ensuring the positive definiteness of suitable Grammatrices during the training. The analysis sheds insight into the dynamics ofthe optimization process, and provides guarantees on the neural networkstrained by stochastic algorithms.</description><author>Bangti Jin, Longjun Wu</author><pubDate>Fri, 29 Aug 2025 12:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21571v1</guid></item><item><title>OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories</title><link>http://arxiv.org/abs/2508.21570v1</link><description>Ocean salinity plays a vital role in circulation, climate, and marineecosystems, yet its measurement is often sparse, irregular, and noisy,especially in drifter-based datasets. Traditional approaches, such as remotesensing and optimal interpolation, rely on linearity and stationarity, and arelimited by cloud cover, sensor drift, and low satellite revisit rates. Whilemachine learning models offer flexibility, they often fail under severesparsity and lack principled ways to incorporate physical covariates withoutspecialized sensors. In this paper, we introduce the OceAn Salinity ImputationSystem (OASIS), a novel diffusion adversarial framework designed to addressthese challenges.</description><author>Bo Li, Yingqi Feng, Ming Jin, Xin Zheng, Yufei Tang, Laurent Cherubin, Alan Wee-Chung Liew, Can Wang, Qinghua Lu, Jingwei Yao, Shirui Pan, Hong Zhang, Xingquan Zhu</author><pubDate>Fri, 29 Aug 2025 12:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21570v1</guid></item><item><title>L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models</title><link>http://arxiv.org/abs/2508.21569v1</link><description>We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERTmodel optimized for regression-based similarity scoring. The MahaSTS datasetconsists of 16,860 Marathi sentence pairs labeled with continuous similarityscores in the range of 0-5. To ensure balanced supervision, the dataset isuniformly distributed across six score-based buckets spanning the full 0-5range, thus reducing label bias and enhancing model stability. We fine-tune theMahaSBERT model on this dataset and benchmark its performance against otheralternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experimentsdemonstrate that MahaSTS enables effective training for sentence similaritytasks in Marathi, highlighting the impact of human-curated annotations,targeted fine-tuning, and structured supervision in low-resource settings. Thedataset and model are publicly shared athttps://github.com/l3cube-pune/MarathiNLP</description><author>Aishwarya Mirashi, Ananya Joshi, Raviraj Joshi</author><pubDate>Fri, 29 Aug 2025 12:24:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21569v1</guid></item><item><title>NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic pruning and dendritic integration</title><link>http://arxiv.org/abs/2508.21566v1</link><description>Spiking neural networks (SNNs) are artificial neural networks based onsimulated biological neurons and have attracted much attention in recentartificial intelligence technology studies. The dendrites in biological neuronshave efficient information processing ability and computational power; however,the neurons of SNNs rarely match the complex structure of the dendrites.Inspired by the nonlinear structure and highly sparse properties of neuronaldendrites, in this study, we propose an efficient, lightweight SNN method withnonlinear pruning and dendritic integration (NSPDI-SNN). In this method, weintroduce nonlinear dendritic integration (NDI) to improve the representationof the spatiotemporal information of neurons. We implement heterogeneous statetransition ratios of dendritic spines and construct a new and flexiblenonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. Weconducted systematic experiments on three benchmark datasets (DVS128 Gesture,CIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks(speech recognition and reinforcement learning-based maze navigation task).Across all tasks, NSPDI-SNN consistently achieved high sparsity with minimalperformance degradation. In particular, our method achieved the bestexperimental results on all three event stream datasets. Further analysisshowed that NSPDI significantly improved the efficiency of synaptic informationtransfer as sparsity increased. In conclusion, our results indicate that thecomplex structure and nonlinear computation of neuronal dendrites provide apromising approach for developing efficient SNN methods.</description><author>Wuque Cai, Hongze Sun, Jiayi He, Qianqian Liao, Yunliang Zang, Duo Chen, Dezhong Yao, Daqing Guo</author><pubDate>Fri, 29 Aug 2025 12:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21566v1</guid></item><item><title>How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images</title><link>http://arxiv.org/abs/2508.21565v1</link><description>Effectively understanding urban scenes requires fine-grained spatialreasoning about objects, layouts, and depth cues. However, how well currentvision-language models (VLMs), pretrained on general scenes, transfer theseabilities to urban domain remains underexplored. To address this gap, weconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,and LLaVA-1.5-evaluating both zero-shot performance and the effects offine-tuning with a synthetic VQA dataset specific to urban scenes. We constructsuch dataset from segmentation, depth, and object detection predictions ofstreet-view images, pairing each question with LLM-generated Chain-of-Thought(CoT) answers for step-by-step reasoning supervision. Results show that whileVLMs perform reasonably well in zero-shot settings, fine-tuning with oursynthetic CoT-supervised dataset substantially boosts performance, especiallyfor challenging question types such as negation and counterfactuals. This studyintroduces urban spatial reasoning as a new challenge for VLMs and demonstratessynthetic dataset construction as a practical path for adapting general-purposemodels to specialized domains.</description><author>Juneyoung Ro, Namwoo Kim, Yoonjin Yoon</author><pubDate>Fri, 29 Aug 2025 12:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21565v1</guid></item><item><title>Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances</title><link>http://arxiv.org/abs/2508.21564v1</link><description>We propose a new framework for discovering landmarks that automaticallygeneralize across a domain. These generalized landmarks are learned from a setof solved instances and describe intermediate goals for planning problems wheretraditional landmark extraction algorithms fall short. Our generalizedlandmarks extend beyond the predicates of a domain by using state functionsthat are independent of the objects of a specific problem and apply to allsimilar objects, thus capturing repetition. Based on these functions, weconstruct a directed generalized landmark graph that defines the landmarkprogression, including loop possibilities for repetitive subplans. We show howto use this graph in a heuristic to solve new problem instances of the samedomain. Our results show that the generalized landmark graphs learned from afew small instances are also effective for larger instances in the same domain.If a loop that indicates repetition is identified, we see a significantimprovement in heuristic performance over the baseline. Generalized landmarkscapture domain information that is interpretable and useful to an automatedplanner. This information can be discovered from a small set of plans for thesame domain.</description><author>Issa Hanou, Sebastijan Dumančić, Mathijs de Weerdt</author><pubDate>Fri, 29 Aug 2025 12:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21564v1</guid></item><item><title>Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification</title><link>http://arxiv.org/abs/2508.21561v1</link><description>Recent studies show the promise of large language models (LLMs) for few-shottabular classification but highlight challenges due to the variability instructured data. To address this, we propose distilling data into actionableinsights to enable robust and effective classification by LLMs. Drawinginspiration from human learning processes, we introduce InsightTab, an insightdistillation framework guided by principles of divide-and-conquer, easy-first,and reflective learning. Our approach integrates rule summarization, strategicexemplification, and insight reflection through deep collaboration between LLMsand data modeling techniques. The obtained insights enable LLMs to better aligntheir general knowledge and capabilities with the particular requirements ofspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.The results demonstrate consistent improvement over state-of-the-art methods.Ablation studies further validate the principle-guided distillation process,while analyses emphasize InsightTab's effectiveness in leveraging labeled dataand managing bias.</description><author>Yifei Yuan, Jiatong Li, Weijia Zhang, Mohammad Aliannejadi, Evangelos Kanoulas, Renjun Hu</author><pubDate>Fri, 29 Aug 2025 12:16:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21561v1</guid></item><item><title>Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation</title><link>http://arxiv.org/abs/2508.21559v1</link><description>Physics-Informed Neural Networks (PINNs) present a transformative approachfor smart grid modeling by integrating physical laws directly into learningframeworks, addressing critical challenges of data scarcity and physicalconsistency in conventional data-driven methods. This paper evaluates PINNs'capabilities as surrogate models for smart grid dynamics, comparing theirperformance against XGBoost, Random Forest, and Linear Regression across threekey experiments: interpolation, cross-validation, and episodic trajectoryprediction. By training PINNs exclusively through physics-based loss functions(enforcing power balance, operational constraints, and grid stability) wedemonstrate their superior generalization, outperforming data-driven models inerror reduction. Notably, PINNs maintain comparatively lower MAE in dynamicgrid operations, reliably capturing state transitions in both random andexpert-driven control scenarios, while traditional models exhibit erraticperformance. Despite slight degradation in extreme operational regimes, PINNsconsistently enforce physical feasibility, proving vital for safety-criticalapplications. Our results contribute to establishing PINNs as aparadigm-shifting tool for smart grid surrogation, bridging data-drivenflexibility with first-principles rigor. This work advances real-time gridcontrol and scalable digital twins, emphasizing the necessity of physics-awarearchitectures in mission-critical energy systems.</description><author>Julen Cestero, Carmine Delle Femine, Kenji S. Muro, Marco Quartulli, Marcello Restelli</author><pubDate>Fri, 29 Aug 2025 12:15:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21559v1</guid></item><item><title>ECHO: Ego-Centric modeling of Human-Object interactions</title><link>http://arxiv.org/abs/2508.21556v1</link><description>Modeling human-object interactions (HOI) from an egocentric perspective is alargely unexplored yet important problem due to the increasing adoption ofwearable devices, such as smart glasses and watches. We investigate how muchinformation about interaction can be recovered from only head and wriststracking. Our answer is ECHO (Ego-Centric modeling of Human-Objectinteractions), which, for the first time, proposes a unified framework torecover three modalities: human pose, object motion, and contact from suchminimal observation. ECHO employs a Diffusion Transformer architecture and aunique three-variate diffusion process, which jointly models human motion,object trajectory, and contact sequence, allowing for flexible inputconfigurations. Our method operates in a head-centric canonical space,enhancing robustness to global orientation. We propose a conveyor-basedinference, which progressively increases the diffusion timestamp with the frameposition, allowing us to process sequences of any length. Through extensiveevaluation, we demonstrate that ECHO outperforms existing methods that do notoffer the same flexibility, setting a state-of-the-art in egocentric HOIreconstruction.</description><author>Ilya A. Petrov, Vladimir Guzov, Riccardo Marin, Emre Aksan, Xu Chen, Daniel Cremers, Thabo Beeler, Gerard Pons-Moll</author><pubDate>Fri, 29 Aug 2025 12:12:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21556v1</guid></item><item><title>Comprehensive Signal Quality Evaluation of a Wearable Textile ECG Garment: A Sex-Balanced Study</title><link>http://arxiv.org/abs/2508.21554v1</link><description>We introduce a novel wearable textile-garment featuring an innovativeelectrode placement aimed at minimizing noise and motion artifacts, therebyenhancing signal fidelity in Electrocardiography (ECG) recordings. We present acomprehensive, sex-balanced evaluation involving 15 healthy males and 15healthy female participants to ensure the device's suitability acrossanatomical and physiological variations. The assessment framework encompassesdistinct evaluation approaches: quantitative signal quality indices toobjectively benchmark device performance; rhythm-based analyzes ofphysiological parameters such as heart rate and heart rate variability; machinelearning classification tasks to assess application-relevant predictiveutility; morphological analysis of ECG features including amplitude andinterval parameters; and investigations of the effects of electrode projectionangle given by the textile / body shape, with all analyzes stratified by sex toelucidate sex-specific influences. Evaluations were conducted across variousactivity phases representing real-world conditions. The results demonstratethat the textile system achieves signal quality highly concordant withreference devices in both rhythm and morphological analyses, exhibits robustclassification performance, and enables identification of key sex-specificdeterminants affecting signal acquisition. These findings underscore thepractical viability of textile-based ECG garments for physiological monitoringas well as psychophysiological state detection. Moreover, we identify theimportance of incorporating sex-specific design considerations to ensureequitable and reliable cardiac diagnostics in wearable health technologies.</description><author>Maximilian P. Oppelt, Tobias S. Zech, Sarah H. Lorenz, Laurenz Ottmann, Jan Steffan, Bjoern M. Eskofier, Nadine R. Lang-Richter, Norman Pfeiffer</author><pubDate>Fri, 29 Aug 2025 12:10:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21554v1</guid></item><item><title>EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting</title><link>http://arxiv.org/abs/2508.21550v1</link><description>Pairwise comparison is often favored over absolute rating or ordinalclassification in subjective or difficult annotation tasks due to its improvedreliability. However, exhaustive comparisons require a massive number ofannotations (O(n^2)). Recent work has greatly reduced the annotation burden(O(n log n)) by actively sampling pairwise comparisons using a sortingalgorithm. We further improve annotation efficiency by (1) roughly pre-orderingitems using the Contrastive Language-Image Pre-training (CLIP) modelhierarchically without training, and (2) replacing easy, obvious humancomparisons with automated comparisons. The proposed EZ-Sort first produces aCLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validationwas conducted using various datasets: face-age estimation (FGNET), historicalimage chronology (DHCI), and retinal image quality assessment (EyePACS). Itshowed that EZ-Sort reduced human annotation cost by 90.5% compared toexhaustive pairwise comparisons and by 19.8% compared to prior work (when n =100), while improving or maintaining inter-rater reliability. These resultsdemonstrate that combining CLIP-based priors with uncertainty-aware samplingyields an efficient and scalable solution for pairwise ranking.</description><author>Yujin Park, Haejun Chung, Ikbeom Jang</author><pubDate>Fri, 29 Aug 2025 12:06:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21550v1</guid></item><item><title>BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models</title><link>http://arxiv.org/abs/2506.15689v2</link><description>Rotations have become essential to state-of-the-art quantization pipelinesfor large language models (LLMs) by effectively smoothing outliers in weightsand activations. However, further optimizing the rotation parameters offersonly limited performance gains and introduces significant training overhead:due to rotation parameter sharing, full-model must be loaded simultaneously toenable backpropagation, resulting in substantial memory consumption and limitedpractical utility. In this work, we identify two fundamental limitations ofcurrent rotational quantization methods: (i) rotation fails to align channelmeans, resulting in wider quantization bounds and increased rounding errors;and (ii) rotation makes the activation distribution more Gaussian-like,increasing energy loss caused by clipping errors. To address these issues, weintroduce \textbf{BASE-Q}, a simple yet powerful approach that combines biascorrection and asymmetric scaling to effectively reduce rounding and clippingerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating theneed for memory-intensive full-model backpropagation. Extensive experiments onvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowingthe accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will bereleased soon.</description><author>Liulu He, Shenli Zheng, Karwei Sun, Yijiang Liu, Yufei Zhao, Chongkang Tan, Huanrui Yang, Yuan Du, Li Du</author><pubDate>Fri, 29 Aug 2025 12:03:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.15689v2</guid></item><item><title>What Data is Really Necessary? A Feasibility Study of Inference Data Minimization for Recommender Systems</title><link>http://arxiv.org/abs/2508.21547v1</link><description>Data minimization is a legal principle requiring personal data processing tobe limited to what is necessary for a specified purpose. Operationalizing thisprinciple for recommender systems, which rely on extensive personal data,remains a significant challenge. This paper conducts a feasibility study onminimizing implicit feedback inference data for such systems. We propose anovel problem formulation, analyze various minimization techniques, andinvestigate key factors influencing their effectiveness. We demonstrate thatsubstantial inference data reduction is technically feasible withoutsignificant performance loss. However, its practicality is criticallydetermined by two factors: the technical setting (e.g., performance targets,choice of model) and user characteristics (e.g., history size, preferencecomplexity). Thus, while we establish its technical feasibility, we concludethat data minimization remains practically challenging and its dependence onthe technical and user context makes a universal standard for data `necessity'difficult to implement.</description><author>Jens Leysen, Marco Favier, Bart Goethals</author><pubDate>Fri, 29 Aug 2025 12:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21547v1</guid></item><item><title>Categorical Data Clustering via Value Order Estimated Distance Metric Learning</title><link>http://arxiv.org/abs/2411.15189v5</link><description>Clustering is a popular machine learning technique for data mining that canprocess and analyze datasets to automatically reveal sample distributionpatterns. Since the ubiquitous categorical data naturally lack a well-definedmetric space such as the Euclidean distance space of numerical data, thedistribution of categorical data is usually under-represented, and thusvaluable information can be easily twisted in clustering. This paper,therefore, introduces a novel order distance metric learning approach tointuitively represent categorical attribute values by learning their optimalorder relationship and quantifying their distance in a line similar to that ofthe numerical attributes. Since subjectively created qualitative categoricalvalues involve ambiguity and fuzziness, the order distance metric is learned inthe context of clustering. Accordingly, a new joint learning paradigm isdeveloped to alternatively perform clustering and order distance metriclearning with low time complexity and a guarantee of convergence. Due to theclustering-friendly order learning mechanism and the homogeneous ordinal natureof the order distance and Euclidean distance, the proposed method achievessuperior clustering accuracy on categorical and mixed datasets. Moreimportantly, the learned order distance metric greatly reduces the difficultyof understanding and managing the non-intuitive categorical data. Experimentswith ablation studies, significance tests, case studies, etc., have validatedthe efficacy of the proposed method. The source code is available athttps://github.com/DAJ0612/OCL_Source_Code.</description><author>Yiqun Zhang, Mingjie Zhao, Hong Jia, Yang Lu, Mengke Li, Yiu-ming Cheung</author><pubDate>Fri, 29 Aug 2025 12:00:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15189v5</guid></item><item><title>Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</title><link>http://arxiv.org/abs/2508.21542v1</link><description>Gaussian splatting typically requires dense observations of the scene and canfail to reconstruct occluded and unobserved areas. We propose a latentdiffusion model to reconstruct a complete 3D scene with Gaussian splats,including the occluded parts, from only a single image during inference.Completing the unobserved surfaces of a scene is challenging due to theambiguity of the plausible surfaces. Conventional methods use aregression-based formulation to predict a single "mode" for occluded andout-of-frustum surfaces, leading to blurriness, implausibility, and failure tocapture multiple possible explanations. Thus, they often address this problempartially, focusing either on objects isolated from the background,reconstructing only visible surfaces, or failing to extrapolate far from theinput views. In contrast, we propose a generative formulation to learn adistribution of 3D representations of Gaussian splats conditioned on a singleinput image. To address the lack of ground-truth training data, we propose aVariational AutoReconstructor to learn a latent space only from 2D images in aself-supervised manner, over which a diffusion model is trained. Our methodgenerates faithful reconstructions and diverse samples with the ability tocomplete the occluded surfaces for high-quality 360-degree renderings.</description><author>Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman</author><pubDate>Fri, 29 Aug 2025 11:55:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21542v1</guid></item><item><title>HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining</title><link>http://arxiv.org/abs/2508.21540v1</link><description>Process mining has emerged as a powerful analytical technique forunderstanding complex healthcare workflows. However, its application facessignificant barriers, including technical complexity, a lack of standardizedapproaches, and limited access to practical training resources. We introduceHealthProcessAI, a GenAI framework designed to simplify process miningapplications in healthcare and epidemiology by providing a comprehensivewrapper around existing Python (PM4PY) and R (bupaR) libraries. To addressunfamiliarity and improve accessibility, the framework integrates multipleLarge Language Models (LLMs) for automated process map interpretation andreport generation, helping translate technical analyses into outputs thatdiverse users can readily understand. We validated the framework using sepsisprogression data as a proof-of-concept example and compared the outputs of fivestate-of-the-art LLM models through the OpenRouter platform. To test itsfunctionality, the framework successfully processed sepsis data across fourproof-of-concept scenarios, demonstrating robust technical performance and itscapability to generate reports through automated LLM analysis. LLM evaluationusing five independent LLMs as automated evaluators revealed distinct modelstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistencyscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. Byintegrating multiple Large Language Models (LLMs) for automated interpretationand report generation, the framework addresses widespread unfamiliarity withprocess mining outputs, making them more accessible to clinicians, datascientists, and researchers. This structured analytics and AI-driveninterpretation combination represents a novel methodological advance intranslating complex process mining results into potentially actionable insightsfor healthcare applications.</description><author>Eduardo Illueca-Fernandez, Kaile Chen, Fernando Seoane, Farhad Abtahi</author><pubDate>Fri, 29 Aug 2025 11:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21540v1</guid></item><item><title>L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models</title><link>http://arxiv.org/abs/2506.00863v2</link><description>Emotion recognition in low-resource languages like Marathi remainschallenging due to limited annotated data. We present L3Cube-MahaEmotions, ahigh-quality Marathi emotion recognition dataset with 11 fine-grained emotionlabels. The training data is synthetically annotated using large languagemodels (LLMs), while the validation and test sets are manually labeled to serveas a reliable gold-standard benchmark. Building on the MahaSent dataset, weapply the Chain-of-Translation (CoTR) prompting technique, where Marathisentences are translated into English and emotion labeled via a single prompt.GPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training dataannotation due to superior label quality. We evaluate model performance usingstandard metrics and explore label aggregation strategies (e.g., Union,Intersection). While GPT-4 predictions outperform fine-tuned BERT models,BERT-based models trained on synthetic labels fail to surpass GPT-4. Thishighlights both the importance of high-quality human-labeled data and theinherent complexity of emotion recognition. An important finding of this workis that generic LLMs like GPT-4 and Llama3-405B generalize better thanfine-tuned BERT for complex low-resource emotion recognition tasks. The datasetand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP</description><author>Nidhi Kowtal, Raviraj Joshi</author><pubDate>Fri, 29 Aug 2025 11:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.00863v2</guid></item><item><title>HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones</title><link>http://arxiv.org/abs/2508.21539v1</link><description>Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks suchas target matching and navigation. However, the wide field of view and complexcompositional semantics in drone scenarios pose challenges for vision-languageunderstanding. Mainstream Vision-Language Models (VLMs) emphasize globalalignment while lacking fine-grained semantics, and existing hierarchicalmethods depend on precise entity partitioning and strict containment, limitingeffectiveness in dynamic environments. To address this, we propose theHierarchical Cross-Granularity Contrastive and Matching learning (HCCM)framework with two components: (1) Region-Global Image-Text ContrastiveLearning (RG-ITC), which avoids precise scene partitioning and captureshierarchical local-to-global semantics by contrasting local visual regions withglobal text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),which dispenses with rigid constraints and instead evaluates local semanticconsistency within global cross-modal representations, enhancing compositionalreasoning. Moreover, drone text descriptions are often incomplete or ambiguous,destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCMachieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (textretrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shotgeneralization with 39.93% mean recall (mR), outperforming fine-tunedbaselines.</description><author>Hao Ruan, Jinliang Lin, Yingxin Lai, Zhiming Luo, Shaozi Li</author><pubDate>Fri, 29 Aug 2025 11:50:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21539v1</guid></item><item><title>Adaptive generative moment matching networks for improved learning of dependence structures</title><link>http://arxiv.org/abs/2508.21531v1</link><description>An adaptive bandwidth selection procedure for the mixture kernel in themaximum mean discrepancy (MMD) for fitting generative moment matching networks(GMMNs) is introduced, and its ability to improve the learning of copula randomnumber generators is demonstrated. Based on the relative error of the trainingloss, the number of kernels is increased during training; additionally, therelative error of the validation loss is used as an early stopping criterion.While training time of such adaptively trained GMMNs (AGMMNs) is similar tothat of GMMNs, training performance is increased significantly in comparison toGMMNs, which is assessed and shown based on validation MMD trajectories,samples and validation MMD values. Superiority of AGMMNs over GMMNs, as well astypical parametric copula models, is demonstrated in terms of threeapplications. First, convergence rates of quasi-random versus pseudo-randomsamples from high-dimensional copulas are investigated for three functionals ofinterest and in dimensions as large as 100 for the first time. Second,replicated validation MMDs, as well as Monte Carlo and quasi-Monte Carloapplications based on the expected payoff of a basked call option and the riskmeasure expected shortfall as functionals are used to demonstrate the improvedtraining of AGMMNs over GMMNs for a copula model fitted to the standardizedresiduals of the 50 constituents of the S&amp;P 500 index after deGARCHing. Last,both the latter dataset and 50 constituents of the FTSE~100 are used todemonstrate that the improved training of AGMMNs over GMMNs and in comparisonto the fitting of classical parametric copula models indeed also translates toan improved model prediction.</description><author>Marius Hofert, Gan Yao</author><pubDate>Fri, 29 Aug 2025 11:38:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21531v1</guid></item><item><title>Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</title><link>http://arxiv.org/abs/2508.21529v1</link><description>Feature foundation models - usually vision transformers - offer rich semanticdescriptors of images, useful for downstream tasks such as (interactive)segmentation and object detection. For computational efficiency thesedescriptors are often patch-based, and so struggle to represent the finefeatures often present in micrographs; they also struggle with the large imagesizes present in materials and biological image analysis. In this work, wetrain a convolutional neural network to upsample low-resolution (i.e, largepatch size) foundation model features with reference to the input image. Weapply this upsampler network (without any further training) to efficientlyfeaturise and then segment a variety of microscopy images, including plantcells, a lithium-ion battery cathode and organic crystals. The richness ofthese upsampled features admits separation of hard to segment phases, likehairline cracks. We demonstrate that interactive segmentation with these deepfeatures produces high-quality segmentations far faster and with far fewerlabels than training or finetuning a more traditional convolutional network.</description><author>Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper</author><pubDate>Fri, 29 Aug 2025 11:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21529v1</guid></item><item><title>KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval</title><link>http://arxiv.org/abs/2508.20417v2</link><description>The integration of knowledge graphs (KGs) with large language models (LLMs)offers significant potential to improve the retrieval phase ofretrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,a novel framework for Contextual Query Retrieval (CQR) that enhances theretrieval phase by enriching the contextual representation of complex inputqueries using a corpus-centric KG. Unlike existing methods that primarilyaddress corpus-level context loss, KG-CQR focuses on query enrichment throughstructured relation representations, extracting and completing relevant KGsubgraphs to generate semantically rich query contexts. Comprising subgraphextraction, completion, and contextual generation modules, KG-CQR operates as amodel-agnostic pipeline, ensuring scalability across LLMs of varying sizeswithout additional training. Experimental results on RAGBench and MultiHop-RAGdatasets demonstrate KG-CQR's superior performance, achieving a 4-6%improvement in mAP and a 2-3% improvement in Recall@25 over strong baselinemodels. Furthermore, evaluations on challenging RAG tasks such as multi-hopquestion answering show that, by incorporating KG-CQR, the performanceconsistently outperforms the existing baseline in terms of retrievaleffectiveness</description><author>Chi Minh Bui, Ngoc Mai Thieu, Van Vinh Nguyen, Jason J. Jung, Khac-Hoai Nam Bui</author><pubDate>Fri, 29 Aug 2025 11:37:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20417v2</guid></item><item><title>C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</title><link>http://arxiv.org/abs/2508.18860v2</link><description>Balancing sensitivity to new tasks and stability for retaining past knowledgeis crucial in continual learning (CL). Recently, sharpness-aware minimizationhas proven effective in transfer learning and has also been adopted incontinual learning (CL) to improve memory retention and learning efficiency.However, relying on zeroth-order sharpness alone may favor sharper minima overflatter ones in certain settings, leading to less robust and potentiallysuboptimal solutions. In this paper, we propose \textbf{C}ontinual\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter losslandscapes tailored for CL. C-Flat offers plug-and-play compatibility, enablingeasy integration with minimal modifications to the code pipeline. Besides, wepresent a general framework that integrates C-Flat into all major CL paradigmsand conduct comprehensive comparisons with loss-minima optimizers andflat-minima-based CL methods. Our results show that C-Flat consistentlyimproves performance across a wide range of settings. In addition, we introduceC-Flat++, an efficient yet effective framework that leverages selectiveflatness-driven promotion, significantly reducing the update cost required byC-Flat. Extensive experiments across multiple CL methods, datasets, andscenarios demonstrate the effectiveness and efficiency of our proposedapproaches. Code is available at https://github.com/WanNaa/C-Flat.</description><author>Wei Li, Hangjie Yuan, Zixiang Zhao, Yifan Zhu, Aojun Lu, Tao Feng, Yanan Sun</author><pubDate>Fri, 29 Aug 2025 11:36:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18860v2</guid></item><item><title>A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement</title><link>http://arxiv.org/abs/2411.04090v3</link><description>Content moderation typically combines the efforts of human moderators andmachine learning models. However, these systems often rely on data wheresignificant disagreement occurs during moderation, reflecting the subjectivenature of toxicity perception. Rather than dismissing this disagreement asnoise, we interpret it as a valuable signal that highlights the inherentambiguity of the content,an insight missed when only the majority label isconsidered. In this work, we introduce a novel content moderation frameworkthat emphasizes the importance of capturing annotation disagreement. Ourapproach uses multitask learning, where toxicity classification serves as theprimary task and annotation disagreement is addressed as an auxiliary task.Additionally, we leverage uncertainty estimation techniques, specificallyConformal Prediction, to account for both the ambiguity in comment annotationsand the model's inherent uncertainty in predicting toxicity anddisagreement.The framework also allows moderators to adjust thresholds forannotation disagreement, offering flexibility in determining when ambiguityshould trigger a review. We demonstrate that our joint approach enhances modelperformance, calibration, and uncertainty estimation, while offering greaterparameter efficiency and improving the review process in comparison tosingle-task methods.</description><author>Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz</author><pubDate>Fri, 29 Aug 2025 11:32:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04090v3</guid></item><item><title>Stochastic Control for Fine-tuning Diffusion Models: Optimality, Regularity, and Convergence</title><link>http://arxiv.org/abs/2412.18164v4</link><description>Diffusion models have emerged as powerful tools for generative modeling,demonstrating exceptional capability in capturing target data distributionsfrom large datasets. However, fine-tuning these massive models for specificdownstream tasks, constraints, and human preferences remains a criticalchallenge. While recent advances have leveraged reinforcement learningalgorithms to tackle this problem, much of the progress has been empirical,with limited theoretical understanding. To bridge this gap, we propose astochastic control framework for fine-tuning diffusion models. Building ondenoising diffusion probabilistic models as the pre-trained reference dynamics,our approach integrates linear dynamics control with Kullback-Leiblerregularization. We establish the well-posedness and regularity of thestochastic control problem and develop a policy iteration algorithm (PI-FT) fornumerical solution. We show that PI-FT achieves global convergence at a linearrate. Unlike existing work that assumes regularities throughout training, weprove that the control and value sequences generated by the algorithm maintainthe regularity. Additionally, we explore extensions of our framework toparametric settings and continuous-time formulations, and demonstrate thepractical effectiveness of the proposed PI-FT algorithm through numericalexperiments. Our code is available athttps://github.com/yinbinhan/fine-tuning-of-diffusion-models.</description><author>Yinbin Han, Meisam Razaviyayn, Renyuan Xu</author><pubDate>Fri, 29 Aug 2025 11:26:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.18164v4</guid></item><item><title>Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators</title><link>http://arxiv.org/abs/2508.21524v1</link><description>Compute-in-memory (CIM) accelerators have emerged as a promising way forenhancing the energy efficiency of convolutional neural networks (CNNs).Deploying CNNs on CIM platforms generally requires quantization of networkweights and activations to meet hardware constraints. However, existingapproaches either prioritize hardware efficiency with binary weight andactivation quantization at the cost of accuracy, or utilize multi-bit weightsand activations for greater accuracy but limited efficiency. In this paper, weintroduce a novel binary weight multi-bit activation (BWMA) method for CNNs onCIM-based accelerators. Our contributions include: deriving closed-formsolutions for weight quantization in each layer, significantly improving therepresentational capabilities of binarized weights; and developing adifferentiable function for activation quantization, approximating the idealmulti-bit function while bypassing the extensive search for optimal settings.Through comprehensive experiments on CIFAR-10 and ImageNet datasets, we showthat BWMA achieves notable accuracy improvements over existing methods,registering gains of 1.44\%-5.46\% and 0.35\%-5.37\% on respective datasets.Moreover, hardware simulation results indicate that 4-bit activationquantization strikes the optimal balance between hardware cost and modelperformance.</description><author>Wenyong Zhou, Zhengwu Liu, Yuan Ren, Ngai Wong</author><pubDate>Fri, 29 Aug 2025 11:24:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21524v1</guid></item><item><title>Counterfactual Scenarios for Automated Planning</title><link>http://arxiv.org/abs/2508.21521v1</link><description>Counterfactual Explanations (CEs) are a powerful technique used to explainMachine Learning models by showing how the input to a model should be minimallychanged for the model to produce a different output. Similar proposals havebeen made in the context of Automated Planning, where CEs have beencharacterised in terms of minimal modifications to an existing plan that wouldresult in the satisfaction of a different goal. While such explanations mayhelp diagnose faults and reason about the characteristics of a plan, they failto capture higher-level properties of the problem being solved. To address thislimitation, we propose a novel explanation paradigm that is based oncounterfactual scenarios. In particular, given a planning problem $P$ and an\ltlf formula $\psi$ defining desired properties of a plan, counterfactualscenarios identify minimal modifications to $P$ such that it admits plans thatcomply with $\psi$. In this paper, we present two qualitative instantiations ofcounterfactual scenarios based on an explicit quantification over plans thatmust satisfy $\psi$. We then characterise the computational complexity ofgenerating such counterfactual scenarios when different types of changes areallowed on $P$. We show that producing counterfactual scenarios is often onlyas expensive as computing a plan for $P$, thus demonstrating the practicalviability of our proposal and ultimately providing a framework to constructpractical algorithms in this area.</description><author>Nicola Gigante, Francesco Leofante, Andrea Micheli</author><pubDate>Fri, 29 Aug 2025 11:16:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21521v1</guid></item><item><title>Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis</title><link>http://arxiv.org/abs/2508.21517v1</link><description>Background: Wisdom is a superordinate construct that embraces perspectivetaking, reflectiveness, prosocial orientation, reflective empathetic action,and intellectual humility. Unlike conventional models of reasoning that arerigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,requiring both graded evaluation and self-reflective humility. Current measuresdepend on self-reports and seldom reflect the humility and uncertainty inherentin wise reasoning. A computational framework that takes into account bothmultidimensionality and confidence has the potential to improve psychologicalscience and allow humane AI. Method: We present a fuzzy inference system with Znumbers, each of the decisions being expressed in terms of a wisdom score(restriction) and confidence score (certainty). As part of this study,participants (N = 100) were exposed to culturally neutral pictorial moraldilemma tasks to which they generated think-aloud linguistic responses, whichwere mapped into five theoretically based components of wisdom. The scores ofeach individual component were combined using a base of 21 rules, withmembership functions tuned via Gaussian kernel density estimation. Results: Ina proof of concept study, the system produced dual attribute wisdomrepresentations that correlated modestly but significantly with establishedscales while showing negligible relations with unrelated traits, supportingconvergent and divergent validity. Contribution: The contribution is toformalize wisdom as a multidimensional, uncertainty-conscious construct,operationalized in the form of Z-numbers. In addition to progressingmeasurement in psychology, it calculates how fuzzy Z numbers can provide AIsystems with interpretable, confidence-sensitive reasoning that affords a safe,middle ground between rigorous computation and human-like judgment.</description><author>Sweta Kaman, Ankita Sharma, Romi Banerjee</author><pubDate>Fri, 29 Aug 2025 11:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21517v1</guid></item><item><title>On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature</title><link>http://arxiv.org/abs/2508.21513v1</link><description>Graph Neural Networks (GNNs) have recently shown promise as solvers forBoolean Satisfiability Problems (SATs) by operating on graph representations oflogical formulas. However, their performance degrades sharply on harderinstances, raising the question of whether this reflects fundamentalarchitectural limitations. In this work, we provide a geometric explanationthrough the lens of graph Ricci Curvature (RC), which quantifies localconnectivity bottlenecks. We prove that bipartite graphs derived from randomk-SAT formulas are inherently negatively curved, and that this curvaturedecreases with instance difficulty. Building on this, we show that GNN-basedSAT solvers are affected by oversquashing, a phenomenon where long-rangedependencies become impossible to compress into fixed-length representations.We validate our claims empirically across different SAT benchmarks and confirmthat curvature is both a strong indicator of problem complexity and can be usedto predict performance. Finally, we connect our findings to design principlesof existing solvers and outline promising directions for future work.</description><author>Geri Skenderi</author><pubDate>Fri, 29 Aug 2025 10:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21513v1</guid></item><item><title>Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches</title><link>http://arxiv.org/abs/2508.21512v1</link><description>Large Language Models (LLMs) are increasingly employed in high-stakesdecision-making tasks, such as loan approvals. While their applications expandacross domains, LLMs struggle to process tabular data, ensuring fairness anddelivering reliable predictions. In this work, we assess the performance andfairness of LLMs on serialized loan approval datasets from three geographicallydistinct regions: Ghana, Germany, and the United States. Our evaluation focuseson the model's zero-shot and in-context learning (ICL) capabilities. Ourresults reveal that the choice of serialization (Serialization refers to theprocess of converting tabular data into text formats suitable for processing byLLMs.) format significantly affects both performance and fairness in LLMs, withcertain formats such as GReat and LIFT yielding higher F1 scores butexacerbating fairness disparities. Notably, while ICL improved modelperformance by 4.9-59.6% relative to zero-shot baselines, its effect onfairness varied considerably across datasets. Our work underscores theimportance of effective tabular data representation methods and fairness-awaremodels to improve the reliability of LLMs in financial decision-making.</description><author>Israel Abebe Azime, Deborah D. Kanubala, Tejumade Afonja, Mario Fritz, Isabel Valera, Dietrich Klakow, Philipp Slusallek</author><pubDate>Fri, 29 Aug 2025 10:51:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21512v1</guid></item><item><title>Testing Conviction: An Argumentative Framework for Measuring LLM Political Stability</title><link>http://arxiv.org/abs/2504.17052v2</link><description>Large Language Models (LLMs) increasingly shape political discourse, yetexhibit inconsistent responses when challenged. While prior researchcategorizes LLMs as left- or right-leaning based on single-prompt responses, acritical question remains: Do these classifications reflect stable ideologiesor superficial mimicry? Existing methods cannot distinguish between genuineideological alignment and performative text generation. To address this, wepropose a framework for evaluating ideological depth through (1) argumentativeconsistency and (2) uncertainty quantification. Testing 12 LLMs on 19 economicpolicies from the Political Compass Test, we classify responses as stable orperformative ideological positioning. Results show 95% of left-leaning modelsand 89% of right-leaning models demonstrate behavior consistent with ourclassifications across different experimental conditions. Furthermore, semanticentropy strongly validates our classifications (AUROC=0.78), revealinguncertainty's relationship to ideological consistency. Our findings demonstratethat ideological stability is topic-dependent and challenge the notion ofmonolithic LLM ideologies, and offer a robust way to distinguish genuinealignment from performative behavior.</description><author>Shariar Kabir, Kevin Esterling, Yue Dong</author><pubDate>Fri, 29 Aug 2025 10:47:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17052v2</guid></item><item><title>DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2501.08005v6</link><description>Out-of-distribution (OOD) detection holds significant importance across manyapplications. While semantic and domain-shift OOD problems are well-studied,this work focuses on covariate shifts - subtle variations in the datadistribution that can degrade machine learning performance. We hypothesize thatdetecting these subtle shifts can improve our understanding of in-distributionboundaries, ultimately improving OOD detection. In adversarial discriminatorstrained with Batch Normalization (BN), real and adversarial samples formdistinct domains with unique batch statistics - a property we exploit for OODdetection. We introduce DisCoPatch, an unsupervised Adversarial VariationalAutoencoder (VAE) framework that harnesses this mechanism. During inference,batches consist of patches from the same image, ensuring a consistent datadistribution that allows the model to rely on batch statistics. DisCoPatch usesthe VAE's suboptimal outputs (generated and reconstructed) as negative samplesto train the discriminator, thereby improving its ability to delineate theboundary between in-distribution samples and covariate shifts. By tighteningthis boundary, DisCoPatch achieves state-of-the-art results in public OODdetection benchmarks. The proposed model not only excels in detecting covariateshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all priormethods on public Near-OOD (95.0%) benchmarks. With a compact model size of25MB, it achieves high OOD detection performance at notably lower latency thanexisting methods, making it an efficient and practical solution for real-worldOOD detection applications. The code is publicly available.</description><author>Francisco Caetano, Christiaan Viviers, Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen</author><pubDate>Fri, 29 Aug 2025 10:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08005v6</guid></item><item><title>A Hybrid Artificial Intelligence Method for Estimating Flicker in Power Systems</title><link>http://arxiv.org/abs/2506.13611v3</link><description>This paper introduces a novel hybrid AI method combining H filtering and anadaptive linear neuron network for flicker component estimation in powerdistribution systems.The proposed method leverages the robustness of the Hfilter to extract the voltage envelope under uncertain and noisy conditionsfollowed by the use of ADALINE to accurately identify flicker frequenciesembedded in the envelope.This synergy enables efficient time domain estimationwith rapid convergence and noise resilience addressing key limitations ofexisting frequency domain approaches.Unlike conventional techniques this hybridAI model handles complex power disturbances without prior knowledge of noisecharacteristics or extensive training.To validate the method performance weconduct simulation studies based on IEC Standard 61000 4 15 supported bystatistical analysis Monte Carlo simulations and real world data.Resultsdemonstrate superior accuracy robustness and reduced computational loadcompared to Fast Fourier Transform and Discrete Wavelet Transform basedestimators.</description><author>Javad Enayati, Pedram Asef, Alexandre Benoit</author><pubDate>Fri, 29 Aug 2025 10:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.13611v3</guid></item><item><title>Toxicity Begets Toxicity: Unraveling Conversational Chains in Political Podcasts</title><link>http://arxiv.org/abs/2501.12640v2</link><description>Tackling toxic behavior in digital communication continues to be a pressingconcern for both academics and industry professionals. While significantresearch has explored toxicity on platforms like social networks and discussionboards, podcasts despite their rapid rise in popularity remain relativelyunderstudied in this context. This work seeks to fill that gap by curating adataset of political podcast transcripts and analyzing them with a focus onconversational structure. Specifically, we investigate how toxicity surfacesand intensifies through sequences of replies within these dialogues, sheddinglight on the organic patterns by which harmful language can escalate acrossconversational turns. Warning: Contains potentially abusive/toxic contents.</description><author>Naquee Rizwan, Nayandeep Deb, Sarthak Roy, Vishwajeet Singh Solanki, Kiran Garimella, Animesh Mukherjee</author><pubDate>Fri, 29 Aug 2025 10:39:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12640v2</guid></item><item><title>Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control</title><link>http://arxiv.org/abs/2508.21505v1</link><description>Reinforcement learning agents based on Transformer architectures haveachieved impressive performance on sequential decision-making tasks, but theirreliance on dense matrix operations makes them ill-suited forenergy-constrained, edge-oriented platforms. Spiking neural networks promiseultra-low-power, event-driven inference, yet no prior work has seamlesslymerged spiking dynamics with return-conditioned sequence modeling. We presentthe Spiking Decision Transformer (SNN-DT), which embeds LeakyIntegrate-and-Fire neurons into each self-attention block, trains end-to-endvia surrogate gradients, and incorporates biologically inspired three-factorplasticity, phase-shifted spike-based positional encodings, and a lightweightdendritic routing module. Our implementation matches or exceeds standardDecision Transformer performance on classic control benchmarks (CartPole-v1,MountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikesper decision, an energy proxy suggesting over four orders-of-magnitudereduction in per inference energy. By marrying sequence modeling withneuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-powercontrol on embedded and wearable devices.</description><author>Vishal Pandey, Debasmita Biswas</author><pubDate>Fri, 29 Aug 2025 10:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21505v1</guid></item><item><title>ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</title><link>http://arxiv.org/abs/2508.21496v1</link><description>Video multimodal large language models (Video-MLLMs) have achieved remarkableprogress in video understanding. However, they remain vulnerable tohallucination-producing content inconsistent with or unrelated to video inputs.Previous video hallucination benchmarks primarily focus on short-videos. Theyattribute hallucinations to factors such as strong language priors, missingframes, or vision-language biases introduced by the visual encoder. While thesecauses indeed account for most hallucinations in short videos, they stilloversimplify the cause of hallucinations. Sometimes, models generate incorrectoutputs but with correct frame-level semantics. We refer to this type ofhallucination as Semantic Aggregation Hallucination (SAH), which arises duringthe process of aggregating frame-level semantics into event-level semanticgroups. Given that SAH becomes particularly critical in long videos due toincreased semantic complexity across multiple events, it is essential toseparate and thoroughly investigate the causes of this type of hallucination.To address the above issues, we introduce ELV-Halluc, the first benchmarkdedicated to long-video hallucination, enabling a systematic investigation ofSAH. Our experiments confirm the existence of SAH and show that it increaseswith semantic complexity. Additionally, we find that models are more prone toSAH on rapidly changing semantics. Moreover, we discuss potential approaches tomitigate SAH. We demonstrate that positional encoding strategy contributes toalleviating SAH, and further adopt DPO strategy to enhance the model's abilityto distinguish semantics within and across events. To support this, we curate adataset of 8K adversarial data pairs and achieve improvements on bothELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.</description><author>Hao Lu, Jiahao Wang, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin, Lewei Lu</author><pubDate>Fri, 29 Aug 2025 10:25:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21496v1</guid></item><item><title>Failure Prediction Is a Better Performance Proxy for Early-Exit Networks Than Calibration</title><link>http://arxiv.org/abs/2508.21495v1</link><description>Early-exit models speed up inference by attaching internal classifiers tointermediate layers of the model and allowing computation to stop once aprediction satisfies an exit criterion. Most early-exit methods rely onconfidence-based exit strategies, which motivated some works to calibrateintermediate classifiers to improve the performance of the entire model. Inthis paper, we show that calibration measures can be misleading indicators ofthe performance of multi-exit models: a well-calibrated classifier may stillwaste computation, and common calibration methods do not preserve the sampleranking within a classifier. We demonstrate empirical cases where miscalibratednetworks outperform calibrated ones. As an alternative, we propose to usefailure prediction as a more useful proxy for early-exit model performance.Unlike calibration, failure prediction accounts for changes in the ranking ofsamples and shows a strong correlation with efficiency improvements, making ita more dependable basis for designing and evaluating early-exit models.</description><author>Piotr Kubaty, Filip Szatkowski, Metod Jazbec, Bartosz Wójcik</author><pubDate>Fri, 29 Aug 2025 10:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21495v1</guid></item><item><title>Discovering Heterogeneous Treatment Effects in Regression Discontinuity Designs</title><link>http://arxiv.org/abs/2106.11640v4</link><description>The paper proposes a causal supervised machine learning algorithm to uncovertreatment effect heterogeneity in sharp and fuzzy regression discontinuity (RD)designs. We develop a criterion for building an honest ``regressiondiscontinuity tree'', where each leaf contains the RD estimate of a treatmentconditional on the values of some pre-treatment covariates. It is a prioriunknown which covariates are relevant for capturing treatment effectheterogeneity, and it is the task of the algorithm to discover them, withoutinvalidating inference, while employing a nonparametric estimator with expectedMSE optimal bandwidth. We study the performance of the method through MonteCarlo simulations and apply it to uncover various sources of heterogeneity inthe impact of attending a better secondary school in Romania.</description><author>Ágoston Reguly</author><pubDate>Fri, 29 Aug 2025 10:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2106.11640v4</guid></item><item><title>Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning</title><link>http://arxiv.org/abs/2508.21488v1</link><description>Uncertainty quantification in reinforcement learning can greatly improveexploration and robustness. Approximate Bayesian approaches have recently beenpopularized to quantify uncertainty in model-free algorithms. However, so farthe focus has been on improving the accuracy of the posterior approximation,instead of studying the accuracy of the prior and likelihood assumptionsunderlying the posterior. In this work, we demonstrate that there is a coldposterior effect in Bayesian deep Q-learning, where contrary to theory,performance increases when reducing the temperature of the posterior. Toidentify and overcome likely causes, we challenge common assumptions made onthe likelihood and priors in Bayesian model-free algorithms. We empiricallystudy prior distributions and show through statistical tests that the commonGaussian likelihood assumption is frequently violated. We argue that developingmore suitable likelihoods and priors should be a key focus in future Bayesianreinforcement learning research and we offer simple, implementable solutionsfor better priors in deep Q-learning that lead to more performant Bayesianalgorithms.</description><author>Pascal R. van der Vaart, Neil Yorke-Smith, Matthijs T. J. Spaan</author><pubDate>Fri, 29 Aug 2025 10:12:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21488v1</guid></item><item><title>Data-driven Discovery of Digital Twins in Biomedical Research</title><link>http://arxiv.org/abs/2508.21484v1</link><description>Recent technological advances have expanded the availability ofhigh-throughput biological datasets, enabling the reliable design of digitaltwins of biomedical systems or patients. Such computational tools represent keyreaction networks driving perturbation or drug response and can guide drugdiscovery and personalized therapeutics. Yet, their development still relies onlaborious data integration by the human modeler, so that automated approachesare critically needed. The success of data-driven system discovery in Physics,rooted in clean datasets and well-defined governing laws, has fueled interestin applying similar techniques in Biology, which presents unique challenges.Here, we reviewed methodologies for automatically inferring digital twins frombiological time series, which mostly involve symbolic or sparse regression. Weevaluate algorithms according to eight biological and methodologicalchallenges, associated to noisy/incomplete data, multiple conditions, priorknowledge integration, latent variables, high dimensionality, unobservedvariable derivatives, candidate library design, and uncertainty quantification.Upon these criteria, sparse regression generally outperformed symbolicregression, particularly when using Bayesian frameworks. We further highlightthe emerging role of deep learning and large language models, which enableinnovative prior knowledge integration, though the reliability and consistencyof such approaches must be improved. While no single method addresses allchallenges, we argue that progress in learning digital twins will come fromhybrid and modular frameworks combining chemical reaction network-basedmechanistic grounding, Bayesian uncertainty quantification, and the generativeand knowledge integration capacities of deep learning. To support theirdevelopment, we further propose a benchmarking framework to evaluate methodsacross all challenges.</description><author>Clémence Métayer, Annabelle Ballesta, Julien Martinelli</author><pubDate>Fri, 29 Aug 2025 10:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21484v1</guid></item><item><title>HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble</title><link>http://arxiv.org/abs/2508.21482v1</link><description>Psychological biases, such as confirmation bias, make individualsparticularly vulnerable to believing and spreading fake news on social media,leading to significant consequences in domains such as public health andpolitics. Machine learning-based fact-checking systems have been widely studiedto mitigate this problem. Among them, ensemble methods are particularlyeffective in combining multiple classifiers to improve robustness. However,their performance heavily depends on the diversity of the constituentclassifiers-selecting genuinely diverse models remains a key challenge,especially when models tend to learn redundant patterns. In this work, wepropose a novel automatic classifier selection approach that prioritizesdiversity, also extended by performance. The method first computes pairwisediversity between classifiers and applies hierarchical clustering to organizethem into groups at different levels of granularity. A HierarchySelect thenexplores these hierarchical levels to select one pool of classifiers per level,each representing a distinct intra-pool diversity. The most diverse pool isidentified and selected for ensemble construction from these. The selectionprocess incorporates an evaluation metric reflecting each classifiers'sperformance to ensure the ensemble also generalises well. We conductexperiments with 40 heterogeneous classifiers across six datasets fromdifferent application domains and with varying numbers of classes. Our methodis compared against the Elbow heuristic and state-of-the-art baselines. Resultsshow that our approach achieves the highest accuracy on two of six datasets.The implementation details are available on the project's repository:https://github.com/SaraBCoutinho/HSFN .</description><author>Sara B. Coutinho, Rafael M. O. Cruz, Francimaria R. S. Nascimento, George D. C. Cavalcanti</author><pubDate>Fri, 29 Aug 2025 10:09:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21482v1</guid></item><item><title>Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward</title><link>http://arxiv.org/abs/2508.12800v3</link><description>Large language models (LLMs) exhibit remarkable problem-solving abilities,but struggle with complex tasks due to static internal knowledge.Retrieval-Augmented Generation (RAG) enhances access to external information,yet remains limited in multi-hop reasoning and strategic search due to rigidworkflows. Recent advancements in agentic deep research empower LLMs toautonomously reason, search, and synthesize information. However, currentapproaches relying on outcome-based reinforcement learning (RL) face criticalissues such as conflicting gradients and reward sparsity, limiting performancegains and training efficiency. To address these, we first propose AtomicThought, a novel LLM thinking paradigm that decomposes reasoning intofine-grained functional units. These units are supervised by Reasoning RewardModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grainedguidance. Building on this, we propose Atom-Searcher, a novel RL framework foragentic deep research that integrates Atomic Thought and ATR. Atom-Searcheruses a curriculum-inspired reward schedule, prioritizing process-level ATRearly and transitioning to outcome rewards, accelerating convergence oneffective reasoning paths. Experiments on seven benchmarks show consistentimprovements over the state-of-the-art. Key advantages include: (1)Atom-Searcher scales computation at test-time. (2) Atomic Thought providessupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)Atom-Searcher exhibits more interpretable, human-like reasoning patterns.</description><author>Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Yuan Wang, Quanxing Zha, Sunhao Dai, Changhua Meng</author><pubDate>Fri, 29 Aug 2025 10:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12800v3</guid></item><item><title>Endmember Extraction from Hyperspectral Images Using Self-Dictionary Approach with Linear Programming</title><link>http://arxiv.org/abs/2404.13098v3</link><description>Hyperspectral imaging technology has a wide range of applications, includingforest management, mineral resource exploration, and Earth surface monitoring.A key step in utilizing this technology is endmember extraction, which aims toidentify the spectral signatures of materials in observed scenes. Theoreticalstudies suggest that self-dictionary methods using linear programming (LP),known as Hottopixx methods, are effective in extracting endmembers. However,their practical application is hindered by high computational costs, as theyrequire solving LP problems whose size grows quadratically with the number ofpixels in the image. As a result, their actual effectiveness remains unclear.To address this issue, we propose an enhanced implementation of Hottopixxdesigned to reduce computational time and improve endmember extractionperformance. We demonstrate its effectiveness through experiments. The resultssuggest that our implementation enables the application of Hottopixx forendmember extraction from real hyperspectral images and allows us to achievereasonably high accuracy in estimating endmember signatures.</description><author>Tomohiko Mizutani</author><pubDate>Fri, 29 Aug 2025 10:01:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13098v3</guid></item><item><title>Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</title><link>http://arxiv.org/abs/2508.21476v1</link><description>Large Language Models (LLMs) have demonstrated remarkable creative writingcapabilities, yet their substantial computational demands hinder widespreaduse. Enhancing Small Language Models (SLMs) offers a promising alternative, butcurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, andReinforcement Learning from Human Feedback (RLHF) is costly. This paperexplores two distinct AI-driven reward strategies within a ReinforcementLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a7B-parameter SLM, specifically for generating Chinese greetings. The firststrategy employs a RM trained on high-quality preference data curated by anovel multi-agent rejection sampling framework designed for creative tasks. Thesecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whosereward function is optimized via an adversarial training scheme with areflection mechanism, to directly provide reward signals. Comprehensiveexperiments reveal that while both approaches significantly enhance creativeoutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yieldssuperior generation quality. Furthermore, it offers notable advantages intraining efficiency and reduced dependency on human-annotated data, presentinga more scalable and effective path towards creative SLMs. Our automatedevaluation methods also exhibit strong alignment with human judgments. Our codeand data are publicly available athttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.</description><author>Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin</author><pubDate>Fri, 29 Aug 2025 10:00:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21476v1</guid></item><item><title>DeepTrans: Deep Reasoning Translation via Reinforcement Learning</title><link>http://arxiv.org/abs/2504.10187v2</link><description>Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shownpromising performance in various downstream tasks. Free translation is animportant and interesting task in the multilingual world, which requires goingbeyond word-for-word translation. However, the task is still under-explored indeep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoningtranslation model that learns free translation via reinforcement learning (RL).Specifically, we carefully build a reward model with pre-defined scoringcriteria on both the translation results and the thought processes. The rewardmodel teaches DeepTrans how to think and free-translate the given sentencesduring RL. Besides, our RL training does not need any labeled translations,avoiding the human-intensive annotation or resource-intensive data synthesis.Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B asthe backbone, DeepTrans improves performance by 16.3% in literaturetranslation, and outperforms strong deep reasoning LLMs. Moreover, we summarizethe failures and interesting findings during our RL exploration. We hope thiswork could inspire other researchers in free translation.</description><author>Jiaan Wang, Fandong Meng, Jie Zhou</author><pubDate>Fri, 29 Aug 2025 09:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.10187v2</guid></item><item><title>MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents</title><link>http://arxiv.org/abs/2508.21475v1</link><description>Large multimodal language models (MLLMs) are increasingly deployed as webagents, yet many multimodal browsing benchmarks can be solved by shallow, fixedworkflows that lean on high-recall image search and nearby text-masking thegenuinely multimodal challenges of fine-grained visual reasoning, provenanceverification, and long-horizon tool use. We introduce MMSearch-Plus, abenchmark of 311 tasks that highly demand multimodal understanding whilepreserving the difficulty profile of strong text-only browsing suites. Eachitem is constructed to contain multiple weak, localized visual signals thatmust be extracted, propagated through iterative text-image search, andcross-validated under retrieval noise before answering. Our curation procedure,Spatial-Temporal Extrapolation, seeds questions whose answers requireextrapolating from spatial cues (micro-text, part-level appearance, layouts,signage) and temporal traces (broadcast overlays, seasonal context) toout-of-image facts such as events, dates, and venues. We provide amodel-agnostic agent framework with browsing tools and evaluate a range ofclosed and open MLLMs. The strongest agent (o3) attains 15.1% without searchand 36.0% accuracy with rollout under our framework, while a strong open-sourcemodel (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20rounds of search. Beyond answer accuracy, we assess bounding-box production andcropped-image search, and conduct an error analysis that surfaces failures insource verification, part-based reasoning, and long-horizon planning.</description><author>Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao, Ziru Liu, Haoli Bai, Rui Liu, Lingpeng Kong</author><pubDate>Fri, 29 Aug 2025 09:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21475v1</guid></item><item><title>Revealing Fine-Grained Values and Opinions in Large Language Models</title><link>http://arxiv.org/abs/2406.19238v3</link><description>Uncovering latent values and opinions embedded in large language models(LLMs) can help identify biases and mitigate potential harm. Recently, this hasbeen approached by prompting LLMs with survey questions and quantifying thestances in the outputs towards morally and politically charged statements.However, the stances generated by LLMs can vary greatly depending on how theyare prompted, and there are many ways to argue for or against a given position.In this work, we propose to address this by analysing a large and robustdataset of 156k LLM responses to the 62 propositions of the Political CompassTest (PCT) generated by 6 LLMs using 420 prompt variations. We performcoarse-grained analysis of their generated stances and fine-grained analysis ofthe plain text justifications for those stances. For fine-grained analysis, wepropose to identify tropes in the responses: semantically similar phrases thatare recurrent and consistent across different prompts, revealing naturalpatterns in the text that a given LLM is prone to produce. We find thatdemographic features added to prompts significantly affect outcomes on the PCT,reflecting bias, as well as disparities between the results of tests wheneliciting closed-form vs. open domain responses. Additionally, patterns in theplain text rationales via tropes show that similar justifications arerepeatedly generated across models and prompts even with disparate stances.</description><author>Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein</author><pubDate>Fri, 29 Aug 2025 09:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.19238v3</guid></item><item><title>Adversarial Patch Attack for Ship Detection via Localized Augmentation</title><link>http://arxiv.org/abs/2508.21472v1</link><description>Current ship detection techniques based on remote sensing imagery primarilyrely on the object detection capabilities of deep neural networks (DNNs).However, DNNs are vulnerable to adversarial patch attacks, which can lead tomisclassification by the detection model or complete evasion of the targets.Numerous studies have demonstrated that data transformation-based methods canimprove the transferability of adversarial examples. However, excessiveaugmentation of image backgrounds or irrelevant regions may introduceunnecessary interference, resulting in false detections of the object detectionmodel. These errors are not caused by the adversarial patches themselves butrather by the over-augmentation of background and non-target areas. This paperproposes a localized augmentation method that applies augmentation only to thetarget regions, avoiding any influence on non-target areas. By reducingbackground interference, this approach enables the loss function to focus moredirectly on the impact of the adversarial patch on the detection model, therebyimproving the attack success rate. Experiments conducted on the HRSC2016dataset demonstrate that the proposed method effectively increases the successrate of adversarial patch attacks and enhances their transferability.</description><author>Chun Liu, Panpan Ding, Zheng Zheng, Hailong Wang, Bingqian Zhu, Tao Xu, Zhigang Han, Jiayao Wang</author><pubDate>Fri, 29 Aug 2025 09:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21472v1</guid></item><item><title>Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration</title><link>http://arxiv.org/abs/2508.21468v1</link><description>Recent advances in Structure-based Drug Design (SBDD) have leveragedgenerative models for 3D molecular generation, predominantly evaluating modelperformance by binding affinity to target proteins. However, practical drugdiscovery necessitates high binding affinity along with synthetic feasibilityand selectivity, critical properties that were largely neglected in previousevaluations. To address this gap, we identify fundamental limitations ofconventional diffusion-based generative models in effectively guiding moleculegeneration toward these diverse pharmacological properties. We propose CByG, anovel framework extending Bayesian Flow Network into a gradient-basedconditional generative model that robustly integrates property-specificguidance. Additionally, we introduce a comprehensive evaluation schemeincorporating practical benchmarks for binding affinity, synthetic feasibility,and selectivity, overcoming the limitations of conventional evaluation methods.Extensive experiments demonstrate that our proposed CByG frameworksignificantly outperforms baseline models across multiple essential evaluationcriteria, highlighting its effectiveness and practicality for real-world drugdiscovery applications.</description><author>Seungyeon Choi, Hwanhee Kim, Chihyun Park, Dahyeon Lee, Seungyong Lee, Yoonju Kim, Hyoungjoon Park, Sein Kwon, Youngwan Jo, Sanghyun Park</author><pubDate>Fri, 29 Aug 2025 09:49:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21468v1</guid></item><item><title>Normalized Maximum Likelihood Code-Length on Riemannian Manifold Data Spaces</title><link>http://arxiv.org/abs/2508.21466v1</link><description>In recent years, with the large-scale expansion of graph data, there has beenan increased focus on Riemannian manifold data spaces other than Euclideanspace. In particular, the development of hyperbolic spaces has been remarkable,and they have high expressive power for graph data with hierarchicalstructures. Normalized Maximum Likelihood (NML) is employed in regretminimization and model selection. However, existing formulations of NML havebeen developed primarily in Euclidean spaces and are inherently dependent onthe choice of coordinate systems, making it non-trivial to extend NML toRiemannian manifolds. In this study, we define a new NML that reflects thegeometric structure of Riemannian manifolds, called the Riemannian manifold NML(Rm-NML). This Rm-NML is invariant under coordinate transformations andcoincides with the conventional NML under the natural parameterization inEuclidean space. We extend existing computational techniques for NML to thesetting of Riemannian manifolds. Furthermore, we derive a method to simplifythe computation of Rm-NML on Riemannian symmetric spaces, which encompass dataspaces of growing interest such as hyperbolic spaces. To illustrate thepractical application of our proposed method, we explicitly computed the Rm-NMLfor normal distributions on hyperbolic spaces.</description><author>Kota Fukuzawa, Atsushi Suzuki, Kenji Yamanishi</author><pubDate>Fri, 29 Aug 2025 09:48:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21466v1</guid></item><item><title>Multi-Method Ensemble for Out-of-Distribution Detection</title><link>http://arxiv.org/abs/2508.21463v1</link><description>Detecting out-of-distribution (OOD) samples is essential for neural networksoperating in open-world settings, particularly in safety-critical applications.Existing methods have improved OOD detection by leveraging two main techniques:feature truncation, which increases the separation between in-distribution (ID)and OOD samples, and scoring functions, which assign scores to distinguishbetween ID and OOD data. However, most approaches either focus on a singlefamily of techniques or evaluate their effectiveness on a specific type of OODdataset, overlooking the potential of combining multiple existing solutions.Motivated by this observation, we theoretically and empirically demonstratethat state-of-the-art feature truncation and scoring functions can beeffectively combined. Moreover, we show that aggregating multiple scoringfunctions enhances robustness against various types of OOD samples. Based onthese insights, we propose the Multi-Method Ensemble (MME) score, which unifiesstate-of-the-art OOD detectors into a single, more effective scoring function.Extensive experiments on both large-scale and small-scale benchmarks, coveringnear-OOD and far-OOD scenarios, show that MME significantly outperforms recentstate-of-the-art methods across all benchmarks. Notably, using the BiT model,our method achieves an average FPR95 of 27.57% on the challenging ImageNet-1Kbenchmark, improving performance by 6% over the best existing baseline.</description><author>Lucas Rakotoarivony</author><pubDate>Fri, 29 Aug 2025 09:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21463v1</guid></item><item><title>Convolutional Rectangular Attention Module</title><link>http://arxiv.org/abs/2503.10875v2</link><description>In this paper, we introduce a novel spatial attention module that can beeasily integrated to any convolutional network. This module guides the model topay attention to the most discriminative part of an image. This enables themodel to attain a better performance by an end-to-end training. In conventionalapproaches, a spatial attention map is typically generated in a position-wisemanner. Thus, it is often resulting in irregular boundaries and so can hampergeneralization to new samples. In our method, the attention region isconstrained to be rectangular. This rectangle is parametrized by only 5parameters, allowing for a better stability and generalization to new samples.In our experiments, our method systematically outperforms the position-wisecounterpart. So that, we provide a novel useful spatial attention mechanism forconvolutional models. Besides, our module also provides the interpretabilityregarding the \textit{where to look} question, as it helps to know the part ofthe input on which the model focuses to produce the prediction.</description><author>Hai-Vy Nguyen, Fabrice Gamboa, Sixin Zhang, Reda Chhaibi, Serge Gratton, Thierry Giaccone</author><pubDate>Fri, 29 Aug 2025 09:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.10875v2</guid></item><item><title>Diffusion-based Multi-modal Synergy Interest Network for Click-through Rate Prediction</title><link>http://arxiv.org/abs/2508.21460v1</link><description>In click-through rate prediction, click-through rate prediction is used tomodel users' interests. However, most of the existing CTR prediction methodsare mainly based on the ID modality. As a result, they are unable tocomprehensively model users' multi-modal preferences. Therefore, it isnecessary to introduce multi-modal CTR prediction. Although it seems appealingto directly apply the existing multi-modal fusion methods to click-through rateprediction models, these methods (1) fail to effectively disentanglecommonalities and specificities across different modalities; (2) fail toconsider the synergistic effects between modalities and model the complexinteractions between modalities. To address the above issues, this paper proposes the Diffusion-basedMulti-modal Synergy Interest Network (Diff-MSIN) framework for click-throughprediction. This framework introduces three innovative modules: the Multi-modalFeature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module,and the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRCModule extract synergistic, common, and special information among differentmodalities. They effectively enhances the representation of the modalities,improving the overall quality of the fusion. To encourage distinctiveness amongdifferent features, we design a Knowledge Decoupling method. Additionally, theFDAF Module focuses on capturing user preferences and reducing fusion noise. Tovalidate the effectiveness of the Diff-MSIN framework, we conducted extensiveexperiments using the Rec-Tmall and three Amazon datasets. The resultsdemonstrate that our approach yields a significant improvement of at least1.67% compared to the baseline, highlighting its potential for enhancingmulti-modal recommendation systems. Our code is available at the followinglink: https://github.com/Cxx-0/Diff-MSIN.</description><author>Xiaoxi Cui, Weihai Lu, Yu Tong, Yiheng Li, Zhejun Zhao</author><pubDate>Fri, 29 Aug 2025 09:46:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21460v1</guid></item><item><title>Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification</title><link>http://arxiv.org/abs/2508.21458v1</link><description>While foundation models (FMs) offer strong potential for AI-based dementiadiagnosis, their integration into federated learning (FL) systems remainsunderexplored. In this benchmarking study, we systematically evaluate theimpact of key design choices: classification head architecture, fine-tuningstrategy, and aggregation method, on the performance and efficiency offederated FM tuning using brain MRI data. Using a large multi-cohort dataset,we find that the architecture of the classification head substantiallyinfluences performance, freezing the FM encoder achieves comparable results tofull fine-tuning, and advanced aggregation methods outperform standardfederated averaging. Our results offer practical insights for deploying FMs indecentralized clinical settings and highlight trade-offs that should guidefuture method development.</description><author>Kaouther Mouheb, Marawan Elbatel, Janne Papma, Geert Jan Biessels, Jurgen Claassen, Huub Middelkoop, Barbara van Munster, Wiesje van der Flier, Inez Ramakers, Stefan Klein, Esther E. Bron</author><pubDate>Fri, 29 Aug 2025 09:43:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21458v1</guid></item><item><title>Morae: Proactively Pausing UI Agents for User Choices</title><link>http://arxiv.org/abs/2508.21456v1</link><description>User interface (UI) agents promise to make inaccessible or complex UIs easierto access for blind and low-vision (BLV) users. However, current UI agentstypically perform tasks end-to-end without involving users in critical choicesor making them aware of important contextual information, thus reducing useragency. For example, in our field study, a BLV participant asked to buy thecheapest available sparkling water, and the agent automatically chose one fromseveral equally priced options, without mentioning alternative products withdifferent flavors or better ratings. To address this problem, we introduceMorae, a UI agent that automatically identifies decision points during taskexecution and pauses so that users can make choices. Morae uses largemultimodal models to interpret user queries alongside UI code and screenshots,and prompt users for clarification when there is a choice to be made. In astudy over real-world web tasks with BLV participants, Morae helped userscomplete more tasks and select options that better matched their preferences,as compared to baseline agents, including OpenAI Operator. More broadly, thiswork exemplifies a mixed-initiative approach in which users benefit from theautomation of UI agents while being able to express their preferences.</description><author>Yi-Hao Peng, Dingzeyu Li, Jeffrey P. Bigham, Amy Pavel</author><pubDate>Fri, 29 Aug 2025 09:39:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21456v1</guid></item><item><title>Retrieval-Augmented Machine Translation with Unstructured Knowledge</title><link>http://arxiv.org/abs/2412.04342v2</link><description>Retrieval-augmented generation (RAG) introduces additional information toenhance large language models (LLMs). In machine translation (MT), previouswork typically retrieves in-context examples from paired MT corpora, ordomain-specific knowledge from knowledge graphs, to enhance MT models. However,a large amount of world knowledge is organized in unstructured documents, andmight not be fully paired across different languages. In this paper, we studyretrieval-augmented MT using unstructured documents. Specifically, we buildRAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmentedMT ability. RAGtrans contains 169K MT samples collected via GPT-4o and humantranslators. Besides, documents from various languages are also provided tosupply the knowledge to these samples. Based on RAGtrans, we further propose amulti-task training method to teach LLMs how to use information frommultilingual documents during their translation. The method uses existingmultilingual corpora to create auxiliary training objectives without additionallabeling requirements. Extensive experiments show that the method improves LLMsby 1.6-3.1 BLEU and 1.0-2.0 COMET scores in En-Zh, and 1.7-2.9 BLEU and 2.1-2.7COMET scores in En-De. We also conclude the critical difficulties that currentLLMs face with this task.</description><author>Jiaan Wang, Fandong Meng, Yingxue Zhang, Jie Zhou</author><pubDate>Fri, 29 Aug 2025 09:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04342v2</guid></item><item><title>From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics</title><link>http://arxiv.org/abs/2508.21452v1</link><description>Large language models (LLMs) are increasingly considered as tutoring aids inscience education. Yet their readiness for unsupervised use in undergraduateinstruction remains uncertain, as reliable teaching requires more than fluentrecall: it demands consistent, principle-grounded reasoning. Thermodynamics,with its compact laws and subtle distinctions between state and path functions,reversibility, and entropy, provides an ideal testbed for evaluating suchcapabilities. Here we present UTQA, a 50-item undergraduate thermodynamicsquestion answering benchmark, covering ideal-gas processes, reversibility, anddiagram interpretation. No leading 2025-era model exceeded our 95\% competencethreshold: the best LLMs achieved 82\% accuracy, with text-only itemsperforming better than image reasoning tasks, which often fell to chancelevels. Prompt phrasing and syntactic complexity showed modest to littlecorrelation with performance. The gap concentrates in finite-rate/irreversiblescenarios and in binding visual features to thermodynamic meaning, indicatingthat current LLMs are not yet suitable for unsupervised tutoring in thisdomain.</description><author>Anna Geißler, Luca-Sophie Bien, Friedrich Schöppler, Tobias Hertel</author><pubDate>Fri, 29 Aug 2025 09:36:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21452v1</guid></item><item><title>Computer-Aided Design of Personalized Occlusal Positioning Splints Using Multimodal 3D Data</title><link>http://arxiv.org/abs/2504.12868v2</link><description>Digital technology plays a crucial role in designing customized medicaldevices, such as occlusal splints, commonly used in the management of disordersof the stomatognathic system. This methodological proof-of-concept studypresents a computer-aided approach for designing and evaluating occlusalpositioning splints. The primary aim is to demonstrate the feasibility andgeometric accuracy of the proposed method at the preclinical stage. In thisapproach, a three-dimensional splint is generated using a transformation matrixto represent the therapeutic mandibular position. An experienced operatordefines this position using a virtual patient model reconstructed fromintraoral scans, CBCT, 3D facial scans, and a digitized plaster model. Weintroduce a novel method for generating splints that reproduces occlusalconditions in the therapeutic position and resolves surface conflicts throughvirtual embossing. The process for obtaining transformation matrices usingdental tools and intraoral devices commonly employed in dental and laboratoryworkflows is described, and the geometric accuracy of both designed and printedsplints is evaluated using profile and surface deviation analysis. The methodsupports reproducible, patient-specific splint fabrication and provides atransparent foundation for future validation studies, supporting multimodalimage registration and quantification of occlusal discrepancies in researchsettings.</description><author>Agnieszka Anna Tomaka, Leszek Luchowski, Michał Tarnawski, Dariusz Pojda</author><pubDate>Fri, 29 Aug 2025 09:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.12868v2</guid></item><item><title>BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions</title><link>http://arxiv.org/abs/2508.19762v2</link><description>Pollinator insects such as honeybees and bumblebees are vital to global foodproduction and ecosystem stability, yet their populations are declining due toanthropogenic and environmental stressors. Scalable, automated monitoring inagricultural environments remains an open challenge due to the difficulty ofdetecting small, fast-moving, and often camouflaged insects. To address this,we present BuzzSet v1.0, a large-scale dataset of high-resolution pollinatorimages collected under real field conditions. BuzzSet contains 7,856 manuallyverified images with more than 8,000 annotated instances across three classes:honeybees, bumblebees, and unidentified insects. Initial annotations wereproduced using a YOLOv12 model trained on external data and refined throughhuman verification with open-source tools. All images were preprocessed into256 x 256 tiles to improve the detection of small insects. We provide baselinesusing the RF-DETR transformer-based object detector. The model achieves strongclassification accuracy with F1 scores of 0.94 and 0.92 for honeybees andbumblebees, with minimal confusion between these categories. The unidentifiedclass remains more difficult due to label ambiguity and fewer samples, yetstill contributes insights for robustness evaluation. Overall detectionperformance (mAP at 0.50 of 0.559) illustrates the challenging nature of thedataset and its potential to drive advances in small object detection underrealistic ecological conditions. Future work focuses on expanding the datasetto version 2.0 with additional annotations and evaluating further detectionstrategies. BuzzSet establishes a benchmark for ecological computer vision,with the primary challenge being reliable detection of insects frequentlycamouflaged within natural vegetation, highlighting an open problem for futureresearch.</description><author>Ahmed Emam, Mohamed Elbassiouny, Julius Miller, Patrick Donworth, Sabine Seidel, Ribana Roscher</author><pubDate>Fri, 29 Aug 2025 09:32:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19762v2</guid></item><item><title>One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist</title><link>http://arxiv.org/abs/2508.21451v1</link><description>Image captioning is fundamental for applications like video instructionsystems and exploration robots, yet deploying such models on local devices ischallenging due to the high computational demands of multimodal large languagemodels (MLLMs). To address this, we first explore lightweight captioning byimplementing a specialist based on a 125M-parameter language model, 56 timessmaller than LLaMA-7B, and evaluating its performance on both single-sentenceand detailed captioning tasks. Surprisingly, we find that our model can achieveperformance comparable to large multimodal generalists, suggesting itspotential to serve as a strong visual specialist for on-device applications.While promising, our model also exhibits a limitation: like other MLLMs, itsuffers from visual blindness, occasionally resulting in semantic captioningerrors. We carry out toy experiments and investigate the underlying causes,where we observe that the problems arise from ineffective attention mechanismsand limited visual representations. To alleviate them, we develop a novelcaptioning framework, Sharp-Eyed Refinement, which enhances caption qualitythrough improved visual grounding. At its core, our DeepLens extracts detailedvisual representations by concentrating on informative regions identifiedduring the initial glance. Our experiments confirm both the advantages of ourspecialist over prior small captioning models and large generalists and theeffectiveness of our framework.</description><author>Junha Song, Yongsik Jo, So Yeon Min, Quanting Xie, Taehwan Kim, Yonatan Bisk, Jaegul Choo</author><pubDate>Fri, 29 Aug 2025 09:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21451v1</guid></item><item><title>Learning Lifted Action Models From Traces of Incomplete Actions and States</title><link>http://arxiv.org/abs/2508.21449v1</link><description>Consider the problem of learning a lifted STRIPS model of the sliding-tilepuzzle from random state-action traces where the states represent the locationof the tiles only, and the actions are the labels up, down, left, and right,with no arguments. Two challenges are involved in this problem. First, thestates are not full STRIPS states, as some predicates are missing, like theatoms representing the position of the ``blank''. Second, the actions are notfull STRIPS either, as they do not reveal all the objects involved in theactions effects and preconditions. Previous approaches have addressed differentversions of this model learning problem, but most assume that actions in thetraces are full STRIPS actions or that the domain predicates are allobservable. The new setting considered in this work is more ``realistic'', asthe atoms observed convey the state of the world but not full STRIPS states,and the actions reveal the arguments needed for selecting the action but notthe ones needed for modeling it in STRIPS. For formulating and addressing thelearning problem, we introduce a variant of STRIPS, which we call STRIPS+,where certain STRIPS action arguments can be left implicit in preconditionswhich can also involve a limited form of existential quantification. Thelearning problem becomes the problem of learning STRIPS+ models from STRIPS+state-action traces. For this, the proposed learning algorithm, called SYNTH,constructs a stratified sequence (conjunction) of precondition expressions or``queries'' for each action, that denote unique objects in the state and groundthe implicit action arguments in STRIPS+. The correctness and completeness ofSYNTH is established, and its scalability is tested on state-action tracesobtained from STRIPS+ models derived from existing STRIPS domains.</description><author>Niklas Jansen, Jonas Gösgens, Hector Geffner</author><pubDate>Fri, 29 Aug 2025 09:27:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21449v1</guid></item><item><title>Beyond the Surface: Probing the Ideological Depth of Large Language Models</title><link>http://arxiv.org/abs/2508.21448v1</link><description>Large Language Models (LLMs) have demonstrated pronounced ideologicalleanings, yet the stability and depth of these positions remain poorlyunderstood. Surface-level responses can often be manipulated through simpleprompt engineering, calling into question whether they reflect a coherentunderlying ideology. This paper investigates the concept of "ideological depth"in LLMs, defined as the robustness and complexity of their internal politicalrepresentations. We employ a dual approach: first, we measure the"steerability" of two well-known open-source LLMs using instruction promptingand activation steering. We find that while some models can easily switchbetween liberal and conservative viewpoints, others exhibit resistance or anincreased rate of refusal, suggesting a more entrenched ideological structure.Second, we probe the internal mechanisms of these models using SparseAutoencoders (SAEs). Preliminary analysis reveals that models with lowersteerability possess more distinct and abstract ideological features. Ourevaluations reveal that one model can contain 7.3x more political features thananother model of similar size. This allows targeted ablation of a corepolitical feature in an ideologically "deep" model, leading to consistent,logical shifts in its reasoning across related topics, whereas the sameintervention in a "shallow" model results in an increase in refusal outputs.Our findings suggest that ideological depth is a quantifiable property of LLMsand that steerability serves as a valuable window into their latent politicalarchitecture.</description><author>Shariar Kabir, Kevin Esterling, Yue Dong</author><pubDate>Fri, 29 Aug 2025 09:27:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21448v1</guid></item><item><title>Towards Understanding Camera Motions in Any Video</title><link>http://arxiv.org/abs/2504.15376v2</link><description>We introduce CameraBench, a large-scale dataset and benchmark designed toassess and improve camera motion understanding. CameraBench consists of ~3,000diverse internet videos, annotated by experts through a rigorous multi-stagequality control process. One of our contributions is a taxonomy of cameramotion primitives, designed in collaboration with cinematographers. We find,for example, that some motions like "follow" (or tracking) requireunderstanding scene content like moving subjects. We conduct a large-scalehuman study to quantify human annotation performance, revealing that domainexpertise and tutorial-based training can significantly enhance accuracy. Forexample, a novice may confuse zoom-in (a change of intrinsics) with translatingforward (a change of extrinsics), but can be trained to differentiate the two.Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-LanguageModels (VLMs), finding that SfM models struggle to capture semantic primitivesthat depend on scene content, while VLMs struggle to capture geometricprimitives that require precise estimation of trajectories. We then fine-tune agenerative VLM on CameraBench to achieve the best of both worlds and showcaseits applications, including motion-augmented captioning, video questionanswering, and video-text retrieval. We hope our taxonomy, benchmark, andtutorials will drive future efforts towards the ultimate goal of understandingcamera motions in any video.</description><author>Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, Rushikesh Zawar, Xue Bai, Yilun Du, Chuang Gan, Deva Ramanan</author><pubDate>Fri, 29 Aug 2025 09:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.15376v2</guid></item><item><title>TrueGL: A Truthful, Reliable, and Unified Engine for Grounded Learning in Full-Stack Search</title><link>http://arxiv.org/abs/2506.12072v2</link><description>In the age of open and free information, a concerning trend of reliance on AIis emerging. However, existing AI tools struggle to evaluate the credibility ofinformation and to justify their assessments. Hence, there is a growing needfor systems that can help users evaluate the trustworthiness of onlineinformation. Although major search engines incorporate AI features, they oftenlack clear reliability indicators. We present TrueGL, a model that makestrustworthy search results more accessible. The model is a fine-tuned versionof IBM's Granite-1B, trained on the custom dataset and integrated into a searchengine with a reliability scoring system. We evaluate the system using promptengineering and assigning each statement a continuous reliability score from0.1 to 1, then instructing the model to return a textual explanation alongsidethe score. Each model's predicted scores are measured against real scores usingstandard evaluation metrics. TrueGL consistently outperforms other small-scaleLLMs and rule-based approaches across all experiments on key evaluationmetrics, including MAE, RMSE, and R2. The model's high accuracy, broad contentcoverage, and ease of use make trustworthy information more accessible and helpreduce the spread of false or misleading content online. Our code is publiclyavailable at https://github.com/AlgazinovAleksandr/TrueGL, and our model ispublicly released at https://huggingface.co/JoydeepC/trueGL.</description><author>Joydeep Chandra, Aleksandr Algazinov, Satyam Kumar Navneet, Rim El Filali, Matt Laing, Andrew Hanna</author><pubDate>Fri, 29 Aug 2025 09:25:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.12072v2</guid></item><item><title>Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content</title><link>http://arxiv.org/abs/2508.21444v1</link><description>3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a keyrequirement for immersive applications. However, the extension of 3DGS todynamic scenes remains limitations on the substantial data volume of denseGaussians and the prolonged training time required for each frame. This paperpresents \M, a scalable Gaussian Splatting framework designed for efficienttraining in streaming tasks. Specifically, Gaussian spheres are hierarchicallyorganized by scale within an anchor-based structure. Coarser-level Gaussiansrepresent the low-resolution structure of the scene, while finer-levelGaussians, responsible for detailed high-fidelity rendering, are selectivelyactivated by the coarser-level Gaussians. To further reduce computationaloverhead, we introduce a hybrid deformation and spawning strategy that modelsmotion of inter-frame through Gaussian deformation and triggers Gaussianspawning to characterize wide-range motion. Additionally, a bidirectionaladaptive masking mechanism enhances training efficiency by removing staticregions and prioritizing informative viewpoints. Extensive experimentsdemonstrate that \M~ achieves superior visual quality while significantlyreducing training time compared to state-of-the-art methods.</description><author>Jiayu Yang, Weijian Su, Songqian Zhang, Yuqi Han, Jinli Suo, Qiang Zhang</author><pubDate>Fri, 29 Aug 2025 09:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21444v1</guid></item><item><title>Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning</title><link>http://arxiv.org/abs/2508.21443v1</link><description>Reinforcement learning (RL) algorithms typically optimize the expectedcumulative reward, i.e., the expected value of the sum of scalar rewards anagent receives over the course of a trajectory. The expected value averages theperformance over an infinite number of trajectories. However, when deployingthe agent in the real world, this ensemble average may be uninformative for theperformance of individual trajectories. Thus, in many applications, optimizingthe long-term performance of individual trajectories might be more desirable.In this work, we propose a novel RL algorithm that combines the standardensemble average with the time-average growth rate, a measure for the long-termperformance of individual trajectories. We first define the Bellman operatorfor the time-average growth rate. We then show that, under multiplicativereward dynamics, the geometric mean aligns with the time-average growth rate.To address more general and unknown reward dynamics, we propose a modifiedgeometric mean with $N$-sliding window that captures the path-dependency as anestimator for the time-average growth rate. This estimator is embedded as aregularizer into the objective, forming a practical algorithm and enabling thepolicy to benefit from ensemble average and time-average simultaneously. Weevaluate our algorithm in challenging simulations, where it outperformsconventional RL methods.</description><author>Xinyi Sheng, Dominik Baumann</author><pubDate>Fri, 29 Aug 2025 09:12:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21443v1</guid></item><item><title>A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions</title><link>http://arxiv.org/abs/2508.21441v1</link><description>Forgetting as a knowledge management operation deliberately ignores parts ofthe knowledge and beliefs of an agent, for various reasons. Forgetting has manyfacets, one may want to forget parts of the syntax, a proposition, or aconditional. In the literature, two main operators suitable for performingforgetting have been proposed and investigated in depth: First, variableelimination is a syntactical method that blends out certain atomic variables tofocus on the rest of the language. It has been mainly used in the area of logicprogramming and answer set programming. Second, contraction in AGM beliefrevision theory effectively removes propositions from belief sets under logicaldeduction. Both operations rely mainly on classical logics. In this article, wetake an epistemic perspective and study forgetting operations in epistemicstates with richer semantic structures, but with clear links to propositionallogic. This allows us to investigate what forgetting in the epistemicbackground means, thereby lifting well-known and novel forgetting operations tothe epistemic level. We present five general types of epistemic forgetting andinstantiate them with seven concrete forgetting operations for Spohn's rankingfunctions. We take inspiration from postulates of forgetting both from logicprogramming and AGM theory to propose a rich landscape of axioms for evaluatingforgetting operations. Finally, we evaluate all concrete forgetting operationsaccording to all postulates, leading to a novel comprehensive overviewhighlighting differences and commonalities among the forgetting operators.</description><author>Christoph Beierle, Alexander Hahn, Diana Howey, Gabriele Kern-Isberner, Kai Sauerwald</author><pubDate>Fri, 29 Aug 2025 09:08:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21441v1</guid></item><item><title>SpecPipe: Accelerating Pipeline Parallelism-based LLM Inference with Speculative Decoding</title><link>http://arxiv.org/abs/2504.04104v2</link><description>The demand for large language model inference is rapidly increasing. Pipelineparallelism offers a cost-effective deployment strategy for distributedinference but suffers from high service latency. While incorporatingspeculative decoding to pipeline parallelism improves performance, it stillfaces challenges of low hardware utilization and narrow speculative window.Inspired by branch prediction in instruction pipelining, we introduce SpecPipe,which fills the pipeline with speculative tokens of a request step-by-step. Bymaximizing the hardware utilization, SpecPipe decodes one token per pipelinestep ideally. Specifically, SpecPipe comprises a dynamic speculative token treeand a pipelined inference framework. The tree dynamically accepts tokens from aspeculative token source and outputs the tokens to the inference pipeline.Since the speculative window relaxed in our framework, a high-accuracy draftmodel is integrated without fine-tuning. The pipeline inference frameworkfollows node-wise computation, pruning propagation, and inter-nodecommunication stages. We implement SpecPipe and a variant SpecPipe-DB withdynamic batching for single- and multi-request inference, respectively. On an8-stage pipeline, SpecPipe improves time between tokens on diversesingle-request workloads by $4.19\times$-$5.53\times$ over standard pipelineparallelism and by $2.08\times$-$2.38\times$ over prior tree-based speculativedecoding methods. For multi-request workloads, SpecPipe-DB achieves$1.64\times$-$2.08\times$ higher throughput and $1.61\times$-$2.06\times$ lowertime between tokens than vLLM.</description><author>Haofei Yin, Mengbai Xiao, Tinghong Li, Xiao Zhang, Dongxiao Yu, Guanghui Zhang</author><pubDate>Fri, 29 Aug 2025 09:07:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.04104v2</guid></item><item><title>Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing</title><link>http://arxiv.org/abs/2508.21438v1</link><description>The development of continuous biomanufacturing processes requires robust andearly anomaly detection, since even minor deviations can compromise yield andstability, leading to disruptions in scheduling, reduced weekly production, anddiminished economic performance. These processes are inherently complex andexhibit non-linear dynamics with intricate relationships between processvariables, thus making advanced methods for anomaly detection essential forefficient operation. In this work, we present a novel framework forunsupervised anomaly detection in continuous biomanufacturing based on anensemble of generative adversarial networks (GANs). We first establish abenchmark dataset simulating both normal and anomalous operation regimes in acontinuous process for the production of a small molecule. We then demonstratethe effectiveness of our GAN-based framework in detecting anomalies caused bysudden feedstock variability. Finally, we evaluate the impact of using a hybridquantum/classical GAN approach with both a simulated quantum circuit and a realphotonic quantum processor on anomaly detection performance. We find that thehybrid approach yields improved anomaly detection rates. Our work shows thepotential of hybrid quantum/classical approaches for solving real-worldproblems in complex continuous biomanufacturing processes.</description><author>Rajiv Kailasanathan, William R. Clements, Mohammad Reza Boskabadi, Shawn M. Gibford, Emmanouil Papadakis, Christopher J. Savoie, Seyed Soheil Mansouri</author><pubDate>Fri, 29 Aug 2025 09:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21438v1</guid></item><item><title>Trees as Gaussians: Large-Scale Individual Tree Mapping</title><link>http://arxiv.org/abs/2508.21437v1</link><description>Trees are key components of the terrestrial biosphere, playing vital roles inecosystem function, climate regulation, and the bioeconomy. However,large-scale monitoring of individual trees remains limited by inadequatemodelling. Available global products have focused on binary tree cover orcanopy height, which do not explicitely identify trees at individual level. Inthis study, we present a deep learning approach for detecting large individualtrees in 3-m resolution PlanetScope imagery at a global scale. We simulate treecrowns with Gaussian kernels of scalable size, allowing the extraction of crowncenters and the generation of binary tree cover maps. Training is based onbillions of points automatically extracted from airborne lidar data, enablingthe model to successfully identify trees both inside and outside forests. Wecompare against existing tree cover maps and airborne lidar withstate-of-the-art performance (fractional cover R$^2 = 0.81$ against aeriallidar), report balanced detection metrics across biomes, and demonstrate howdetection can be further improved through fine-tuning with manual labels. Ourmethod offers a scalable framework for global, high-resolution tree monitoring,and is adaptable to future satellite missions offering improved imagery.</description><author>Dimitri Gominski, Martin Brandt, Xiaoye Tong, Siyu Liu, Maurice Mugabowindekwe, Sizhuo Li, Florian Reiner, Andrew Davies, Rasmus Fensholt</author><pubDate>Fri, 29 Aug 2025 09:04:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21437v1</guid></item><item><title>Discovering Semantic Subdimensions through Disentangled Conceptual Representations</title><link>http://arxiv.org/abs/2508.21436v1</link><description>Understanding the core dimensions of conceptual semantics is fundamental touncovering how meaning is organized in language and the brain. Existingapproaches often rely on predefined semantic dimensions that offer only broadrepresentations, overlooking finer conceptual distinctions. This paper proposesa novel framework to investigate the subdimensions underlying coarse-grainedsemantic dimensions. Specifically, we introduce a Disentangled ContinuousSemantic Representation Model (DCSRM) that decomposes word embeddings fromlarge language models into multiple sub-embeddings, each encoding specificsemantic information. Using these sub-embeddings, we identify a set ofinterpretable semantic subdimensions. To assess their neural plausibility, weapply voxel-wise encoding models to map these subdimensions to brainactivation. Our work offers more fine-grained interpretable semanticsubdimensions of conceptual meaning. Further analyses reveal that semanticdimensions are structured according to distinct principles, with polarityemerging as a key factor driving their decomposition into subdimensions. Theneural correlates of the identified subdimensions support their cognitive andneuroscientific plausibility.</description><author>Yunhao Zhang, Shaonan Wang, Nan Lin, Xinyi Dong, Chong Li, Chengqing Zong</author><pubDate>Fri, 29 Aug 2025 09:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21436v1</guid></item><item><title>MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation</title><link>http://arxiv.org/abs/2508.21435v1</link><description>Synthetic medical data offers a scalable solution for training robust models,but significant domain gaps limit its generalizability to real-world clinicalsettings. This paper addresses the challenge of cross-domain translationbetween synthetic and real X-ray images of the head, focusing on bridgingdiscrepancies in attenuation behavior, noise characteristics, and soft tissuerepresentation. We propose MedShift, a unified class-conditional generativemodel based on Flow Matching and Schrodinger Bridges, which enableshigh-fidelity, unpaired image translation across multiple domains. Unlike priorapproaches that require domain-specific training or rely on paired data,MedShift learns a shared domain-agnostic latent space and supports seamlesstranslation between any pair of domains seen during training. We introduceX-DigiSkull, a new dataset comprising aligned synthetic and real skull X-raysunder varying radiation doses, to benchmark domain translation models.Experimental results demonstrate that, despite its smaller model size comparedto diffusion-based approaches, MedShift offers strong performance and remainsflexible at inference time, as it can be tuned to prioritize either perceptualfidelity or structural consistency, making it a scalable and generalizablesolution for domain adaptation in medical imaging. The code and dataset areavailable at https://caetas.github.io/medshift.html</description><author>Francisco Caetano, Christiaan Viviers, Peter H. H. de With, Fons van der Sommen</author><pubDate>Fri, 29 Aug 2025 09:04:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21435v1</guid></item><item><title>The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management</title><link>http://arxiv.org/abs/2508.21433v1</link><description>Large Language Model (LLM)-based agents solve complex tasks through iterativereasoning, exploration, and tool-use, a process that can result in long,expensive context histories. While state-of-the-art Software Engineering ( SE)agents like OpenHands or Cursor use LLM-based summarization to tackle thisissue, it is unclear whether the increased complexity offers tangibleperformance benefits compared to simply omitting older observations. We presenta systematic comparison of these strategies within SWE-agent on SWE-benchVerified across five diverse model configurations. We find that a simpleobservation-masking strategy halves cost relative to a raw agent whilematching, and sometimes slightly exceeding, the solve rate of LLMsummarization. For example, with Qwen3-Coder 480B, masking improves solve ratefrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarizationat a lower cost. These results suggest that, at least within SWE-agent onSWE-bench Verified, the most effective and efficient context management can bethe simplest. We release code and data for reproducibility</description><author>Tobias Lindenbauer, Igor Slinko, Ludwig Felder, Egor Bogomolov, Yaroslav Zharov</author><pubDate>Fri, 29 Aug 2025 09:02:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21433v1</guid></item><item><title>Explicit Residual-Based Scalable Image Coding for Humans and Machines</title><link>http://arxiv.org/abs/2506.19297v2</link><description>Scalable image compression is a technique that progressively reconstructsmultiple versions of an image for different requirements. In recent years,images have increasingly been consumed not only by humans but also by imagerecognition models. This shift has drawn growing attention to scalable imagecompression methods that serve both machine and human vision (ICMH). Manyexisting models employ neural network-based codecs, known as learned imagecompression, and have made significant strides in this field by carefullydesigning the loss functions. In some cases, however, models are overly relianton their learning capacity, and their architectural design is not sufficientlyconsidered. In this paper, we enhance the coding efficiency andinterpretability of ICMH framework by integrating an explicit residualcompression mechanism, which is commonly employed in resolution scalable codingmethods such as JPEG2000. Specifically, we propose two complementary methods:Feature Residual-based Scalable Coding (FR-ICMH) and Pixel Residual-basedScalable Coding (PR-ICMH). These proposed methods are applicable to variousmachine vision tasks. Moreover, they provide flexibility to choose betweenencoder complexity and compression performance, making it adaptable to diverseapplication requirements. Experimental results demonstrate the effectiveness ofour proposed methods, with PR-ICMH achieving up to 29.57% BD-rate savings overthe previous work.</description><author>Yui Tatsumi, Ziyue Zeng, Hiroshi Watanabe</author><pubDate>Fri, 29 Aug 2025 09:02:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19297v2</guid></item><item><title>Unified Path Planner with Adaptive Safety and Optimality</title><link>http://arxiv.org/abs/2505.23197v2</link><description>Path planning for autonomous robots presents a fundamental trade-off betweenoptimality and safety. While conventional algorithms typically prioritize oneof these objectives, we introduce the Unified Path Planner (UPP), a unifiedframework that simultaneously addresses both. UPP is a graph-search-basedalgorithm that employs a modified heuristic function incorporating a dynamicsafety cost, enabling an adaptive balance between path length and obstacleclearance. We establish theoretical sub-optimality bounds for the planner anddemonstrate that its safety-to-optimality ratio can be tuned via adjustableparameters, with a trade-off in computational complexity. Extensive simulationsshow that UPP achieves a high success rate, generating near-optimal paths withonly a negligible increase in cost over traditional A*, while ensuring safetymargins that closely approach those of the classical Voronoi planner. Finally,the practical efficacy of UPP is validated through a hardware implementation ona TurtleBot, confirming its ability to navigate cluttered environments bygenerating safe, sub-optimal paths.</description><author>Jatin Kumar Arora, Soutrik Bandyopadhyay, Shubhendu Bhasin</author><pubDate>Fri, 29 Aug 2025 08:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23197v2</guid></item><item><title>Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models</title><link>http://arxiv.org/abs/2508.21430v1</link><description>Multimodal large language models (MLLMs) hold significant potential inmedical applications, including disease diagnosis and clinical decision-making.However, these tasks require highly accurate, context-sensitive, andprofessionally aligned responses, making reliable reward models and judgescritical. Despite their importance, medical reward models (MRMs) and judgesremain underexplored, with no dedicated benchmarks addressing clinicalrequirements. Existing benchmarks focus on general MLLM capabilities orevaluate models as solvers, neglecting essential evaluation dimensions likediagnostic accuracy and clinical relevance. To address this, we introduceMed-RewardBench, the first benchmark specifically designed to evaluate MRMs andjudges in medical scenarios. Med-RewardBench features a multimodal datasetspanning 13 organ systems and 8 clinical departments, with 1,026expert-annotated cases. A rigorous three-step process ensures high-qualityevaluation data across six clinically critical dimensions. We evaluate 32state-of-the-art MLLMs, including open-source, proprietary, andmedical-specific models, revealing substantial challenges in aligning outputswith expert judgment. Additionally, we develop baseline models that demonstratesubstantial performance improvements through fine-tuning.</description><author>Meidan Ding, Jipeng Zhang, Wenxuan Wang, Cheng-Yi Li, Wei-Chieh Fang, Hsin-Yu Wu, Haiqin Zhong, Wenting Chen, Linlin Shen</author><pubDate>Fri, 29 Aug 2025 08:58:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21430v1</guid></item><item><title>ALow-Cost Real-Time Framework for Industrial Action Recognition Using Foundation Models</title><link>http://arxiv.org/abs/2403.08420v2</link><description>Action recognition (AR) in industrial environments -- particularly foridentifying actions and operational gestures -- faces persistent challenges dueto high deployment costs, poor cross-scenario generalization, and limitedreal-time performance. To address these issues, we propose a low-cost real-timeframework for industrial action recognition using foundation models, denoted asLRIAR, to enhance recognition accuracy and transferability while minimizinghuman annotation and computational overhead. The proposed framework constructsan automatically labeled dataset by coupling Grounding DINO with the pretrainedBLIP-2 image encoder, enabling efficient and scalable action labeling.Leveraging the constructed dataset, we train YOLOv5 for real-time actiondetection, and a Vision Transformer (ViT) classifier is deceloped viaLoRA-based fine-tuning for action classification. Extensive experimentsconducted in real-world industrial settings validate the effectiveness ofLRIAR, demonstrating consistent improvements over state-of-the-art methods inrecognition accuracy, scenario generalization, and deployment efficiency.</description><author>Zhicheng Wang, Wensheng Liang, Ruiyan Zhuang, Shuai Li, Jianwei Tan, Xiaoguang Ma</author><pubDate>Fri, 29 Aug 2025 08:56:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.08420v2</guid></item><item><title>THEME: Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics</title><link>http://arxiv.org/abs/2508.16936v2</link><description>Thematic investing, which aims to construct portfolios aligned withstructural trends, remains a challenging endeavor due to overlapping sectorboundaries and evolving market dynamics. A promising direction is to buildsemantic representations of investment themes from textual data. However,despite their power, general-purpose LLM embedding models are not well-suitedto capture the nuanced characteristics of financial assets, since the semanticrepresentation of investment assets may differ fundamentally from that ofgeneral financial text. To address this, we introduce THEME, a framework thatfine-tunes embeddings using hierarchical contrastive learning. THEME alignsthemes and their constituent stocks using their hierarchical relationship, andsubsequently refines these embeddings by incorporating stock returns. Thisprocess yields representations effective for retrieving thematically alignedassets with strong return potential. Empirical results demonstrate that THEMEexcels in two key areas. For thematic asset retrieval, it significantlyoutperforms leading large language models. Furthermore, its constructedportfolios demonstrate compelling performance. By jointly modeling thematicrelationships from text and market dynamics from returns, THEME generates stockembeddings specifically tailored for a wide range of practical investmentapplications.</description><author>Hoyoung Lee, Wonbin Ahn, Suhwan Park, Jaehoon Lee, Minjae Kim, Sungdong Yoo, Taeyoon Lim, Woohyung Lim, Yongjae Lee</author><pubDate>Fri, 29 Aug 2025 08:56:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16936v2</guid></item><item><title>TorchCP: A Python Library for Conformal Prediction</title><link>http://arxiv.org/abs/2402.12683v4</link><description>Conformal prediction (CP) is a powerful statistical framework that generatesprediction intervals or sets with guaranteed coverage probability. While CPalgorithms have evolved beyond traditional classifiers and regressors tosophisticated deep learning models like deep neural networks (DNNs), graphneural networks (GNNs), and large language models (LLMs), existing CP librariesoften lack the model support and scalability for large-scale DL scenarios. Thispaper introduces TorchCP, a PyTorch-native library designed to integratestate-of-the-art CP algorithms into deep learning techniques, includingDNN-based classifier/regressor, GNN, and LLM. Released under the LGPL-3.0license, TorchCP comprises about 16k lines of code, validated with 100% unittest coverage and detailed documentation. Notably, TorchCP enables CP-specifictraining algorithms, online prediction, and GPU-accelerated batch processing,achieving up to 90% reduction in inference time on large datasets. With itslow-coupling design, comprehensive suite of advanced methods, and full GPUscalability, TorchCP empowers researchers and practitioners to enhanceuncertainty quantification across cutting-edge applications.</description><author>Jianguo Huang, Jianqing Song, Xuanning Zhou, Bingyi Jing, Hongxin Wei</author><pubDate>Fri, 29 Aug 2025 08:56:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12683v4</guid></item><item><title>Unsupervised Incremental Learning Using Confidence-Based Pseudo-Labels</title><link>http://arxiv.org/abs/2508.21424v1</link><description>Deep learning models have achieved state-of-the-art performance in manycomputer vision tasks. However, in real-world scenarios, novel classes thatwere unseen during training often emerge, requiring models to acquire newknowledge incrementally. Class-Incremental Learning (CIL) methods enable amodel to learn novel classes while retaining knowledge of previous classes.However, these methods make the strong assumption that the incremental datasetis fully labeled, which is unrealistic in practice. In this work, we propose anunsupervised Incremental Learning method using Confidence-based Pseudo-labels(ICPL), which replaces human annotations with pseudo-labels, enablingincremental learning from unlabeled datasets. We integrate these pseudo-labelsinto various CIL methods with confidence-based selection and evaluateperformance degradation on CIFAR100 and ImageNet100. Then, we compare ourapproach to popular Class Incremental Novel Category Discovery (class-iNCD)methods addressing similar challenges. Additionally, we apply our method tofine-grained datasets to demonstrate its real-world practicality and measureits computational complexity to validate its suitability forresource-constrained environments. ICPL achieves competitive results comparedto supervised methods and outperforms state-of-the-art class-iNCD methods bymore than 5% in final accuracy.</description><author>Lucas Rakotoarivony</author><pubDate>Fri, 29 Aug 2025 08:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21424v1</guid></item><item><title>Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework</title><link>http://arxiv.org/abs/2508.21422v1</link><description>Large Language Models (LLMs) have great potential to accelerate and supportscholarly peer review and are increasingly used as fully automatic reviewgenerators (ARGs). However, potential biases and systematic errors may posesignificant risks to scientific integrity; understanding the specificcapabilities and limitations of state-of-the-art ARGs is essential. We focus ona core reviewing skill that underpins high-quality peer review: detectingfaulty research logic. This involves evaluating the internal consistencybetween a paper's results, interpretations, and claims. We present a fullyautomated counterfactual evaluation framework that isolates and tests thisskill under controlled conditions. Testing a range of ARG approaches, we findthat, contrary to expectation, flaws in research logic have no significanteffect on their output reviews. Based on our findings, we derive threeactionable recommendations for future work and release our counterfactualdataset and evaluation framework publicly.</description><author>Nils Dycke, Iryna Gurevych</author><pubDate>Fri, 29 Aug 2025 08:48:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21422v1</guid></item><item><title>Macro Graph of Experts for Billion-Scale Multi-Task Recommendation</title><link>http://arxiv.org/abs/2506.10520v3</link><description>Graph-based multi-task learning at billion-scale presents a significantchallenge, as different tasks correspond to distinct billion-scale graphs.Traditional multi-task learning methods often neglect these graph structures,relying solely on individual user and item embeddings. However, disregardinggraph structures overlooks substantial potential for improving performance. Inthis paper, we introduce the Macro Graph of Expert (MGOE) framework, the firstapproach capable of leveraging macro graph embeddings to capture task-specificmacro features while modeling the correlations between task-specific experts.Specifically, we propose the concept of a Macro Graph Bottom, which, for thefirst time, enables multi-task learning models to incorporate graph informationeffectively. We design the Macro Prediction Tower to dynamically integratemacro knowledge across tasks. MGOE has been deployed at scale, poweringmulti-task learning for the homepage of a leading billion-scale recommendersystem. Extensive offline experiments conducted on three public benchmarkdatasets demonstrate its superiority over state-of-the-art multi-task learningmethods, establishing MGOE as a breakthrough in multi-task graph-basedrecommendation. Furthermore, online A/B tests confirm the superiority of MGOEin billion-scale recommender systems.</description><author>Hongyu Yao, Zijin Hong, Hao Chen, Zhiqing Li, Qijie Shen, Zuobin Ying, Qihua Feng, Huan Gong, Feiran Huang</author><pubDate>Fri, 29 Aug 2025 08:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.10520v3</guid></item><item><title>Geoff: The Generic Optimization Framework &amp; Frontend for Particle Accelerator Controls</title><link>http://arxiv.org/abs/2506.03796v2</link><description>Geoff is a collection of Python packages that form a framework for automationof particle accelerator controls. With particle accelerator laboratories aroundthe world researching machine learning techniques to improve acceleratorperformance and uptime, a multitude of approaches and algorithms have emerged.The purpose of Geoff is to harmonize these approaches and to minimize frictionwhen comparing or migrating between them. It provides standardized interfacesfor optimization problems, utility functions to speed up development, and areference GUI application that ties everything together. Geoff is anopen-source library developed at CERN and maintained and updated incollaboration between CERN and GSI as part of the EURO-LABS project. This papergives an overview over Geoff's design, features, and current usage.</description><author>Penelope Madysa, Sabrina Appel, Verena Kain, Michael Schenk</author><pubDate>Fri, 29 Aug 2025 08:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.03796v2</guid></item><item><title>Rethinking Layer-wise Model Merging through Chain of Merges</title><link>http://arxiv.org/abs/2508.21421v1</link><description>Fine-tuning pretrained models has become a standard pathway to achievestate-of-the-art performance across a wide range of domains, leading to aproliferation of task-specific model variants. As the number of suchspecialized modules in-creases, merging them into a unified model withoutretraining has become a critical challenge. Existing merging techniques oftenrely on interference heuristics,importance weighting, or activation matchingwhile treating each layer independently, thereby failing to account for theinter-layer dependencies inherent in deep networks. This simplification leadsto distributional mismatches, especially inactivation-based methods, whenchanges in early layers are not properly reflected in downstream ones. Weidentify these mismatches as a form of internal covariate shift, comparable tothe phenomenon encountered in the initial phases of neural networks training.To address it, we propose Chain of Merges (CoM), a layer-wise merging procedurethat updates activation statistics in an auto-regressive fashion, explicitlyaccounting for cross-layer interactions. CoM produces a coherent merged modelthrough a series of conditionally optimal updates, effectively mitigatingdegradation caused by covariate shift. Experiments on standard bench-marksdemonstrate that CoM achieves state-of-the-art performance.</description><author>Pietro Buzzega, Riccardo Salami, Angelo Porrello, Simone Calderara</author><pubDate>Fri, 29 Aug 2025 08:44:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21421v1</guid></item><item><title>Benchmarking the State of Networks with a Low-Cost Method Based on Reservoir Computing</title><link>http://arxiv.org/abs/2508.21420v1</link><description>Using data from mobile network utilization in Norway, we showcase thepossibility of monitoring the state of communication and mobility networks witha non-invasive, low-cost method. This method transforms the network data into amodel within the framework of reservoir computing and then measures the model'sperformance on proxy tasks. Experimentally, we show how the performance onthese proxies relates to the state of the network. A key advantage of thisapproach is that it uses readily available data sets and leverages thereservoir computing framework for an inexpensive and largely agnostic method.Data from mobile network utilization is available in an anonymous, aggregatedform with multiple snapshots per day. This data can be treated like a weightednetwork. Reservoir computing allows the use of weighted, but untrained networksas a machine learning tool. The network, initialized as a so-called echo statenetwork (ESN), projects incoming signals into a higher dimensional space, onwhich a single trained layer operates. This consumes less energy than deepneural networks in which every weight of the network is trained. We useneuroscience inspired tasks and trained our ESN model to solve them. We thenshow how the performance depends on certain network configurations and also howit visibly decreases when perturbing the network. While this work serves asproof of concept, we believe it can be elevated to be used for near-real-timemonitoring as well as the identification of possible weak spots of both mobilecommunication networks as well as transportation networks.</description><author>Felix Simon Reimers, Carl-Hendrik Peters, Stefano Nichele</author><pubDate>Fri, 29 Aug 2025 08:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21420v1</guid></item><item><title>Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives</title><link>http://arxiv.org/abs/2508.21418v1</link><description>A Whole Slide Image (WSI) is a high-resolution digital image created byscanning an entire glass slide containing a biological specimen, such as tissuesections or cell samples, at multiple magnifications. These images can beviewed, analyzed, shared digitally, and are used today for ArtificialIntelligence (AI) algorithm development. WSIs are used in a variety of fields,including pathology for diagnosing diseases and oncology for cancer research.They are also utilized in neurology, veterinary medicine, hematology,microbiology, dermatology, pharmacology, toxicology, immunology, and forensicscience. When assembling cohorts for the training or validation of an AI algorithm, itis essential to know what is present on such a WSI. However, there is currentlyno standard for this metadata, so such selection has mainly been done throughmanual inspection, which is not suitable for large collections with severalmillion objects. We propose a general framework to generate a 2D index map for WSI and aprofiling mechanism for specific application domains. We demonstrate thisapproach in the field of clinical pathology, using common syntax and semanticsto achieve interoperability between different catalogs. Our approach augments each WSI collection with a detailed tissue map thatprovides fine-grained information about the WSI content. The tissue map isorganized into three layers: source, tissue type, and pathological alterations,with each layer assigning segments of the WSI to specific classes. We illustrate the advantages and applicability of the proposed standardthrough specific examples in WSI catalogs, Machine Learning (ML), andgraph-based WSI representations.</description><author>Gernot Fiala, Markus Plass, Robert Harb, Peter Regitnig, Kristijan Skok, Wael Al Zoughbi, Carmen Zerner, Paul Torke, Michaela Kargl, Heimo Müller, Tomas Brazdil, Matej Gallo, Jaroslav Kubín, Roman Stoklasa, Rudolf Nenutil, Norman Zerbe, Andreas Holzinger, Petr Holub</author><pubDate>Fri, 29 Aug 2025 08:39:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21418v1</guid></item><item><title>Molecular Machine Learning in Chemical Process Design</title><link>http://arxiv.org/abs/2508.20527v2</link><description>We present a perspective on molecular machine learning (ML) in the field ofchemical process engineering. Recently, molecular ML has demonstrated greatpotential in (i) providing highly accurate predictions for properties of purecomponents and their mixtures, and (ii) exploring the chemical space for newmolecular structures. We review current state-of-the-art molecular ML modelsand discuss research directions that promise further advancements. Thisincludes ML methods, such as graph neural networks and transformers, which canbe further advanced through the incorporation of physicochemical knowledge in ahybrid or physics-informed fashion. Then, we consider leveraging molecular MLat the chemical process scale, which is highly desirable yet rather unexplored.We discuss how molecular ML can be integrated into process design andoptimization formulations, promising to accelerate the identification of novelmolecules and processes. To this end, it will be essential to create moleculeand process design benchmarks and practically validate proposed candidates,possibly in collaboration with the chemical industry.</description><author>Jan G. Rittig, Manuel Dahmen, Martin Grohe, Philippe Schwaller, Alexander Mitsos</author><pubDate>Fri, 29 Aug 2025 08:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20527v2</guid></item><item><title>TrustGeoGen: Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving</title><link>http://arxiv.org/abs/2504.15780v2</link><description>Mathematical geometric problem solving (GPS) demands verifiable logicalcoherence and multimodal reasoning capabilities. While large language models(LLMs) have shown rapid progress in GPS, their advancement is hindered by thelack of reliable benchmarks and systematic methodologies. A critical challengeis the inherent hallucination in LLMs, which leads to synthetic GPS datasetsthat are often noisy, unverified, and self-contradictory. To address this, weintroduce TrustGeoGen, a data engine that generates formally verified geometricproblems to establish a principled and trustworthy benchmark. Our engineintegrates four key innovations: 1) Multimodal Alignment, which synchronizesthe generation of diagrams, text, and step-by-step solutions; 2) FormalVerification, ensuring all reasoning paths are rule-compliant; 3) ConnectionThinking, bridging formal deduction with human-like logical steps; and 4) our\textit{GeoExplore} series algorithms, which produce diverse problem variantswith multiple solutions and self-reflective backtracking. Using this engine, wecreate the GeoTrust-200K dataset and the corresponding GeoTrust-test benchmark,both with guaranteed cross-modal integrity. Experiments reveal thatstate-of-the-art models achieve only 45.83\% accuracy on GeoTrust-test,highlighting its significant challenge. Furthermore, training on oursynthesized data substantially improves model performance on GPS tasks, withstrong generalization to out-of-domain (OOD) benchmarks. Our code and data areavailable at https://github.com/Alpha-Innovator/TrustGeoGen</description><author>Daocheng Fu, Jianlong Chen, Renqiu Xia, Zijun Chen, Qi Liu, Yuan Feng, Hongbin Zhou, Renrui Zhang, Shiyang Feng, Peng Gao, Hongyuan Zha, Junchi Yan, Botian Shi, Yu Qiao, Bo Zhang</author><pubDate>Fri, 29 Aug 2025 08:38:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.15780v2</guid></item><item><title>CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN</title><link>http://arxiv.org/abs/2508.21411v1</link><description>User-friendly modeling and virtual simulation of urban traffic scenarios withdifferent types of interacting agents such as pedestrians, cyclists andautonomous vehicles remains a challenge. We present CARJAN, a novel tool forsemi-automated generation and simulation of such scenarios based on themulti-agent engineering framework AJAN and the driving simulator CARLA. CARJANprovides a visual user interface for the modeling, storage and maintenance oftraffic scenario layouts, and leverages SPARQL Behavior Tree-baseddecision-making and interactions for agents in dynamic scenario simulations inCARLA. CARJAN provides a first integrated approach for interactive, intelligentagent-based generation and simulation of virtual traffic scenarios in CARLA.</description><author>Leonard Frank Neis, Andre Antakli, Matthias Klusch</author><pubDate>Fri, 29 Aug 2025 08:33:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21411v1</guid></item><item><title>BrainGPT: Unleashing the Potential of EEG Generalist Foundation Model by Autoregressive Pre-training</title><link>http://arxiv.org/abs/2410.19779v2</link><description>Electroencephalogram (EEG) signals are pivotal in providing insights intospontaneous brain activity, highlighting their significant importance inneuroscience research. However, the exploration of versatile EEG models isconstrained by diverse data formats, outdated pre-training paradigms, andlimited transfer learning methods, only leading to specialist models on singledataset. In this paper, we introduce EEGPT, the first generalist EEG foundationmodel designed to address these challenges. First, we propose an electrode-wisemodeling strategy that treats each electrode as a fundamental unit, enablingthe integration of diverse EEG datasets collected from up to 138 electrodes,amassing 37.5M pre-training samples. Second, we develop the firstautoregressive EEG pre-trained model, moving away from traditional maskedautoencoder approaches to a next signal prediction task that better capturesthe sequential and temporal dependencies of EEG data. We also explore scalinglaws with model up to 1.1B parameters: the largest in EEG research to date.Third, we introduce a multi-task transfer learning paradigm using a learnableelectrode graph network shared across tasks, which for the first time confirmsmulti-task compatibility and synergy. As the first generalist EEG foundationmodel, EEGPT shows broad compatibility with various signal acquisition devices,subjects, and tasks. It supports up to 138 electrodes and any combinationthereof as input. Furthermore, we simultaneously evaluate it on 5 distincttasks across 12 benchmarks. EEGPT consistently outperforms existing specialistmodels across all downstream tasks, with its effectiveness further validatedthrough extensive ablation studies. This work sets a new direction forgeneralist EEG modeling, offering improved scalability, transferability, andadaptability for a wide range of EEG applications. The code and models will bereleased.</description><author>Tongtian Yue, Xuange Gao, Shuning Xue, Yepeng Tang, Longteng Guo, Jie Jiang, Jing Liu</author><pubDate>Fri, 29 Aug 2025 08:30:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19779v2</guid></item><item><title>DRASP: A Dual-Resolution Attentive Statistics Pooling Framework for Automatic MOS Prediction</title><link>http://arxiv.org/abs/2508.21407v1</link><description>A pooling mechanism is essential for mean opinion score (MOS) prediction,facilitating the transformation of variable-length audio features into aconcise fixed-size representation that effectively encodes speech quality.Existing pooling methods typically operate at a singular granularity,concentrating either on a comprehensive global perspective or a detailedframe-level analysis, which may overlook complementary perceptual insights. Toaddress this limitation, we introduce the Dual-Resolution Attentive StatisticsPooling (DRASP) framework. DRASP integrates both coarse-grained, globalstatistical summaries and fine-grained, attentive analyses of perceptuallysignificant segments. This dual-view architecture empowers our model toformulate a more thorough and robust representation, capturing both theoverarching structural context and salient local details concurrently.Extensive experiments validate the effectiveness and strong generalizationability of the proposed framework. It consistently outperforms various baselinemethods across diverse datasets (MusicEval and AES-Natural), MOS predictionbackbones (including a CLAP-based model and AudioBox-Aesthetics), and differentaudio generation systems, achieving a relative improvement of 10.39% insystem-level Spearman's rank correlation coefficient (SRCC) over thewidely-used average pooling approach.</description><author>Cheng-Yeh Yang, Kuan-Tang Huang, Chien-Chun Wang, Hung-Shin Lee, Hsin-Min Wang, Berlin Chen</author><pubDate>Fri, 29 Aug 2025 08:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21407v1</guid></item><item><title>Adaptive Optimisation of Ride-Pooling Personalised Fares in a Stochastic Framework</title><link>http://arxiv.org/abs/2508.20723v2</link><description>Ride-pooling systems, to succeed, must provide an attractive service, namelycompensate perceived costs with an appealing price. However, because of astrong heterogeneity in a value-of-time, each traveller has his own acceptableprice, unknown to the operator. Here, we show that individual acceptance levelscan be learned by the operator (over $90\%$ accuracy for pooled travellers in$10$ days) to optimise personalised fares. We propose an adaptive pricingpolicy, where every day the operator constructs an offer that progressivelymeets travellers' expectations and attracts a growing demand. Our resultssuggest that operators, by learning behavioural traits of individualtravellers, may improve performance not only for travellers (increased utility)but also for themselves (increased profit). Moreover, such knowledge allows theoperator to remove inefficient pooled rides and focus on attractive andprofitable combinations.</description><author>Michal Bujak, Rafal Kucharski</author><pubDate>Fri, 29 Aug 2025 08:19:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20723v2</guid></item><item><title>SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing</title><link>http://arxiv.org/abs/2508.21402v1</link><description>Self-supervised learning has emerged as a powerful tool for remote sensing,where large amounts of unlabeled data are available. In this work, weinvestigate the use of DINO, a contrastive self-supervised method, forpretraining on remote sensing imagery. We introduce SatDINO, a model tailoredfor representation learning in satellite imagery. Through extensive experimentson multiple datasets in multiple testing setups, we demonstrate that SatDINOoutperforms other state-of-the-art methods based on much more common maskedautoencoders (MAE) and achieves competitive results in multiple benchmarks. We also provide a rigorous ablation study evaluating SatDINO's individualcomponents. Finally, we propose a few novel enhancements, such as a new way toincorporate ground sample distance (GSD) encoding and adaptive view sampling.These enhancements can be used independently on our SatDINO model. Our code andtrained models are available at: https://github.com/strakaj/SatDINO.</description><author>Jakub Straka, Ivan Gruber</author><pubDate>Fri, 29 Aug 2025 08:19:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21402v1</guid></item><item><title>Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation</title><link>http://arxiv.org/abs/2309.08289v3</link><description>Accurate 3D modeling of human organs is critical for constructing digitalphantoms in virtual imaging trials. However, organs such as the large intestineremain particularly challenging due to their complex geometry and shapevariability. We propose CLAP, a novel Conditional LAtent Point-diffusion modelthat combines geometric deep learning with denoising diffusion models toenhance 3D representations of the large intestine. Given point clouds sampledfrom segmentation masks, we employ a hierarchical variational autoencoder tolearn both global and local latent shape representations. Two conditionaldiffusion models operate within this latent space to refine the organ shape. Apretrained surface reconstruction model is then used to convert the refinedpoint clouds into meshes. CLAP achieves substantial improvements in shapemodeling accuracy, reducing Chamfer distance by 26% and Hausdorff distance by36% relative to the initial suboptimal shapes. This approach offers a robustand extensible solution for high-fidelity organ modeling, with potentialapplicability to a wide range of anatomical structures.</description><author>Kaouther Mouheb, Mobina Ghojogh Nejad, Lavsen Dahal, Ehsan Samei, Kyle J. Lafata, W. Paul Segars, Joseph Y. Lo</author><pubDate>Fri, 29 Aug 2025 08:17:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08289v3</guid></item></channel></rss>