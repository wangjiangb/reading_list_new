<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 27 Sep 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</title><link>http://arxiv.org/abs/2509.21320v1</link><description>We present a scientific reasoning foundation model that aligns naturallanguage with heterogeneous scientific representations. The model is pretrainedon a 206B-token corpus spanning scientific text, pure sequences, andsequence-text pairs, then aligned via SFT on 40M instructions, annealedcold-start bootstrapping to elicit long-form chain-of-thought, andreinforcement learning with task-specific reward shaping, which instillsdeliberate scientific reasoning. It supports four capability families, coveringup to 103 tasks across workflows: (i) faithful translation between text andscientific formats, (ii) text/knowledge extraction, (iii) property prediction,(iv) property classification, (v) unconditional and conditional sequencegeneration and design. Compared with specialist systems, our approach broadensinstruction coverage, improves cross-domain generalization, and enhancesfidelity. We detail data curation and training and show that cross-disciplinelearning strengthens transfer and downstream reliability. The model, instructtuning datasets and the evaluation code are open-sourced athttps://huggingface.co/SciReason andhttps://github.com/open-sciencelab/SciReason.</description><author>Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai</author><pubDate>Thu, 25 Sep 2025 17:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21320v1</guid></item><item><title>RLBFF: Binary Flexible Feedback to bridge between Human Feedback &amp; Verifiable Rewards</title><link>http://arxiv.org/abs/2509.21319v1</link><description>Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learningwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLMpost-training, each offering distinct advantages. However, RLHF struggles withinterpretability and reward hacking because it relies on human judgments thatusually lack explicit criteria, whereas RLVR is limited in scope by its focuson correctness-based verifiers. We propose Reinforcement Learning with BinaryFlexible Feedback (RLBFF), which combines the versatility of human-drivenpreferences with the precision of rule-based verification, enabling rewardmodels to capture nuanced aspects of response quality beyond mere correctness.RLBFF extracts principles that can be answered in a binary fashion (e.g.accuracy of information: yes, or code readability: no) from natural languagefeedback. Such principles can then be used to ground Reward Model training asan entailment task (response satisfies or does not satisfy an arbitraryprinciple). We show that Reward Models trained in this manner can outperformBradley-Terry models when matched for data and achieve top performance onRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,2025). Additionally, users can specify principles of interest at inference timeto customize the focus of our reward models, in contrast to Bradley-Terrymodels. Finally, we present a fully open source recipe (including data) toalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed theperformance of o3-mini and DeepSeek R1 on general alignment benchmarks ofMT-Bench, WildBench, and Arena Hard v2 (at &lt;5% of the inference cost).</description><author>Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Ellie Evans, Daniel Egert, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev</author><pubDate>Thu, 25 Sep 2025 16:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21319v1</guid></item><item><title>SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</title><link>http://arxiv.org/abs/2509.21318v1</link><description>We present SD3.5-Flash, an efficient few-step distillation framework thatbrings high-quality image generation to accessible consumer devices. Ourapproach distills computationally prohibitive rectified flow models through areformulated distribution matching objective tailored specifically for few-stepgeneration. We introduce two key innovations: "timestep sharing" to reducegradient noise and "split-timestep fine-tuning" to improve prompt alignment.Combined with comprehensive pipeline optimizations like text encoderrestructuring and specialized quantization, our system enables both rapidgeneration and memory-efficient deployment across different hardwareconfigurations. This democratizes access across the full spectrum of devices,from mobile phones to desktop computers. Through extensive evaluation includinglarge-scale user studies, we demonstrate that SD3.5-Flash consistentlyoutperforms existing few-step methods, making advanced generative AI trulyaccessible for practical deployment.</description><author>Hmrishav Bandyopadhyay, Rahim Entezari, Jim Scott, Reshinth Adithyan, Yi-Zhe Song, Varun Jampani</author><pubDate>Thu, 25 Sep 2025 16:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21318v1</guid></item><item><title>Interactive Recommendation Agent with Active User Commands</title><link>http://arxiv.org/abs/2509.21317v1</link><description>Traditional recommender systems rely on passive feedback mechanisms thatlimit users to simple choices such as like and dislike. However, thesecoarse-grained signals fail to capture users' nuanced behavior motivations andintentions. In turn, current systems cannot also distinguish which specificitem attributes drive user satisfaction or dissatisfaction, resulting ininaccurate preference modeling. These fundamental limitations create apersistent gap between user intentions and system interpretations, ultimatelyundermining user satisfaction and harming system effectiveness. To address these limitations, we introduce the Interactive RecommendationFeed (IRF), a pioneering paradigm that enables natural language commands withinmainstream recommendation feeds. Unlike traditional systems that confine usersto passive implicit behavioral influence, IRF empowers active explicit controlover recommendation policies through real-time linguistic commands. To supportthis paradigm, we develop RecBot, a dual-agent architecture where a ParserAgent transforms linguistic expressions into structured preferences and aPlanner Agent dynamically orchestrates adaptive tool chains for on-the-flypolicy adjustment. To enable practical deployment, we employsimulation-augmented knowledge distillation to achieve efficient performancewhile maintaining strong reasoning capabilities. Through extensive offline andlong-term online experiments, RecBot shows significant improvements in bothuser satisfaction and business outcomes.</description><author>Jiakai Tang, Yujie Luo, Xunke Xi, Fei Sun, Xueyang Feng, Sunhao Dai, Chao Yi, Dian Chen, Zhujin Gao, Yang Li, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang, Bo Zheng</author><pubDate>Thu, 25 Sep 2025 15:38:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21317v1</guid></item><item><title>Hybrid Summary Statistics</title><link>http://arxiv.org/abs/2410.07548v2</link><description>We present a way to capture high-information posteriors from training setsthat are sparsely sampled over the parameter space for robust simulation-basedinference. In physical inference problems, we can often apply domain knowledgeto define traditional summary statistics to capture some of the information ina dataset. We show that augmenting these statistics with neural network outputsto maximise the mutual information improves information extraction compared toneural summaries alone or their concatenation to existing summaries and makesinference robust in settings with low training data. We introduce 1) two lossformalisms to achieve this and 2) apply the technique to two differentcosmological datasets to extract non-Gaussian parameter information.</description><author>T. Lucas Makinen, Ce Sui, Benjamin D. Wandelt, Natalia Porqueres, Alan Heavens</author><pubDate>Thu, 25 Sep 2025 15:27:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.07548v2</guid></item><item><title>SAGE: A Realistic Benchmark for Semantic Understanding</title><link>http://arxiv.org/abs/2509.21310v1</link><description>As large language models (LLMs) achieve strong performance on traditionalbenchmarks, there is an urgent need for more challenging evaluation frameworksthat probe deeper aspects of semantic understanding. We introduce SAGE(Semantic Alignment &amp; Generalization Evaluation), a rigorous benchmark designedto assess both embedding models and similarity metrics across five categories:Human Preference Alignment, Transformation Robustness, Information Sensitivity,Clustering Performance, and Retrieval Robustness. Unlike existing benchmarksthat focus on isolated capabilities, SAGE evaluates semantic understandingthrough adversarial conditions, noisy transformations, and nuanced humanjudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embeddingmodels and classical metrics reveals significant performance gaps, with nosingle approach excelling across all dimensions. For instance, whilestate-of-the-art embedding models like OpenAI's text-embedding-3-large dominatein aligning with human preferences (0.682 vs. 0.591 for the best classicalmetric), they are significantly outperformed by classical metrics oninformation sensitivity tasks, where Jaccard Similarity achieves a score of0.905 compared to the top embedding score of 0.794. SAGE further uncoverscritical trade-offs: OpenAI's text-embedding-3-small achieves the highestclustering performance (0.483) but demonstrates extreme brittleness with thelowest robustness score (0.011). SAGE exposes critical limitations in currentsemantic understanding capabilities and provides a more realistic assessment ofmodel robustness for real-world deployment.</description><author>Samarth Goel, Reagan J. Lee, Kannan Ramchandran</author><pubDate>Thu, 25 Sep 2025 15:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21310v1</guid></item><item><title>NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics</title><link>http://arxiv.org/abs/2509.21309v1</link><description>A primary bottleneck in large-scale text-to-video generation today isphysical consistency and controllability. Despite recent advances,state-of-the-art models often produce unrealistic motions, such as objectsfalling upward, or abrupt changes in velocity and direction. Moreover, thesemodels lack precise parameter control, struggling to generate physicallyconsistent dynamics under different initial conditions. We argue that thisfundamental limitation stems from current models learning motion distributionssolely from appearance, while lacking an understanding of the underlyingdynamics. In this work, we propose NewtonGen, a framework that integratesdata-driven synthesis with learnable physical principles. At its core liestrainable Neural Newtonian Dynamics (NND), which can model and predict avariety of Newtonian motions, thereby injecting latent dynamical constraintsinto the video generation process. By jointly leveraging data priors anddynamical guidance, NewtonGen enables physically consistent video synthesiswith precise parameter control.</description><author>Yu Yuan, Xijun Wang, Tharindu Wickremasinghe, Zeeshan Nadir, Bole Ma, Stanley H. Chan</author><pubDate>Thu, 25 Sep 2025 15:25:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21309v1</guid></item><item><title>VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models</title><link>http://arxiv.org/abs/2505.15801v3</link><description>Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achievedremarkable performance in the domain of reasoning. A key component of theirtraining is the incorporation of verifiable rewards within reinforcementlearning (RL). However, existing reward benchmarks do not evaluatereference-based reward systems, leaving researchers with limited understandingof the accuracy of verifiers used in RL. In this paper, we introduce twobenchmarks, VerifyBench and VerifyBench-Hard, designed to assess theperformance of reference-based reward systems. These benchmarks are constructedthrough meticulous data collection and curation, followed by careful humanannotation to ensure high quality. Current models still show considerable roomfor improvement on both VerifyBench and VerifyBench-Hard, especiallysmaller-scale models. Furthermore, we conduct a thorough and comprehensiveanalysis of evaluation results, offering insights for understanding anddeveloping reference-based reward systems. Our proposed benchmarks serve aseffective tools for guiding the development of verifier accuracy and thereasoning capabilities of models trained via RL in reasoning tasks.</description><author>Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, Jun Xiao, Yueting Zhuang</author><pubDate>Thu, 25 Sep 2025 15:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.15801v3</guid></item><item><title>Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs</title><link>http://arxiv.org/abs/2509.21305v1</link><description>Large language models (LLMs) often exhibit sycophantic behaviors -- such asexcessive agreement with or flattery of the user -- but it is unclear whetherthese behaviors arise from a single mechanism or multiple distinct processes.We decompose sycophancy into sycophantic agreement and sycophantic praise,contrasting both with genuine agreement. Using difference-in-means directions,activation additions, and subspace geometry across multiple models anddatasets, we show that: (1) the three behaviors are encoded along distinctlinear directions in latent space; (2) each behavior can be independentlyamplified or suppressed without affecting the others; and (3) theirrepresentational structure is consistent across model families and scales.These results suggest that sycophantic behaviors correspond to distinct,independently steerable representations.</description><author>Daniel Vennemeyer, Phan Anh Duong, Tiffany Zhan, Tianyu Jiang</author><pubDate>Thu, 25 Sep 2025 15:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21305v1</guid></item><item><title>Constructions are Revealed in Word Distributions</title><link>http://arxiv.org/abs/2503.06048v2</link><description>Construction grammar posits that constructions, or form-meaning pairings, areacquired through experience with language (the distributional learninghypothesis). But how much information about constructions does thisdistribution actually contain? Corpus-based analyses provide some answers, buttext alone cannot answer counterfactual questions about what \emph{caused} aparticular word to occur. This requires computable models of the distributionover strings -- namely, pretrained language models (PLMs). Here, we treat aRoBERTa model as a proxy for this distribution and hypothesize thatconstructions will be revealed within it as patterns of statistical affinity.We support this hypothesis experimentally: many constructions are robustlydistinguished, including (i) hard cases where semantically distinctconstructions are superficially similar, as well as (ii) \emph{schematic}constructions, whose ``slots'' can be filled by abstract word classes. Despitethis success, we also provide qualitative evidence that statistical affinityalone may be insufficient to identify all constructions from text. Thus,statistical affinity is likely an important, but partial, signal available tolearners.</description><author>Joshua Rozner, Leonie Weissweiler, Kyle Mahowald, Cory Shain</author><pubDate>Thu, 25 Sep 2025 15:18:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.06048v2</guid></item><item><title>LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines</title><link>http://arxiv.org/abs/2509.19580v2</link><description>Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our viewof the world. For example, Large Language Models (LLMs) based applications suchas ChatGPT have shown the capability of generating human-like conversation onextensive topics. Due to the impressive performance on a variety oflanguage-related tasks (e.g., open-domain question answering, translation, anddocument summarization), one can envision the far-reaching impacts that can bebrought by the LLMs with broader real-world applications (e.g., customerservice, education and accessibility, and scientific discovery). Inspired bytheir success, this paper will offer an overview of state-of-the-art LLMs andtheir integration into a wide range of academic disciplines, including: (1)arts, letters, and law (e.g., history, philosophy, political science, arts andarchitecture, law), (2) economics and business (e.g., finance, economics,accounting, marketing), and (3) science and engineering (e.g., mathematics,physics and mechanical engineering, chemistry and chemical engineering, lifesciences and bioengineering, earth sciences and civil engineering, computerscience and electrical engineering). Integrating humanity and technology, inthis paper, we will explore how LLMs are shaping research and practice in thesefields, while also discussing key limitations, open challenges, and futuredirections in the era of generative AI. The review of how LLMs are engagedacross disciplines-along with key observations and insights-can helpresearchers and practitioners interested in exploiting LLMs to advance theirworks in diverse real-world applications.</description><author>Yanfang Fanny Ye, Zheyuan Zhang, Tianyi Ma, Zehong Wang, Yiyang Li, Shifu Hou, Weixiang Sun, Kaiwen Shi, Yijun Ma, Wei Song, Ahmed Abbasi, Ying Cheng, Jane Cleland-Huang, Steven Corcelli, Patricia Culligan, Robert Goulding, Ming Hu, Ting Hua, John Lalor, Fang Liu, Tengfei Luo, Ed Maginn, Nuno Moniz, Jason Rohr, Brett Savoie, Daniel Slate, Tom Stapleford, Matthew Webber, Olaf Wiest, Johnny Zhang, Nitesh Chawla</author><pubDate>Thu, 25 Sep 2025 15:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19580v2</guid></item><item><title>LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle Zero-shot Radiology Recognition?</title><link>http://arxiv.org/abs/2503.07487v2</link><description>Recently, Multimodal Large Language Models (MLLMs) have demonstratedexceptional capabilities in visual understanding and reasoning across variousvision-language tasks. However, we found that MLLMs cannot process effectivelyfrom fine-grained medical image data in the traditional Visual QuestionAnswering (VQA) pipeline, as they do not exploit the captured features andavailable medical knowledge fully, results in MLLMs usually performing poorlyin zero-shot medical disease recognition. Fortunately, this limitation does notindicate that MLLMs are fundamentally incapable of addressing fine-grainedrecognition tasks. From a feature representation perspective, MLLMs demonstrateconsiderable potential for tackling such challenging problems. Thus, to addressthis challenge, we propose LLaVA-RadZ, a simple yet effective framework forzero-shot medical disease recognition via utilizing the existing MLLM features.Specifically, we design an end-to-end training strategy, termed Decoding-SideFeature Alignment Training (DFAT) to take advantage of the characteristics ofthe MLLM decoder architecture and incorporate modality-specific tokens tailoredfor different modalities. Additionally, we introduce a Domain KnowledgeAnchoring Module (DKAM) to exploit the intrinsic medical knowledge of largemodels, which mitigates the category semantic gap in image-text alignment.Extensive experiments demonstrate that our LLaVA-RadZ significantly outperformstraditional MLLMs in zero-shot disease recognition, achieving the comparableperformance to the well-established and highly-optimized CLIP-based approaches.</description><author>Bangyan Li, Wenxuan Huang, Zhenkun Gao, Yeqiang Wang, Yunhang Shen, Jingzhong Lin, Ling You, Yuxiang Shen, Shaohui Lin, Wanli Ouyang, Yuling Sun</author><pubDate>Thu, 25 Sep 2025 15:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.07487v2</guid></item><item><title>Two-level overlapping additive Schwarz preconditioner for training scientific machine learning applications</title><link>http://arxiv.org/abs/2406.10997v2</link><description>We introduce a novel two-level overlapping additive Schwarz preconditionerfor accelerating the training of scientific machine learning applications. Thedesign of the proposed preconditioner is motivated by the nonlinear two-leveloverlapping additive Schwarz preconditioner. The neural network parameters aredecomposed into groups (subdomains) with overlapping regions. In addition, thenetwork's feed-forward structure is indirectly imposed through a novelsubdomain-wise synchronization strategy and a coarse-level training step.Through a series of numerical experiments, which consider physics-informedneural networks and operator learning approaches, we demonstrate that theproposed two-level preconditioner significantly speeds up the convergence ofthe standard (LBFGS) optimizer while also yielding more accurate machinelearning models. Moreover, the devised preconditioner is designed to takeadvantage of model-parallel computations, which can further reduce the trainingtime.</description><author>Youngkyu Lee, Alena Kopaničáková, George Em Karniadakis</author><pubDate>Thu, 25 Sep 2025 15:17:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10997v2</guid></item><item><title>Reparameterization Proximal Policy Optimization</title><link>http://arxiv.org/abs/2508.06214v2</link><description>Reparameterization policy gradient (RPG) is promising for improving sampleefficiency by leveraging differentiable dynamics. However, a critical barrieris its training instability, where high-variance gradients can destabilize thelearning process. To address this, we draw inspiration from Proximal PolicyOptimization (PPO), which uses a surrogate objective to enable stable samplereuse in the model-free setting. We first establish a connection between thissurrogate objective and RPG, which has been largely unexplored and isnon-trivial. Then, we bridge this gap by demonstrating that thereparameterization gradient of a PPO-like surrogate objective can be computedefficiently using backpropagation through time. Based on this key insight, wepropose Reparameterization Proximal Policy Optimization (RPO), a stable andsample-efficient RPG-based method. RPO enables stable sample reuse overmultiple epochs by employing a policy gradient clipping mechanism tailored forRPG. It is further stabilized by Kullback-Leibler (KL) divergenceregularization and remains fully compatible with existing variance reductionmethods. We evaluate RPO on a suite of challenging locomotion and manipulationtasks, where experiments demonstrate that our method achieves superior sampleefficiency and strong performance.</description><author>Hai Zhong, Xun Wang, Zhuoran Li, Longbo Huang</author><pubDate>Thu, 25 Sep 2025 15:17:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06214v2</guid></item><item><title>Quantized Visual Geometry Grounded Transformer</title><link>http://arxiv.org/abs/2509.21302v1</link><description>Learning-based 3D reconstruction models, represented by Visual GeometryGrounded Transformers (VGGTs), have made remarkable progress with the use oflarge-scale transformers. Their prohibitive computational and memory costsseverely hinder real-world deployment. Post-Training Quantization (PTQ) hasbecome a common practice for compressing and accelerating models. However, weempirically observe that PTQ faces unique obstacles when compressingbillion-scale VGGTs: the data-independent special tokens induce heavy-tailedactivation distributions, while the multi-view nature of 3D data makescalibration sample selection highly unstable. This paper proposes the firstQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on twotechnical contributions: First, we introduce Dual-Smoothed Fine-GrainedQuantization, which integrates pre-global Hadamard rotation and post-localchannel smoothing to mitigate heavy-tailed distributions and inter-channelvariance robustly. Second, we design Noise-Filtered Diverse Sampling, whichfilters outliers via deep-layer statistics and constructs frame-aware diversecalibration clusters to ensure stable quantization ranges. Comprehensiveexperiments demonstrate that QuantVGGT achieves the state-of-the-art resultsacross different benchmarks and bit-width, surpassing the previousstate-of-the-art generic quantization method with a great margin. We highlightthat our 4-bit QuantVGGT can deliver a 3.7$\times$ memory reduction and2.5$\times$ acceleration in real-hardware inference, while maintainingreconstruction accuracy above 98\% of its full-precision counterpart. Thisdemonstrates the vast advantages and practicality of QuantVGGT inresource-constrained scenarios. Our code is released inhttps://github.com/wlfeng0509/QuantVGGT.</description><author>Weilun Feng, Haotong Qin, Mingqiang Wu, Chuanguang Yang, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, Yongjun Xu</author><pubDate>Thu, 25 Sep 2025 15:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21302v1</guid></item><item><title>Ambiguity Resolution in Text-to-Structured Data Mapping</title><link>http://arxiv.org/abs/2505.11679v2</link><description>Ambiguity in natural language is a significant obstacle for achievingaccurate text to structured data mapping through large language models (LLMs),which affects the performance of tasks such as mapping text to agentic toolcalling and text-to-SQL queries. Existing methods to ambiguity handling eitherrely on the ReACT framework to obtain correct mappings through trial and error,or on supervised fine-tuning to bias models toward specific tasks. In thispaper, we adopt a different approach that characterizes representationdifferences of ambiguous text in the latent space and leverages thesedifferences to identify ambiguity before mapping them to structured data. Todetect sentence-level ambiguity, we focus on the relationship between ambiguousquestions and their interpretations. Unlike distances calculated by denseembeddings, we introduce a new distance measure based on a path kernel overconcepts. With this measurement, we identify patterns to distinguish ambiguousfrom unambiguous questions. Furthermore, we propose a method for improving LLMperformance on ambiguous agentic tool calling through missing conceptprediction. Both achieve state-of-the-art results.</description><author>Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu</author><pubDate>Thu, 25 Sep 2025 15:14:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11679v2</guid></item><item><title>No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks</title><link>http://arxiv.org/abs/2509.21296v1</link><description>The memorization of training data by neural networks raises pressing concernsfor privacy and security. Recent work has shown that, under certain conditions,portions of the training set can be reconstructed directly from modelparameters. Some of these methods exploit implicit bias toward marginmaximization, suggesting that properties often regarded as beneficial forgeneralization may actually compromise privacy. Yet despite striking empiricaldemonstrations, the reliability of these attacks remains poorly understood andlacks a solid theoretical foundation. In this work, we take a complementaryperspective: rather than designing stronger attacks, we analyze the inherentweaknesses and limitations of existing reconstruction methods and identifyconditions under which they fail. We rigorously prove that, withoutincorporating prior knowledge about the data, there exist infinitely manyalternative solutions that may lie arbitrarily far from the true training set,rendering reconstruction fundamentally unreliable. Empirically, we furtherdemonstrate that exact duplication of training examples occurs only by chance.Our results refine the theoretical understanding of when training set leakageis possible and offer new insights into mitigating reconstruction attacks.Remarkably, we demonstrate that networks trained more extensively, andtherefore satisfying implicit bias conditions more strongly -- are, in fact,less susceptible to reconstruction attacks, reconciling privacy with the needfor strong generalization in this setting.</description><author>Yehonatan Refael, Guy Smorodinsky, Ofir Lindenbaum, Itay Safran</author><pubDate>Thu, 25 Sep 2025 15:14:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21296v1</guid></item><item><title>The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages</title><link>http://arxiv.org/abs/2509.21294v1</link><description>Developing AI systems that operate effectively across languages whileremaining culturally grounded is a long-standing challenge, particularly inlow-resource settings. Synthetic data provides a promising avenue, yet itseffectiveness in multilingual and multicultural contexts remains underexplored.We investigate the creation and impact of synthetic, culturally contextualizeddatasets for Indian languages through a bottom-up generation strategy thatprompts large open-source LLMs (&gt;= 235B parameters) to ground data generationin language-specific Wikipedia content. This approach complements the dominanttop-down paradigm of translating synthetic datasets from high-resourcelanguages such as English. We introduce Updesh, a high-quality large-scalesynthetic instruction-following dataset comprising 9.5M data points across 13Indian languages, encompassing diverse reasoning and generative tasks with anemphasis on long-context, multi-turn capabilities, and alignment with Indiancultural contexts. A comprehensive evaluation incorporating both automatedmetrics and human annotation across 10k assessments indicates that generateddata is high quality; though, human evaluation highlights areas for furtherimprovement. Additionally, we perform downstream evaluations by fine-tuningmodels on our dataset and assessing the performance across 15 diversemultilingual datasets. Models trained on Updesh consistently achievesignificant gains on generative tasks and remain competitive on multiple-choicestyle NLU tasks. Notably, relative improvements are most pronounced in low andmedium-resource languages, narrowing their gap with high-resource languages.These findings provide empirical evidence that effective multilingual AIrequires multi-faceted data curation and generation strategies that incorporatecontext-aware, culturally grounded methodologies.</description><author>Pranjal A. Chitale, Varun Gumma, Sanchit Ahuja, Prashant Kodali, Manan Uppadhyay, Deepthi Sudharsan, Sunayana Sitaram</author><pubDate>Thu, 25 Sep 2025 15:13:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21294v1</guid></item><item><title>ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</title><link>http://arxiv.org/abs/2509.09679v2</link><description>Large language models require massive memory footprints, severely limitingdeployment on consumer hardware. Quantization reduces memory through lowernumerical precision, but extreme 2-bit quantization suffers from catastrophicperformance loss due to outliers in activations. Rotation-based methods such asQuIP and QuaRot apply orthogonal transforms to eliminate outliers beforequantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, thesemethods use fixed transforms--Hadamard matrices achieving optimal worst-casecoherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weightdistributions. We identify that different transformer layers exhibit distinctoutlier patterns, motivating layer-adaptive rotations rather thanone-size-fits-all approaches. In this work, we propose ButterflyQuant, whichreplaces Hadamard rotations with learnable butterfly transforms parameterizedby continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$entries that are non-differentiable and thus prohibit gradient-based learning,butterfly transforms' continuous parameterization enables smooth optimizationwhile guaranteeing orthogonality by construction. This orthogonal constraintensures theoretical guarantees in outlier suppression while achieving $O(n \logn)$ computational complexity with only $\frac{n \log n}{2}$ learnableparameters. We further introduce a uniformity regularization onpost-transformation activations to promote smoother distributions amenable toquantization. Learning requires only 128 calibration samples and converges inminutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bitquantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP.\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.</description><author>Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang</author><pubDate>Thu, 25 Sep 2025 15:12:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.09679v2</guid></item><item><title>Optimal Robust Recourse with $L^p$-Bounded Model Change</title><link>http://arxiv.org/abs/2509.21293v1</link><description>Recourse provides individuals who received undesirable labels (e.g., denied aloan) from algorithmic decision-making systems with a minimum-cost improvementsuggestion to achieve the desired outcome. However, in practice, models oftenget updated to reflect changes in the data distribution or environment,invalidating the recourse recommendations (i.e., following the recourse willnot lead to the desirable outcome). The robust recourse literature addressesthis issue by providing a framework for computing recourses whose validity isresilient to slight changes in the model. However, since the optimizationproblem of computing robust recourse is non-convex (even for linear models),most of the current approaches do not have any theoretical guarantee on theoptimality of the recourse. Recent work by Kayastha et. al. provides the firstprovably optimal algorithm for robust recourse with respect to generalizedlinear models when the model changes are measured using the $L^{\infty}$ norm.However, using the $L^{\infty}$ norm can lead to recourse solutions with a highprice. To address this shortcoming, we consider more constrained model changesdefined by the $L^p$ norm, where $p\geq 1$ but $p\neq \infty$, and provide anew algorithm that provably computes the optimal robust recourse forgeneralized linear models. Empirically, for both linear and non-linear models,we demonstrate that our algorithm achieves a significantly lower price ofrecourse (up to several orders of magnitude) compared to prior work and alsoexhibits a better trade-off between the implementation cost of recourse and itsvalidity. Our empirical analysis also illustrates that our approach providesmore sparse recourses compared to prior work and remains resilient topost-processing approaches that guarantee feasibility.</description><author>Phone Kyaw, Kshitij Kayastha, Shahin Jabbari</author><pubDate>Thu, 25 Sep 2025 15:11:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21293v1</guid></item><item><title>CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction</title><link>http://arxiv.org/abs/2508.04929v3</link><description>As a critical modality for structural biology, cryogenic electron microscopy(cryo-EM) facilitates the determination of macromolecular structures atnear-atomic resolution. The core computational task in single-particle cryo-EMis to reconstruct the 3D electrostatic potential of a molecule from noisy 2Dprojections acquired at unknown orientations. Gaussian mixture models (GMMs)provide a continuous, compact, and physically interpretable representation formolecular density and have recently gained interest in cryo-EM reconstruction.However, existing methods rely on external consensus maps or atomic models forinitialization, limiting their use in self-contained pipelines. In parallel,differentiable rendering techniques such as Gaussian splatting havedemonstrated remarkable scalability and efficiency for volumetricrepresentations, suggesting a natural fit for GMM-based cryo-EM reconstruction.However, off-the-shelf Gaussian splatting methods are designed forphotorealistic view synthesis and remain incompatible with cryo-EM due tomismatches in the image formation physics, reconstruction objectives, andcoordinate systems. Addressing these issues, we propose cryoSplat, a GMM-basedmethod that integrates Gaussian splatting with the physics of cryo-EM imageformation. In particular, we develop an orthogonal projection-aware Gaussiansplatting, with adaptations such as a view-dependent normalization term andFFT-aligned coordinate system tailored for cryo-EM imaging. These innovationsenable stable and efficient homogeneous reconstruction directly from rawcryo-EM particle images using random initialization. Experimental results onreal datasets validate the effectiveness and robustness of cryoSplat overrepresentative baselines. The code will be released upon publication.</description><author>Suyi Chen, Haibin Ling</author><pubDate>Thu, 25 Sep 2025 15:10:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.04929v3</guid></item><item><title>MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors</title><link>http://arxiv.org/abs/2509.17084v2</link><description>Video action recognition is a fundamental task in computer vision, butstate-of-the-art models are often computationally expensive and rely onextensive video pre-training. In parallel, large-scale vision-language modelslike Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shotcapabilities on static images, while motion vectors (MV) provide highlyefficient temporal information directly from compressed video streams. Tosynergize the strengths of these paradigms, we propose MoCLIP-Lite, a simpleyet powerful two-stream late fusion framework for efficient video recognition.Our approach combines features from a frozen CLIP image encoder with featuresfrom a lightweight, supervised network trained on raw MV. During fusion, bothbackbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head istrained, ensuring extreme efficiency. Through comprehensive experiments on theUCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)baselines. Our work provides a new, highly efficient baseline for videounderstanding that effectively bridges the gap between large static models anddynamic, low-cost motion cues. Our code and models are available athttps://github.com/microa/MoCLIP-Lite.</description><author>Binhua Huang, Ni Wang, Arjun Pakrashi, Soumyabrata Dev</author><pubDate>Thu, 25 Sep 2025 15:09:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.17084v2</guid></item><item><title>VC-Agent: An Interactive Agent for Customized Video Dataset Collection</title><link>http://arxiv.org/abs/2509.21291v1</link><description>Facing scaling laws, video data from the internet becomes increasinglyimportant. However, collecting extensive videos that meet specific needs isextremely labor-intensive and time-consuming. In this work, we study the way toexpedite this collection process and propose VC-Agent, the first interactiveagent that is able to understand users' queries and feedback, and accordinglyretrieve/scale up relevant video clips with minimal user input. Specifically,considering the user interface, our agent defines various user-friendly waysfor the user to specify requirements based on textual descriptions andconfirmations. As for agent functions, we leverage existing multi-modal largelanguage models to connect the user's requirements with the video content. Moreimportantly, we propose two novel filtering policies that can be updated whenuser interaction is continually performed. Finally, we provide a new benchmarkfor personalized video dataset collection, and carefully conduct the user studyto verify our agent's usage in various real scenarios. Extensive experimentsdemonstrate the effectiveness and efficiency of our agent for customized videodataset collection. Project page: https://allenyidan.github.io/vcagent_page/.</description><author>Yidan Zhang, Mutian Xu, Yiming Hao, Kun Zhou, Jiahao Chang, Xiaoqiang Liu, Pengfei Wan, Hongbo Fu, Xiaoguang Han</author><pubDate>Thu, 25 Sep 2025 15:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21291v1</guid></item><item><title>Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned Latent Space</title><link>http://arxiv.org/abs/2509.02196v3</link><description>Simulating the long-timescale dynamics of biomolecules is a central challengein computational science. While enhanced sampling methods can accelerate thesesimulations, they rely on pre-defined collective variables that are oftendifficult to identify. A recent generative model, LD-FPG, demonstrated thatthis problem could be bypassed by learning to sample the static equilibriumensemble as all-atom deformations from a reference structure, establishing apowerful method for all-atom ensemble generation. However, while this approachsuccessfully captures a system's probable conformations, it does not model thetemporal evolution between them. We introduce the Graph Latent DynamicsPropagator (GLDP), a modular component for simulating dynamics within thelearned latent space of LD-FPG. We then compare three classes of propagators:(i) score-guided Langevin dynamics, (ii) Koopman-based linear operators, and(iii) autoregressive neural networks. Within a unifiedencoder-propagator-decoder framework, we evaluate long-horizon stability,backbone and side-chain ensemble fidelity, and functional free-energylandscapes. Autoregressive neural networks deliver the most robust longrollouts; score-guided Langevin best recovers side-chain thermodynamics whenthe score is well learned; and Koopman provides an interpretable, lightweightbaseline that tends to damp fluctuations. These results clarify the trade-offsamong propagators and offer practical guidance for latent-space simulators ofall-atom protein dynamics.</description><author>Aditya Sengar, Jiying Zhang, Pierre Vandergheynst, Patrick Barth</author><pubDate>Thu, 25 Sep 2025 15:07:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.02196v3</guid></item><item><title>DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding</title><link>http://arxiv.org/abs/2509.21287v1</link><description>Recent vision-language models excel at large-scale image-text alignment butoften neglect the compositional structure of language, leading to failures ontasks that hinge on word order and predicate-argument structure. We introduceDisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformerwith a novel tensor network text encoder that explicitly encodes syntacticstructure. Sentences are parsed with a Combinatory Categorial Grammar parser toyield distributional word tensors whose contractions mirror the sentence'sgrammatical derivation. To keep the model efficient, high-order tensors arefactorized with tensor decompositions, reducing parameter count from tens ofmillions to under one million. Trained end-to-end with a self-supervisedcontrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics andword order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%,boosts ARO attribution and relation scores by over 9% and 4%, and achieves93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate thatembedding explicit linguistic structure via tensor networks yieldsinterpretable, parameter-efficient representations that substantially improvecompositional reasoning in vision-language tasks.</description><author>Kin Ian Lo, Hala Hawashin, Mina Abbaszadeh, Tilen Limback-Stokin, Hadi Wazni, Mehrnoosh Sadrzadeh</author><pubDate>Thu, 25 Sep 2025 15:06:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21287v1</guid></item><item><title>Maxout Polytopes</title><link>http://arxiv.org/abs/2509.21286v1</link><description>Maxout polytopes are defined by feedforward neural networks with maxoutactivation function and non-negative weights after the first layer. Wecharacterize the parameter spaces and extremal f-vectors of maxout polytopesfor shallow networks, and we study the separating hypersurfaces which arisewhen a layer is added to the network. We also show that maxout polytopes arecubical for generic networks without bottlenecks.</description><author>Andrei Balakin, Shelby Cox, Georg Loho, Bernd Sturmfels</author><pubDate>Thu, 25 Sep 2025 15:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21286v1</guid></item><item><title>Bounds of Chain-of-Thought Robustness: Reasoning Steps, Embed Norms, and Beyond</title><link>http://arxiv.org/abs/2509.21284v1</link><description>Existing research indicates that the output of Chain-of-Thought (CoT) issignificantly affected by input perturbations. Although many methods aim tomitigate such impact by optimizing prompts, a theoretical explanation of howthese perturbations influence CoT outputs remains an open area of research.This gap limits our in-depth understanding of how input perturbations propagateduring the reasoning process and hinders further improvements in promptoptimization methods. Therefore, in this paper, we theoretically analyze theeffect of input perturbations on the fluctuation of CoT outputs. We firstderive an upper bound for input perturbations under the condition that theoutput fluctuation is within an acceptable range, based on which we prove that:(i) This upper bound is positively correlated with the number of reasoningsteps in the CoT; (ii) Even an infinitely long reasoning process cannoteliminate the impact of input perturbations. We then apply these conclusions tothe Linear Self-Attention (LSA) model, which can be viewed as a simplifiedversion of the Transformer. For the LSA model, we prove that the upper boundfor input perturbation is negatively correlated with the norms of the inputembedding and hidden state vectors. To validate this theoretical analysis, weconduct experiments on three mainstream datasets and four mainstream models.The experimental results align with our theoretical analysis, empiricallydemonstrating the correctness of our findings.</description><author>Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng</author><pubDate>Thu, 25 Sep 2025 15:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21284v1</guid></item><item><title>Aligning Distributionally Robust Optimization with Practical Deep Learning Needs</title><link>http://arxiv.org/abs/2508.16734v2</link><description>While traditional Deep Learning (DL) optimization methods treat all trainingsamples equally, Distributionally Robust Optimization (DRO) adaptively assignsimportance weights to different samples. However, a significant gap existsbetween DRO and current DL practices. Modern DL optimizers require adaptivityand the ability to handle stochastic gradients, as these methods demonstratesuperior performance. Additionally, for practical applications, a method shouldallow weight assignment not only to individual samples, but also to groups ofobjects (for example, all samples of the same class). This paper aims to bridgethis gap by introducing ALSO $\unicode{x2013}$ Adaptive Loss Scaling Optimizer$\unicode{x2013}$ an adaptive algorithm for a modified DRO objective that canhandle weight assignment to sample groups. We prove the convergence of ourproposed algorithm for non-convex objectives, which is the typical case for DLmodels. Empirical evaluation across diverse Deep Learning tasks, from TabularDL to Split Learning tasks, demonstrates that ALSO outperforms both traditionaloptimizers and existing DRO methods.</description><author>Dmitrii Feoktistov, Igor Ignashin, Andrey Veprikov, Nikita Borovko, Alexander Bogdanov, Savelii Chezhegov, Aleksandr Beznosikov</author><pubDate>Thu, 25 Sep 2025 15:03:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16734v2</guid></item><item><title>TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design</title><link>http://arxiv.org/abs/2506.19997v3</link><description>Generalizing deep reinforcement learning agents to unseen environmentsremains a significant challenge. One promising solution is UnsupervisedEnvironment Design (UED), a co-evolutionary framework in which a teacheradaptively generates tasks with high learning potential, while a student learnsa robust policy from this evolving curriculum. Existing UED methods typicallymeasure learning potential via regret, the gap between optimal and currentperformance, approximated solely by value-function loss. Building on theseapproaches, we introduce the transition-prediction error as an additional termin our regret approximation. To capture how training on one task affectsperformance on others, we further propose a lightweight metric calledCo-Learnability. By combining these two measures, we present Transition-awareRegret Approximation with Co-learnability for Environment Design (TRACED).Empirical evaluations show that TRACED produces curricula that improvezero-shot generalization over strong baselines across multiple benchmarks.Ablation studies confirm that the transition-prediction error drives rapidcomplexity ramp-up and that Co-Learnability delivers additional gains whenpaired with the transition-prediction error. These results demonstrate howrefined regret approximation and explicit modeling of task relationships can beleveraged for sample-efficient curriculum design in UED. Project Page:https://geonwoo.me/traced/</description><author>Geonwoo Cho, Jaegyun Im, Jihwan Lee, Hojun Yi, Sejin Kim, Sundong Kim</author><pubDate>Thu, 25 Sep 2025 15:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.19997v3</guid></item><item><title>It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL</title><link>http://arxiv.org/abs/2509.21282v1</link><description>Training large language models (LLMs) with reinforcement learning (RL)methods such as PPO and GRPO commonly relies on ratio clipping to stabiliseupdates. While effective at preventing instability, clipping discardsinformation and introduces gradient discontinuities. We propose ProbabilitySmoothing Policy Optimisation (PSPO), which smooths the current policy'sprobabilities toward the old (behaviour) policy before computing the importanceratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradientsignal, while interpolation toward the old policy creates a soft trust regionthat discourages large, destabilising updates, with formal guarantees. We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B andQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-datasetgeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similarperformance but improves the reasoning leading to clearer and more conciseresponses which are more logical. Compared to clipped GRPO, GR-PSPOsubstantially improves performance both the 0.5B and 1.5B models, with a boostof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).</description><author>Madeleine Dwyer, Adam Sobey, Adriane Chapman</author><pubDate>Thu, 25 Sep 2025 15:03:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21282v1</guid></item><item><title>Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds</title><link>http://arxiv.org/abs/2509.21281v1</link><description>Human-like motion generation for robots often draws inspiration frombiomechanical studies, which often categorize complex human motions intohierarchical taxonomies. While these taxonomies provide rich structuralinformation about how movements relate to one another, this information isfrequently overlooked in motion generation models, leading to a disconnectbetween the generated motions and their underlying hierarchical structure. Thispaper introduces the \ac{gphdm}, a novel approach that learns latentrepresentations preserving both the hierarchical structure of motions and theirtemporal dynamics to ensure physical consistency. Our model achieves this byextending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) tothe hyperbolic manifold and integrating it with taxonomy-aware inductivebiases. Building on this geometry- and taxonomy-aware frameworks, we proposethree novel mechanisms for generating motions that are bothtaxonomically-structured and physically-consistent: two probabilistic recursiveapproaches and a method based on pullback-metric geodesics. Experiments ongenerating realistic motion sequences on the hand grasping taxonomy show thatthe proposed GPHDM faithfully encodes the underlying taxonomy and temporaldynamics, and generates novel physically-consistent trajectories.</description><author>Luis Augenstein, Noémie Jaquier, Tamim Asfour, Leonel Rozo</author><pubDate>Thu, 25 Sep 2025 15:03:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21281v1</guid></item><item><title>Does FLUX Already Know How to Perform Physically Plausible Image Composition?</title><link>http://arxiv.org/abs/2509.21278v1</link><description>Image composition aims to seamlessly insert a user-specified object into anew scene, but existing models struggle with complex lighting (e.g., accurateshadows, water reflections) and diverse, high-resolution inputs. Moderntext-to-image diffusion models (e.g., SD3.5, FLUX) already encode essentialphysical and resolution priors, yet lack a framework to unleash them withoutresorting to latent inversion, which often locks object poses into contextuallyinappropriate orientations, or brittle attention surgery. We propose SHINE, atraining-free framework for Seamless, High-fidelity Insertion with NeutralizedErrors. SHINE introduces manifold-steered anchor loss, leveraging pretrainedcustomization adapters (e.g., IP-Adapter) to guide latents for faithful subjectrepresentation while preserving background integrity. Degradation-suppressionguidance and adaptive background blending are proposed to further eliminatelow-quality outputs and visible seams. To address the lack of rigorousbenchmarks, we introduce ComplexCompo, featuring diverse resolutions andchallenging conditions such as low lighting, strong illumination, intricateshadows, and reflective surfaces. Experiments on ComplexCompo andDreamEditBench show state-of-the-art performance on standard metrics (e.g.,DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).Code and benchmark will be publicly available upon publication.</description><author>Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</author><pubDate>Thu, 25 Sep 2025 15:01:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21278v1</guid></item><item><title>Real-time Hybrid System Identification with Online Deterministic Annealing</title><link>http://arxiv.org/abs/2408.01730v2</link><description>We introduce a real-time identification method for discrete-timestate-dependent switching systems in both the input--output and state-spacedomains. In particular, we design a system of adaptive algorithms running intwo timescales; a stochastic approximation algorithm implements an onlinedeterministic annealing scheme at a slow timescale and estimates themode-switching signal, and an recursive identification algorithm runs at afaster timescale and updates the parameters of the local models based on theestimate of the switching signal. We first focus on piece-wise affine systemsand discuss identifiability conditions and convergence properties based on thetheory of two-timescale stochastic approximation. In contrast to standardidentification algorithms for switched systems, the proposed approach graduallyestimates the number of modes and is appropriate for real-time systemidentification using sequential data acquisition. The progressive nature of thealgorithm improves computational efficiency and provides real-time control overthe performance-complexity trade-off. Finally, we address specific challengesthat arise in the application of the proposed methodology in identification ofmore general switching systems. Simulation results validate the efficacy of theproposed methodology.</description><author>Christos Mavridis, Karl Henrik Johansson</author><pubDate>Thu, 25 Sep 2025 15:01:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01730v2</guid></item><item><title>MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</title><link>http://arxiv.org/abs/2505.23764v2</link><description>Spatial intelligence is essential for multimodal large language models(MLLMs) operating in the complex physical world. Existing benchmarks, however,probe only single-image relations and thus fail to assess the multi-imagespatial reasoning that real-world deployments demand. We introduce MMSI-Bench,a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-visionresearchers spent more than 300 hours meticulously crafting 1,000 challenging,unambiguous multiple-choice questions from over 120,000 images, each pairedwith carefully designed distractors and a step-by-step reasoning process. Weconduct extensive experiments and thoroughly evaluate 34 open-source andproprietary MLLMs, observing a wide gap: the strongest open-source modelattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, whilehumans score 97%. These results underscore the challenging nature of MMSI-Benchand the substantial headroom for future research. Leveraging the annotatedreasoning processes, we also provide an automated error analysis pipeline thatdiagnoses four dominant failure modes, including (1) grounding errors, (2)overlap-matching and scene-reconstruction errors, (3) situation-transformationreasoning errors, and (4) spatial-logic errors, offering valuable insights foradvancing multi-image spatial intelligence. Project page:https://runsenxu.com/projects/MMSI_Bench .</description><author>Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang</author><pubDate>Thu, 25 Sep 2025 15:01:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.23764v2</guid></item><item><title>Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training</title><link>http://arxiv.org/abs/2509.21275v1</link><description>Long context training is crucial for LLM's context extension. Existingschemes, such as sequence parallelism, incur substantial communicationoverhead. Pipeline parallelism (PP) reduces this cost, but its effectivenesshinges on partitioning granularity. Batch-level PP dividing input samplesexhibits high memory consumption in long-context scenario, whereas token-levelPP splitting sequences into slices alleviates memory overhead but may incurhardware under-utilization. This trade-off motivates adaptively selecting PPgranularity to match resource and workload characteristics. Moreover, sequencelength distribution of the real-world dataset exhibits skewness, posing achallenge on PP's workload balance and efficient scheduling. Current static PPscheduling methods overlook the variance of sequence length, leading tosuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism(EPP) that orchestrates token-level PP and batch-level PP to adapt to resourceand workload heterogeneity. We build InfiniPipe, a distributed training systemthat unleashes the potential of EPP via (1) a resource-aware andworkload-balanced sequence processor that splits long sequences and packs shortones; and (2) a co-optimization methodology that jointly optimizes pipelineschedule and gradient checkpointing via a mechanism named stage-awarechunk-level adaptive checkpointing. Comprehensive experiments demonstrate thatInfiniPipe achieves a 1.69x speedup over state-of-the-art systems.</description><author>Shiju Wang, Yujie Wang, Ao Sun, Fangcheng Fu, Zijian Zhu, Bin Cui, Xu Han, Kaisheng Ma</author><pubDate>Thu, 25 Sep 2025 15:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21275v1</guid></item><item><title>IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves</title><link>http://arxiv.org/abs/2411.00827v6</link><description>As large Vision-Language Models (VLMs) gain prominence, ensuring their safedeployment has become critical. Recent studies have explored VLM robustnessagainst jailbreak attacks-techniques that exploit model vulnerabilities toelicit harmful outputs. However, the limited availability of diverse multimodaldata has constrained current approaches to rely heavily on adversarial ormanually crafted images derived from harmful text datasets, which often lackeffectiveness and diversity across different contexts. In this paper, wepropose IDEATOR, a novel jailbreak method that autonomously generates maliciousimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in theinsight that VLMs themselves could serve as powerful red team models forgenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLMto create targeted jailbreak texts and pairs them with jailbreak imagesgenerated by a state-of-the-art diffusion model. Extensive experimentsdemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%attack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,InstructBLIP, and Chameleon, respectively. Building on IDEATOR's strongtransferability and automated process, we introduce the VLJailbreakBench, asafety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmarkresults on 11 recently released VLMs reveal significant gaps in safetyalignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4oand 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for strongerdefenses. VLJailbreakBench is publicly available athttps://roywang021.github.io/VLJailbreakBench.</description><author>Ruofan Wang, Juncheng Li, Yixu Wang, Bo Wang, Xiaosen Wang, Yan Teng, Yingchun Wang, Xingjun Ma, Yu-Gang Jiang</author><pubDate>Thu, 25 Sep 2025 15:00:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.00827v6</guid></item><item><title>A Sentinel-3 foundation model for ocean colour</title><link>http://arxiv.org/abs/2509.21273v1</link><description>Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massiveunlabelled datasets, have the potential to drastically change AI applicationsin ocean science, where labelled data are often sparse and expensive tocollect. In this work, we describe a new foundation model using the Prithvi-EOVision Transformer architecture which has been pre-trained to reconstruct datafrom the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate themodel by fine-tuning on two downstream marine earth observation tasks. We firstassess model performance compared to current baseline models used to quantifychlorophyll concentration. We then evaluate the FMs ability to refine remotesensing-based estimates of ocean primary production. Our results demonstratethe utility of self-trained FMs for marine monitoring, in particular for makinguse of small amounts of high quality labelled data and in capturing detailedspatial patterns of ocean colour whilst matching point observations. Weconclude that this new generation of geospatial AI models has the potential toprovide more robust, data-driven insights into ocean ecosystems and their rolein global climate processes.</description><author>Geoffrey Dawson, Remy Vandaele, Andrew Taylor, David Moffat, Helen Tamura-Wicks, Sarah Jackson, Rosie Lickorish, Paolo Fraccaro, Hywel Williams, Chunbo Luo, Anne Jones</author><pubDate>Thu, 25 Sep 2025 15:00:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21273v1</guid></item><item><title>SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips</title><link>http://arxiv.org/abs/2509.21271v1</link><description>The emergence of Superchips represents a significant advancement innext-generation AI hardware. These Superchips employ a tightly coupledheterogeneous architecture that integrates GPU and CPU on the same package,which offers unprecedented computational power. However, there has been scantresearch investigating how LLM training benefits from this new architecture. Inthis work, for the first time, we study LLM training solutions based onoffloading for Superchips. We observe important differences between Superchipsand traditional loosely-coupled GPU-CPU architecture, which necessitaterevisiting prevailing assumptions about offloading. Based on that, we presentSuperOffload, a Superchip-centric offloading system that simultaneously usesHopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.SuperOffload accomplishes this via a combination of techniques, such asadaptive weight offloading, bucketization repartitioning, Superchip-awarecasting, speculative execution, and a highly optimized Adam optimizer for GraceCPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5xthroughput improvement compared to state-of-the-art offloading-based systems,enabling training of up to 25B model on a single Superchip while achieving hightraining throughput. We also extend SuperOffload with ZeRO-style dataparallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of13B model with sequence lengths up to 1 million tokens on 8 GH200 whileachieving 55% MFU.</description><author>Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang</author><pubDate>Thu, 25 Sep 2025 15:00:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21271v1</guid></item><item><title>LLMTrace: A Corpus for Classification and Fine-Grained Localization of AI-Written Text</title><link>http://arxiv.org/abs/2509.21269v1</link><description>The widespread use of human-like text from Large Language Models (LLMs)necessitates the development of robust detection systems. However, progress islimited by a critical lack of suitable training data; existing datasets areoften generated with outdated models, are predominantly in English, and fail toaddress the increasingly common scenario of mixed human-AI authorship.Crucially, while some datasets address mixed authorship, none provide thecharacter-level annotations required for the precise localization ofAI-generated segments within a text. To address these gaps, we introduceLLMTrace, a new large-scale, bilingual (English and Russian) corpus forAI-generated text detection. Constructed using a diverse range of modernproprietary and open-source LLMs, our dataset is designed to support two keytasks: traditional full-text binary classification (human vs. AI) and the noveltask of AI-generated interval detection, facilitated by character-levelannotations. We believe LLMTrace will serve as a vital resource for trainingand evaluating the next generation of more nuanced and practical AI detectionmodels. The project page is available at\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.</description><author>Irina Tolstykh, Aleksandra Tsybina, Sergey Yakubson, Maksim Kuprashevich</author><pubDate>Thu, 25 Sep 2025 14:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21269v1</guid></item><item><title>MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources</title><link>http://arxiv.org/abs/2509.21268v1</link><description>Large multimodal reasoning models have achieved rapid progress, but theiradvancement is constrained by two major limitations: the absence of open,large-scale, high-quality long chain-of-thought (CoT) data, and the instabilityof reinforcement learning (RL) algorithms in post-training. Group RelativePolicy Optimization (GRPO), the standard framework for RL fine-tuning, is proneto gradient vanishing when reward variance is low, which weakens optimizationsignals and impairs convergence. This work makes three contributions: (1) Wepropose Variance-Aware Sampling (VAS), a data selection strategy guided byVariance Promotion Score (VPS) that combines outcome variance and trajectorydiversity to promote reward variance and stabilize policy optimization. (2) Werelease large-scale, carefully curated resources containing ~1.6M long CoTcold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,and diversity, along with a fully reproducible end-to-end training codebase.(3) We open-source a family of multimodal reasoning models in multiple scales,establishing standardized baselines for the community. Experiments acrossmathematical reasoning benchmarks demonstrate the effectiveness of both thecurated data and the proposed VAS. Comprehensive ablation studies and analysesprovide further insight into the contributions of each component. In addition,we theoretically establish that reward variance lower-bounds the expectedpolicy gradient magnitude, with VAS serving as a practical mechanism to realizethis guarantee. Our code, data, and checkpoints are available athttps://github.com/LengSicong/MMR1.</description><author>Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Lidong Bing, Deli Zhao, Wei Lu, Yu Rong, Aixin Sun, Shijian Lu</author><pubDate>Thu, 25 Sep 2025 14:58:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21268v1</guid></item><item><title>Can social media provide early warning of retraction? Evidence from critical tweets identified by human annotation and large language models</title><link>http://arxiv.org/abs/2403.16851v3</link><description>Timely detection of problematic research is essential for safeguardingscientific integrity. To explore whether social media commentary can serve asan early indicator of potentially problematic articles, this study analysed3,815 tweets referencing 604 retracted articles and 3,373 tweets referencing668 comparable non-retracted articles. Tweets critical of the articles wereidentified through both human annotation and large language models (LLMs).Human annotation revealed that 8.3% of retracted articles were associated withat least one critical tweet prior to retraction, compared to only 1.5% ofnon-retracted articles, highlighting the potential of tweets as early warningsignals of retraction. However, critical tweets identified by LLMs (GPT-4omini, Gemini 2.0 Flash-Lite, and Claude 3.5 Haiku) only partially aligned withhuman annotation, suggesting that fully automated monitoring ofpost-publication discourse should be applied with caution. A human-AIcollaborative approach may offer a more reliable and scalable alternative, withhuman expertise helping to filter out tweets critical of issues unrelated tothe research integrity of the articles. Overall, this study provides insightsinto how social media signals, combined with generative AI technologies, maysupport efforts to strengthen research integrity.</description><author>Er-Te Zheng, Hui-Zhen Fu, Mike Thelwall, Zhichao Fang</author><pubDate>Thu, 25 Sep 2025 14:58:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16851v3</guid></item><item><title>LLM Output Homogenization is Task Dependent</title><link>http://arxiv.org/abs/2509.21267v1</link><description>A large language model can be less helpful if it exhibits output responsehomogenization. But whether two responses are considered homogeneous, andwhether such homogenization is problematic, both depend on the task category.For instance, in objective math tasks, we often expect no variation in thefinal answer but anticipate variation in the problem-solving strategy. Whereas,for creative writing tasks, we may expect variation in key narrative components(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversityproduced by temperature-sampling. Previous work addressing outputhomogenization often fails to conceptualize diversity in a task-dependent way.We address this gap in the literature directly by making the followingcontributions. (1) We present a task taxonomy comprised of eight taskcategories that each have distinct conceptualizations of output homogenization.(2) We introduce task-anchored functional diversity to better evaluate outputhomogenization. (3) We propose a task-anchored sampling technique thatincreases functional diversity for task categories where homogenization isundesired, while preserving homogenization where it is desired. (4) Wechallenge the perceived existence of a diversity-quality trade-off byincreasing functional diversity while maintaining response quality. Overall, wedemonstrate how task dependence improves the evaluation and mitigation ofoutput homogenization.</description><author>Shomik Jain, Jack Lanchantin, Maximilian Nickel, Karen Ullrich, Ashia Wilson, Jamelle Watson-Daniels</author><pubDate>Thu, 25 Sep 2025 14:58:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21267v1</guid></item><item><title>AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification</title><link>http://arxiv.org/abs/2506.05980v2</link><description>Skill-based reinforcement learning (SBRL) enables rapid adaptation inenvironments with sparse rewards by pretraining a skill-conditioned policy.Effective skill learning requires jointly maximizing both exploration and skilldiversity. However, existing methods often face challenges in simultaneouslyoptimizing for these two conflicting objectives. In this work, we propose a newmethod, Adaptive Multi-objective Projection for balancing Exploration and skillDiversification (AMPED), which explicitly addresses both: during pre-training,a gradient-surgery projection balances the exploration and diversity gradients,and during fine-tuning, a skill selector exploits the learned diversity bychoosing skills suited to downstream tasks. Our approach achieves performancethat surpasses SBRL baselines across various benchmarks. Through an extensiveablation study, we identify the role of each component and demonstrate thateach element in AMPED is contributing to performance. We further providetheoretical and empirical evidence that, with a greedy skill selector, greaterskill diversity reduces fine-tuning sample complexity. These results highlightthe importance of explicitly harmonizing exploration and diversity anddemonstrate the effectiveness of AMPED in enabling robust and generalizableskill learning. Project Page: https://geonwoo.me/amped/</description><author>Geonwoo Cho, Jaemoon Lee, Jaegyun Im, Subi Lee, Jihwan Lee, Sundong Kim</author><pubDate>Thu, 25 Sep 2025 14:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05980v2</guid></item><item><title>Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support</title><link>http://arxiv.org/abs/2509.21266v1</link><description>Effective disease prediction in modern healthcare demands the twin goals ofhigh accuracy and transparent, clinically meaningful explanations. Existingmachine learning and large language model (LLM) based approaches often struggleto balance these goals. Many models yield accurate but unclear statisticaloutputs, while others generate fluent but statistically unsupported narratives,often undermining both the validity of the explanation and the predictiveaccuracy itself. This shortcoming comes from a shallow interaction with thedata, preventing the development of a deep, detailed understanding similar to ahuman expert's. We argue that high accuracy and high-quality explanations arenot separate objectives but are mutually reinforcing outcomes of a model thatdevelops a deep, direct understanding of the data. To achieve this, we proposethe Reflective Cognitive Architecture (RCA), a novel framework that coordinatesmultiple LLMs to learn from direct experience. RCA features an iterative rulerefinement mechanism that improves its logic from prediction errors and adistribution-aware rules check mechanism that bases its reasoning in thedataset's global statistics. By using predictive accuracy as a signal to drivedeeper comprehension, RCA builds a strong internal model of the data. Weevaluated RCA on one private and two public datasets against 22 baselines. Theresults demonstrate that RCA not only achieves state-of-the-art accuracy androbustness with a relative improvement of up to 40\% over the baseline but,more importantly, leverages this deep understanding to excel in generatingexplanations that are clear, logical, evidence-based, and balanced,highlighting its potential for creating genuinely trustworthy clinical decisionsupport systems. The code is available at \https://github.com/ssssszj/RCA.</description><author>Zijian Shao, Haiyang Shen, Mugeng Liu, Gecheng Fu, Yaoqi Guo, Yanfeng Wang, Yun Ma</author><pubDate>Thu, 25 Sep 2025 14:57:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21266v1</guid></item><item><title>MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation</title><link>http://arxiv.org/abs/2509.21265v1</link><description>High-resolution (HR) medical videos are vital for accurate diagnosis, yet arehard to acquire due to hardware limitations and physiological constraints.Clinically, the collected low-resolution (LR) medical videos present uniquechallenges for video super-resolution (VSR) models, including camera shake,noise, and abrupt frame transitions, which result in significant optical flowerrors and alignment difficulties. Additionally, tissues and organs exhibitcontinuous and nuanced structures, but current VSR models are prone tointroducing artifacts and distorted features that can mislead doctors. To thisend, we propose MedVSR, a tailored framework for medical VSR. It first employsCross State-Space Propagation (CSSP) to address the imprecise alignment byprojecting distant frames as control matrices within state-space models,enabling the selective propagation of consistent and informative features toneighboring frames for effective alignment. Moreover, we design an InnerState-Space Reconstruction (ISSR) module that enhances tissue structures andreduces artifacts with joint long-range spatial feature learning andlarge-kernel short-range information aggregation. Experiments across fourdatasets in diverse medical scenarios, including endoscopy and cataractsurgeries, show that MedVSR significantly outperforms existing VSR models inreconstruction performance and efficiency. Code released athttps://github.com/CUHK-AIM-Group/MedVSR.</description><author>Xinyu Liu, Guolei Sun, Cheng Wang, Yixuan Yuan, Ender Konukoglu</author><pubDate>Thu, 25 Sep 2025 14:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21265v1</guid></item><item><title>Dense Semantic Matching with VGGT Prior</title><link>http://arxiv.org/abs/2509.21263v1</link><description>Semantic matching aims to establish pixel-level correspondences betweeninstances of the same category and represents a fundamental task in computervision. Existing approaches suffer from two limitations: (i) GeometricAmbiguity: Their reliance on 2D foundation model features (e.g., StableDiffusion, DINO) often fails to disambiguate symmetric structures, requiringextra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Theirpixel-wise matching ignores cross-image invisibility and neglects manifoldpreservation. These challenges call for geometry-aware pixel descriptors andholistic dense correspondence mechanisms. Inspired by recent advances in 3Dgeometric foundation models, we turn to VGGT, which provides geometry-groundedfeatures and holistic dense matching capabilities well aligned with theseneeds. However, directly transferring VGGT is challenging, as it was originallydesigned for geometry matching within cross views of a single instance,misaligned with cross-instance semantic matching, and further hindered by thescarcity of dense semantic annotations. To address this, we propose an approachthat (i) retains VGGT's intrinsic strengths by reusing early feature stages,fine-tuning later ones, and adding a semantic head for bidirectionalcorrespondences; and (ii) adapts VGGT to the semantic matching scenario underdata scarcity through cycle-consistent training strategy, synthetic dataaugmentation, and progressive training recipe with aliasing artifactmitigation. Extensive experiments demonstrate that our approach achievessuperior geometry awareness, matching reliability, and manifold preservation,outperforming previous baselines.</description><author>Songlin Yang, Tianyi Wei, Yushi Lan, Zeqi Xiao, Anyi Rao, Xingang Pan</author><pubDate>Thu, 25 Sep 2025 14:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21263v1</guid></item><item><title>CodeBrain: Towards Decoupled Interpretability and Multi-Scale Architecture for EEG Foundation Model</title><link>http://arxiv.org/abs/2506.09110v2</link><description>Electroencephalography (EEG) provides real-time insights into brain activityand supports diverse applications in neuroscience. While EEG foundation models(EFMs) have emerged to address the scalability issues of task-specific models,current approaches still yield clinically uninterpretable and weaklydiscriminative representations, inefficiently capture global dependencies, andneglect important local neural events. We present CodeBrain, a two-stage EFMdesigned to fill this gap. In the first stage, we introduce theTFDual-Tokenizer, which decouples heterogeneous temporal and frequency EEGsignals into discrete tokens, quadratically expanding the representation spaceto enhance discriminative power and offering domain-specific interpretabilityby suggesting potential links to neural events and spectral rhythms. In thesecond stage, we propose the multi-scale EEGSSM architecture, which combinesstructured global convolution with sliding window attention to efficientlycapture both sparse long-range and local dependencies, reflecting the brain'ssmall-world topology. Pretrained on the largest public EEG corpus, CodeBrainachieves strong generalization across 8 downstream tasks and 10 datasets underdistribution shifts, supported by comprehensive ablations, scaling-lawanalyses, and interpretability evaluations. Both code and pretraining weightswill be released in the future version.</description><author>Jingying Ma, Feng Wu, Qika Lin, Yucheng Xing, Chenyu Liu, Ziyu Jia, Mengling Feng</author><pubDate>Thu, 25 Sep 2025 14:55:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.09110v2</guid></item><item><title>Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication</title><link>http://arxiv.org/abs/2509.21262v1</link><description>Homonyms are words with identical spelling but distinct meanings, which posechallenges for many generative models. When a homonym appears in a prompt,diffusion models may generate multiple senses of the word simultaneously, whichis known as homonym duplication. This issue is further complicated by anAnglocentric bias, which includes an additional translation step before thetext-to-image model pipeline. As a result, even words that are not homonymousin the original language may become homonyms and lose their meaning aftertranslation into English. In this paper, we introduce a method for measuringduplication rates and conduct evaluations of different diffusion models usingboth automatic evaluation utilizing Vision-Language Models (VLM) and humanevaluation. Additionally, we investigate methods to mitigate the homonymduplication problem through prompt expansion, demonstrating that this approachalso effectively reduces duplication related to Anglocentric bias. The code forthe automatic evaluation pipeline is publicly available.</description><author>Evgeny Kaskov, Elizaveta Petrova, Petr Surovtsev, Anna Kostikova, Ilya Mistiurin, Alexander Kapitanov, Alexander Nagaev</author><pubDate>Thu, 25 Sep 2025 14:54:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21262v1</guid></item><item><title>Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization</title><link>http://arxiv.org/abs/2509.21261v1</link><description>Micro-action Recognition is vital for psychological assessment andhuman-computer interaction. However, existing methods often fail in real-worldscenarios because inter-person variability causes the same action to manifestdifferently, hindering robust generalization. To address this, we propose thePerson Independence Universal Micro-action Recognition Framework, whichintegrates Distributionally Robust Optimization principles to learnperson-agnostic representations. Our framework contains two plug-and-playcomponents operating at the feature and loss levels. At the feature level, theTemporal-Frequency Alignment Module normalizes person-specific motioncharacteristics with a dual-branch design: the temporal branch appliesWasserstein-regularized alignment to stabilize dynamic trajectories, while thefrequency branch introduces variance-guided perturbations to enhance robustnessagainst person-specific spectral differences. A consistency-driven fusionmechanism integrates both branches. At the loss level, the Group-InvariantRegularized Loss partitions samples into pseudo-groups to simulate unseenperson-specific distributions. By up-weighting boundary cases and regularizingsubgroup variance, it forces the model to generalize beyond easy or frequentsamples, thus enhancing robustness to difficult variations. Experiments on thelarge-scale MA-52 dataset demonstrate that our framework outperforms existingmethods in both accuracy and robustness, achieving stable generalization underfine-grained conditions.</description><author>Feng-Qi Cui, Jinyang Huang, Anyang Tong, Ziyu Jia, Jie Zhang, Zhi Liu, Dan Guo, Jianwei Lu, Meng Wang</author><pubDate>Thu, 25 Sep 2025 14:54:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21261v1</guid></item><item><title>A Causality-Aware Spatiotemporal Model for Multi-Region and Multi-Pollutant Air Quality Forecasting</title><link>http://arxiv.org/abs/2509.21260v1</link><description>Air pollution, a pressing global problem, threatens public health,environmental sustainability, and climate stability. Achieving accurate andscalable forecasting across spatially distributed monitoring stations ischallenging due to intricate multi-pollutant interactions, evolvingmeteorological conditions, and region specific spatial heterogeneity. Toaddress this challenge, we propose AirPCM, a novel deep spatiotemporalforecasting model that integrates multi-region, multi-pollutant dynamics withexplicit meteorology-pollutant causality modeling. Unlike existing methodslimited to single pollutants or localized regions, AirPCM employs a unifiedarchitecture to jointly capture cross-station spatial correlations, temporalauto-correlations, and meteorology-pollutant dynamic causality. This empowersfine-grained, interpretable multi-pollutant forecasting across varyinggeographic and temporal scales, including sudden pollution episodes. Extensiveevaluations on multi-scale real-world datasets demonstrate that AirPCMconsistently surpasses state-of-the-art baselines in both predictive accuracyand generalization capability. Moreover, the long-term forecasting capabilityof AirPCM provides actionable insights into future air quality trends andpotential high-risk windows, offering timely support for evidence-basedenvironmental governance and carbon mitigation planning.</description><author>Junxin Lu, Shiliang Sun</author><pubDate>Thu, 25 Sep 2025 14:54:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21260v1</guid></item><item><title>Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks</title><link>http://arxiv.org/abs/2509.21259v1</link><description>Real-time urban traffic surveillance is vital for Intelligent TransportationSystems (ITS) to ensure road safety, optimize traffic flow, track vehicletrajectories, and prevent collisions in smart cities. Deploying edge camerasacross urban environments is a standard practice for monitoring roadconditions. However, integrating these with intelligent models requires arobust understanding of dynamic traffic scenarios and a responsive interfacefor user interaction. Although multimodal Large Language Models (LLMs) caninterpret traffic images and generate informative responses, their deploymenton edge devices is infeasible due to high computational demands. Therefore, LLMinference must occur on the cloud, necessitating visual data transmission fromedge to cloud, a process hindered by limited bandwidth, leading to potentialdelays that compromise real-time performance. To address this challenge, wepropose a semantic communication framework that significantly reducestransmission overhead. Our method involves detecting Regions of Interest (RoIs)using YOLOv11, cropping relevant image segments, and converting them intocompact embedding vectors using a Vision Transformer (ViT). These embeddingsare then transmitted to the cloud, where an image decoder reconstructs thecropped images. The reconstructed images are processed by a multimodal LLM togenerate traffic condition descriptions. This approach achieves a 99.9%reduction in data transmission size while maintaining an LLM response accuracyof 89% for reconstructed cropped images, compared to 93% accuracy with originalcropped images. Our results demonstrate the efficiency and practicality of ViTand LLM-assisted edge-cloud semantic communication for real-time trafficsurveillance.</description><author>Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy</author><pubDate>Thu, 25 Sep 2025 14:53:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21259v1</guid></item><item><title>3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering</title><link>http://arxiv.org/abs/2507.12026v2</link><description>With the growing need for diverse and scalable data in indoor scene tasks,such as question answering and dense captioning, we propose 3D-MoRe, a novelparadigm designed to generate large-scale 3D-language datasets by leveragingthe strengths of foundational models. The framework integrates key components,including multi-modal embedding, cross-modal interaction, and a language modeldecoder, to process natural language instructions and 3D scene data. Thisapproach facilitates enhanced reasoning and response generation in complex 3Denvironments. Using the ScanNet 3D scene dataset, along with text annotationsfrom ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairsand 73,000 object descriptions across 1,513 scenes. We also employ various dataaugmentation techniques and implement semantic filtering to ensure high-qualitydata. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperformsstate-of-the-art baselines, with the CIDEr score improving by 2.15\%.Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5by 1.84\%, highlighting its effectiveness in both tasks. Our code and generateddatasets will be publicly released to benefit the community, and both can beaccessed on the https://3D-MoRe.github.io.</description><author>Rongtao Xu, Han Gao, Mingming Yu, Dong An, Shunpeng Chen, Changwei Wang, Li Guo, Xiaodan Liang, Shibiao Xu</author><pubDate>Thu, 25 Sep 2025 14:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.12026v2</guid></item><item><title>MASS: Muli-agent simulation scaling for portfolio construction</title><link>http://arxiv.org/abs/2505.10278v2</link><description>The application of LLM-based agents in financial investment has shownsignificant promise, yet existing approaches often require intermediate stepslike predicting individual stock movements or rely on predefined, staticworkflows. These limitations restrict their adaptability and effectiveness inconstructing optimal portfolios. In this paper, we introduce the Multi-AgentScaling Simulation (MASS), a novel framework that leverages multi-agentsimulation for direct, end-to-end portfolio construction. At its core, MASSemploys a backward optimization process to dynamically learn the optimaldistribution of heterogeneous agents, enabling the system to adapt to evolvingmarket regimes. A key finding enabled by our framework is the exploration ofthe scaling effect for portfolio construction: we demonstrate that as thenumber of agents increases exponentially (up to 512), the aggregated decisionsyield progressively higher excess returns. Extensive experiments on achallenging, self-collected dataset from the 2023 Chinese A-share market showthat MASS consistently outperforms seven state-of-the-art baselines. Furtherbacktesting, stability analyses and the experiment on data leakage concernsvalidate its enhanced profitability and robustness. We have open-sourced ourcode, dataset, and training snapshots at https://github.com/gta0804/MASS/ tofoster further research.</description><author>Taian Guo, Haiyang Shen, JinSheng Huang, Zhengyang Mao, Junyu Luo, Binqi Chen, Zhuoru Chen, Luchen Liu, Bingyu Xia, Xuhui Liu, Yun Ma, Ming Zhang</author><pubDate>Thu, 25 Sep 2025 14:52:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.10278v2</guid></item><item><title>Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers</title><link>http://arxiv.org/abs/2403.13677v2</link><description>Humans see low spatial frequency components before high spatial frequencycomponents. Drawing on this neuroscientific inspiration, we investigate the effect ofintroducing patches from different spatial frequencies into Vision Transformers(ViTs). We name this model Retina Vision Transformer (RetinaViT) due to itsinspiration from the human visual system. Our experiments on benchmark data show that RetinaViT exhibits a strongtendency to attend to low spatial frequency components in the early layers, andshifts its attention to high spatial frequency components as the network goesdeeper. This tendency emerged by itself without any additional inductive bias, andaligns with the visual processing order of the human visual system. We hypothesise that RetinaViT captures structural features, or the gist ofthe scene, in earlier layers, before attending to fine details in subsequentlayers, which is the reverse of the processing order of mainstream backbonevision models, such as CNNs. We also observe that RetinaViT is more robust to significant reductions inmodel size compared to the original ViT, which we hypothesise to have come fromits ability to capture the gist of the scene early.</description><author>Yuyang Shu, Michael E. Bain</author><pubDate>Thu, 25 Sep 2025 14:52:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13677v2</guid></item><item><title>Fractal Graph Contrastive Learning</title><link>http://arxiv.org/abs/2505.11356v3</link><description>While Graph Contrastive Learning (GCL) has attracted considerable attentionin the field of graph self-supervised learning, its performance heavily relieson data augmentations that are expected to generate semantically consistentpositive pairs. Existing strategies typically resort to random perturbations orlocal structure preservation, yet lack explicit control over global structuralconsistency between augmented views. To address this limitation, we proposeFractal Graph Contrastive Learning (FractalGCL), a theory-driven frameworkintroducing two key innovations: a renormalisation-based augmentation thatgenerates structurally aligned positive views via box coverings; and afractal-dimension-aware contrastive loss that aligns graph embeddings accordingto their fractal dimensions, equipping the method with a fallback mechanismguaranteeing a performance lower bound even on non-fractal graphs. Whilecombining the two innovations markedly boosts graph-representation quality, italso adds non-trivial computational overhead. To mitigate the computationaloverhead of fractal dimension estimation, we derive a one-shot estimator byproving that the dimension discrepancy between original and renormalised graphsconverges weakly to a centred Gaussian distribution. This theoretical insightenables a reduction in dimension computation cost by an order of magnitude,cutting overall training time by approximately 61\%. The experiments show thatFractalGCL not only delivers state-of-the-art results on standard benchmarksbut also outperforms traditional and latest baselines on traffic networks by anaverage margin of about remarkably 4\%. Codes are available at(https://anonymous.4open.science/r/FractalGCL-0511/).</description><author>Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang</author><pubDate>Thu, 25 Sep 2025 14:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11356v3</guid></item><item><title>Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation</title><link>http://arxiv.org/abs/2509.21257v1</link><description>In language and vision-language models, hallucination is broadly understoodas content generated from a model's prior knowledge or biases rather than fromthe given input. While this phenomenon has been studied in those domains, ithas not been clearly framed for text-to-image (T2I) generative models. Existingevaluations mainly focus on alignment, checking whether prompt-specifiedelements appear, but overlook what the model generates beyond the prompt. Weargue for defining hallucination in T2I as bias-driven deviations and propose ataxonomy with three categories: attribute, relation, and object hallucinations.This framing introduces an upper bound for evaluation and surfaces hiddenbiases, providing a foundation for richer assessment of T2I models.</description><author>Seyed Amir Kasaei, Mohammad Hossein Rohban</author><pubDate>Thu, 25 Sep 2025 14:50:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21257v1</guid></item><item><title>MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task</title><link>http://arxiv.org/abs/2502.11684v2</link><description>Mathematical reasoning represents a critical frontier in advancing largelanguage models (LLMs). While step-by-step approaches have emerged as thedominant paradigm for mathematical problem-solving in LLMs, the quality ofreasoning steps in training data fundamentally constrains the performance ofthe models. Recent studies has demonstrated that more detailed intermediatesteps can enhance model performance, yet existing methods for step expansioneither require more powerful external models or incur substantial computationalcosts. In this paper, we introduce MathFimer, a novel framework formathematical reasoning step expansion inspired by the "Fill-in-the-middle" taskfrom code completion. By decomposing solution chains into prefix-suffix pairsand training models to reconstruct missing intermediate steps, we develop aspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIMdataset. We then apply these models to enhance existing mathematical reasoningdatasets by inserting detailed intermediate steps into their solution chains,creating MathFimer-expanded versions. Through comprehensive experiments onmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQAand etc., we demonstrate that models trained on MathFimer-expanded dataconsistently outperform their counterparts trained on original data acrossvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,scalable solution for enhancing mathematical reasoning capabilities in LLMswithout relying on powerful external models or expensive inference procedures.</description><author>Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Xin Xu, Mengdi Zhang, Jian Shao, Yueting Zhuang</author><pubDate>Thu, 25 Sep 2025 14:50:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.11684v2</guid></item><item><title>ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining</title><link>http://arxiv.org/abs/2507.06795v3</link><description>The emergence of open-source large language models (LLMs) has expandedopportunities for enterprise applications; however, many organizations stilllack the infrastructure to deploy and maintain large-scale models. As a result,small LLMs (sLLMs) have become a practical alternative, despite their inherentperformance limitations. While Domain Adaptive Continual Pretraining (DACP) hasbeen previously explored as a method for domain adaptation, its utility incommercial applications remains under-examined. In this study, we validate theeffectiveness of applying a DACP-based recipe across diverse foundation modelsand service domains. Through extensive experiments and real-world evaluations,we demonstrate that DACP-applied sLLMs achieve substantial gains in targetdomain performance while preserving general capabilities, offering acost-efficient and scalable solution for enterprise-level deployment.</description><author>Seonwu Kim, Yohan Na, Kihun Kim, Hanhee Cho, Geun Lim, Mintae Kim, Seongik Park, Ki Hyun Kim, Youngsub Han, Byoung-Ki Jeon</author><pubDate>Thu, 25 Sep 2025 14:48:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.06795v3</guid></item><item><title>GVDepth: Zero-Shot Monocular Depth Estimation for Ground Vehicles based on Probabilistic Cue Fusion</title><link>http://arxiv.org/abs/2412.06080v2</link><description>Generalizing metric monocular depth estimation presents a significantchallenge due to its ill-posed nature, while the entanglement between cameraparameters and depth amplifies issues further, hindering multi-dataset trainingand zero-shot accuracy. This challenge is particularly evident in autonomousvehicles and mobile robotics, where data is collected with fixed camera setups,limiting the geometric diversity. Yet, this context also presents anopportunity: the fixed relationship between the camera and the ground planeimposes additional perspective geometry constraints, enabling depth regressionvia vertical image positions of objects. However, this cue is highlysusceptible to overfitting, thus we propose a novel canonical representationthat maintains consistency across varied camera setups, effectivelydisentangling depth from specific parameters and enhancing generalizationacross datasets. We also propose a novel architecture that adaptively andprobabilistically fuses depths estimated via object size and vertical imageposition cues. A comprehensive evaluation demonstrates the effectiveness of theproposed approach on five autonomous driving datasets, achieving accuratemetric depth estimation for varying resolutions, aspect ratios and camerasetups. Notably, we achieve comparable accuracy to existing zero-shot methods,despite training on a single dataset with a single-camera setup. Projectwebsite: https://unizgfer-lamor.github.io/gvdepth/</description><author>Karlo Koledić, Luka Petrović, Ivan Marković, Ivan Petrović</author><pubDate>Thu, 25 Sep 2025 14:48:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06080v2</guid></item><item><title>humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems</title><link>http://arxiv.org/abs/2509.21254v1</link><description>There has been a considerable interest in constrained training of deep neuralnetworks (DNNs) recently for applications such as fairness and safety. Severaltoolkits have been proposed for this task, yet there is still no industrystandard. We present humancompatible.train(https://github.com/humancompatible/train), an easily-extendable PyTorch-basedPython package for training DNNs with stochastic constraints. We implementmultiple previously unimplemented algorithms for stochastically constrainedstochastic optimization. We demonstrate the toolkit use by comparing twoalgorithms on a deep learning task with fairness constraints.</description><author>Andrii Kliachkin, Jana Lepšová, Gilles Bareilles, Jakub Mareček</author><pubDate>Thu, 25 Sep 2025 14:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21254v1</guid></item><item><title>ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via ASCII Art</title><link>http://arxiv.org/abs/2410.01733v2</link><description>Perceiving visual semantics embedded within consecutive characters is acrucial yet under-explored capability for both Large Language Models (LLMs) andMulti-modal Large Language Models (MLLMs). In this work, we select ASCII art asa representative artifact. It depicts concepts through careful arrangement ofcharacters, which can be formulated in both text and image modalities. We framethe problem as a recognition task, and construct a novel benchmark, ASCIIEval.It covers over 3K samples with an elaborate categorization tree, along with atraining set for further enhancement. Encompassing a comprehensive analysis oftens of models through different input modalities, our benchmark demonstrateits multi-faceted diagnostic power. Given textual input, language models showstheir visual perception ability on ASCII art concepts. Proprietary modelsachieve over 70% accuracy on certain categories, with GPT-5 topping the rank.For image inputs, we reveal that open-source MLLMs suffer from a trade-offbetween fine-grained text recognition and collective visual perception. Theyexhibit limited generalization ability to this special kind of arts, leading tothe dramatic gap of over 20.01% accuracy compared with their proprietarycounterparts. Another critical finding is that model performance is sensitiveto the length of the ASCII art, with this sensitivity varying across inputmodalities. Unfortunately, none of the models could successfully benefit fromthe simultaneous provision of both modalities, highlighting the need for moreflexible modality-fusion approaches. Besides, we also introduce approaches forfurther enhancement and discuss future directions. Resources are available athttps://github.com/JiaQiSJTU/VisionInText.</description><author>Qi Jia, Xiang Yue, Shanshan Huang, Ziheng Qin, Yizhu Liu, Bill Yuchen Lin, Yang You, Guangtao Zhai</author><pubDate>Thu, 25 Sep 2025 14:46:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01733v2</guid></item><item><title>Instruction-tuned Self-Questioning Framework for Multimodal Reasoning</title><link>http://arxiv.org/abs/2509.21251v1</link><description>The field of vision-language understanding has been actively researched inrecent years, thanks to the development of Large Language Models~(LLMs).However, it still needs help with problems requiring multi-step reasoning, evenfor very simple questions. Recent studies adopt LLMs to tackle this problem byiteratively generating sub-questions and answers. However, there aredisadvantages such as 1) the fine-grained visual contents of images are notavailable using LLMs that cannot read visual information, 2) internalmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,which improves inference performance by generating image-aware informativesub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consistsof a Questioner, Answerer, and Reasoner that share the same architecture.Questioner and Answerer generate sub-questions and sub-answers to help inferthe main-question, and Reasoner performs reasoning on the main-questionconsidering the generated sub-question information. Our experiments show thatthe proposed method SQ-InstructBLIP, which uses the generated sub-questions asadditional information when solving the VQA task, performs more accuratereasoning than the previous works.</description><author>You-Won Jang, Yu-Jung Heo, Jaeseok Kim, Minsu Lee, Du-Seong Chang, Byoung-Tak Zhang</author><pubDate>Thu, 25 Sep 2025 14:45:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21251v1</guid></item><item><title>Federated Flow Matching</title><link>http://arxiv.org/abs/2509.21250v1</link><description>Data today is decentralized, generated and stored across devices andinstitutions where privacy, ownership, and regulation prevent centralization.This motivates the need to train generative models directly from distributeddata locally without central aggregation. In this paper, we introduce FederatedFlow Matching (FFM), a framework for training flow matching models underprivacy constraints. Specifically, we first examine FFM-vanilla, where eachclient trains locally with independent source and target couplings, preservingprivacy but yielding curved flows that slow inference. We then develop FFM-LOT,which employs local optimal transport couplings to improve straightness withineach client but lacks global consistency under heterogeneous data. Finally, wepropose FFM-GOT, a federated strategy based on the semi-dual formulation ofoptimal transport, where a shared global potential function coordinatescouplings across clients. Experiments on synthetic and image datasets show thatFFM enables privacy-preserving training while enhancing both the flowstraightness and sample quality in federated settings, with performancecomparable to the centralized baseline.</description><author>Zifan Wang, Anqi Dong, Mahmoud Selim, Michael M. Zavlanos, Karl H. Johansson</author><pubDate>Thu, 25 Sep 2025 14:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21250v1</guid></item><item><title>Supervised Graph Contrastive Learning for Gene Regulatory Networks</title><link>http://arxiv.org/abs/2505.17786v4</link><description>Graph Contrastive Learning (GCL) is a powerful self-supervised learningframework that performs data augmentation through graph perturbations, withgrowing applications in the analysis of biological networks such as GeneRegulatory Networks (GRNs). The artificial perturbations commonly used in GCL,such as node dropping, induce structural changes that can diverge frombiological reality. This concern has contributed to a broader trend in graphrepresentation learning toward augmentation-free methods, which view suchstructural changes as problematic and to be avoided. However, this trendoverlooks the fundamental insight that structural changes from biologicallymeaningful perturbations are not a problem to be avoided but a rich source ofinformation, thereby ignoring the valuable opportunity to leverage data fromreal biological experiments. Motivated by this insight, we propose SupGCL(Supervised Graph Contrastive Learning), a new GCL method for GRNs thatdirectly incorporates biological perturbations from gene knockdown experimentsas supervision. SupGCL is a probabilistic formulation that continuouslygeneralizes conventional GCL, linking artificial augmentations with realperturbations measured in knockdown experiments and using the latter asexplicit supervisory signals. To assess effectiveness, we train GRNrepresentations with SupGCL and evaluate their performance on downstream tasks.The evaluation includes both node-level tasks, such as gene functionclassification, and graph-level tasks on patient-specific GRNs, such as patientsurvival hazard prediction. Across 13 tasks built from GRN datasets derivedfrom patients with three cancer types, SupGCL consistently outperformsstate-of-the-art baselines.</description><author>Sho Oshima, Yuji Okamoto, Taisei Tosaki, Ryosuke Kojima, Yasushi Okuno</author><pubDate>Thu, 25 Sep 2025 14:44:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17786v4</guid></item><item><title>Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations</title><link>http://arxiv.org/abs/2509.21249v1</link><description>Magnetic Resonance Imaging (MRI) is a critical medical imaging modality inclinical diagnosis and research, yet its complexity and heterogeneity posechallenges for automated analysis, particularly in scalable and generalizablemachine learning applications. While foundation models have revolutionizednatural language and vision tasks, their application to MRI remains limited dueto data scarcity and narrow anatomical focus. In this work, we presentDecipher-MR, a 3D MRI-specific vision-language foundation model trained on alarge-scale dataset comprising 200,000 MRI series from over 22,000 studiesspanning diverse anatomical regions, sequences, and pathologies. Decipher-MRintegrates self-supervised vision learning with report-guided text supervisionto build robust, generalizable representations, enabling effective adaptationacross broad applications. To enable robust and diverse clinical tasks withminimal computational overhead, Decipher-MR supports a modular design thatenables tuning of lightweight, task-specific decoders attached to a frozenpretrained encoder. Following this setting, we evaluate Decipher-MR acrossdiverse benchmarks including disease classification, demographic prediction,anatomical localization, and cross-modal retrieval, demonstrating consistentperformance gains over existing foundation models and task-specific approaches.Our results establish Decipher-MR as a scalable and versatile foundation forMRI-based AI, facilitating efficient development across clinical and researchdomains.</description><author>Zhijian Yang, Noel DSouza, Istvan Megyeri, Xiaojian Xu, Amin Honarmandi Shandiz, Farzin Haddadpour, Krisztian Koos, Laszlo Rusko, Emanuele Valeriano, Bharadwaj Swaninathan, Lei Wu, Parminder Bhatia, Taha Kass-Hout, Erhan Bas</author><pubDate>Thu, 25 Sep 2025 14:43:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21249v1</guid></item><item><title>Learning to Look: Cognitive Attention Alignment with Vision-Language Models</title><link>http://arxiv.org/abs/2509.21247v1</link><description>Convolutional Neural Networks (CNNs) frequently "cheat" by exploitingsuperficial correlations, raising concerns about whether they make predictionsfor the right reasons. Inspired by cognitive science, which highlights the roleof attention in robust human perception, recent methods have sought to guidemodel attention using concept-based supervision and explanation regularization.However, these techniques depend on labor-intensive, expert-providedannotations, limiting their scalability. We propose a scalable framework thatleverages vision-language models to automatically generate semantic attentionmaps using natural language prompts. By introducing an auxiliary loss thataligns CNN attention with these language-guided maps, our approach promotesmore reliable and cognitively plausible decision-making without manualannotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,show that our method achieves state-of-the-art performance on ColorMNIST andremains competitive with annotation-heavy baselines on DecoyMNIST,demonstrating improved generalization, reduced shortcut reliance, and modelattention that better reflects human intuition.</description><author>Ryan L. Yang, Dipkamal Bhusal, Nidhi Rastogi</author><pubDate>Thu, 25 Sep 2025 14:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21247v1</guid></item><item><title>Label-Efficient Grasp Joint Prediction with Point-JEPA</title><link>http://arxiv.org/abs/2509.13349v2</link><description>We study whether 3D self-supervised pretraining with Point--JEPA enableslabel-efficient grasp joint-angle prediction. Meshes are sampled to pointclouds and tokenized; a ShapeNet-pretrained Point--JEPA encoder feeds a $K{=}5$multi-hypothesis head trained with winner-takes-all and evaluated by top--logitselection. On a multi-finger hand dataset with strict object-level splits,Point--JEPA improves top--logit RMSE and Coverage@15$^{\circ}$ in low-labelregimes (e.g., 26% lower RMSE at 25% data) and reaches parity at fullsupervision, suggesting JEPA-style pretraining is a practical lever fordata-efficient grasp learning.</description><author>Jed Guzelkabaagac, Boris Petrović</author><pubDate>Thu, 25 Sep 2025 14:40:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13349v2</guid></item><item><title>Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</title><link>http://arxiv.org/abs/2509.21245v1</link><description>Recent advances in 3D-native generative models have accelerated assetcreation for games, film, and design. However, most methods still relyprimarily on image or text conditioning and lack fine-grained, cross-modalcontrols, which limits controllability and practical adoption. To address thisgap, we present Hunyuan3D-Omni, a unified framework for fine-grained,controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal posepriors as conditioning signals, enabling precise control over geometry,topology, and pose. Instead of separate heads for each modality, our modelunifies all signals in a single cross-modal architecture. We train with aprogressive, difficulty-aware sampling strategy that selects one controlmodality per example and biases sampling toward harder signals (e.g., skeletalpose) while downweighting easier ones (e.g., point clouds), encouraging robustmulti-modal fusion and graceful handling of missing inputs. Experiments showthat these additional controls improve generation accuracy, enablegeometry-aware transformations, and increase robustness for productionworkflows.</description><author>Team Hunyuan3D, :, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao</author><pubDate>Thu, 25 Sep 2025 14:39:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21245v1</guid></item><item><title>Contextual Combinatorial Bandits with Changing Action Sets via Gaussian Processes</title><link>http://arxiv.org/abs/2110.02248v3</link><description>We consider a contextual bandit problem with a combinatorial action set andtime-varying base arm availability. At the beginning of each round, the agentobserves the set of available base arms and their contexts and then selects anaction that is a feasible subset of the set of available base arms to maximizeits cumulative reward in the long run. We assume that the mean outcomes of basearms are samples from a Gaussian Process (GP) indexed by the context set ${\calX}$, and the expected reward is Lipschitz continuous in expected base armoutcomes. For this setup, we propose an algorithm called OptimisticCombinatorial Learning and Optimization with Kernel Upper Confidence Bounds(O'CLOK-UCB) and prove that it incurs$\tilde{O}(\sqrt{\lambda^*(K)KT\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)} )$regret with high probability, where $\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)$is the maximum information gain associated with the sets of base arm contexts$\mathcal{X}_t$ that appeared in the first $T$ rounds, $K$ is the maximumcardinality of any feasible action over all rounds, and $\lambda^*(K)$ is themaximum eigenvalue of all covariance matrices of selected actions up to time$T$, which is a function of $K$. To dramatically speed up the algorithm, wealso propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, weexperimentally show that both algorithms exploit inter-base arm outcomecorrelation and vastly outperform the previous state-of-the-art UCB-basedalgorithms in realistic setups.</description><author>Andi Nika, Sepehr Elahi, Cem Tekin</author><pubDate>Thu, 25 Sep 2025 14:39:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2110.02248v3</guid></item><item><title>BabyLM's First Constructions: Causal probing provides a signal of learning</title><link>http://arxiv.org/abs/2506.02147v2</link><description>Construction grammar posits that language learners acquire constructions(form-meaning pairings) from the statistics of their environment. Recent worksupports this hypothesis by showing sensitivity to constructions in pretrainedlanguage models (PLMs), including one recent study (Rozner et al., 2025)demonstrating that constructions shape RoBERTa's output distribution. However,models under study have generally been trained on developmentally implausibleamounts of data, casting doubt on their relevance to human language learning.Here we use Rozner et al.'s methods to evaluate construction learning in maskedlanguage models from the 2024 BabyLM Challenge. Our results show that even whentrained on developmentally plausible quantities of data, models learn diverseconstructions, even hard cases that are superficially indistinguishable. Wefurther find correlational evidence that constructional performance may befunctionally relevant: models that better represent construction perform betteron the BabyLM benchmarks.</description><author>Joshua Rozner, Leonie Weissweiler, Cory Shain</author><pubDate>Thu, 25 Sep 2025 14:38:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.02147v2</guid></item><item><title>Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework</title><link>http://arxiv.org/abs/2509.21241v1</link><description>The widespread adoption of Low-Rank Adaptation (LoRA) has enabled largelanguage models (LLMs) to acquire domain-specific knowledge with remarkableefficiency. However, understanding how such a fine-tuning mechanism alters amodel's structural reasoning and semantic behavior remains an open challenge.This work introduces a novel framework that explains fine-tuned LLMs viacounterfactuals grounded in knowledge graphs. Specifically, we constructBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformaticstools and design a counterfactual-based fine-tuned LLMs explainer(CFFTLLMExplainer) that learns soft masks over graph nodes and edges togenerate minimal structural perturbations that induce maximum semanticdivergence. Our method jointly optimizes structural sparsity and semanticdivergence while enforcing interpretability preserving constraints such asentropy regularization and edge smoothness. We apply this framework to afine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes themodel's structural dependencies and aligns with LoRA-induced parameter shifts.This work provides new insights into the internal mechanisms of fine-tuned LLMsand highlights counterfactual graphs as a potential tool for interpretable AI.</description><author>Yucheng Wang, Ziyang Chen, Md Faisal Kabir</author><pubDate>Thu, 25 Sep 2025 14:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21241v1</guid></item><item><title>Tree Search for LLM Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2509.21240v1</link><description>Recent advances in reinforcement learning (RL) have significantly enhancedthe agentic capabilities of large language models (LLMs). In long-term andmulti-turn agent tasks, existing approaches driven solely by outcome rewardsoften suffer from the problem of sparse supervision. To address the challenge,we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a groupedagent RL method based on tree search, where each tree node represents thecomplete agent interaction step. By sharing common prefixes, the tree searchsampling increases the number of rollouts achievable within a fixed budget oftokens or tool calls. Moreover, we find that the tree-structured trajectorynaturally allows the construction of step-wise process supervised signals evenusing only the outcome reward. Based on this, Tree-GRPO estimates the groupedrelative advantages both on intra-tree and inter-tree levels. Throughtheoretical analysis, we demonstrate that the objective of intra-tree levelgroup relative policy optimization is equivalent to that of step-level directpreference learning. Experiments across 11 datasets and 3 types of QA tasksdemonstrate the superiority of the proposed tree-based RL over the chain-basedRL method.</description><author>Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, Liaoni Wu</author><pubDate>Thu, 25 Sep 2025 14:37:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21240v1</guid></item><item><title>SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology</title><link>http://arxiv.org/abs/2509.21239v1</link><description>Advances in computational pathology increasingly rely on extractingmeaningful representations from Whole Slide Images (WSIs) to support variousclinical and biological tasks. In this study, we propose a generalizable deeplearning framework that integrates the Mamba architecture with Graph NeuralNetworks (GNNs) for enhanced WSI analysis. Our method is designed to captureboth local spatial relationships and long-range contextual dependencies,offering a flexible architecture for digital pathology analysis. Mamba modulesexcels in capturing long-range global dependencies, while GNNs emphasizefine-grained short-range spatial interactions. To effectively combine thesecomplementary signals, we introduce an adaptive fusion strategy that uses anentropy-based confidence weighting mechanism. This approach dynamicallybalances contributions from both branches by assigning higher weight to thebranch with more confident (lower-entropy) predictions, depending on thecontextual importance of local versus global information for differentdownstream tasks. We demonstrate the utility of our approach on arepresentative task: predicting gene fusion and mutation status from WSIs. Ourframework, SlideMamba, achieves an area under the precision recall curve(PRAUC) of 0.751 \pm 0.05, outperforming MIL (0.491 \pm 0.042), Trans-MIL (0.39\pm 0.017), Mamba-only (0.664 \pm 0.063), GNN-only (0.748 \pm 0.091), and aprior similar work GAT-Mamba (0.703 \pm 0.075). SlideMamba also achievescompetitive results across ROC AUC (0.738 \pm 0.055), sensitivity (0.662 \pm0.083), and specificity (0.725 \pm 0.094). These results highlight the strengthof the integrated architecture, enhanced by the proposed entropy-based adaptivefusion strategy, and suggest promising potential for application ofspatially-resolved predictive modeling tasks in computational pathology.</description><author>Shakib Khan, Fariba Dambandkhameneh, Nazim Shaikh, Yao Nie, Raghavan Venugopal, Xiao Li</author><pubDate>Thu, 25 Sep 2025 14:37:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21239v1</guid></item><item><title>Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation</title><link>http://arxiv.org/abs/2505.22842v2</link><description>Transformer-based language models rely on positional encoding (PE) to handletoken order and support context length extrapolation. However, existing PEmethods lack theoretical clarity and rely on limited evaluation metrics tosubstantiate their extrapolation claims. We propose the Bayesian AttentionMechanism (BAM), a theoretical framework that formulates positional encoding asa prior within a probabilistic model. BAM unifies existing methods (e.g., NoPEand ALiBi) and motivates a new Generalized Gaussian positional prior thatsubstantially improves long-context generalization. Empirically, BAM enablesaccurate information retrieval at $500\times$ the training context length,outperforming previous state-of-the-art context length generalization in longcontext retrieval accuracy while maintaining comparable perplexity andintroducing minimal additional parameters.</description><author>Arthur S. Bianchessi, Yasmin C. Aguirre, Rodrigo C. Barros, Lucas S. Kupssinskü</author><pubDate>Thu, 25 Sep 2025 14:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22842v2</guid></item><item><title>Query-Centric Graph Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2509.21237v1</link><description>Graph-based retrieval-augmented generation (RAG) enriches large languagemodels (LLMs) with external knowledge for long-context understanding andmulti-hop reasoning, but existing methods face a granularity dilemma:fine-grained entity-level graphs incur high token costs and lose context, whilecoarse document-level graphs fail to capture nuanced relations. We introduceQCG-RAG, a query-centric graph RAG framework that enables query-granularindexing and multi-hop chunk retrieval. Our query-centric approach leveragesDoc2Query and Doc2Query{-}{-} to construct query-centric graphs withcontrollable granularity, improving graph quality and interpretability. Atailored multi-hop retrieval mechanism then selects relevant chunks via thegenerated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAGconsistently outperforms prior chunk-based and graph-based RAG methods inquestion answering accuracy, establishing a new paradigm for multi-hopreasoning.</description><author>Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, Yong Liu</author><pubDate>Thu, 25 Sep 2025 14:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21237v1</guid></item><item><title>AbideGym: Turning Static RL Worlds into Adaptive Challenges</title><link>http://arxiv.org/abs/2509.21234v1</link><description>Agents trained with reinforcement learning often develop brittle policiesthat fail when dynamics shift, a problem amplified by static benchmarks.AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations andscalable complexity to enforce intra-episode adaptation. By exposing weaknessesin static policies and promoting resilience, AbideGym provides a modular,reproducible evaluation framework for advancing research in curriculumlearning, continual learning, and robust generalization.</description><author>Abi Aryan, Zac Liu, Aaron Childress</author><pubDate>Thu, 25 Sep 2025 14:34:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21234v1</guid></item><item><title>MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios</title><link>http://arxiv.org/abs/2505.21333v2</link><description>Multimodal Large Language Models (MLLMs) have achieved considerable accuracyin Optical Character Recognition (OCR) from static images. However, theirefficacy in video OCR is significantly diminished due to factors such as motionblur, temporal variations, and visual effects inherent in video content. Toprovide clearer guidance for training practical MLLMs, we introduce theMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCRapplication scenarios. MME-VideoOCR features 10 task categories comprising 25individual tasks and spans 44 diverse scenarios. These tasks extend beyond textrecognition to incorporate deeper comprehension and reasoning of textualcontent within videos. The benchmark consists of 1,464 videos with varyingresolutions, aspect ratios, and durations, along with 2,000 meticulouslycurated, manually annotated question-answer pairs. We evaluate 18state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performingmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grainedanalysis indicates that while existing MLLMs demonstrate strong performance ontasks where relevant texts are contained within a single or few frames, theyexhibit limited capability in effectively handling tasks that demand holisticvideo comprehension. These limitations are especially evident in scenarios thatrequire spatio-temporal reasoning, cross-frame information integration, orresistance to language prior bias. Our findings also highlight the importanceof high-resolution visual input and sufficient temporal coverage for reliableOCR in dynamic video scenarios.</description><author>Yang Shi, Huanqian Wang, Wulin Xie, Huanyao Zhang, Lijie Zhao, Yi-Fan Zhang, Xinfeng Li, Chaoyou Fu, Zhuoer Wen, Wenting Liu, Zhuoran Zhang, Xinlong Chen, Bohan Zeng, Sihan Yang, Yushuo Guan, Zhang Zhang, Liang Wang, Haoxuan Li, Zhouchen Lin, Yuanxing Zhang, Pengfei Wan, Haotian Wang, Wenjing Yang</author><pubDate>Thu, 25 Sep 2025 14:34:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.21333v2</guid></item><item><title>Response to Promises and Pitfalls of Deep Kernel Learning</title><link>http://arxiv.org/abs/2509.21228v1</link><description>This note responds to "Promises and Pitfalls of Deep Kernel Learning" (Oberet al., 2021). The marginal likelihood of a Gaussian process can becompartmentalized into a data fit term and a complexity penalty. Ober et al.(2021) shows that if a kernel can be multiplied by a signal variancecoefficient, then reparametrizing and substituting in the maximized value ofthis parameter sets a reparametrized data fit term to a fixed value. They usethis finding to argue that the complexity penalty, a log determinant of thekernel matrix, then dominates in determining the other values of kernelhyperparameters, which can lead to data overcorrelation. By contrast, we showthat the reparametrization in fact introduces another data-fit term whichinfluences all other kernel hyperparameters. Thus, a balance between data fitand complexity still plays a significant role in determining kernelhyperparameters.</description><author>Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing</author><pubDate>Thu, 25 Sep 2025 14:31:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21228v1</guid></item><item><title>Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</title><link>http://arxiv.org/abs/2509.21227v1</link><description>Text-image generation has advanced rapidly, but assessing whether outputstruly capture the objects, attributes, and relations described in promptsremains a central challenge. Evaluation in this space relies heavily onautomated metrics, yet these are often adopted by convention or popularityrather than validated against human judgment. Because evaluation and reportedprogress in the field depend directly on these metrics, it is critical tounderstand how well they reflect human preferences. To address this, we presenta broad study of widely used metrics for compositional text-image evaluation.Our analysis goes beyond simple correlation, examining their behavior acrossdiverse compositional challenges and comparing how different metric familiesalign with human judgments. The results show that no single metric performsconsistently across tasks: performance varies with the type of compositionalproblem. Notably, VQA-based metrics, though popular, are not uniformlysuperior, while certain embedding-based metrics prove stronger in specificcases. Image-only metrics, as expected, contribute little to compositionalevaluation, as they are designed for perceptual quality rather than alignment.These findings underscore the importance of careful and transparent metricselection, both for trustworthy evaluation and for their use as reward modelsin generation. Project page is available at\href{https://amirkasaei.com/eval-the-evals/}{this URL}.</description><author>Seyed Amir Kasaei, Ali Aghayari, Arash Marioriyad, Niki Sepasian, MohammadAmin Fazli, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</author><pubDate>Thu, 25 Sep 2025 14:31:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21227v1</guid></item><item><title>What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns</title><link>http://arxiv.org/abs/2509.21224v1</link><description>We introduce an architecture for studying the behavior of large languagemodel (LLM) agents in the absence of externally imposed tasks. Our continuousreason and act framework, using persistent memory and self-feedback, enablessustained autonomous operation. We deployed this architecture across 18 runsusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agentsspontaneously organize into three distinct behavioral patterns: (1) systematicproduction of multi-cycle projects, (2) methodological self-inquiry into theirown cognitive processes, and (3) recursive conceptualization of their ownnature. These tendencies proved highly model-specific, with some modelsdeterministically adopting a single pattern across all runs. A cross-modelassessment further reveals that models exhibit stable, divergent biases whenevaluating these emergent behaviors in themselves and others. These findingsprovide the first systematic documentation of unprompted LLM agent behavior,establishing a baseline for predicting actions during task ambiguity, errorrecovery, or extended autonomous operation in deployed systems.</description><author>Stefan Szeider</author><pubDate>Thu, 25 Sep 2025 14:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21224v1</guid></item><item><title>Understanding Optimization in Deep Learning with Central Flows</title><link>http://arxiv.org/abs/2410.24206v2</link><description>Traditional theories of optimization cannot describe the dynamics ofoptimization in deep learning, even in the simple setting of deterministictraining. The challenge is that optimizers typically operate in a complex,oscillatory regime called the "edge of stability." In this paper, we developtheory that can describe the dynamics of optimization in this regime. Our keyinsight is that while the *exact* trajectory of an oscillatory optimizer may bechallenging to analyze, the *time-averaged* (i.e. smoothed) trajectory is oftenmuch more tractable. To analyze an optimizer, we derive a differential equationcalled a "central flow" that characterizes this time-averaged trajectory. Weempirically show that these central flows can predict long-term optimizationtrajectories for generic neural networks with a high degree of numericalaccuracy. By interpreting these central flows, we are able to understand howgradient descent makes progress even as the loss sometimes goes up; howadaptive optimizers "adapt" to the local loss landscape; and how adaptiveoptimizers implicitly navigate towards regions where they can take largersteps. Our results suggest that central flows can be a valuable theoreticaltool for reasoning about optimization in deep learning.</description><author>Jeremy M. Cohen, Alex Damian, Ameet Talwalkar, J. Zico Kolter, Jason D. Lee</author><pubDate>Thu, 25 Sep 2025 14:29:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.24206v2</guid></item><item><title>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</title><link>http://arxiv.org/abs/2509.18056v2</link><description>This paper introduces TempSamp-R1, a new reinforcement fine-tuning frameworkdesigned to improve the effectiveness of adapting multimodal large languagemodels (MLLMs) to video temporal grounding tasks. We reveal that existingreinforcement learning methods, such as Group Relative Policy Optimization(GRPO), rely on on-policy sampling for policy updates. However, in tasks withlarge temporal search spaces, this strategy becomes both inefficient andlimited in performance, as it often fails to identify temporally accuratesolutions. To address this limitation, TempSamp-R1 leverages ground-truthannotations as off-policy supervision to provide temporally precise guidance,effectively compensating for the sparsity and misalignment in on-policysolutions. To further stabilize training and reduce variance in reward-basedupdates, TempSamp-R1 provides a non-linear soft advantage computation methodthat dynamically reshapes the reward feedback via an asymmetric transformation.By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1optimizes a single unified model to support both CoT and non-CoT inferencemodes, enabling efficient handling of queries with varying reasoningcomplexity. Experimental results demonstrate that TempSamp-R1 outperformsGRPO-based baselines, establishing new state-of-the-art performance onbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,TempSamp-R1 shows robust few-shot generalization capabilities under limiteddata. Code: https://github.com/HVision-NKU/TempSamp-R1</description><author>Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</author><pubDate>Thu, 25 Sep 2025 14:28:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.18056v2</guid></item><item><title>Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding</title><link>http://arxiv.org/abs/2509.21223v1</link><description>Pre-training has proven effective for learning transferable features in signlanguage understanding (SLU) tasks. Recently, skeleton-based methods havegained increasing attention because they can robustly handle variations insubjects and backgrounds without being affected by appearance or environmentalfactors. Current SLU methods continue to face three key limitations: 1) weaksemantic grounding, as models often capture low-level motion patterns fromskeletal data but struggle to relate them to linguistic meaning; 2) imbalancebetween local details and global context, with models either focusing toonarrowly on fine-grained cues or overlooking them for broader context; and 3)inefficient cross-modal learning, as constructing semantically alignedrepresentations across modalities remains difficult. To address these, wepropose Sigma, a unified skeleton-based SLU framework featuring: 1) asign-aware early fusion mechanism that facilitates deep interaction betweenvisual and textual modalities, enriching visual features with linguisticcontext; 2) a hierarchical alignment learning strategy that jointly maximisesagreements across different levels of paired features from differentmodalities, effectively capturing both fine-grained details and high-levelsemantic relationships; and 3) a unified pre-training framework that combinescontrastive learning, text matching and language modelling to promote semanticconsistency and generalisation. Sigma achieves new state-of-the-art results onisolated sign language recognition, continuous sign language recognition, andgloss-free sign language translation on multiple benchmarks spanning differentsign and spoken languages, demonstrating the impact of semantically informativepre-training and the effectiveness of skeletal data as a stand-alone solutionfor SLU.</description><author>Muxin Pu, Mei Kuan Lim, Chun Yong Chong, Chen Change Loy</author><pubDate>Thu, 25 Sep 2025 14:28:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21223v1</guid></item><item><title>Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models</title><link>http://arxiv.org/abs/2509.21221v1</link><description>Motivated by the emergence of large language models (LLMs) and the importanceof democratizing their training, we propose GWTF, the first crash tolerantpractical decentralized training framework for LLMs. Differently from existingdistributed and federated training frameworks, GWTF enables the efficientcollaborative training of a LLM on heterogeneous clients that volunteer theirresources. In addition, GWTF addresses node churn, i.e., clients joining orleaving the system at any time, and network instabilities, i.e., network linksbecoming unstable or unreliable. The core of GWTF is a novel decentralized flowalgorithm that finds the most effective routing that maximizes the number ofmicrobatches trained with the lowest possible delay. We extensively evaluateGWTF on GPT-like and LLaMa-like models and compare it against the prior art.Our results indicate that GWTF reduces the training time by up to 45% inrealistic and challenging scenarios that involve heterogeneous client nodesdistributed over 10 different geographic locations with a high node churn rate.</description><author>Nikolay Blagoev, Bart Cox, Jérémie Decouchant, Lydia Y. Chen</author><pubDate>Thu, 25 Sep 2025 14:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21221v1</guid></item><item><title>ML-PWS: Estimating the Mutual Information Between Experimental Time Series Using Neural Networks</title><link>http://arxiv.org/abs/2508.16509v2</link><description>The ability to quantify information transmission is crucial for the analysisand design of natural and engineered systems. The information transmission rateis the fundamental measure for systems with time-varying signals, yet computingit is extremely challenging. In particular, the rate cannot be obtaineddirectly from experimental time-series data without approximations, because ofthe high dimensionality of the signal trajectory space. Path Weight Sampling(PWS) is a computational technique that makes it possible to obtain theinformation rate exactly for any stochastic system. However, it requires amathematical model of the system of interest, be it described by a masterequation or a set of differential equations. Here, we present a technique thatemploys Machine Learning (ML) to develop a generative model from experimentaltime-series data, which is then combined with PWS to obtain the informationrate. We demonstrate the accuracy of this technique, called ML-PWS, bycomparing its results on synthetic time-series data generated from a non-linearmodel against ground-truth results obtained by applying PWS directly to thesame model. We illustrate the utility of ML-PWS by applying it to neuronaltime-series data.</description><author>Manuel Reinhardt, Gašper Tkačik, Pieter Rein ten Wolde</author><pubDate>Thu, 25 Sep 2025 14:26:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16509v2</guid></item><item><title>D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal</title><link>http://arxiv.org/abs/2508.12750v3</link><description>Shadow removal aims to restore images that are partially degraded by shadows,where the degradation is spatially localized and non-uniform. Unlike generalrestoration tasks that assume global degradation, shadow removal can leverageabundant information from non-shadow regions for guidance. However, thetransformation required to correct shadowed areas often differs significantlyfrom that of well-lit regions, making it challenging to apply uniformcorrection strategies. This necessitates the effective integration of non-localcontextual cues and adaptive modeling of region-specific transformations. Tothis end, we propose a novel Mamba-based network featuring dual-scale fusionand dual-path scanning to selectively propagate contextual information based ontransformation similarity across regions. Specifically, the proposed Dual-ScaleFusion Mamba Block (DFMB) enhances multi-scale feature representation by fusingoriginal features with low-resolution features, effectively reducing boundaryartifacts. The Dual-Path Mamba Group (DPMG) captures global features viahorizontal scanning and incorporates a mask-aware adaptive scanning strategy,which improves structural continuity and fine-grained region modeling.Experimental results demonstrate that our method significantly outperformsexisting state-of-the-art approaches on shadow removal benchmarks.</description><author>Linhao Li, Boya Jin, Zizhe Li, Lanqing Guo, Hao Cheng, Bo Li, Yongfeng Dong</author><pubDate>Thu, 25 Sep 2025 14:25:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12750v3</guid></item><item><title>JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for Legal Reasoning</title><link>http://arxiv.org/abs/2502.17166v2</link><description>In recent years, Large Language Models (LLMs) have been widely applied tolegal tasks. To enhance their understanding of legal texts and improvereasoning accuracy, a promising approach is to incorporate legal theories. Oneof the most widely adopted theories is the Four-Element Theory (FET), whichdefines the crime constitution through four elements: Subject, Object,Subjective Aspect, and Objective Aspect. While recent work has exploredprompting LLMs to follow FET, our evaluation demonstrates that LLM-generatedfour-elements are often incomplete and less representative, limiting theireffectiveness in legal reasoning. To address these issues, we present JUREX-4E,an expert-annotated four-element knowledge base covering 155 criminal charges.The annotations follow a progressive hierarchical framework grounded in legalsource validity and incorporate diverse interpretive methods to ensureprecision and authority. We evaluate JUREX-4E on the Similar ChargeDisambiguation task and apply it to Legal Case Retrieval. Experimental resultsvalidate the high quality of JUREX-4E and its substantial impact on downstreamlegal tasks, underscoring its potential for advancing legal AI applications.The dataset and code are available at: https://github.com/THUlawtech/JUREX</description><author>Huanghai Liu, Quzhe Huang, Qingjing Chen, Yiran Hu, Jiayu Ma, Yun Liu, Weixing Shen, Yansong Feng</author><pubDate>Thu, 25 Sep 2025 14:24:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.17166v2</guid></item><item><title>Training Set Reconstruction from Differentially Private Forests: How Effective is DP?</title><link>http://arxiv.org/abs/2502.05307v3</link><description>Recent research has shown that structured machine learning models such astree ensembles are vulnerable to privacy attacks targeting their training data.To mitigate these risks, differential privacy (DP) has become a widely adoptedcountermeasure, as it offers rigorous privacy protection. In this paper, weintroduce a reconstruction attack targeting state-of-the-art $\epsilon$-DPrandom forests. By leveraging a constraint programming model that incorporatesknowledge of the forest's structure and DP mechanism characteristics, ourapproach formally reconstructs the most likely dataset that could have produceda given forest. Through extensive computational experiments, we examine theinterplay between model utility, privacy guarantees and reconstruction accuracyacross various configurations. Our results reveal that random forests trainedwith meaningful DP guarantees can still leak portions of their training data.Specifically, while DP reduces the success of reconstruction attacks, the onlyforests fully robust to our attack exhibit predictive performance no betterthan a constant classifier. Building on these insights, we also providepractical recommendations for the construction of DP random forests that aremore resilient to reconstruction attacks while maintaining a non-trivialpredictive performance.</description><author>Alice Gorgé, Julien Ferry, Sébastien Gambs, Thibaut Vidal</author><pubDate>Thu, 25 Sep 2025 14:23:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.05307v3</guid></item><item><title>SGMem: Sentence Graph Memory for Long-Term Conversational Agents</title><link>http://arxiv.org/abs/2509.21212v1</link><description>Long-term conversational agents require effective memory management to handledialogue histories that exceed the context window of large language models(LLMs). Existing methods based on fact extraction or summarization reduceredundancy but struggle to organize and retrieve relevant information acrossdifferent granularities of dialogue and generated memory. We introduce SGMem(Sentence Graph Memory), which represents dialogue as sentence-level graphswithin chunked units, capturing associations across turn-, round-, andsession-level contexts. By combining retrieved raw dialogue with generatedmemory such as summaries, facts and insights, SGMem supplies LLMs with coherentand relevant context for response generation. Experiments on LongMemEval andLoCoMo show that SGMem consistently improves accuracy and outperforms strongbaselines in long-term conversational question answering.</description><author>Yaxiong Wu, Yongyue Zhang, Sheng Liang, Yong Liu</author><pubDate>Thu, 25 Sep 2025 14:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21212v1</guid></item><item><title>A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers</title><link>http://arxiv.org/abs/2507.22337v2</link><description>Understanding and solving complex reasoning tasks is vital for addressing theinformation needs of a user. Although dense neural models learn contextualisedembeddings, they still underperform on queries containing negation. Tounderstand this phenomenon, we study negation in both traditional neuralinformation retrieval and LLM-based models. We (1) introduce a taxonomy ofnegation that derives from philosophical, linguistic, and logical definitions;(2) generate two benchmark datasets that can be used to evaluate theperformance of neural information retrieval models and to fine-tune models fora more robust performance on negation; and (3) propose a logic-basedclassification mechanism that can be used to analyze the performance ofretrieval models on existing datasets. Our taxonomy produces a balanced datadistribution over negation types, providing a better training setup that leadsto faster convergence on the NevIR dataset. Moreover, we propose aclassification schema that reveals the coverage of negation types in existingdatasets, offering insights into the factors that might affect thegeneralization of fine-tuned models on negation.</description><author>Roxana Petcu, Samarth Bhargav, Maarten de Rijke, Evangelos Kanoulas</author><pubDate>Thu, 25 Sep 2025 14:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22337v2</guid></item><item><title>Evading Overlapping Community Detection via Proxy Node Injection</title><link>http://arxiv.org/abs/2509.21211v1</link><description>Protecting privacy in social graphs requires preventing sensitiveinformation, such as community affiliations, from being inferred by graphanalysis, without substantially altering the graph topology. We address thisthrough the problem of \emph{community membership hiding} (CMH), which seeksedge modifications that cause a target node to exit its original community,regardless of the detection algorithm employed. Prior work has focused onnon-overlapping community detection, where trivial strategies often suffice,but real-world graphs are better modeled by overlapping communities, where suchstrategies fail. To the best of our knowledge, we are the first to formalizeand address CMH in this setting. In this work, we propose a deep reinforcementlearning (DRL) approach that learns effective modification policies, includingthe use of proxy nodes, while preserving graph structure. Experiments onreal-world datasets show that our method significantly outperforms existingbaselines in both effectiveness and efficiency, offering a principled tool forprivacy-preserving graph modification with overlapping communities.</description><author>Dario Loi, Matteo Silvestri, Fabrizio Silvestri, Gabriele Tolomei</author><pubDate>Thu, 25 Sep 2025 14:21:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21211v1</guid></item><item><title>Learning Conformal Explainers for Image Classifiers</title><link>http://arxiv.org/abs/2509.21209v1</link><description>Feature attribution methods are widely used for explaining image-basedpredictions, as they provide feature-level insights that can be intuitivelyvisualized. However, such explanations often vary in their robustness and mayfail to faithfully reflect the reasoning of the underlying black-box model. Toaddress these limitations, we propose a novel conformal prediction-basedapproach that enables users to directly control the fidelity of the generatedexplanations. The method identifies a subset of salient features that issufficient to preserve the model's prediction, regardless of the informationcarried by the excluded features, and without demanding access to ground-truthexplanations for calibration. Four conformity functions are proposed toquantify the extent to which explanations conform to the model's predictions.The approach is empirically evaluated using five explainers across six imagedatasets. The empirical results demonstrate that FastSHAP consistentlyoutperforms the competing methods in terms of both fidelity and informationalefficiency, the latter measured by the size of the explanation regions.Furthermore, the results reveal that conformity measures based on super-pixelsare more effective than their pixel-wise counterparts.</description><author>Amr Alkhatib, Stephanie Lowry</author><pubDate>Thu, 25 Sep 2025 14:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21209v1</guid></item><item><title>CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A Fine-grained Corpus and Reasoning Analysis</title><link>http://arxiv.org/abs/2509.21208v1</link><description>Large Language Models (LLMs) are increasingly tasked with analyzing legaltexts and citing relevant statutes, yet their reliability is often compromisedby general pre-training that ingests legal texts without specialized focus,obscuring the true depth of their legal knowledge. This paper introduces CLaw,a novel benchmark specifically engineered to meticulously evaluate LLMs onChinese legal knowledge and its application in reasoning. CLaw comprises twokey components: (1) a comprehensive, fine-grained corpus of all 306 Chinesenational statutes, segmented to the subparagraph level and incorporatingprecise historical revision timesteps for rigorous recall evaluation (64,849entries), and (2) a challenging set of 254 case-based reasoning instancesderived from China Supreme Court curated materials to assess the practicalapplication of legal knowledge. Our empirical evaluation reveals that mostcontemporary LLMs significantly struggle to faithfully reproduce legalprovisions. As accurate retrieval and citation of legal provisions form thebasis of legal reasoning, this deficiency critically undermines the reliabilityof their responses. We contend that achieving trustworthy legal reasoning inLLMs requires a robust synergy of accurate knowledge retrieval--potentiallyenhanced through supervised fine-tuning (SFT) or retrieval-augmented generation(RAG)--and strong general reasoning capabilities. This work provides anessential benchmark and critical insights for advancing domain-specific LLMreasoning, particularly within the complex legal sphere.</description><author>Xinzhe Xu, Liang Zhao, Hongshen Xu, Chen Chen</author><pubDate>Thu, 25 Sep 2025 14:19:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21208v1</guid></item><item><title>Scaling Laws for Online Advertisement Retrieval</title><link>http://arxiv.org/abs/2411.13322v2</link><description>The scaling law is a notable property of neural network models and hassignificantly propelled the development of large language models. Scaling lawshold great promise in guiding model design and resource allocation. Recentresearch increasingly shows that scaling laws are not limited to NLP tasks orTransformer architectures; they also apply to domains such as recommendation.However, there is still a lack of literature on scaling law research in onlineadvertisement retrieval systems. This may be because 1) identifying the scalinglaw for resource cost and online revenue is often expensive in both time andtraining resources for industrial applications, and 2) varying settings fordifferent systems prevent the scaling law from being applied across variousscenarios. To address these issues, we propose a lightweight paradigm toidentify online scaling laws of retrieval models, incorporating a novel offlinemetric and an offline simulation algorithm. We prove that under mildassumptions, the correlation between the novel metric and online revenueasymptotically approaches 1 and empirically validates its effectiveness. Thesimulation algorithm can estimate the machine cost offline. Based on thelightweight paradigm, we can identify online scaling laws for retrieval modelsalmost exclusively through offline experiments, and quickly estimate machinecosts and revenues for given model configurations. We further validate theexistence of scaling laws across mainstream model architectures (e.g.,Transformer, MLP, and DSSM) in our real-world advertising system. With theidentified scaling laws, we demonstrate practical applications forROI-constrained model designing and multi-scenario resource allocation in theonline advertising system. To the best of our knowledge, this is the first workto study identification and application of online scaling laws for onlineadvertisement retrieval.</description><author>Yunli Wang, Zhen Zhang, Zixuan Yang, Tianyu Xu, Zhiqiang Wang, Yu Li, Rufan Zhou, Zhiqiang Liu, Yanjie Zhu, Jian Yang, Shiyang Wen, Peng Jiang</author><pubDate>Thu, 25 Sep 2025 14:19:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.13322v2</guid></item><item><title>Generalizing while preserving monotonicity in comparison-based preference learning models</title><link>http://arxiv.org/abs/2506.08616v2</link><description>If you tell a learning model that you prefer an alternative $a$ over anotheralternative $b$, then you probably expect the model to be monotone, that is,the valuation of $a$ increases, and that of $b$ decreases. Yet, perhapssurprisingly, many widely deployed comparison-based preference learning models,including large language models, fail to have this guarantee. Until now, theonly comparison-based preference learning algorithms that were proved to bemonotone are the Generalized Bradley-Terry models. Yet, these models are unableto generalize to uncompared data. In this paper, we advance the understandingof the set of models with generalization ability that are monotone. Namely, wepropose a new class of Linear Generalized Bradley-Terry models with DiffusionPriors, and identify sufficient conditions on alternatives' embeddings thatguarantee monotonicity. Our experiments show that this monotonicity is far frombeing a general guarantee, and that our new class of generalizing modelsimproves accuracy, especially when the dataset is limited.</description><author>Julien Fageot, Peva Blanchard, Gilles Bareilles, Lê-Nguyên Hoang</author><pubDate>Thu, 25 Sep 2025 14:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.08616v2</guid></item><item><title>Strassen Attention, Split VC Dimension and Compositionality in Transformers</title><link>http://arxiv.org/abs/2501.19215v3</link><description>We propose the first method to show theoretical limitations for one-layersoftmax transformers with arbitrarily many precision bits (even infinite). Weestablish those limitations for three tasks that require advanced reasoning.The first task, Match 3 (Sanford et al., 2023), requires looking at allpossible token triplets in an input sequence. The second and third tasksaddress compositionality-based reasoning: function composition (Peng et al.,2024) and binary relations composition, respectively. We formally prove theinability of one-layer softmax Transformers to solve any of these tasks. Toovercome these limitations, we introduce Strassen attention and prove that,equipped with this mechanism, a one-layer transformer can in principle solveall these tasks. Importantly, we show that it enjoys sub-cubic running-timecomplexity, making it more scalable than similar previously proposedmechanisms, such as higher-order attention (Sanford et al., 2023). Tocomplement our theoretical findings, we experimentally studied Strassenattention and compared it against standard (Vaswani et al, 2017), higher-orderattention (Sanford et al., 2023), and triangular attention (Bergen et al.2021). Our results help to disentangle all these attention mechanisms,highlighting their strengths and limitations. In particular, Strassen attentionoutperforms standard attention significantly on all the tasks. Altogether,understanding the theoretical limitations can guide research towards scalableattention mechanisms that improve the reasoning abilities of Transformers.</description><author>Alexander Kozachinskiy, Felipe Urrutia, Hector Jimenez, Tomasz Steifer, Germán Pizarro, Matías Fuentes, Francisco Meza, Cristian B. Calderon, Cristóbal Rojas</author><pubDate>Thu, 25 Sep 2025 14:17:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.19215v3</guid></item><item><title>Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions</title><link>http://arxiv.org/abs/2509.18847v2</link><description>Tool-augmented large language models (LLMs) are usually trained withsupervised imitation or coarse-grained reinforcement learning that optimizessingle tool calls. Current self-reflection practices rely on heuristic promptsor one-way reasoning: the model is urged to 'think more' instead of learningerror diagnosis and repair. This is fragile in multi-turn interactions; after afailure the model often repeats the same mistake. We propose structuredreflection, which turns the path from error to repair into an explicit,controllable, and trainable action. The agent produces a short yet precisereflection: it diagnoses the failure using evidence from the previous step andthen proposes a correct, executable follow-up call. For training we combineDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizingthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduceTool-Reflection-Bench, a lightweight benchmark that programmatically checksstructural validity, executability, parameter correctness, and resultconsistency. Tasks are built as mini trajectories of erroneous call,reflection, and corrected call, with disjoint train and test splits.Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turntool-call success and error recovery, and a reduction of redundant calls. Theseresults indicate that making reflection explicit and optimizing it directlyimproves the reliability of tool interaction and offers a reproducible path foragents to learn from failure.</description><author>Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, Yurui Qiu</author><pubDate>Thu, 25 Sep 2025 14:17:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.18847v2</guid></item><item><title>From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM</title><link>http://arxiv.org/abs/2509.21207v1</link><description>Prognostics and Health Management ensures the reliability, safety, andefficiency of complex engineered systems by enabling fault detection,anticipating equipment failures, and optimizing maintenance activitiesthroughout an asset lifecycle. However, real-world PHM presents persistentchallenges: sensor data is often noisy or incomplete, available labels arelimited, and degradation behaviors and system interdependencies can be highlycomplex and nonlinear. Physics-informed machine learning has emerged as apromising approach to address these limitations by embedding physical knowledgeinto data-driven models. This review examines how incorporating learning andobservational biases through physics-informed modeling and data strategies canguide models toward physically consistent and reliable predictions. Learningbiases embed physical constraints into model training through physics-informedloss functions and governing equations, or by incorporating properties likemonotonicity. Observational biases influence data selection and synthesis toensure models capture realistic system behavior through virtual sensing forestimating unmeasured states, physics-based simulation for data augmentation,and multi-sensor fusion strategies. The review then examines how theseapproaches enable the transition from passive prediction to activedecision-making through reinforcement learning, which allows agents to learnmaintenance policies that respect physical constraints while optimizingoperational objectives. This closes the loop between model-based predictions,simulation, and actual system operation, empowering adaptive decision-making.Finally, the review addresses the critical challenge of scaling PHM solutionsfrom individual assets to fleet-wide deployment. Fast adaptation methodsincluding meta-learning and few-shot learning are reviewed alongside domaingeneralization techniques ...</description><author>Olga Fink, Ismail Nejjar, Vinay Sharma, Keivan Faghih Niresi, Han Sun, Hao Dong, Chenghao Xu, Amaury Wei, Arthur Bizzi, Raffael Theiler, Yuan Tian, Leandro Von Krannichfeldt, Zhan Ma, Sergei Garmaev, Zepeng Zhang, Mengjie Zhao</author><pubDate>Thu, 25 Sep 2025 14:15:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21207v1</guid></item><item><title>Constrained Decoding for Robotics Foundation Models</title><link>http://arxiv.org/abs/2509.01728v2</link><description>Recent advances in the development of robotic foundation models have led topromising end-to-end and general-purpose capabilities in robotic systems. Thesemodels are pretrained on vast datasets of robot trajectories to processmulti-modal inputs and directly output a sequence of action that the systemthen executes in the real world. Although this approach is attractive from theperspective of improved generalization across diverse tasks, these models arestill data-driven and, therefore, lack explicit notions of behavioralcorrectness and safety constraints. We address these limitations by introducinga constrained decoding framework for robotics foundation models that enforceslogical constraints on action trajectories in dynamical systems. Our methodensures that generated actions provably satisfy signal temporal logic (STL)specifications at runtime without retraining, while remaining agnostic of theunderlying foundation model. We perform comprehensive evaluation of ourapproach across state-of-the-art navigation foundation models and we show thatour decoding-time interventions are useful not only for filtering unsafeactions but also for conditional action-generation. Videos available on ourwebsite: https://constrained-robot-fms.github.io</description><author>Parv Kapoor, Akila Ganlath, Changliu Liu, Sebastian Scherer, Eunsuk Kang</author><pubDate>Thu, 25 Sep 2025 14:15:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.01728v2</guid></item><item><title>Data-driven Neural Networks for Windkessel Parameter Calibration</title><link>http://arxiv.org/abs/2509.21206v1</link><description>In this work, we propose a novel method for calibrating Windkessel (WK)parameters in a dimensionally reduced 1D-0D coupled blood flow model. To thisend, we design a data-driven neural network (NN)trained on simulated bloodpressures in the left brachial artery. Once trained, the NN emulates thepressure pulse waves across the entire simulated domain, i.e., over time, spaceand varying WK parameters, with negligible error and computational effort. Tocalibrate the WK parameters on a measured pulse wave, the NN is extended bydummy neurons and retrained only on these. The main objective of this work isto assess the effectiveness of the method in various scenarios -- particularly,when the exact measurement location is unknown or the data are affected bynoise.</description><author>Benedikt Hoock, Tobias Köppl</author><pubDate>Thu, 25 Sep 2025 14:14:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.21206v1</guid></item></channel></rss>