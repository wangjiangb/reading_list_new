<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivfresh papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 03 Sep 2025 13:00:16 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>End to End Autoencoder MLP Framework for Sepsis Prediction</title><link>http://arxiv.org/abs/2508.18688v2</link><description>Sepsis is a life threatening condition that requires timely detection inintensive care settings. Traditional machine learning approaches, includingNaive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, oftenrely on manual feature engineering and struggle with irregular, incompletetime-series data commonly present in electronic health records. We introduce anend-to-end deep learning framework integrating an unsupervised autoencoder forautomatic feature extraction with a multilayer perceptron classifier for binarysepsis risk prediction. To enhance clinical applicability, we implement acustomized down sampling strategy that extracts high information densitysegments during training and a non-overlapping dynamic sliding window mechanismfor real-time inference. Preprocessed time series data are represented as fixeddimension vectors with explicit missingness indicators, mitigating bias andnoise. We validate our approach on three ICU cohorts. Our end-to-end modelachieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,respectively, consistently outperforming traditional machine learningbaselines. These results demonstrate the framework's superior robustness,generalizability, and clinical utility for early sepsis detection acrossheterogeneous ICU environments.</description><author>Hejiang Cai, Di Wu, Ji Xu, Xiang Liu, Yiziting Zhu, Xin Shu, Yujie Li, Bin Yi</author><pubDate>Tue, 02 Sep 2025 12:20:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18688v2</guid></item><item><title>Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks</title><link>http://arxiv.org/abs/2508.17690v2</link><description>Out-of-distribution (OOD) detection remains challenging in text-richnetworks, where textual features intertwine with topological structures.Existing methods primarily address label shifts or rudimentary domain-basedsplits, overlooking the intricate textual-structural diversity. For example, insocial networks, where users represent nodes with textual features (name, bio)while edges indicate friendship status, OOD may stem from the distinct languagepatterns between bot and normal users. To address this gap, we introduce theTextTopoOOD framework for evaluating detection across diverse OOD scenarios:(1) attribute-level shifts via text augmentations and embedding perturbations;(2) structural shifts through edge rewiring and semantic connections; (3)thematically-guided label shifts; and (4) domain-based divisions. Furthermore,we propose TNT-OOD to model the complex interplay between Text aNd Topologyusing: 1) a novel cross-attention module to fuse local structure intonode-level text representations, and 2) a HyperNetwork to generatenode-specific transformation parameters. This aligns topological and semanticfeatures of ID nodes, enhancing ID/OOD distinction across structural andtextual shifts. Experiments on 11 datasets across four OOD scenariosdemonstrate the nuanced challenge of TextTopoOOD for evaluating OOD detectionin text-rich networks.</description><author>Danny Wang, Ruihong Qiu, Guangdong Bai, Zi Huang</author><pubDate>Tue, 02 Sep 2025 11:53:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17690v2</guid></item><item><title>NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation</title><link>http://arxiv.org/abs/2507.01463v2</link><description>Instance segmentation of novel objects instances in RGB images, given someexample images for each object, is a well known problem in computer vision.Designing a model general enough to be employed for all kinds of novel objectswithout (re-) training has proven to be a difficult task. To handle this, wepresent a new training-free framework, called: Novel Object Cyclic Thresholdbased Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models:Grounded-SAM 2 for object proposals with precise bounding boxes andcorresponding segmentation masks; and DINOv2 for robust class and patchembeddings, due to its zero-shot capabilities. Internally, the proposal-objectmatching is realized by determining an object matching score based on thesimilarity of the class embeddings and the average maximum similarity of thepatch embeddings with a new cyclic thresholding (CT) mechanism that mitigatesunstable matches caused by repetitive textures or visually similar patterns.Beyond CT, NOCTIS introduces: (i) an appearance score that is unaffected byobject selection bias; (ii) the usage of the average confidence of theproposals bounding box and mask as a scoring component; and (iii) an RGB-onlypipeline that performs even better than RGB-D ones. We empirically show thatNOCTIS, without further training/fine tuning, attains state-of-the-art resultsregarding the mean AP score, w.r.t. the best RGB and RGB-D methods on the sevencore datasets of the BOP 2023 challenge for the "Model-based 2D segmentation ofunseen objects" task.</description><author>Max Gandyra, Alessandro Santonicola, Michael Beetz</author><pubDate>Tue, 02 Sep 2025 11:45:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01463v2</guid></item><item><title>SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</title><link>http://arxiv.org/abs/2508.15212v2</link><description>Long-context inference in large language models (LLMs) is increasinglyconstrained by the KV cache bottleneck: memory usage grows linearly withsequence length, while attention computation scales quadratically. Existingapproaches address this issue by compressing the KV cache along the temporalaxis through strategies such as token eviction or merging to reduce memory andcomputational overhead. However, these methods often neglect fine-grainedimportance variations across feature dimensions (i.e., the channel axis),thereby limiting their ability to effectively balance efficiency and modelaccuracy. In reality, we observe that channel saliency varies dramaticallyacross both queries and positions: certain feature channels carry near-zeroinformation for a given query, while others spike in relevance. To address thisoversight, we propose SPARK, a training-free plug-and-play method that appliesunstructured sparsity by pruning KV at the channel level, while dynamicallyrestoring the pruned entries during attention score computation. Notably, ourapproach is orthogonal to existing KV compression and quantization techniques,making it compatible for integration with them to achieve further acceleration.By reducing channel-level redundancy, SPARK enables processing of longersequences within the same memory budget. For sequences of equal length, SPARKnot only preserves or improves model accuracy but also reduces KV cache storageby over 30% compared to eviction-based methods. Furthermore, even with anaggressive pruning ratio of 80%, SPARK maintains performance with lessdegradation than 5% compared to the baseline eviction method, demonstrating itsrobustness and effectiveness. Our code will be available athttps://github.com/Xnhyacinth/SparK.</description><author>Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu</author><pubDate>Tue, 02 Sep 2025 11:29:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15212v2</guid></item><item><title>MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation</title><link>http://arxiv.org/abs/2508.14146v2</link><description>With the rapid growth of academic publications, peer review has become anessential yet time-consuming responsibility within the research community.Large Language Models (LLMs) have increasingly been adopted to assist in thegeneration of review comments; however, current LLM-based review tasks lack aunified evaluation benchmark to rigorously assess the models' ability toproduce comprehensive, accurate, and human-aligned assessments, particularly inscenarios involving multimodal content such as figures and tables. To addressthis gap, we propose \textbf{MMReview}, a comprehensive benchmark that spansmultiple disciplines and modalities. MMReview includes multimodal content andexpert-written review comments for 240 papers across 17 research domains withinfour major academic disciplines: Artificial Intelligence, Natural Sciences,Engineering Sciences, and Social Sciences. We design a total of 13 tasksgrouped into four core categories, aimed at evaluating the performance of LLMsand Multimodal LLMs (MLLMs) in step-wise review generation, outcomeformulation, alignment with human preferences, and robustness to adversarialinput manipulation. Extensive experiments conducted on 16 open-source modelsand 5 advanced closed-source models demonstrate the thoroughness of thebenchmark. We envision MMReview as a critical step toward establishing astandardized foundation for the development of automated peer review systems.</description><author>Xian Gao, Jiacheng Ruan, Zongyun Zhang, Jingsheng Gao, Ting Liu, Yuzhuo Fu</author><pubDate>Tue, 02 Sep 2025 11:28:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14146v2</guid></item><item><title>Programmable k-local Ising Machines and all-optical Kolmogorov-Arnold Networks on Photonic Platforms</title><link>http://arxiv.org/abs/2508.17440v2</link><description>Photonic computing promises energy-efficient acceleration for optimizationand learning, yet discrete combinatorial search and continuous functionapproximation have largely required distinct devices and control stacks. Herewe unify k-local Ising optimization and optical Kolmogorov-Arnold network (KAN)learning on a single photonic platform, establishing a critical convergencepoint in optical computing. We introduce an SLM-centric primitive thatrealizes, in one stroke, all-optical k-local Ising interactions and fullyoptical KAN layers. The key idea is to convert the structural nonlinearity of anominally linear scatterer into a per-window computational resource by adding asingle relay pass through the same spatial light modulator: a folded 4f relayre-images the first Fourier plane onto the SLM so that each selected clique orchannel occupies a disjoint window with its own second pass phase patch.Propagation remains linear in the optical field, yet the measured intensity ineach window becomes a freely programmable polynomial of the clique sum orprojection amplitude. This yields native, per clique k-local couplings withoutnonlinear media and, in parallel, the many independent univariatenonlinearities required by KAN layers, all trainable with in-situ physicalgradients using two frames (forward and adjoint). We outline implementations onspatial photonic Ising machines, injection-locked vertical cavity surfaceemitting laser (VCSEL) arrays, and Microsoft analog optical computers; in allcases the hardware change is one extra lens and a fold (or an on-chip 4f loop),enabling a minimal overhead, massively parallel route to high-order Isingoptimization and trainable, all-optical KAN processing on one platform.</description><author>Nikita Stroev, Natalia G. Berloff</author><pubDate>Tue, 02 Sep 2025 10:53:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17440v2</guid></item><item><title>SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA</title><link>http://arxiv.org/abs/2508.03590v2</link><description>Accurate 24-hour solar irradiance forecasting is essential for the safe andeconomic operation of solar photovoltaic systems. Traditional numerical weatherprediction (NWP) models represent the state-of-the-art in forecastingperformance but rely on computationally costly data assimilation and solvingcomplicated partial differential equations (PDEs) that simulate atmosphericphysics. Here, we introduce SolarSeer, an end-to-end large artificialintelligence (AI) model for solar irradiance forecasting across the ContiguousUnited States (CONUS). SolarSeer is designed to directly map the historicalsatellite observations to future forecasts, eliminating the computationaloverhead of data assimilation and PDEs solving. This efficiency allowsSolarSeer to operate over 1,500 times faster than traditional NWP, generating24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometerresolution in under 3 seconds. Compared with the state-of-the-art NWP in theCONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantlyreduces the root mean squared error of solar irradiance forecasting by 27.28%in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectivelycaptures solar irradiance fluctuations and significantly enhances thefirst-order irradiance difference forecasting accuracy. SolarSeer's ultrafast,accurate 24-hour solar irradiance forecasts provide strong support for thetransition to sustainable, net-zero energy systems.</description><author>Mingliang Bai, Zuliang Fang, Shengyu Tao, Siqi Xiang, Jiang Bian, Yanfei Xiang, Pengcheng Zhao, Weixin Jin, Jonathan A. Weyn, Haiyu Dong, Bin Zhang, Hongyu Sun, Kit Thambiratnam, Qi Zhang, Hongbin Sun, Xuan Zhang, Qiuwei Wu</author><pubDate>Tue, 02 Sep 2025 10:49:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03590v2</guid></item><item><title>Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization</title><link>http://arxiv.org/abs/2508.12927v2</link><description>Unsupervised anomaly detection aims to detect defective parts of a sample byhaving access, during training, to a set of normal, i.e. defect-free, data. Ithas many applications in fields, such as industrial inspection or medicalimaging, where acquiring labels is costly or when we want to avoid introducingbiases in the type of anomalies that can be spotted. In this work, we propose anovel UAD method based on prototype learning and introduce a metric to comparea structured set of embeddings that balances a feature-based cost and aspatial-based cost. We leverage this metric to learn local and globalprototypes with optimal transport from latent representations extracted with apre-trained image encoder. We demonstrate that our approach can enforce astructural constraint when learning the prototypes, allowing to capture theunderlying organization of the normal samples, thus improving the detection ofincoherencies in images. Our model achieves performance that is on par withstrong baselines on two reference benchmarks for anomaly detection onindustrial images.</description><author>Robin Trombetta, Carole Lartizien</author><pubDate>Tue, 02 Sep 2025 10:31:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12927v2</guid></item><item><title>Micro-splatting: Multistage Isotropy-informed Covariance Regularization Optimization for High-Fidelity 3D Gaussian Splatting</title><link>http://arxiv.org/abs/2504.05740v2</link><description>High-fidelity 3D Gaussian Splatting methods excel at capturing fine texturesbut often overlook model compactness, resulting in massive splat counts,bloated memory, long training, and complex post-processing. We presentMicro-Splatting: Two-Stage Adaptive Growth and Refinement, a unified,in-training pipeline that preserves visual detail while drastically reducingmodel complexity without any post-processing or auxiliary neural modules. InStage I (Growth), we introduce a trace-based covariance regularization tomaintain near-isotropic Gaussians, mitigating low-pass filtering inhigh-frequency regions and improving spherical-harmonic color fitting. We thenapply gradient-guided adaptive densification that subdivides splats only invisually complex regions, leaving smooth areas sparse. In Stage II(Refinement), we prune low-impact splats using a simple opacity-scaleimportance score and merge redundant neighbors via lightweight spatial andfeature thresholds, producing a lean yet detail-rich model. On fourobject-centric benchmarks, Micro-Splatting reduces splat count and model sizeby up to 60% and shortens training by 20%, while matching or surpassingstate-of-the-art PSNR, SSIM, and LPIPS in real-time rendering. These resultsdemonstrate that Micro-Splatting delivers both compactness and high fidelity ina single, efficient, end-to-end framework.</description><author>Jee Won Lee, Hansol Lim, Sooyeun Yang, Jongseong Brad Choi</author><pubDate>Tue, 02 Sep 2025 10:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05740v2</guid></item><item><title>A theoretical framework for self-supervised contrastive learning for continuous dependent data</title><link>http://arxiv.org/abs/2506.09785v2</link><description>Self-supervised learning (SSL) has emerged as a powerful approach to learningrepresentations, particularly in the field of computer vision. However, itsapplication to dependent data, such as temporal and spatio-temporal domains,remains underexplored. Besides, traditional contrastive SSL methods oftenassume \emph{semantic independence between samples}, which does not hold fordependent data exhibiting complex correlations. We propose a novel theoreticalframework for contrastive SSL tailored to \emph{continuous dependent data},which allows the nearest samples to be semantically close to each other. Inparticular, we propose two possible \textit{ground truth similarity measures}between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derivean analytical form for the \textit{estimated similarity matrix} thataccommodates both types of closeness between samples, thereby introducingdependency-aware loss functions. We validate our approach, \emph{DependentTS2Vec}, on temporal and spatio-temporal downstream problems. Given thedependency patterns presented in the data, our approach surpasses modern onesfor dependent data, highlighting the effectiveness of our theoreticallygrounded loss functions for SSL in capturing spatio-temporal dependencies.Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, withaccuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, onthe drought classification task, which involves complex spatio-temporalpatterns, our method achieves a $7$\% higher ROC-AUC score.</description><author>Alexander Marusov, Aleksandr Yugay, Alexey Zaytsev</author><pubDate>Tue, 02 Sep 2025 09:59:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.09785v2</guid></item><item><title>Identifying Macro Causal Effects in a C-DMG over ADMGs</title><link>http://arxiv.org/abs/2504.01551v2</link><description>Causal effect identification using causal graphs is a fundamental challengein causal inference. While extensive research has been conducted in this area,most existing methods assume the availability of fully specified directedacyclic graphs or acyclic directed mixed graphs. However, in complex domainssuch as medicine and epidemiology, complete causal knowledge is oftenunavailable, and only partial information about the system is accessible. Thispaper focuses on causal effect identification within partially specified causalgraphs, with particular emphasis on cluster-directed mixed graphs (C-DMGs)which can represent many different acyclic directed mixed graphs (ADMGs). Thesegraphs provide a higher-level representation of causal relationships bygrouping variables into clusters, offering a more practical approach forhandling complex systems. Unlike fully specified ADMGs, C-DMGs can containcycles, which complicate their analysis and interpretation. Furthermore, theircluster-based nature introduces new challenges, as it gives rise to twodistinct types of causal effects: macro causal effects and micro causaleffects, each with different properties. In this work, we focus on macro causaleffects, which describe the effects of entire clusters on other clusters. Weestablish that the do-calculus is both sound and complete for identifying theseeffects in C-DMGs over ADMGs when the cluster sizes are either unknown or ofsize greater than one. Additionally, we provide a graphical characterization ofnon-identifiability for macro causal effects in these graphs.</description><author>Simon Ferreira, Charles K. Assaad</author><pubDate>Tue, 02 Sep 2025 09:50:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.01551v2</guid></item><item><title>A Multi-Stage Auto-Context Deep Learning Framework for Tissue and Nuclei Segmentation and Classification in H&amp;E-Stained Histological Images of Advanced Melanoma</title><link>http://arxiv.org/abs/2503.23958v2</link><description>Melanoma is the most lethal form of skin cancer, with an increasing incidencerate worldwide. Analyzing histological images of melanoma by localizing andclassifying tissues and cell nuclei is considered the gold standard method fordiagnosis and treatment options for patients. While many computerizedapproaches have been proposed for automatic analysis, most perform tissue-basedanalysis and nuclei (cell)-based analysis as separate tasks, which might besuboptimal. In this work, using the PUMA challenge dataset, we propose a novelmulti-stage deep learning approach by combining tissue and nuclei informationin a unified framework based on the auto-context concept to performsegmentation and classification in histological images of melanoma. Throughpre-training and further post-processing, our approach achieved second andfirst place rankings in the PUMA challenge, with average micro Dice tissuescore and summed nuclei F1-score of 73.40% for Track 1 and 63.48% for Track 2,respectively. Furthermore, through a comprehensive ablation study andadditional evaluation on an external dataset, we demonstrated the effectivenessof the framework components as well as the generalization capabilities of theproposed approach. Our implementation for training and testing is available at:https://github.com/NimaTorbati/PumaSubmit</description><author>Nima Torbati, Anastasia Meshcheryakova, Ramona Woitek, Sepideh Hatamikia, Diana Mechtcheriakova, Amirreza Mahbod</author><pubDate>Tue, 02 Sep 2025 09:46:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.23958v2</guid></item><item><title>ViEEG: Hierarchical Visual Neural Representation for EEG Brain Decoding</title><link>http://arxiv.org/abs/2505.12408v3</link><description>Understanding and decoding brain activity into visual representations is afundamental challenge at the intersection of neuroscience and artificialintelligence. While EEG visual decoding has shown promise due to itsnon-invasive, and low-cost nature, existing methods suffer from HierarchicalNeural Encoding Neglect (HNEN)-a critical limitation where flat neuralrepresentations fail to model the brain's hierarchical visual processinghierarchy. Inspired by the hierarchical organization of visual cortex, wepropose ViEEG, a neuro-We further adopt hierarchical contrastive learning forEEG-CLIP representation alignment, enabling zero-shot object recognition.Extensive experiments on the THINGS-EEG dataset demonstrate that ViEEGsignificantly outperforms previous methods by a large margin in bothsubject-dependent and subject-independent settings. Results on the THINGS-MEGdataset further confirm ViEEG's generalization to different neural modalities.Our framework not only advances the performance frontier but also sets a newparadigm for EEG brain decoding. inspired framework that addresses HNEN. ViEEGdecomposes each visual stimulus into three biologically alignedcomponents-contour, foreground object, and contextual scene-serving as anchorsfor a three-stream EEG encoder. These EEG features are progressively integratedvia cross-attention routing, simulating cortical information flow fromlow-level to high-level vision.</description><author>Minxu Liu, Donghai Guan, Chuhang Zheng, Chunwei Tian, Jie Wen, Qi Zhu</author><pubDate>Tue, 02 Sep 2025 09:03:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12408v3</guid></item><item><title>High-Fidelity Prediction of Perturbed Optical Fields using Fourier Feature Networks</title><link>http://arxiv.org/abs/2508.19751v2</link><description>Predicting the effects of physical perturbations on optical channels iscritical for advanced photonic devices, but existing modelling techniques areoften computationally intensive or require exhaustive characterisation. Wepresent a novel data-efficient machine learning framework that learns theperturbation-dependent transmission matrix of a multimode fibre. To overcomethe challenge of modelling the resulting highly oscillatory functions, weencode the perturbation into a Fourier Feature basis, enabling a compactmulti-layer perceptron to learn the mapping with high fidelity. On experimentaldata from a compressed fibre, our model predicts the output field with a 0.995complex correlation to the ground truth, improving accuracy by an order ofmagnitude over standard networks while using 85\% fewer parameters. Thisapproach provides a general tool for modelling complex optical systems fromsparse measurements.</description><author>Joshua R. Jandrell, Mitchell A. Cox</author><pubDate>Tue, 02 Sep 2025 09:02:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19751v2</guid></item><item><title>Compressed Feature Quality Assessment: Dataset and Baselines</title><link>http://arxiv.org/abs/2506.07412v2</link><description>The widespread deployment of large models in resource-constrainedenvironments has underscored the need for efficient transmission ofintermediate feature representations. In this context, feature coding, whichcompresses features into compact bitstreams, becomes a critical component forscenarios involving feature transmission, storage, and reuse. However, thiscompression process inevitably introduces semantic degradation that isdifficult to quantify with traditional metrics. To address this, we formalizethe research problem of Compressed Feature Quality Assessment (CFQA), aiming toevaluate the semantic fidelity of compressed features. To advance CFQAresearch, we propose the first benchmark dataset, comprising 300 originalfeatures and 12000 compressed features derived from three vision tasks and fourfeature codecs. Task-specific performance degradation is provided as truesemantic distortion for evaluating CFQA metrics. We systematically assess threewidely used metrics -- MSE, cosine similarity, and Centered Kernel Alignment(CKA) -- in terms of their ability to capture semantic degradation. Ourfindings demonstrate the representativeness of the proposed dataset whileunderscoring the need for more sophisticated metrics capable of measuringsemantic distortion in compressed features. This work advances the field byestablishing a foundational benchmark and providing a critical resource for thecommunity to explore CFQA. To foster further research, we release the datasetand all associated source code athttps://github.com/chansongoal/Compressed-Feature-Quality-Assessment.</description><author>Changsheng Gao, Wei Zhou, Guosheng Lin, Weisi Lin</author><pubDate>Tue, 02 Sep 2025 08:58:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07412v2</guid></item><item><title>Fine-grained Image Quality Assessment for Perceptual Image Restoration</title><link>http://arxiv.org/abs/2508.14475v2</link><description>Recent years have witnessed remarkable achievements in perceptual imagerestoration (IR), creating an urgent demand for accurate image qualityassessment (IQA), which is essential for both performance comparison andalgorithm optimization. Unfortunately, the existing IQA metrics exhibitinherent weakness for IR task, particularly when distinguishing fine-grainedquality differences among restored images. To address this dilemma, wecontribute the first-of-its-kind fine-grained image quality assessment datasetfor image restoration, termed FGRestore, comprising 18,408 restored imagesacross six common IR tasks. Beyond conventional scalar quality scores,FGRestore was also annotated with 30,886 fine-grained pairwise preferences.Based on FGRestore, a comprehensive benchmark was conducted on the existing IQAmetrics, which reveal significant inconsistencies between score-based IQAevaluations and the fine-grained restoration quality. Motivated by thesefindings, we further propose FGResQ, a new IQA model specifically designed forimage restoration, which features both coarse-grained score regression andfine-grained quality ranking. Extensive experiments and comparisons demonstratethat FGResQ significantly outperforms state-of-the-art IQA metrics. Codes andmodel weights have been released in https://pxf0429.github.io/FGResQ/</description><author>Xiangfei Sheng, Xiaofeng Pan, Zhichao Yang, Pengfei Chen, Leida Li</author><pubDate>Tue, 02 Sep 2025 08:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14475v2</guid></item><item><title>DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation</title><link>http://arxiv.org/abs/2506.16495v2</link><description>Like image coding in visual data transmission, feature coding is essentialfor the distributed deployment of large models by significantly reducingtransmission and storage burden. However, prior studies have mostly targetedtask- or model-specific scenarios, leaving the challenge of universal featurecoding across diverse large models largely unexplored. In this paper, wepresent the first systematic study on universal feature coding for largemodels. The key challenge lies in the inherently diverse and distributionallyincompatible nature of features extracted from different models. For example,features from DINOv2 exhibit highly peaky, concentrated distributions, whilethose from Stable Diffusion 3 (SD3) are more dispersed and uniform. Thisdistributional heterogeneity severely hampers both compression efficiency andcross-model generalization. To address this, we propose a learnedpeaky-to-balanced distribution transformation, which reshapes highly skewedfeature distributions into a common, balanced target space. This transformationis non-uniform, data-driven, and plug-and-play, enabling effective alignment ofheterogeneous distributions without modifying downstream codecs. With thisalignment, a universal codec trained on the balanced target distribution caneffectively generalize to features from different models and tasks. We validateour approach on three representative large models (LLaMA3, DINOv2, and SD3)across multiple tasks and modalities. Extensive experiments show that ourmethod achieves notable improvements in both compression efficiency andcross-model generalization over task-specific baselines. All source code hasbeen made available at https://github.com/chansongoal/DT-UFC.</description><author>Changsheng Gao, Zijie Liu, Li Li, Dong Liu, Xiaoyan Sun, Weisi Lin</author><pubDate>Tue, 02 Sep 2025 08:47:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.16495v2</guid></item><item><title>Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty Quantification for LoRA</title><link>http://arxiv.org/abs/2502.12122v2</link><description>Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of largelanguage models by decomposing weight updates into low-rank matrices,significantly reducing storage and computational overhead. While effective,standard LoRA lacks mechanisms for uncertainty quantification, leading tooverconfident and poorly calibrated models. Bayesian variants of LoRA addressthis limitation, but at the cost of a significantly increased number oftrainable parameters, partially offsetting the original efficiency gains.Additionally, these models are harder to train and may suffer from unstableconvergence. In this work, we propose a novel parameter-efficient Bayesian LoRAvia subspace inference, demonstrating that effective uncertainty quantificationcan be achieved in very low-dimensional parameter spaces. The proposed methodachieves strong performance with improved calibration and generalization whilemaintaining computational efficiency. Our empirical findings show that, withthe appropriate projection of the weight space: (1) uncertainty can beeffectively modeled in a low-dimensional space, and (2) weight covariancesexhibit low ranks.</description><author>Patryk Marszałek, Klaudia Bałazy, Jacek Tabor, Tomasz Kuśmierczyk</author><pubDate>Tue, 02 Sep 2025 08:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.12122v2</guid></item><item><title>Learnable cut flow for high energy physics</title><link>http://arxiv.org/abs/2503.22498v3</link><description>Neural networks have emerged as a powerful paradigm for tasks in high energyphysics, yet their opaque training process renders them as a black box. Incontrast, the traditional cut flow method offers simplicity andinterpretability but requires extensive manual tuning to identify optimal cutboundaries. To merge the strengths of both approaches, we propose the LearnableCut Flow (LCF), a neural network that transforms the traditional cut selectioninto a fully differentiable, data-driven process. LCF implements two cutstrategies-parallel, where observable distributions are treated independently,and sequential, where prior cuts shape subsequent ones-to flexibly determineoptimal boundaries. Building on this strategy, we introduce the LearnableImportance, a metric that quantifies feature importance and adjusts theircontributions to the loss accordingly, offering model-driven insights unlikead-hoc metrics. To ensure differentiability, a modified loss function replaceshard cuts with mask operations, preserving data shape throughout the trainingprocess. LCF is tested on six varied mock datasets and a realistic diboson vs.QCD dataset. Results demonstrate that LCF 1. accurately learns cut boundariesacross typical feature distributions in both parallel and sequentialstrategies, 2. assigns higher importance to discriminative features withminimal overlap, 3. handles redundant or correlated features robustly, and 4.performs effectively in real-world scenarios. In the diboson dataset, LCFinitially underperforms boosted decision trees and multiplayer perceptrons whenusing all observables. LCF bridges the gap between traditional cut flow methodand modern black-box neural networks, delivering actionable insights into thetraining process and feature importance. Source code and experimental data areavailable at https://github.com/Star9daisy/learnable-cut-flow.</description><author>Jing Li, Hao Sun</author><pubDate>Tue, 02 Sep 2025 08:33:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.22498v3</guid></item><item><title>Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</title><link>http://arxiv.org/abs/2508.11452v2</link><description>Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)have ushered in a new era of AI capabilities, demonstrating near-human-levelperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve thedevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourcedgeneral-domain prompts, often falling short of reflecting performance inreal-world applications. To bridge this critical gap, we present InclusionArena, a live leaderboard that ranks models based on human feedback collecteddirectly from AI-powered applications. Our platform integrates pairwise modelcomparisons into natural user interactions, ensuring evaluations reflectpractical usage scenarios. For robust model ranking, we employ theBradley-Terry model augmented with two key innovations: (1) Placement Matches,a cold-start mechanism to quickly estimate initial ratings for newly integratedmodels, and (2) Proximity Sampling, an intelligent comparison strategy thatprioritizes battles between models of similar capabilities to maximizeinformation gain and enhance rating stability. Extensive empirical analyses andsimulations demonstrate that Inclusion Arena yields reliable and stablerankings, exhibits higher data transitivity compared to general crowdsourceddatasets, and significantly mitigates the risk of malicious manipulation. Byfostering an open alliance between foundation models and real-worldapplications, Inclusion Arena aims to accelerate the development of LLMs andMLLMs truly optimized for practical, user-centric deployments. The platform ispublicly accessible at https://www.tbox.cn/about/model-ranking.</description><author>Kangyu Wang, Hongliang He, Lin Liu, Ruiqi Liang, Zhenzhong Lan, Jianguo Li</author><pubDate>Tue, 02 Sep 2025 08:20:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11452v2</guid></item><item><title>TNet: Terrace Convolutional Decoder Network for Remote Sensing Image Semantic Segmentation</title><link>http://arxiv.org/abs/2508.04061v2</link><description>In remote sensing, most segmentation networks adopt the UNet architecture,often incorporating modules such as Transformers or Mamba to enhanceglobal-local feature interactions within decoder stages. However, theseenhancements typically focus on intra-scale relationships and neglect theglobal contextual dependencies across multiple resolutions. To address thislimitation, we introduce the Terrace Convolutional Decoder Network (TNet), asimple yet effective architecture that leverages only convolution and additionoperations to progressively integrate low-resolution features (rich in globalcontext) into higher-resolution features (rich in local details) acrossdecoding stages. This progressive fusion enables the model to learnspatially-aware convolutional kernels that naturally blend global and localinformation in a stage-wise manner. We implement TNet with a ResNet-18 encoder(TNet-R) and evaluate it on three benchmark datasets. TNet-R achievescompetitive performance with a mean Intersection-over-Union (mIoU) of 85.35\%on ISPRS Vaihingen, 87.05\% on ISPRS Potsdam, and 52.19\% on LoveDA, whilemaintaining high computational efficiency. Code is publicly available.</description><author>Chengqian Dai, Yonghong Guo, Hongzhao Xiang, Yigui Luo</author><pubDate>Tue, 02 Sep 2025 08:11:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.04061v2</guid></item><item><title>An effective potential for generative modelling with active matter</title><link>http://arxiv.org/abs/2508.08146v2</link><description>Score-based diffusion models generate samples from a complex underlying datadistribution by time-reversal of a diffusion process and represent thestate-of-the-art in many generative AI applications. Here, I show how agenerative diffusion model can be implemented based on an underlying activeparticle process with finite correlation time. Time reversal is achieved byimposing an effective time-dependent potential on the position coordinate,which can be readily implemented in simulations and experiments to generate newsynthetic data samples driven by active fluctuations. The effective potentialis valid to first order in the persistence time and leads to a force field thatis fully determined by the standard score function and its derivatives up to2nd order. Numerical experiments for artificial data distributions confirm thevalidity of the effective potential, which opens up new avenues to exploitfluctuations in active and living systems for generative AI purposes.</description><author>Adrian Baule</author><pubDate>Tue, 02 Sep 2025 08:01:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.08146v2</guid></item><item><title>Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification</title><link>http://arxiv.org/abs/2504.14212v2</link><description>Large language models (LLMs) acquire general linguistic knowledge frommassive-scale pretraining. However, pretraining data mainly comprised ofweb-crawled texts contain undesirable social biases which can be perpetuated oreven amplified by LLMs. In this study, we propose an efficient yet effectiveannotation pipeline to investigate social biases in the pretraining corpora.Our pipeline consists of protected attribute detection to identify diversedemographics, followed by regard classification to analyze the languagepolarity towards each attribute. Through our experiments, we demonstrate theeffect of our bias analysis and mitigation measures, focusing on Common Crawlas the most representative pretraining corpus.</description><author>Takuma Udagawa, Yang Zhao, Hiroshi Kanayama, Bishwaranjan Bhattacharjee</author><pubDate>Tue, 02 Sep 2025 07:57:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.14212v2</guid></item><item><title>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings</title><link>http://arxiv.org/abs/2508.18733v2</link><description>Computer-Aided Design (CAD) generative modeling is driving significantinnovations across industrial applications. Recent works have shown remarkableprogress in creating solid models from various inputs such as point clouds,meshes, and text descriptions. However, these methods fundamentally divergefrom traditional industrial workflows that begin with 2D engineering drawings.The automatic generation of parametric CAD models from these 2D vector drawingsremains underexplored despite being a critical step in engineering design. Toaddress this gap, our key insight is to reframe CAD generation as asequence-to-sequence learning problem where vector drawing primitives directlyinform the generation of parametric CAD operations, preserving geometricprecision and design intent throughout the transformation process. We proposeDrawing2CAD, a framework with three key technical components: anetwork-friendly vector primitive representation that preserves precisegeometric information, a dual-decoder transformer architecture that decouplescommand type and parameter generation while maintaining precise correspondence,and a soft target distribution loss function accommodating inherent flexibilityin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,a dataset of paired engineering drawings and parametric CAD models, and conductthorough experiments to demonstrate the effectiveness of our method. Code anddataset are available at https://github.com/lllssc/Drawing2CAD.</description><author>Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu</author><pubDate>Tue, 02 Sep 2025 07:34:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18733v2</guid></item><item><title>VSRM: A Robust Mamba-Based Framework for Video Super-Resolution</title><link>http://arxiv.org/abs/2506.22762v2</link><description>Video super-resolution remains a major challenge in low-level vision tasks.To date, CNN- and Transformer-based methods have delivered impressive results.However, CNNs are limited by local receptive fields, while Transformersstruggle with quadratic complexity, posing challenges for processing longsequences in VSR. Recently, Mamba has drawn attention for its long-sequencemodeling, linear complexity, and large receptive fields. In this work, wepropose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolutionframework that leverages the power of \textbf{M}amba. VSRM introducesSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extractlong-range spatio-temporal features and enhance receptive fields efficiently.To better align adjacent frames, we propose Deformable Cross-Mamba Alignmentmodule. This module utilizes a deformable cross-mamba mechanism to make thecompensation stage more dynamic and flexible, preventing feature distortions.Finally, we minimize the frequency domain gaps between reconstructed andground-truth frames by proposing a simple yet effective FrequencyCharbonnier-like loss that better preserves high-frequency content and enhancesvisual quality. Through extensive experiments, VSRM achieves state-of-the-artresults on diverse benchmarks, establishing itself as a solid foundation forfuture research.</description><author>Dinh Phu Tran, Dao Duy Hung, Daeyoung Kim</author><pubDate>Tue, 02 Sep 2025 07:31:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.22762v2</guid></item><item><title>Removing Averaging: Personalized Lip-Sync Driven Characters Based on Identity Adapter</title><link>http://arxiv.org/abs/2503.06397v2</link><description>Recent advances in diffusion-based lip-syncing generative models havedemonstrated their ability to produce highly synchronized talking face videosfor visual dubbing. Although these models excel at lip synchronization, theyoften struggle to maintain fine-grained control over facial details ingenerated images. In this work, we identify "lip averaging" phenomenon wherethe model fails to preserve subtle facial details when dubbing unseenin-the-wild videos. This issue arises because the commonly used UNet backboneprimarily integrates audio features into visual representations in the latentspace via cross-attention mechanisms and multi-scale fusion, but it strugglesto retain fine-grained lip details in the generated faces. To address thisissue, we propose UnAvgLip, which extracts identity embeddings from referencevideos to generate highly faithful facial sequences while maintaining accuratelip synchronization. Specifically, our method comprises two primary components:(1) an Identity Perceiver module that encodes facial embeddings to align withconditioned audio features; and (2) an ID-CrossAttn module that injects facialembeddings into the generation process, enhancing model's capability ofidentity retention. Extensive experiments demonstrate that, at a modesttraining and inference cost, UnAvgLip effectively mitigates the "averaging"phenomenon in lip inpainting, significantly preserving unique facialcharacteristics while maintaining precise lip synchronization. Compared withthe original approach, our method demonstrates significant improvements of 5%on the identity consistency metric and 2% on the SSIM metric across twobenchmark datasets (HDTF and LRW).</description><author>Yanyu Zhu, Lichen Bai, Jintao Xu, Hai-tao Zheng</author><pubDate>Tue, 02 Sep 2025 07:24:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.06397v2</guid></item><item><title>Affective Polarization across European Parliaments</title><link>http://arxiv.org/abs/2508.18916v2</link><description>Affective polarization, characterized by increased negativity and hostilitytowards opposing groups, has become a prominent feature of political discourseworldwide. Our study examines the presence of this type of polarization in aselection of European parliaments in a fully automated manner. Utilizing acomprehensive corpus of parliamentary speeches from the parliaments of sixEuropean countries, we employ natural language processing techniques toestimate parliamentarian sentiment. By comparing the levels of negativityconveyed in references to individuals from opposing groups versus one's own, wediscover patterns of affectively polarized interactions. The findingsdemonstrate the existence of consistent affective polarization across all sixEuropean parliaments. Although activity correlates with negativity, there is noobserved difference in affective polarization between less active and moreactive members of parliament. Finally, we show that reciprocity is acontributing mechanism in affective polarization between parliamentariansacross all six parliaments.</description><author>Bojan Evkoski, Igor Mozetič, Nikola Ljubešić, Petra Kralj Novak</author><pubDate>Tue, 02 Sep 2025 06:19:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18916v2</guid></item><item><title>Can Uncertainty Quantification Improve Learned Index Benefit Estimation?</title><link>http://arxiv.org/abs/2410.17748v2</link><description>Index tuning is crucial for optimizing database performance by selectingoptimal indexes based on workload. The key to this process lies in an accurateand efficient benefit estimator. Traditional methods relying on what-if toolsoften suffer from inefficiency and inaccuracy. In contrast, learning-basedmodels provide a promising alternative but face challenges such as instability,lack of interpretability, and complex management. To overcome theselimitations, we adopt a novel approach: quantifying the uncertainty inlearning-based models' results, thereby combining the strengths of bothtraditional and learning-based methods for reliable index tuning. We propose Beauty, the first uncertainty-aware framework that enhanceslearning-based models with uncertainty quantification and uses what-if tools asa complementary mechanism to improve reliability and reduce managementcomplexity. Specifically, we introduce a novel method that combines AutoEncoderand Monte Carlo Dropout to jointly quantify uncertainty, tailored to thecharacteristics of benefit estimation tasks. In experiments involving sixteen models, our approach outperformed existinguncertainty quantification methods in the majority of cases. We also conductedindex tuning tests on six datasets. By applying the Beauty framework, weeliminated worst-case scenarios and more than tripled the occurrence ofbest-case scenarios.</description><author>Tao Yu, Zhaonian Zou, Hao Xiong</author><pubDate>Tue, 02 Sep 2025 06:14:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17748v2</guid></item><item><title>Federated Retrieval-Augmented Generation: A Systematic Mapping Study</title><link>http://arxiv.org/abs/2505.18906v2</link><description>Federated Retrieval-Augmented Generation (Federated RAG) combines FederatedLearning (FL), which enables distributed model training without exposing rawdata, with Retrieval-Augmented Generation (RAG), which improves the factualaccuracy of language models by grounding outputs in external knowledge. Aslarge language models are increasingly deployed in privacy-sensitive domainssuch as healthcare, finance, and personalized assistance, Federated RAG offersa promising framework for secure, knowledge-intensive natural languageprocessing (NLP). To the best of our knowledge, this paper presents the firstsystematic mapping study of Federated RAG, covering literature publishedbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-basedsoftware engineering, we develop a structured classification of researchfocuses, contribution types, and application domains. We analyze architecturalpatterns, temporal trends, and key challenges, including privacy-preservingretrieval, cross-client heterogeneity, and evaluation limitations. Our findingssynthesize a rapidly evolving body of research, identify recurring designpatterns, and surface open questions, providing a foundation for future work atthe intersection of RAG and federated systems.</description><author>Abhijit Chakraborty, Chahana Dahal, Vivek Gupta</author><pubDate>Tue, 02 Sep 2025 06:08:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.18906v2</guid></item><item><title>VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework</title><link>http://arxiv.org/abs/2502.00711v2</link><description>Visual reasoning refers to the task of solving questions about visualinformation. Current visual reasoning methods typically employ pre-trainedvision-language model (VLM) strategies or deep neural network approaches.However, existing efforts are constrained by limited reasoninginterpretability, while hindering by the phenomenon of underspecification inthe question text. Additionally, the absence of fine-grained visual knowledgelimits the precise understanding of subject behavior in visual reasoning tasks.To address these issues, we propose VIKSER (Visual Knowledge-DrivenSelf-Reinforcing Reasoning Framework). Specifically, VIKSER, trained usingknowledge distilled from large language models, extracts fine-grained visualknowledge with the assistance of visual relationship detection techniques.Subsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase thequestion with underspecification. Additionally, we design a novel promptingmethod called Chain-of-Evidence (CoE), which leverages the power of "evidencefor reasoning" to endow VIKSER with interpretable reasoning capabilities.Meanwhile, the integration of self-reflection technology empowers VIKSER withthe ability to learn and improve from its mistakes. Experiments conducted onwidely used datasets demonstrate that VIKSER achieves new state-of-the-art(SOTA) results in relevant tasks. Moreover, VIKSER achieves performance on parwith leading proprietary models, such as the latest ChatGPT-5.</description><author>Chao Wang, Chunbai Zhang, Yongxiao Tian, Yang Zhou, Yan Peng</author><pubDate>Tue, 02 Sep 2025 05:28:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.00711v2</guid></item><item><title>DesCLIP: Robust Continual Learning via General Attribute Descriptions for VLM-Based Visual Recognition</title><link>http://arxiv.org/abs/2502.00618v2</link><description>Continual learning of vision-language models (VLMs) focuses on leveragingcross-modal pretrained knowledge to incrementally adapt to expanding downstreamtasks and datasets, while tackling the challenge of knowledge forgetting.Existing research often focuses on connecting visual features with specificclass text in downstream tasks, overlooking the latent relationships betweengeneral and specialized knowledge. Our findings reveal that forcing models tooptimize inappropriate visual-text matches exacerbates forgetting of VLM'srecognition ability. To tackle this issue, we propose DesCLIP, which leveragesgeneral attribute (GA) descriptions to guide the understanding of specificclass objects, enabling VLMs to establish robust vision-GA-class trilateralassociations rather than relying solely on vision-class connections.Specifically, we introduce a language assistant to generate concrete GAdescription candidates via proper request prompts. Then, an anchor-basedembedding filter is designed to obtain highly relevant GA descriptionembeddings, which are leveraged as the paired text embeddings forvisual-textual instance matching, thereby tuning the visual encoder.Correspondingly, the class text embeddings are gradually calibrated to alignwith these shared GA description embeddings. Extensive experiments demonstratethe advancements and efficacy of our proposed method, with comprehensiveempirical evaluations highlighting its superior performance in VLM-basedrecognition compared to existing continual learning methods.</description><author>Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li</author><pubDate>Tue, 02 Sep 2025 05:22:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.00618v2</guid></item><item><title>Distilling Large Language Models for Network Active Queue Management</title><link>http://arxiv.org/abs/2501.16734v3</link><description>The growing complexity of network traffic and demand for ultra-low latencycommunication require smarter packet traffic management. Existing DeepLearning-based queuing approaches struggle with dynamic network scenarios anddemand high engineering effort. We propose AQM-LLM, distilling Large LanguageModels (LLMs) with few-shot learning, contextual understanding, and patternrecognition to improve Active Queue Management (AQM) [RFC 9330] with minimalmanual effort. We consider a specific case where AQM is Low Latency, Low Loss,and Scalable Throughput (L4S) and our design of AQM-LLM builds on speculativedecoding and reinforcement-based distilling of LLM by tackling congestionprevention in the L4S architecture using Explicit Congestion Notification (ECN)[RFC 9331] and periodic packet dropping. We develop a new open-sourceexperimental platform by executing L4S-AQM on FreeBSD-14, providinginteroperable modules to support LLM integration and facilitate IETFrecognition through wider testing. Our extensive evaluations show L4S-LLMenhances queue management, prevents congestion, reduces latency, and boostsnetwork performance, showcasing LLMs' adaptability and efficiency in upliftingAQM systems.</description><author>Shiva Raj Pokhrel, Deol Satish, Jonathan Kua, Anwar Walid</author><pubDate>Tue, 02 Sep 2025 05:02:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.16734v3</guid></item><item><title>Can NeRFs See without Cameras?</title><link>http://arxiv.org/abs/2505.22441v2</link><description>Neural Radiance Fields (NeRFs) have been remarkably successful atsynthesizing novel views of 3D scenes by optimizing a volumetric scenefunction. This scene function models how optical rays bring color informationfrom a 3D object to the camera pixels. Radio frequency (RF) or audio signalscan also be viewed as a vehicle for delivering information about theenvironment to a sensor. However, unlike camera pixels, an RF/audio sensorreceives a mixture of signals that contain many environmental reflections (alsocalled "multipath"). Is it still possible to infer the environment using suchmultipath signals? We show that with redesign, NeRFs can be taught to learnfrom multipath signals, and thereby "see" the environment. As a groundingapplication, we aim to infer the indoor floorplan of a home from sparse WiFimeasurements made at multiple locations inside the home. Although a difficultinverse problem, our implicitly learnt floorplans look promising, and enablesforward applications, such as indoor signal prediction and basic ray tracing.</description><author>Chaitanya Amballa, Sattwik Basu, Yu-Lin Wei, Zhijian Yang, Mehmet Ergezer, Romit Roy Choudhury</author><pubDate>Tue, 02 Sep 2025 04:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.22441v2</guid></item><item><title>CF3: Compact and Fast 3D Feature Fields</title><link>http://arxiv.org/abs/2508.05254v3</link><description>3D Gaussian Splatting (3DGS) has begun incorporating rich information from 2Dfoundation models. However, most approaches rely on a bottom-up optimizationprocess that treats raw 2D features as ground truth, incurring increasedcomputational costs. We propose a top-down pipeline for constructing compactand fast 3D Gaussian feature fields, namely, CF3. We first perform a fastweighted fusion of multi-view 2D features with pre-trained Gaussians. Thisapproach enables training a per-Gaussian autoencoder directly on the liftedfeatures, instead of training autoencoders in the 2D domain. As a result, theautoencoder better aligns with the feature distribution. More importantly, weintroduce an adaptive sparsification method that optimizes the Gaussianattributes of the feature field while pruning and merging the redundantGaussians, constructing an efficient representation with preserved geometricdetails. Our approach achieves a competitive 3D feature field using as littleas 5% of the Gaussians compared to Feature-3DGS.</description><author>Hyunjoon Lee, Joonkyu Min, Jaesik Park</author><pubDate>Tue, 02 Sep 2025 04:41:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05254v3</guid></item><item><title>Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner</title><link>http://arxiv.org/abs/2508.15044v2</link><description>Aligning large language models (LLMs) with human preferences has become acritical step in their development. Recent research has increasingly focused ontest-time alignment, where additional compute is allocated during inference toenhance LLM safety and reasoning capabilities. However, these test-timealignment techniques often incur substantial inference costs, limiting theirpractical application. We are inspired by the speculative samplingacceleration, which leverages a small draft model to efficiently predict futuretokens, to address the efficiency bottleneck of test-time alignment. Weintroduce the reward-shifted speculative sampling (SSS) algorithm, in which thedraft model is aligned with human preferences, while the target model remainsunchanged. We theoretically demonstrate that the distributional shift betweenthe aligned draft model and the unaligned target model can be exploited torecover the RLHF optimal solution without actually obtaining it, by modifyingthe acceptance criterion and bonus token distribution. Our algorithm achievessuperior gold reward scores at a significantly reduced inference cost intest-time weak-to-strong alignment experiments, thereby validating both itseffectiveness and efficiency.</description><author>Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang</author><pubDate>Tue, 02 Sep 2025 04:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15044v2</guid></item><item><title>Visual Evolutionary Optimization on Graph-Structured Combinatorial Problems with MLLMs: A Case Study of Influence Maximization</title><link>http://arxiv.org/abs/2505.06850v3</link><description>Graph-structured combinatorial problems in complex networks are prevalent inmany domains, and are computationally demanding due to their complexity andnon-linear nature. Traditional evolutionary algorithms (EAs), while robust,often face obstacles due to content-shallow encoding limitations and lack ofstructural awareness, necessitating hand-crafted modifications for effectiveapplication. In this work, we introduce an original framework, visualevolutionary optimization (VEO), leveraging multimodal large language models(MLLMs) as the backbone evolutionary optimizer in this context. Specifically,we propose a context-aware encoding way, representing the solution of thenetwork as an image. In this manner, we can utilize MLLMs' image processingcapabilities to intuitively comprehend network configurations, thus enablingmachines to solve these problems in a human-like way. We have developedMLLM-based operators tailored for various evolutionary optimization stages,including initialization, crossover, and mutation. {Furthermore, we proposethat graph sparsification can effectively enhance the applicability andscalability of VEO on large-scale networks, owing to the scale-free nature ofreal-world networks.} We demonstrate the effectiveness of our method using awell-known task in complex networks, influence maximization, and validate it oneight different real-world networks of various structures. The results haveconfirmed VEO's reliability and enhanced effectiveness compared to traditionalevolutionary optimization.</description><author>Jie Zhao, Kang Hao Cheong</author><pubDate>Tue, 02 Sep 2025 04:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.06850v3</guid></item><item><title>A Vision-Language Agent System for Compositional Reasoning with VLM-assisted Script and Executable Generation</title><link>http://arxiv.org/abs/2506.07778v2</link><description>The advancement in large language models (LLMs) and large vision models hasfueled the rapid progress in multi-modal vision-text reasoning capabilities.However, existing vision-language models (VLMs) to date offer poor performancefor compositional reasoning. This paper presents VLAgent, a vision-languageagent system for vision-text compositional reasoning with three novel features.First, VLAgent leverages a pre-trained LLM with few-shot context learning togenerate the planning script for each compositional reasoning task and providesa backend engine to generate and perform executable runtime, which maps theplanning script into executable code using the VLAgent library for VLAgentexecutor. Second, VLAgent introduces the SS-parser, which identifies andcorrects logic errors embedded in the LLM-generated planning script, to furtherenhance the quality of script-executable mapping. Third, VLAgent introduces thecompositional reasoning output verifier, which validates and refines the outputof complex compositional reasoning steps, by leveraging complementary reasoningtechniques, e.g., ensemble learning and caption analysis. Extensive experimentsare conducted on six visual benchmarks and compared to a dozen of the SoTAvisual reasoning models. The results show that VLAgent outperforms existingrepresentative approaches for compositional text-visual reasoning. Our code anddatasets with outputs will be made available upon acceptance.</description><author>Yichang Xu, Gaowen Liu, Ramana Rao Kompella, Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Furkan Tekin, Zachary Yahn, Ling Liu</author><pubDate>Tue, 02 Sep 2025 03:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07778v2</guid></item><item><title>Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</title><link>http://arxiv.org/abs/2508.19005v2</link><description>As AI advances toward general intelligence, the focus is shifting fromsystems optimized for static tasks to creating open-ended agents that learncontinuously. In this paper, we introduce Experience-driven Lifelong Learning(ELL), a framework for building self-evolving agents capable of continuousgrowth through real-world interaction. The framework is built on four coreprinciples: (1) Experience Exploration: Agents learn through continuous,self-motivated interaction with dynamic environments, navigating interdependenttasks and generating rich experiential trajectories. (2) Long-term Memory:Agents preserve and structure historical knowledge, including personalexperiences, domain expertise, and commonsense reasoning, into a persistentmemory system. (3) Skill Learning: Agents autonomously improve by abstractingrecurring patterns from experience into reusable skills, which are activelyrefined and validated for application in new tasks. (4) KnowledgeInternalization: Agents internalize explicit and discrete experiences intoimplicit and intuitive capabilities as "second nature". We also introduce StuLife, a benchmark dataset for ELL that simulates astudent's holistic college journey, from enrollment to academic and personaldevelopment, across three core phases and ten detailed sub-scenarios. StuLifeis designed around three key paradigm</description><author>Yuxuan Cai, Yipeng Hao, Jie Zhou, Hang Yan, Zhikai Lei, Rui Zhen, Zhenhua Han, Yutao Yang, Junsong Li, Qianjun Pan, Tianyu Huai, Qin Chen, Xin Li, Kai Chen, Bo Zhang, Xipeng Qiu, Liang He</author><pubDate>Tue, 02 Sep 2025 03:28:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19005v2</guid></item><item><title>Multi-Agent System for Comprehensive Soccer Understanding</title><link>http://arxiv.org/abs/2505.03735v2</link><description>Recent advances in soccer understanding have demonstrated rapid progress, yetexisting research predominantly focuses on isolated or narrow tasks. To bridgethis gap, we propose a comprehensive framework for holistic soccerunderstanding. Concretely, we make the following contributions in this paper:(i) we construct SoccerWiki, the first large-scale multimodal soccer knowledgebase, integrating rich domain knowledge about players, teams, referees, andvenues to enable knowledge-driven reasoning; (ii) we present SoccerBench, thelargest and most comprehensive soccer-specific benchmark, featuring around 10Kmultimodal (text, image, video) multi-choice QA pairs across 13 distinct tasks;(iii) we introduce SoccerAgent, a novel multi-agent system that decomposescomplex soccer questions via collaborative reasoning, leveraging domainexpertise from SoccerWiki and achieving robust performance; (iv) extensiveevaluations and comparisons with representative MLLMs on SoccerBench highlightthe superiority of our agentic system.</description><author>Jiayuan Rao, Zifeng Li, Haoning Wu, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Tue, 02 Sep 2025 03:17:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.03735v2</guid></item><item><title>Bridging Synthetic-to-Real Gaps: Frequency-Aware Perturbation and Selection for Single-shot Multi-Parametric Mapping Reconstruction</title><link>http://arxiv.org/abs/2503.03475v2</link><description>Data-centric artificial intelligence (AI) has remarkably advanced medicalimaging, with emerging methods using synthetic data to address data scarcitywhile introducing synthetic-to-real gaps. Unsupervised domain adaptation (UDA)shows promise in ground truth-scarce tasks, but its application inreconstruction remains underexplored. Although multiple overlapping-echodetachment (MOLED) achieves ultra-fast multi-parametric reconstruction,extending its application to various clinical scenarios, the quality suffersfrom deficiency in mitigating the domain gap, difficulty in maintainingstructural integrity, and inadequacy in ensuring mapping accuracy. To resolvethese issues, we proposed frequency-aware perturbation and selection (FPS),comprising Wasserstein distance-modulated frequency-aware perturbation (WDFP)and hierarchical frequency-aware selection network (HFSNet), which integratesfrequency-aware adaptive selection (FAS), compact FAS (cFAS) and feature-awarearchitecture integration (FAI). Specifically, perturbation activatesdomain-invariant feature learning within uncertainty, while selection refinesoptimal solutions within perturbation, establishing a robust and closed-looplearning pathway. Extensive experiments on synthetic data, along with diversereal clinical cases from 5 healthy volunteers, 94 ischemic stroke patients, and46 meningioma patients, demonstrate the superiority and clinical applicabilityof FPS. Furthermore, FPS is applied to diffusion tensor imaging (DTI),underscoring its versatility and potential for broader medical applications.The code is available at https://github.com/flyannie/FPS.</description><author>Linyu Fan, Che Wang, Ming Ye, Qizhi Yang, Zejun Wu, Xinghao Ding, Yue Huang, Jianfeng Bao, Shuhui Cai, Congbo Cai</author><pubDate>Tue, 02 Sep 2025 03:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.03475v2</guid></item><item><title>ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</title><link>http://arxiv.org/abs/2505.04802v2</link><description>Sparse observations and coarse-resolution climate models limit effectiveregional decision-making, underscoring the need for robust downscaling.However, existing AI methods struggle with generalization across variables andgeographies and are constrained by the quadratic complexity of VisionTransformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundationmodel for global, hyper-resolution climate downscaling. ORBIT-2 incorporatestwo key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecturewith residual learning and Bayesian regularization for efficient, robustprediction; and (2) TILES, a tile-wise sequence scaling algorithm that reducesself-attention complexity from quadratic to linear, enabling long-sequenceprocessing and massive parallelism. ORBIT-2 scales to 10 billion parametersacross 65,536 GPUs, achieving up to 4.1 exaFLOPS sustained throughput and74--98% strong scaling efficiency. It supports downscaling to 0.9 km globalresolution and processes sequences up to 4.2 billion tokens. On 7 km resolutionbenchmarks, ORBIT-2 achieves high accuracy with $R^2$ scores in the range of0.98--0.99 against observational data.</description><author>Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, Xi Xiao, David Pugmire, Ming Fan, Nasik M. Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu</author><pubDate>Tue, 02 Sep 2025 03:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.04802v2</guid></item><item><title>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs</title><link>http://arxiv.org/abs/2505.11423v3</link><description>Reasoning-enhanced large language models (RLLMs), whether explicitly trainedfor reasoning or prompted via chain-of-thought (CoT), have achievedstate-of-the-art performance on many complex reasoning tasks. However, weuncover a surprising and previously overlooked phenomenon: explicit CoTreasoning can significantly degrade instruction-following accuracy. Evaluating15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)and ComplexBench (with complex, compositional constraints), we consistentlyobserve performance drops when CoT prompting is applied. Through large-scalecase studies and an attention-based analysis, we identify common patterns wherereasoning either helps (e.g., with formatting or lexical precision) or hurts(e.g., by neglecting simple constraints or introducing unnecessary content). Wepropose a metric, constraint attention, to quantify model focus duringgeneration and show that CoT reasoning often diverts attention away frominstruction-relevant tokens. To mitigate these effects, we introduce andevaluate four strategies: in-context learning, self-reflection, self-selectivereasoning, and classifier-selective reasoning. Our results demonstrate thatselective reasoning strategies, particularly classifier-selective reasoning,can substantially recover lost performance. To our knowledge, this is the firstwork to systematically expose reasoning-induced failures ininstruction-following and offer practical mitigation strategies.</description><author>Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal</author><pubDate>Tue, 02 Sep 2025 02:55:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11423v3</guid></item><item><title>Exploiting Diffusion Prior for Task-driven Image Restoration</title><link>http://arxiv.org/abs/2507.22459v2</link><description>Task-driven image restoration (TDIR) has recently emerged to addressperformance drops in high-level vision tasks caused by low-quality (LQ) inputs.Previous TDIR methods struggle to handle practical scenarios in which imagesare degraded by multiple complex factors, leaving minimal clues forrestoration. This motivates us to leverage the diffusion prior, one of the mostpowerful natural image priors. However, while the diffusion prior can helpgenerate visually plausible results, using it to restore task-relevant detailsremains challenging, even when combined with recent TDIR methods. To addressthis, we propose EDTR, which effectively harnesses the power of diffusion priorto restore task-relevant details. Specifically, we propose directly leveraginguseful clues from LQ images in the diffusion process by generating frompixel-error-based pre-restored LQ images with mild noise added. Moreover, weemploy a small number of denoising steps to prevent the generation of redundantdetails that dilute crucial task-related information. We demonstrate that ourmethod effectively utilizes diffusion prior for TDIR, significantly enhancingtask performance and visual quality across diverse tasks with multiple complexdegradations.</description><author>Jaeha Kim, Junghun Oh, Kyoung Mu Lee</author><pubDate>Tue, 02 Sep 2025 02:46:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22459v2</guid></item><item><title>Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster</title><link>http://arxiv.org/abs/2503.00229v3</link><description>Armijo line-search (Armijo-LS) is a standard method to set the step-size forgradient descent (GD). For smooth functions, Armijo-LS alleviates the need toknow the global smoothness constant L and adapts to the ``local'' smoothness,enabling GD to converge faster. Existing theoretical analyses show that GD withArmijo-LS (GD-LS) can result in constant factor improvements over GD with a 1/Lstep-size (denoted as GD(1/L)). We strengthen these results and show that ifthe objective function satisfies a certain non-uniform smoothness condition,GD-LS can result in a faster convergence rate than GD(1/L). In particular, weprove that for convex objectives corresponding to logistic regression andmulti-class classification, GD-LS can converge to the optimum at a linear rate,and hence improves over the sublinear convergence of GD(1/L). Furthermore, fornon-convex objectives satisfying gradient domination (e.g., those correspondingto the softmax policy gradient in RL or generalized linear models with alogistic link function), GD-LS can match the fast convergence of algorithmstailored for these specific settings. Finally, we analyze the convergence ofstochastic GD with a stochastic line-search on convex losses under theinterpolation assumption.</description><author>Sharan Vaswani, Reza Babanezhad</author><pubDate>Tue, 02 Sep 2025 02:33:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.00229v3</guid></item><item><title>MedVAL: Toward Expert-Level Medical Text Validation with Language Models</title><link>http://arxiv.org/abs/2507.03152v3</link><description>With the growing use of language models (LMs) in clinical environments, thereis an immediate need to evaluate the accuracy and safety of LM-generatedmedical text. Currently, such evaluation relies solely on manual physicianreview. However, detecting errors in LM-generated text is challenging because1) manual review is costly and 2) expert-composed reference outputs are oftenunavailable in real-world settings. While the "LM-as-judge" paradigm (a LMevaluating another LM) offers scalable evaluation, even frontier LMs can misssubtle but clinically significant errors. To address these challenges, wepropose MedVAL, a self-supervised framework that leverages synthetic data totrain evaluator LMs to assess whether LM-generated medical outputs arefactually consistent with inputs, without requiring physician labels orreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, adataset containing 840 outputs annotated by physicians, following aphysician-defined taxonomy of risk levels and error categories. Across 6diverse medical tasks and 10 state-of-the-art LMs spanning open-source,proprietary, and medically adapted models, MedVAL fine-tuning significantlyimproves (p &lt; 0.001) alignment with physicians on both seen and unseen tasks,increasing average F1 scores from 66% to 83%, with per-sample safetyclassification scores up to 86%. MedVAL improves the performance of even thebest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,risk-aware pathway towards clinical integration, we open-source the 1) codebase(https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench(https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B(https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performingopen-source LM. Our research provides the first evidence of LMs approachingexpert-level validation ability for medical text.</description><author>Asad Aali, Vasiliki Bikia, Maya Varma, Nicole Chiou, Sophie Ostmeier, Arnav Singhvi, Magdalini Paschali, Ashwin Kumar, Andrew Johnston, Karimar Amador-Martinez, Eduardo Juan Perez Guerrero, Paola Naovi Cruz Rivera, Sergios Gatidis, Christian Bluethgen, Eduardo Pontes Reis, Eddy D. Zandee van Rilland, Poonam Laxmappa Hosamani, Kevin R Keet, Minjoung Go, Evelyn Ling, David B. Larson, Curtis Langlotz, Roxana Daneshjou, Jason Hom, Sanmi Koyejo, Emily Alsentzer, Akshay S. Chaudhari</author><pubDate>Tue, 02 Sep 2025 02:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.03152v3</guid></item><item><title>ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</title><link>http://arxiv.org/abs/2508.18190v3</link><description>Semi-structured tables, widely used in real-world applications (e.g.,financial reports, medical records, transactional orders), often involveflexible and complex layouts (e.g., hierarchical headers and merged cells).These tables generally rely on human analysts to interpret table layouts andanswer relevant natural language questions, which is costly and inefficient. Toautomate the procedure, existing methods face significant challenges. First,methods like NL2SQL require converting semi-structured tables into structuredones, which often causes substantial information loss. Second, methods likeNL2Code and multi-modal LLM QA struggle to understand the complex layouts ofsemi-structured tables and cannot accurately answer corresponding questions. Tothis end, we propose ST-Raptor, a tree-based framework for semi-structuredtable question answering using large language models. First, we introduce theHierarchical Orthogonal Tree (HO-Tree), a structural model that capturescomplex semi-structured table layouts, along with an effective algorithm forconstructing the tree. Second, we define a set of basic tree operations toguide LLMs in executing common QA tasks. Given a user question, ST-Raptordecomposes it into simpler sub-questions, generates corresponding treeoperation pipelines, and conducts operation-table alignment for accuratepipeline execution. Third, we incorporate a two-stage verification mechanism:forward validation checks the correctness of execution steps, while backwardvalidation evaluates answer reliability by reconstructing queries frompredicted answers. To benchmark the performance, we present SSTQA, a dataset of764 questions over 102 real-world semi-structured tables. Experiments show thatST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The codeis available at https://github.com/weAIDB/ST-Raptor.</description><author>Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu</author><pubDate>Tue, 02 Sep 2025 02:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18190v3</guid></item><item><title>Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality</title><link>http://arxiv.org/abs/2508.05025v2</link><description>Augmented Reality (AR) systems, while enhancing task performance throughreal-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus onvirtual content that compromises situational awareness (SA) in safety-criticalscenarios. This paper investigates SA in AR-guided cardiopulmonaryresuscitation (CPR), where responders must balance effective compressions withvigilance to unpredictable hazards (e.g., patient vomiting). We developed an ARapp on a Magic Leap 2 that overlays real-time CPR feedback (compression depthand rate) and conducted a user study with simulated unexpected incidents (e.g.,bleeding) to evaluate SA, in which SA metrics were collected via observationand questionnaires administered during freeze-probe events. Eye trackinganalysis revealed that higher SA levels were associated with greater saccadicamplitude and velocity, and with reduced proportion and frequency of fixationson virtual content. To predict SA, we propose FixGraphPool, a graph neuralnetwork that structures gaze events (fixations, saccades) into spatiotemporalgraphs, effectively capturing dynamic attentional patterns. Our model achieved83.0% accuracy (F1=81.0%), outperforming feature-based machine learning andstate-of-the-art time-series models by leveraging domain knowledge andspatial-temporal information encoded in ET data. These findings demonstrate thepotential of eye tracking for SA modeling in AR and highlight its utility indesigning AR systems that ensure user safety and situational awareness.</description><author>Zhehan Qu, Tianyi Hu, Christian Fronk, Maria Gorlatova</author><pubDate>Tue, 02 Sep 2025 02:29:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05025v2</guid></item><item><title>Agent Trading Arena: A Study on Numerical Understanding in LLM-Based Agents</title><link>http://arxiv.org/abs/2502.17967v2</link><description>Large language models (LLMs) have demonstrated remarkable capabilities innatural language tasks, yet their performance in dynamic, real-world financialenvironments remains underexplored. Existing approaches are limited tohistorical backtesting, where trading actions cannot influence market pricesand agents train only on static data. To address this limitation, we presentthe Agent Trading Arena, a virtual zero-sum stock market in which LLM-basedagents engage in competitive multi-agent trading and directly impact pricedynamics. By simulating realistic bid-ask interactions, our platform enablestraining in scenarios that closely mirror live markets, thereby narrowing thegap between training and evaluation. Experiments reveal that LLMs struggle withnumerical reasoning when given plain-text data, often overfitting to localpatterns and recent values. In contrast, chart-based visualizationssignificantly enhance both numerical reasoning and trading performance.Furthermore, incorporating a reflection module yields additional improvements,especially with visual inputs. Evaluations on NASDAQ and CSI datasetsdemonstrate the superiority of our method, particularly under high volatility.All code and data are available athttps://github.com/wekjsdvnm/Agent-Trading-Arena.</description><author>Tianmi Ma, Jiawei Du, Wenxin Huang, Wenjie Wang, Liang Xie, Xian Zhong, Joey Tianyi Zhou</author><pubDate>Tue, 02 Sep 2025 02:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.17967v2</guid></item><item><title>Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring</title><link>http://arxiv.org/abs/2507.22369v3</link><description>Classroom behavior monitoring is a critical aspect of educational research,with significant implications for student engagement and learning outcomes.Recent advancements in Visual Question Answering (VQA) models offer promisingtools for automatically analyzing complex classroom interactions from videorecordings. In this paper, we investigate the applicability of severalstate-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, andNVILA, in the context of classroom behavior analysis. To facilitate rigorousevaluation, we introduce our BAV-Classroom-VQA dataset derived from real-worldclassroom video recordings at the Banking Academy of Vietnam. We present themethodology for data collection, annotation, and benchmark the performance ofthe selected VQA models on this dataset. Our initial experimental resultsdemonstrate that all four models achieve promising performance levels inanswering behavior-related visual questions, showcasing their potential infuture classroom analytics and intervention systems.</description><author>Sinh Trong Vu, Hieu Trung Pham, Dung Manh Nguyen, Hieu Minh Hoang, Nhu Hoang Le, Thu Ha Pham, Tai Tan Mai</author><pubDate>Tue, 02 Sep 2025 02:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22369v3</guid></item><item><title>Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation</title><link>http://arxiv.org/abs/2508.17796v2</link><description>Contextual automatic speech recognition (ASR) systems allow for recognizingout-of-vocabulary (OOV) words, such as named entities or rare words. However,it remains challenging due to limited training data and ambiguous orinconsistent pronunciations. In this paper, we propose a synthesis-drivenmulti-pronunciation contextual biasing method that performs zero-shotcontextual ASR on a pretrained Whisper model. Specifically, we leveragetext-to-speech (TTS) systems to synthesize diverse speech samples containingeach target rare word, and then use the pretrained Whisper model to extractmultiple predicted pronunciation variants. These variant token sequences arecompiled into a prefix-trie, which assigns rewards to beam hypotheses in ashallow-fusion manner during beam-search decoding. Subsequently, any recognizedvariant is mapped back to the original rare word in the final transcription.The evaluation results on the LibriSpeech dataset show that our method reducesbiased-word error rate (B-WER) by 43% on test-clean and 44% on test-other whilemaintaining unbiased-WER (U-WER) essentially unchanged.</description><author>Changsong Liu, Yizhou Peng, Eng Siong Chng</author><pubDate>Tue, 02 Sep 2025 02:20:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17796v2</guid></item><item><title>Machine Learning the 6d Supergravity Landscape</title><link>http://arxiv.org/abs/2505.16131v2</link><description>In this paper, we apply both supervised and unsupervised machine learningalgorithms to the study of the string landscape and swampland in 6-dimensions.Our data are the (almost) anomaly-free 6-dimensional $\mathcal{N} = (1,0)$supergravity models, characterised by the Gram matrix of anomaly coefficients.Our work demonstrates the ability of machine learning algorithms to efficientlylearn highly complex features of the landscape and swampland. Employing anautoencoder for unsupervised learning, we provide an auto-classification ofthese models by compressing the Gram matrix data to 2-dimensions. Throughcompression, similar models cluster together, and we identify prominentfeatures of these clusters. The autoencoder also identifies outlier modelswhich are difficult to reconstruct. One of these outliers proves to beincredibly difficult to combine with other models such that the$\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscapeextremely rare. Further, we utilise supervised learning to build twoclassifiers predicting (1) model consistency under probe string insertion(precision: 0.78, predicting consistency for 214,837 models with reasonablecertainty) and (2) inconsistency under anomaly inflow (precision: 0.91,predicting inconsistency for 1,909,359 models). Notably, projecting thesepredictions onto the autoencoder's 2-dimensional latent layer shows consistentmodels clustering together, further indicating that the autoencoder has learntinteresting and complex features of the set of models and potentially offers anovel approach to mapping the landscape and swampland of 6-dimensionalsupergravity theories.</description><author>Nathan Brady, David Tennyson, Thomas Vandermeulen</author><pubDate>Tue, 02 Sep 2025 02:14:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.16131v2</guid></item><item><title>Explainable post-training bias mitigation with distribution-based fairness metrics</title><link>http://arxiv.org/abs/2504.01223v2</link><description>We develop a novel optimization framework with distribution-based fairnessconstraints for efficiently producing demographically blind, explainable modelsacross a wide range of fairness levels. This is accomplished throughpost-processing, avoiding the need for retraining. Our framework, which isbased on stochastic gradient descent, can be applied to a wide range of modeltypes, with a particular emphasis on the post-processing of gradient-boosteddecision trees. Additionally, we design a broad class of interpretable globalbias metrics compatible with our method by building on previous work. Weempirically test our methodology on a variety of datasets and compare it toother methods.</description><author>Ryan Franks, Alexey Miroshnikov, Konstandinos Kotsiopoulos</author><pubDate>Tue, 02 Sep 2025 01:43:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.01223v2</guid></item><item><title>Demographic-aware fine-grained classification of pediatric wrist fractures</title><link>http://arxiv.org/abs/2507.12964v4</link><description>Wrist pathologies are frequently observed, particularly among children whoconstitute the majority of fracture cases. Computer vision presents a promisingavenue, contingent upon the availability of extensive datasets, a notablechallenge in medical imaging. Therefore, reliance solely on one modality, suchas images, proves inadequate, especially in an era of diverse and plentifuldata types. This study addresses the problem using a multifaceted approach:framing it as a fine-grained recognition task, fusing patient metadata withX-rays, and leveraging weights from a separate fine-grained dataset rather thanfrom a coarse-grained dataset like ImageNet. Unlike prior work, this is thefirst application of metadata integration for wrist pathology recognition. Ourresults show that combining fine-grained transformer approach, fine-grainedpre-training, and metadata integration improves diagnostic accuracy by 2% onsmall custom curated dataset and over 10% on a larger fracture dataset.</description><author>Ammar Ahmed, Ali Shariq Imran, Zenun Kastrati, Sher Muhammad Daudpota</author><pubDate>Tue, 02 Sep 2025 01:32:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.12964v4</guid></item><item><title>Pathology-Aware Adaptive Watermarking for Text-Driven Medical Image Synthesis</title><link>http://arxiv.org/abs/2503.08346v2</link><description>As recent text-conditioned diffusion models have enabled the generation ofhigh-quality images, concerns over their potential misuse have also grown. Thisissue is critical in the medical domain, where text-conditioned generatedmedical images could enable insurance fraud or falsified records, highlightingthe urgent need for reliable safeguards against unethical use. Whilewatermarking techniques have emerged as a promising solution in general imagedomains, their direct application to medical imaging presents significantchallenges. A key challenge is preserving fine-grained disease manifestations,as even minor distortions from a watermark may lead to clinicalmisinterpretation, which compromises diagnostic integrity. To overcome thisgap, we present MedSign, a deep learning-based watermarking frameworkspecifically designed for text-to-medical image synthesis, which preservespathologically significant regions by adaptively adjusting watermark strength.Specifically, we generate a pathology localization map using cross-attentionbetween medical text tokens and the diffusion denoising network, aggregatingtoken-wise attention across layers, heads, and time steps. Leveraging this map,we optimize the LDM decoder to incorporate watermarking during image synthesis,ensuring cohesive integration while minimizing interference in diagnosticallycritical regions. Experimental results show that our MedSign preservesdiagnostic integrity while ensuring watermark robustness, achievingstate-of-the-art performance in image quality and detection accuracy onMIMIC-CXR and OIA-ODIR datasets.</description><author>Chanyoung Kim, Dayun Ju, Jinyeong Kim, Woojung Han, Roberto Alcover-Couso, Seong Jae Hwang</author><pubDate>Tue, 02 Sep 2025 01:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.08346v2</guid></item><item><title>FedSPD: A Soft-clustering Approach for Personalized Decentralized Federated Learning</title><link>http://arxiv.org/abs/2410.18862v2</link><description>Federated learning has recently gained popularity as a framework fordistributed clients to collaboratively train a machine learning model usinglocal data. While traditional federated learning relies on a central server formodel aggregation, recent advancements adopt a decentralized framework,enabling direct model exchange between clients and eliminating the single pointof failure. However, existing decentralized frameworks often assume all clientstrain a shared model. Personalizing each client's model can enhanceperformance, especially with heterogeneous client data distributions. Wepropose FedSPD, an efficient personalized federated learning algorithm for thedecentralized setting, and show that it learns accurate models even inlow-connectivity networks. To provide theoretical guarantees on convergence, weintroduce a clustering-based framework that enables consensus on models fordistinct data clusters while personalizing to unique mixtures of these clustersat different clients. This flexibility, allowing selective model updates basedon data distribution, substantially reduces communication costs compared toprior work on personalized federated learning in decentralized settings.Experimental results on real-world datasets show that FedSPD outperformsmultiple decentralized variants of personalized federated learning algorithms,especially in scenarios with low-connectivity networks.</description><author>I-Cheng Lin, Osman Yagan, Carlee Joe-Wong</author><pubDate>Tue, 02 Sep 2025 00:47:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.18862v2</guid></item><item><title>Training and Evaluating with Human Label Variation: An Empirical Study</title><link>http://arxiv.org/abs/2502.01891v4</link><description>Human label variation (HLV) challenges the standard assumption that alabelled instance has a single ground truth, instead embracing the naturalvariation in human annotation to train and evaluate models. While varioustraining methods and metrics for HLV have been proposed, it is still unclearwhich methods and metrics perform best in what settings. We propose newevaluation metrics for HLV leveraging fuzzy set theory. Since these newproposed metrics are differentiable, we then in turn experiment with employingthese metrics as training objectives. We conduct an extensive study over 6 HLVdatasets testing 14 training methods and 6 evaluation metrics. We find thattraining on either disaggregated annotations or soft labels performs bestacross metrics, outperforming training using the proposed training objectiveswith differentiable metrics. We also show that our proposed soft micro F1 scoreis one of the best metrics for HLV data.</description><author>Kemal Kurniawan, Meladel Mistica, Timothy Baldwin, Jey Han Lau</author><pubDate>Tue, 02 Sep 2025 00:36:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.01891v4</guid></item><item><title>DobLIX: A Dual-Objective Learned Index for Log-Structured Merge Trees</title><link>http://arxiv.org/abs/2502.05369v2</link><description>In this paper, we introduce DobLIX, a dual-objective learned indexspecifically designed for Log-Structured Merge(LSM) tree-based key-valuestores. Although traditional learned indexes focus exclusively on optimizingindex lookups, they often overlook the impact of data access from storage,resulting in performance bottlenecks. DobLIX addresses this by incorporating asecond objective, data access optimization, into the learned index trainingprocess. This dual-objective approach ensures that both index lookup efficiencyand data access costs are minimized, leading to significant improvements inread performance while maintaining write efficiency in real-world LSM-treesystems. Additionally, DobLIX features a reinforcement learning agent thatdynamically tunes the system parameters, allowing it to adapt to varyingworkloads in real-time. Experimental results using real-world datasetsdemonstrate that DobLIX reduces indexing overhead and improves throughput by1.19 to 2.21 times compared to state-of-the-art methods within RocksDB, awidely used LSM-tree-based storage engine.</description><author>Alireza Heidari, Amirhossein Ahmadi, Wei Zhang</author><pubDate>Tue, 02 Sep 2025 00:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.05369v2</guid></item><item><title>Benchmarking LLM Privacy Recognition for Social Robot Decision Making</title><link>http://arxiv.org/abs/2507.16124v2</link><description>While robots have previously utilized rule-based systems or probabilisticmodels for user interaction, the rapid evolution of large language models(LLMs) presents new opportunities to develop LLM-powered robots for enhancedhuman-robot interaction (HRI). To fully realize these capabilities, however,robots need to collect data such as audio, fine-grained images, video, andlocations. As a result, LLMs often process sensitive personal information,particularly within private environments, such as homes. Given the tensionbetween utility and privacy risks, evaluating how current LLMs manage sensitivedata is critical. Specifically, we aim to explore the extent to whichout-of-the-box LLMs are privacy-aware in the context of household robots. Inthis work, we present a set of privacy-relevant scenarios developed using theContextual Integrity (CI) framework. We first surveyed users' privacypreferences regarding in-home robot behaviors and then examined how theirprivacy orientations affected their choices of these behaviors (N = 450). Wethen provided the same set of scenarios and questions to state-of-the-art LLMs(N = 10) and found that the agreement between humans and LLMs was generallylow. To further investigate the capabilities of LLMs as potential privacycontrollers, we implemented four additional prompting strategies and comparedtheir results. We discuss the performance of the evaluated models as well asthe implications and potential of AI privacy awareness in human-robotinteraction.</description><author>Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz</author><pubDate>Mon, 01 Sep 2025 23:52:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.16124v2</guid></item><item><title>Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids</title><link>http://arxiv.org/abs/2502.20396v2</link><description>Learning generalizable robot manipulation policies, especially for complexmulti-fingered humanoids, remains a significant challenge. Existing approachesprimarily rely on extensive data collection and imitation learning, which areexpensive, labor-intensive, and difficult to scale. Sim-to-real reinforcementlearning (RL) offers a promising alternative, but has mostly succeeded insimpler state-based or single-hand setups. How to effectively extend this tovision-based, contact-rich bimanual manipulation tasks remains an openquestion. In this paper, we introduce a practical sim-to-real RL recipe thattrains a humanoid robot to perform three challenging dexterous manipulationtasks: grasp-and-reach, box lift and bimanual handover. Our method features anautomated real-to-sim tuning module, a generalized reward formulation based oncontact and object goals, a divide-and-conquer policy distillation framework,and a hybrid object representation strategy with modality-specificaugmentation. We demonstrate high success rates on unseen objects and robust,adaptive policy behaviors -- highlighting that vision-based dexterousmanipulation via sim-to-real RL is not only viable, but also scalable andbroadly applicable to real-world humanoid manipulation tasks.</description><author>Toru Lin, Kartik Sachdev, Linxi Fan, Jitendra Malik, Yuke Zhu</author><pubDate>Mon, 01 Sep 2025 23:08:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.20396v2</guid></item><item><title>Debiased maximum-likelihood estimators for hazard ratios under machine-learning adjustment</title><link>http://arxiv.org/abs/2507.17686v2</link><description>Previous studies have shown that hazard ratios between treatment groupsestimated with the Cox model are uninterpretable because the indefinitebaseline hazard of the model fails to identify temporal change in the risk setcomposition due to treatment assignment and unobserved factors among multiple,contradictory scenarios. To alleviate this problem, especially in studies basedon observational data with uncontrolled dynamic treatment and real-timemeasurement of many covariates, we propose abandoning the baseline hazard andusing machine learning to explicitly model the change in the risk set with orwithout latent variables. For this framework, we clarify the context in whichhazard ratios can be causally interpreted, and then develop a method based onNeyman orthogonality to compute debiased maximum-likelihood estimators ofhazard ratios. Numerical simulations confirm that the proposed methodidentifies the ground truth with minimal bias. These results lay the foundationfor developing a useful, alternative method for causal inference withuncontrolled, observational data in modern epidemiology.</description><author>Takashi Hayakawa, Satoshi Asai</author><pubDate>Mon, 01 Sep 2025 22:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.17686v2</guid></item><item><title>VSLLaVA: a pipeline of large multimodal foundation model for industrial vibration signal analysis</title><link>http://arxiv.org/abs/2409.07482v2</link><description>While Large Multimodal Models (LMMs) excel in general multimodal tasks, theylack the domain-specific knowledge for industrial vibration signal analysis.This paper introduces VSLLaVA, a comprehensive pipeline that utilizes expertknowledge-guided instruction tuning and evaluation to create an end-to-end LMMfor signal analysis. To achieve this, we construct a novelSignal-Question-Answer (SQA) dataset using an expert rule-based signalgenerator. This dataset facilitates a two-stage learning procedure. The firststep is efficient instruction fine-tuning with Low-Rank Adaptation (LoRA),which imparts specialized signal identification capabilities. Subsequently, wedesigned a tailored Group Relative Policy Optimization (GRPO) to refine thereasoning capabilities and enhance classification robustness. Then, a dual-modeevaluation framework is proposed, combining an LLM referee with expert rulesfor semantic assessment using quantitative metrics for numerical and textualaccuracy, which reveals that VSLLaVA significantly improves performance insignal type identification and parameter analysis, and makes progress in theidentification and parameter analysis of fault-related signals. This researchdemonstrates a viable approach for developing specialized foundational modelsfor complex industrial applications and marks a transition from conventionaltask-specific systems to a cohesive, interactive foundational model.</description><author>Qi Li, Xinran Zhang, Jinfeng Huang, Hongliang He, Feibin Zhang, Zhaoye Qin, Fulei Chu</author><pubDate>Mon, 01 Sep 2025 21:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07482v2</guid></item><item><title>APEX$^2$: Adaptive and Extreme Summarization for Personalized Knowledge Graphs</title><link>http://arxiv.org/abs/2412.17336v2</link><description>Knowledge graphs (KGs), which store an extensive number of relational facts,serve various applications. Recently, personalized knowledge graphs (PKGs) haveemerged as a solution to optimize storage costs by customizing their content toalign with users' specific interests within particular domains. In the realworld, on one hand, user queries and their underlying interests are inherentlyevolving, requiring PKGs to adapt continuously; on the other hand, thesummarization is constantly expected to be as small as possible in terms ofstorage cost. However, the existing PKG summarization methods implicitly assumethat the user's interests are constant and do not shift. Furthermore, when thesize constraint of PKG is extremely small, the existing methods cannotdistinguish which facts are more of immediate interest and guarantee theutility of the summarized PKG. To address these limitations, we proposeAPEX$^2$, a highly scalable PKG summarization framework designed with robusttheoretical guarantees to excel in adaptive summarization tasks with extremelysmall size constraints. To be specific, after constructing an initial PKG,APEX$^2$ continuously tracks the interest shift and adjusts the previoussummary. We evaluate APEX$^2$ under an evolving query setting on benchmark KGscontaining up to 12 million triples, summarizing with compression ratios $\leq0.1\%$. The experiments show that APEX outperforms state-of-the-art baselinesin terms of both query-answering accuracy and efficiency. Code is available athttps://github.com/iDEA-iSAIL-Lab-UIUC/APEX.</description><author>Zihao Li, Dongqi Fu, Mengting Ai, Jingrui He</author><pubDate>Mon, 01 Sep 2025 21:12:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.17336v2</guid></item><item><title>Debiasing Guidance for Discrete Diffusion with Sequential Monte Carlo</title><link>http://arxiv.org/abs/2502.06079v3</link><description>Discrete diffusion models are a class of generative models that producesamples from an approximated data distribution within a discrete state space.Often, there is a need to target specific regions of the data distribution.Current guidance methods aim to sample from a distribution with massproportional to $p_0(x_0) p(\zeta|x_0)^\alpha$ but fail to achieve this inpractice. We introduce a Sequential Monte Carlo algorithm that generatesunbiasedly from this target distribution, utilising the learnt unconditionaland guided process. We validate our approach on low-dimensional distributions,controlled images and text generations. For text generation, our methodprovides strong control while maintaining low perplexity compared toguidance-based approaches.</description><author>Cheuk Kit Lee, Paul Jeha, Jes Frellsen, Pietro Lio, Michael Samuel Albergo, Francisco Vargas</author><pubDate>Mon, 01 Sep 2025 20:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.06079v3</guid></item><item><title>Two-Sided Nearest Neighbors: An adaptive and minimax optimal procedure for matrix completion</title><link>http://arxiv.org/abs/2411.12965v2</link><description>Nearest neighbor (NN) algorithms have been extensively used for missing dataproblems in recommender systems and sequential decision-making systems. Priortheoretical analysis has established favorable guarantees for NN when theunderlying data is sufficiently smooth and the missingness probabilities arelower bounded. Here we analyze NN with non-smooth non-linear functions withvast amounts of missingness. In particular, we consider matrix completionsettings where the entries of the underlying matrix follow a latent non-linearfactor model, with the non-linearity belonging to a \Holder function class thatis less smooth than Lipschitz. Our results establish following favorableproperties for a suitable two-sided NN: (1) The mean squared error (MSE) of NNadapts to the smoothness of the non-linearity, (2) under certain regularityconditions, the NN error rate matches the rate obtained by an oracle equippedwith the knowledge of both the row and column latent factors, and finally (3)NN's MSE is non-trivial for a wide range of settings even when several matrixentries might be missing deterministically. We support our theoretical findingsvia extensive numerical simulations and a case study with data from a mobilehealth study, HeartSteps.</description><author>Tathagata Sadhukhan, Manit Paul, Raaz Dwivedi</author><pubDate>Mon, 01 Sep 2025 20:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12965v2</guid></item><item><title>Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need</title><link>http://arxiv.org/abs/2507.13966v2</link><description>Language models traditionally used for cross-domain generalization haverecently demonstrated task-specific reasoning. However, their top-down trainingapproach on general corpora is insufficient for acquiring abstractions neededfor deep domain expertise. This may require a bottom-up approach that acquiresexpertise by learning to compose simple domain concepts into more complex ones.A knowledge graph (KG) provides this compositional structure, where domainprimitives are represented as head-relation-tail edges and their paths encodehigher-level concepts. We present a task generation pipeline that synthesizestasks directly from KG primitives, enabling models to acquire and compose themfor reasoning. We fine-tune language models on the resultant KG-groundedcurriculum to demonstrate domain-specific superintelligence. While broadlyapplicable, we validate our approach in medicine, where reliable KGs exist.Using a medical KG, we curate 24,000 reasoning tasks paired with thinkingtraces derived from diverse medical primitives. We fine-tune the QwQ-32B modelon this curriculum to obtain QwQ-Med-3 that takes a step towards medicalsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantifyreasoning abilities across 15 medical domains. Our experiments demonstrate thatQwQ-Med-3 significantly outperforms state-of-the-art reasoning models onICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquiredprimitives to widen the performance gap on the hardest tasks of ICD-Bench.Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3transfers acquired expertise to enhance the base model's performance. While theindustry's approach to artificial general intelligence (AGI) emphasizes broadexpertise, we envision a future in which AGI emerges from the composableinteraction of efficient domain-specific superintelligent agents.</description><author>Bhishma Dedhia, Yuval Kansal, Niraj K. Jha</author><pubDate>Mon, 01 Sep 2025 20:28:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.13966v2</guid></item><item><title>Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models</title><link>http://arxiv.org/abs/2505.16959v2</link><description>Diffusion probabilistic models have become a cornerstone of modern generativeAI, yet the mechanisms underlying their generalization remain poorlyunderstood. In fact, if these models were perfectly minimizing their trainingloss, they would just generate data belonging to their training set, i.e.,memorize, as empirically found in the overparameterized regime. We revisit thisview by showing that, in highly overparameterized diffusion models,generalization in natural data domains is progressively achieved duringtraining before the onset of memorization. Our results, ranging from image tolanguage diffusion models, systematically support the empirical law thatmemorization time is proportional to the dataset size. Generalization vs.memorization is then best understood as a competition between time scales. Weshow that this phenomenology is recovered in diffusion models learning a simpleprobabilistic context-free grammar with random rules, where generalizationcorresponds to the hierarchical acquisition of deeper grammar rules as trainingtime grows, and the generalization cost of early stopping can be characterized.We summarize these results in a phase diagram. Overall, our results supportthat a principled early-stopping criterion - scaling with dataset size - caneffectively optimize generalization while avoiding memorization, with directimplications for hyperparameter transfer and privacy-sensitive applications.</description><author>Alessandro Favero, Antonio Sclocchi, Matthieu Wyart</author><pubDate>Mon, 01 Sep 2025 20:18:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.16959v2</guid></item><item><title>Self-Supervised Learning-Based Path Planning and Obstacle Avoidance Using PPO and B-Splines in Unknown Environments</title><link>http://arxiv.org/abs/2412.02176v2</link><description>This paper introduces SmartBSP, an advanced self-supervised learningframework for real-time path planning and obstacle avoidance in autonomousrobotics navigating through complex environments. The proposed systemintegrates Proximal Policy Optimization (PPO) with Convolutional NeuralNetworks (CNN) and Actor-Critic architecture to process limited LIDAR inputsand compute spatial decision-making probabilities. The robot's perceptual fieldis discretized into a grid format, which the CNN analyzes to produce a spatialprobability distribution. During the training process a nuanced cost functionis minimized that accounts for path curvature, endpoint proximity, and obstacleavoidance. Simulations results in different scenarios validate the algorithm'sresilience and adaptability across diverse operational scenarios. Subsequently,Real-time experiments, employing the Robot Operating System (ROS), were carriedout to assess the efficacy of the proposed algorithm.</description><author>Shahab Shokouhi, Oguzhan Oruc, May-Win Thein</author><pubDate>Mon, 01 Sep 2025 20:13:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.02176v2</guid></item><item><title>ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation</title><link>http://arxiv.org/abs/2507.14201v2</link><description>We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x onthe task of Cyber Threat Investigation through security questions derived frominvestigation graphs. Real-world security analysts must sift through a largenumber of heterogeneous alert signals and security logs, follow multi-hopchains of evidence, and compile an incident report. With the developments ofLLMs, building LLM-based agents for automatic thread investigation is apromising direction. To assist the development and evaluation of LLM agents, weconstruct a dataset from a controlled Azure tenant that covers 8 simulatedreal-world multi-step attacks, 57 log tables from Microsoft Sentinel andrelated services, and 589 automatically generated questions. We leveragesecurity logs extracted with expert-crafted detection logic to build threatinvestigation graphs, and then generate questions with LLMs using paired nodeson the graph, taking the start node as background context and the end node asanswer. Anchoring each question to these explicit nodes and edges not onlyprovides automatic, explainable ground truth answers but also makes thepipeline reusable and readily extensible to new logs. This also enables theautomatic generation of procedural tasks with verifiable rewards, which can benaturally extended to training agents via reinforcement learning. Ourcomprehensive experiments with different models confirm the difficulty of thetask: with the base setting, the average reward across all evaluated models is0.249, and the best achieved is 0.368, leaving substantial headroom for futureresearch. Code and data are coming soon!</description><author>Yiran Wu, Mauricio Velazco, Andrew Zhao, Manuel Raúl Meléndez Luján, Srisuma Movva, Yogesh K Roy, Quang Nguyen, Roberto Rodriguez, Qingyun Wu, Michael Albada, Julia Kiseleva, Anand Mudgerikar</author><pubDate>Mon, 01 Sep 2025 20:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14201v2</guid></item><item><title>Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing Demand Forecasting Models</title><link>http://arxiv.org/abs/2508.13057v2</link><description>Inventory management in dynamic and competitive business environmentspresents multidimensional challenges, particularly in the face of demanduncertainty and logistical and financial constraints. In this context, accuratedemand forecasting is critical for optimizing resources and anticipating marketfluctuations. However, the isolated use of traditional metrics such as MeanAbsolute Error (MAE) or Root Mean Squared Error (RMSE) can lead to biasedevaluations and limit model robustness. To address this limitation, we proposethe Hierarchical Evaluation Function (HEF), a composite function thatintegrates R2, MAE, and RMSE under a hierarchical and dynamic framework,complemented by adaptive penalties. The study implements HEF in theoptimization of multiple prediction models, applying Grid Search, ParticleSwarm Optimization (PSO), and Optuna, and evaluating their performance onreference databases (Walmart, M3, M4, and M5). The results, validated usingstatistical tests, confirm that HEF consistently outperforms the MAE used asthe evaluation function in global metrics such as R2, Global RelativePrecision, RMSE, and RMSSE, improving explanatory power and stability againstextreme errors. In contrast, the MAE retains advantages in simplicity andcomputational efficiency. In summary, HEF constitutes a robust and adaptivealternative for highly variable environments, providing a solid framework formodel selection and hyperparameter optimization.</description><author>Adolfo González, Víctor Parada</author><pubDate>Mon, 01 Sep 2025 19:54:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13057v2</guid></item><item><title>Evaluating Language Models as Synthetic Data Generators</title><link>http://arxiv.org/abs/2412.03679v2</link><description>Given the increasing use of synthetic data in language model (LM)post-training, an LM's ability to generate high-quality data has become nearlyas crucial as its ability to solve problems directly. While prior works havefocused on developing effective data generation methods, they lack systematiccomparison of different LMs as data generators in a unified setting. To addressthis gap, we propose AgoraBench, a benchmark that provides standardizedsettings and metrics to evaluate LMs' data generation abilities. Throughsynthesizing 1.26 million training instances using 6 LMs and training 99student models, we uncover key insights about LMs' data generationcapabilities. First, we observe that LMs exhibit distinct strengths. Forinstance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnetperforms better at enhancing existing ones. Furthermore, our analysis revealsthat an LM's data generation ability doesn't necessarily correlate with itsproblem-solving ability. Instead, multiple intrinsic features of dataquality-including response quality, perplexity, and instructiondifficulty-collectively serve as better indicators. Finally, we demonstratethat strategic choices in output format and cost-conscious model selectionsignificantly impact data generation effectiveness.</description><author>Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee, Yizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, Graham Neubig</author><pubDate>Mon, 01 Sep 2025 19:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03679v2</guid></item><item><title>Intrinsic Test of Unlearning Using Parametric Knowledge Traces</title><link>http://arxiv.org/abs/2406.11614v3</link><description>The task of "unlearning" certain concepts in large language models (LLMs) hasattracted immense attention recently, due to its importance in mitigatingundesirable model behaviours, such as the generation of harmful, private, orincorrect information. Current protocols to evaluate unlearning methods largelyrely on behavioral tests, without monitoring the presence of unlearnedknowledge within the model's parameters. This residual knowledge can beadversarially exploited to recover the erased information post-unlearning. Weargue that unlearning should also be evaluated internally, by consideringchanges in the parametric knowledge traces of the unlearned concepts. To thisend, we propose a general evaluation methodology that leverages vocabularyprojections to inspect concepts encoded in model parameters. We use thisapproach to localize "concept vectors" - parameter vectors that encode concreteconcepts - and construct ConceptVectors, a benchmark dataset containinghundreds of common concepts and their parametric knowledge traces within twoopen-source LLMs. Evaluation on ConceptVectors shows that existing unlearningmethods minimally impact concept vectors and mostly suppress them duringinference, while directly ablating these vectors demonstrably removes theassociated knowledge and significantly reduces the model's susceptibility toadversarial manipulation. Our results highlight limitations in behavioral-basedunlearning evaluations and call for future work to include parameter-basedevaluations. To support this, we release our code and benchmark athttps://github.com/yihuaihong/ConceptVectors.</description><author>Yihuai Hong, Lei Yu, Haiqin Yang, Shauli Ravfogel, Mor Geva</author><pubDate>Mon, 01 Sep 2025 18:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11614v3</guid></item><item><title>Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation</title><link>http://arxiv.org/abs/2505.05287v2</link><description>Humans naturally exhibit bilateral symmetry in their gross manipulationskills, effortlessly mirroring simple actions between left and right hands.Bimanual robots-which also feature bilateral symmetry-should similarly exploitthis property to perform tasks with either hand. Unlike humans, who often favora dominant hand for fine dexterous skills, robots should ideally executeambidextrous manipulation with equal proficiency. To this end, we introduceSYMDEX (SYMmetric DEXterity), a reinforcement learning framework forambidextrous bi-manipulation that leverages the robot's inherent bilateralsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulationtasks into per-hand subtasks and trains dedicated policies for each. Byexploiting bilateral symmetry via equivariant neural networks, experience fromone arm is inherently leveraged by the opposite arm. We then distill thesubtask policies into a global ambidextrous policy that is independent of thehand-task assignment. We evaluate SYMDEX on six challenging simulatedmanipulation tasks and demonstrate successful real-world deployment on two ofthem. Our approach strongly outperforms baselines on complex task in which theleft and right hands perform different roles. We further demonstrate SYMDEX'sscalability by extending it to a four-arm manipulation setup, where oursymmetry-aware policies enable effective multi-arm collaboration andcoordination. Our results highlight how structural symmetry as inductive biasin policy learning enhances sample efficiency, robustness, and generalizationacross diverse dexterous manipulation tasks.</description><author>Zechu Li, Yufeng Jin, Daniel Ordonez Apraez, Claudio Semini, Puze Liu, Georgia Chalvatzaki</author><pubDate>Mon, 01 Sep 2025 18:49:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.05287v2</guid></item><item><title>How Does Knowledge Selection Help Retrieval Augmented Generation?</title><link>http://arxiv.org/abs/2410.13258v4</link><description>Retrieval-augmented generation (RAG) is a powerful method for enhancingnatural language generation by integrating external knowledge into a model'soutput. While prior work has demonstrated the importance of improving knowledgeretrieval for boosting generation quality, the role of knowledge selection,a.k.a. reranking or filtering, remains less clear. This paper empiricallyanalyzes how knowledge selection influences downstream generation performancein RAG systems. By simulating different retrieval and selection conditionsthrough a controlled mixture of gold and distractor knowledge, we assess theimpact of these factors on generation outcomes. Our findings indicate that thedownstream generator model's capability, as well as the complexity of the taskand dataset, significantly influence the impact of knowledge selection on theoverall RAG system performance. In typical scenarios, improving the knowledgerecall score is key to enhancing generation outcomes, with the knowledgeselector providing limited benefit when a strong generator model is used onclear, well-defined tasks. For weaker generator models or more ambiguous tasksand datasets, the knowledge F1 score becomes a critical factor, and theknowledge selector plays a more prominent role in improving overallperformance.</description><author>Xiangci Li, Jessica Ouyang</author><pubDate>Mon, 01 Sep 2025 18:35:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13258v4</guid></item><item><title>Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning</title><link>http://arxiv.org/abs/2502.03275v2</link><description>Large Language Models (LLMs) excel at reasoning and planning when trained onchainof-thought (CoT) data, where the step-by-step thought process isexplicitly outlined by text tokens. However, this results in lengthy inputswhere many words support textual coherence rather than core reasoninginformation, and processing these inputs consumes substantial computationresources. In this work, we propose a hybrid representation of the reasoningprocess, where we partially abstract away the initial reasoning steps usinglatent discrete tokens generated by VQ-VAE, significantly reducing the lengthof reasoning traces. We explore the use of latent trace abstractions in twoscenarios: 1) training the model from scratch for the Keys-Finding Mazeproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabularyincluding unseen latent tokens, for both logical and mathematical reasoningproblems. To facilitate effective learning, we introduce a simple trainingprocedure that randomly mixes latent and text tokens, which enables fastadaptation to new latent tokens. Our approach consistently outperforms thebaselines methods in various benchmarks.</description><author>DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng</author><pubDate>Mon, 01 Sep 2025 18:25:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.03275v2</guid></item><item><title>Mitosis detection in domain shift scenarios: a Mamba-based approach</title><link>http://arxiv.org/abs/2508.21033v2</link><description>Mitosis detection in histopathology images plays a key role in tumorassessment. Although machine learning algorithms could be exploited for aidingphysicians in accurately performing such a task, these algorithms suffer fromsignificative performance drop when evaluated on images coming from domainsthat are different from the training ones. In this work, we propose aMamba-based approach for mitosis detection under domain shift, inspired by thepromising performance demonstrated by Mamba in medical imaging segmentationtasks. Specifically, our approach exploits a VM-UNet architecture for carryingout the addressed task, as well as stain augmentation operations for furtherimproving model robustness against domain shift. Our approach has beensubmitted to the track 1 of the MItosis DOmain Generalization (MIDOG)challenge. Preliminary experiments, conducted on the MIDOG++ dataset, showlarge room for improvement for the proposed method.</description><author>Gennaro Percannella, Mattia Sarno, Francesco Tortorella, Mario Vento</author><pubDate>Mon, 01 Sep 2025 18:09:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21033v2</guid></item><item><title>How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench</title><link>http://arxiv.org/abs/2508.20931v2</link><description>Recent advances in reasoning and planning capabilities of large languagemodels (LLMs) have enabled their potential as autonomous agents capable of tooluse in dynamic environments. However, in multi-turn conversational environmentslike $\tau$-bench, these agents often struggle with consistent reasoning,adherence to domain-specific policies, and extracting correct information overa long horizon of tool-calls and conversation. To capture and mitigate thesefailures, we conduct a comprehensive manual analysis of the common errorsoccurring in the conversation trajectories. We then experiment withreformulations of inputs to the tool-calling agent for improvement in agentdecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)framework, which automatically reformulates user queries augmented withrelevant domain rules and tool suggestions for the tool-calling agent to focuson. The results show that IRMA significantly outperforms ReAct, FunctionCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, inoverall pass^5 scores. These findings highlight the superior reliability andconsistency of IRMA compared to other methods in dynamic environments.</description><author>Venkatesh Mishra, Amir Saeidi, Satyam Raj, Mutsumi Nakamura, Jayanth Srinivasa, Gaowen Liu, Ali Payani, Chitta Baral</author><pubDate>Mon, 01 Sep 2025 18:05:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20931v2</guid></item><item><title>Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination</title><link>http://arxiv.org/abs/2403.17231v2</link><description>This paper introduces Dynamic Learning from Learned Hallucination(Dyna-LfLH), a self-supervised method for training motion planners to navigateenvironments with dense and dynamic obstacles. Classical planners struggle withdense, unpredictable obstacles due to limited computation, while learning-basedplanners face challenges in acquiring high- quality demonstrations forimitation learning or dealing with exploration inefficiencies in reinforcementlearning. Building on Learning from Hallucination (LfH), which synthesizestraining data from past successful navigation experiences in simplerenvironments, Dyna-LfLH incorporates dynamic obstacles by generating themthrough a learned latent distribution. This enables efficient and safe motionplanner training. We evaluate Dyna-LfLH on a ground robot in both simulated andreal environments, achieving up to a 25% improvement in success rate comparedto baselines.</description><author>Saad Abdul Ghani, Zizhao Wang, Peter Stone, Xuesu Xiao</author><pubDate>Mon, 01 Sep 2025 18:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17231v2</guid></item><item><title>UniBERT: Adversarial Training for Language-Universal Representations</title><link>http://arxiv.org/abs/2503.12608v3</link><description>This paper presents UniBERT, a compact multilingual language model that usesan innovative training framework that integrates three components: maskedlanguage modeling, adversarial training, and knowledge distillation.Pre-trained on a meticulously curated Wikipedia corpus spanning 107 languages,UniBERT is designed to reduce the computational demands of large-scale modelswhile maintaining competitive performance across various natural languageprocessing tasks. Comprehensive evaluations on four tasks - named entityrecognition, natural language inference, question answering, and semantictextual similarity - demonstrate that our multilingual training strategyenhanced by an adversarial objective significantly improves cross-lingualgeneralization. Specifically, UniBERT models show an average relativeimprovement of 7.72% over traditional baselines, which achieved an averagerelative improvement of only 1.17%, and statistical analysis confirms thesignificance of these gains (p-value = 0.0181). This work highlights thebenefits of combining adversarial training and knowledge distillation to buildscalable and robust language models, thus advancing the field of multilingualand cross-lingual natural language processing.</description><author>Andrei-Marius Avram, Marian Lupaşcu, Dumitru-Clementin Cercel, Ionuţ Mironică, Ştefan Trăuşan-Matu</author><pubDate>Mon, 01 Sep 2025 18:01:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.12608v3</guid></item><item><title>Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures</title><link>http://arxiv.org/abs/2408.14875v2</link><description>The emergence of deep learning models has revolutionized various industriesover the last decade, leading to a surge in connected devices andinfrastructures. However, these models can be tricked into making incorrectpredictions with high confidence, leading to disastrous failures and securityconcerns. To this end, we explore the impact of adversarial attacks onmultivariate time-series forecasting and investigate methods to counter them.Specifically, we employ untargeted white-box attacks, namely the Fast GradientSign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputsto the training process, effectively misleading the model. We also illustratethe subtle modifications to the inputs after the attack, which makes detectingthe attack using the naked eye quite difficult. Having demonstrated thefeasibility of these attacks, we develop robust models through adversarialtraining and model hardening. We are among the first to showcase thetransferability of these attacks and defenses by extrapolating our work fromthe benchmark electricity data to a larger, 10-year real-world data used forpredicting the time-to-failure of hard disks. Our experimental results confirmthat the attacks and defenses achieve the desired security thresholds, leadingto a 72.41% and 94.81% decrease in RMSE for the electricity and hard diskdatasets respectively after implementing the adversarial defenses.</description><author>Pooja Krishan, Rohan Mohapatra, Sanchari Das, Saptarshi Sengupta</author><pubDate>Mon, 01 Sep 2025 17:42:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.14875v2</guid></item><item><title>Data-driven Discovery of Digital Twins in Biomedical Research</title><link>http://arxiv.org/abs/2508.21484v2</link><description>Recent technological advances have expanded the availability ofhigh-throughput biological datasets, enabling the reliable design of digitaltwins of biomedical systems or patients. Such computational tools represent keyreaction networks driving perturbation or drug response and can guide drugdiscovery and personalized therapeutics. Yet, their development still relies onlaborious data integration by the human modeler, so that automated approachesare critically needed. The success of data-driven system discovery in Physics,rooted in clean datasets and well-defined governing laws, has fueled interestin applying similar techniques in Biology, which presents unique challenges.Here, we reviewed methodologies for automatically inferring digital twins frombiological time series, which mostly involve symbolic or sparse regression. Weevaluate algorithms according to eight biological and methodologicalchallenges, associated to noisy/incomplete data, multiple conditions, priorknowledge integration, latent variables, high dimensionality, unobservedvariable derivatives, candidate library design, and uncertainty quantification.Upon these criteria, sparse regression generally outperformed symbolicregression, particularly when using Bayesian frameworks. We further highlightthe emerging role of deep learning and large language models, which enableinnovative prior knowledge integration, though the reliability and consistencyof such approaches must be improved. While no single method addresses allchallenges, we argue that progress in learning digital twins will come fromhybrid and modular frameworks combining chemical reaction network-basedmechanistic grounding, Bayesian uncertainty quantification, and the generativeand knowledge integration capacities of deep learning. To support theirdevelopment, we further propose a benchmarking framework to evaluate methodsacross all challenges.</description><author>Clémence Métayer, Annabelle Ballesta, Julien Martinelli</author><pubDate>Mon, 01 Sep 2025 17:06:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21484v2</guid></item><item><title>Controlled Latent Diffusion Models for 3D Porous Media Reconstruction</title><link>http://arxiv.org/abs/2503.24083v3</link><description>Note: The final version of this article was published in Computers andGeosciences, Volume 206, January 2026, 106038. DOI:10.1016/j.cageo.2025.106038. Readers should refer to the published version forthe most up-to-date content. Three-dimensional digital reconstruction of porousmedia presents a fundamental challenge in geoscience, requiring simultaneousresolution of fine-scale pore structures while capturing representativeelementary volumes. We introduce a computational framework that addresses thischallenge through latent diffusion models operating within the EDM framework.Our approach reduces dimensionality via a custom variational autoencodertrained in binary geological volumes, improving efficiency and also enablingthe generation of larger volumes than previously possible with diffusionmodels. A key innovation is our controlled unconditional sampling methodology,which enhances distribution coverage by first sampling target statistics fromtheir empirical distributions, then generating samples conditioned on thesevalues. Extensive testing on four distinct rock types demonstrates thatconditioning on porosity - a readily computable statistic - is sufficient toensure a consistent representation of multiple complex properties, includingpermeability, two-point correlation functions, and pore size distributions. Theframework achieves better generation quality than pixel-space diffusion whileenabling significantly larger volume reconstruction (256-cube voxels) withsubstantially reduced computational requirements, establishing a newstate-of-the-art for digital rock physics applications.</description><author>Danilo Naiff, Bernardo P. Schaeffer, Gustavo Pires, Dragan Stojkovic, Thomas Rapstine, Fabio Ramos</author><pubDate>Mon, 01 Sep 2025 17:02:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.24083v3</guid></item><item><title>The challenge of hidden gifts in multi-agent reinforcement learning</title><link>http://arxiv.org/abs/2505.20579v4</link><description>Sometimes we benefit from actions that others have taken even when we areunaware that they took those actions. For example, if your neighbor chooses notto take a parking spot in front of your house when you are not there, you canbenefit, even without being aware that they took this action. These "hiddengifts" represent an interesting challenge for multi-agent reinforcementlearning (MARL), since assigning credit when the beneficial actions of othersare hidden is non-trivial. Here, we study the impact of hidden gifts with avery simple MARL task. In this task, agents in a grid-world environment haveindividual doors to unlock in order to obtain individual rewards. As well, ifall the agents unlock their door the group receives a larger collective reward.However, there is only one key for all of the doors, such that the collectivereward can only be obtained when the agents drop the key for others after theyuse it. Notably, there is nothing to indicate to an agent that the other agentshave dropped the key, thus the act of dropping the key for others is a "hiddengift". We show that several different state-of-the-art RL algorithms, includingMARL algorithms, fail to learn how to obtain the collective reward in thissimple task. Interestingly, we find that independent model-free policy gradientagents can solve the task when we provide them with information about their ownaction history, but MARL agents still cannot solve the task with actionhistory. Finally, we derive a correction term for these independent agents,inspired by learning aware approaches, which reduces the variance in learningand helps them to converge to collective success more reliably. These resultsshow that credit assignment in multi-agent settings can be particularlychallenging in the presence of "hidden gifts", and demonstrate that learningawareness in independent agents can benefit these settings.</description><author>Dane Malenfant, Blake A. Richards</author><pubDate>Mon, 01 Sep 2025 16:56:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.20579v4</guid></item><item><title>TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for Traversability Estimation</title><link>http://arxiv.org/abs/2406.01395v5</link><description>This paper presents TE-NeXt, a novel and efficient architecture forTraversability Estimation (TE) from sparse LiDAR point clouds based on aresidual convolution block. TE-NeXt block fuses notions of current trends suchas attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstratehigh capacity for generalisation in a variety of urban and naturalenvironments, using well-known and accessible datasets such as SemanticKITTI,Rellis-3D and SemanticUSL. Thus, the designed architecture ouperformsstate-of-the-art methods in the problem of semantic segmentation, demonstratingbetter results in unstructured environments and maintaining high reliabilityand robustness in urbans environments, which leads to better abstraction.Implementation is available in a open repository to the scientific communitywith the aim of ensuring the reproducibility of results.</description><author>Antonio Santo, Juan J. Cabrera, David Valiente, Carlos Viegas, Arturo Gil</author><pubDate>Mon, 01 Sep 2025 16:49:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01395v5</guid></item><item><title>BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation</title><link>http://arxiv.org/abs/2505.12620v5</link><description>Advances in AI generative models facilitate super-realistic video synthesis,amplifying misinformation risks via social media and eroding trust in digitalcontent. Several research works have explored new deepfake detection methods onAI-generated images to alleviate these risks. However, with the fastdevelopment of video generation models, such as Sora and WanX, there iscurrently a lack of large-scale, high-quality AI-generated video datasets forforgery detection. In addition, existing detection approaches predominantlytreat the task as binary classification, lacking explainability in modeldecision-making and failing to provide actionable insights or guidance for thepublic. To address these challenges, we propose \textbf{GenBuster-200K}, alarge-scale AI-generated video dataset featuring 200K high-resolution videoclips, diverse latest generative techniques, and real-world scenes. We furtherintroduce \textbf{BusterX}, a novel AI-generated video detection andexplanation framework leveraging multimodal large language model (MLLM) andreinforcement learning for authenticity determination and explainablerationale. To our knowledge, GenBuster-200K is the {\it \textbf{first}}large-scale, high-quality AI-generated video dataset that incorporates thelatest generative techniques for real-world scenarios. BusterX is the {\it\textbf{first}} framework to integrate MLLM with reinforcement learning forexplainable AI-generated video detection. Extensive comparisons withstate-of-the-art methods and ablation studies validate the effectiveness andgeneralizability of BusterX. The code, models, and datasets will be released.</description><author>Haiquan Wen, Yiwei He, Zhenglin Huang, Tianxiao Li, Zihan Yu, Xingru Huang, Lu Qi, Baoyuan Wu, Xiangtai Li, Guangliang Cheng</author><pubDate>Mon, 01 Sep 2025 16:38:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12620v5</guid></item><item><title>Adversarial Combinatorial Semi-bandits with Graph Feedback</title><link>http://arxiv.org/abs/2502.18826v6</link><description>In combinatorial semi-bandits, a learner repeatedly selects from acombinatorial decision set of arms, receives the realized sum of rewards, andobserves the rewards of the individual selected arms as feedback. In thispaper, we extend this framework to include \emph{graph feedback}, where thelearner observes the rewards of all neighboring arms of the selected arms in afeedback graph $G$. We establish that the optimal regret over a time horizon$T$ scales as $\widetilde{\Theta}(S\sqrt{T}+\sqrt{\alpha ST})$, where $S$ isthe size of the combinatorial decisions and $\alpha$ is the independence numberof $G$. This result interpolates between the known regrets$\widetilde\Theta(S\sqrt{T})$ under full information (i.e., $G$ is complete)and $\widetilde\Theta(\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$has only self-loops), where $K$ is the total number of arms. A key technicalingredient is to realize a convexified action using a random decision vectorwith negative correlations. We also show that online stochastic mirror descent(OSMD) that only realizes convexified actions in expectation is suboptimal. Inaddition, we describe the problem of \emph{combinatorial semi-bandits withgeneral capacity} and apply our results to derive an improved regret upperbound, which may be of independent interest.</description><author>Yuxiao Wen</author><pubDate>Mon, 01 Sep 2025 16:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.18826v6</guid></item><item><title>Addressing Key Challenges of Adversarial Attacks and Defenses in the Tabular Domain: A Methodological Framework for Coherence and Consistency</title><link>http://arxiv.org/abs/2412.07326v3</link><description>Machine learning models trained on tabular data are vulnerable to adversarialattacks, even in realistic scenarios where attackers only have access to themodel's outputs. Since tabular data contains complex interdependencies amongfeatures, it presents a unique challenge for adversarial samples which mustmaintain coherence and respect these interdependencies to remainindistinguishable from benign data. Moreover, existing attack evaluationmetrics-such as the success rate, perturbation magnitude, and query count-failto account for this challenge. To address those gaps, we propose a techniquefor perturbing dependent features while preserving sample coherence. Inaddition, we introduce Class-Specific Anomaly Detection (CSAD), an effectivenovel anomaly detection approach, along with concrete metrics for assessing thequality of tabular adversarial attacks. CSAD evaluates adversarial samplesrelative to their predicted class distribution, rather than a broad benigndistribution. It ensures that subtle adversarial perturbations, which mayappear coherent in other classes, are correctly identified as anomalies. Weintegrate SHAP explainability techniques to detect inconsistencies in modeldecision-making, extending CSAD for SHAP-based anomaly detection. Ourevaluation incorporates both anomaly detection rates with SHAP-basedassessments to provide a more comprehensive measure of adversarial samplequality. We evaluate various attack strategies, examining black-box query-basedand transferability-based gradient attacks across four target models.Experiments on benchmark tabular datasets reveal key differences in theattacker's risk and effort and attack quality, offering insights into thestrengths, limitations, and trade-offs faced by attackers and defenders. Ourfindings lay the groundwork for future research on adversarial attacks anddefense development in the tabular domain.</description><author>Yael Itzhakev, Amit Giloni, Yuval Elovici, Asaf Shabtai</author><pubDate>Mon, 01 Sep 2025 16:11:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.07326v3</guid></item><item><title>TRACE-CS: A Hybrid Logic-LLM System for Explainable Course Scheduling</title><link>http://arxiv.org/abs/2409.03671v3</link><description>We present TRACE-CS, a novel hybrid system that combines symbolic reasoningwith large language models (LLMs)to address contrastive queries in coursescheduling problems. TRACE-CS leverages logic-based techniques to encodescheduling constraints and generate provably correct explanations, whileutilizing an LLM to process natural language queries and refine logicalexplanations into user friendly responses. This system showcases how combiningsymbolic KR methods with LLMs creates explainable AI agents that balancelogical correctness with natural language accessibility, addressing afundamental challenge in deployed scheduling systems.</description><author>Stylianos Loukas Vasileiou, William Yeoh</author><pubDate>Mon, 01 Sep 2025 16:10:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03671v3</guid></item><item><title>Can Large Language Models be Effective Online Opinion Miners?</title><link>http://arxiv.org/abs/2505.15695v2</link><description>The surge of user-generated online content presents a wealth of insights intocustomer preferences and market trends. However, the highly diverse, complex,and context-rich nature of such contents poses significant challenges totraditional opinion mining approaches. To address this, we introduce OnlineOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocoldesigned to assess the ability of large language models (LLMs) to mine opinionseffectively from diverse and intricate online environments. OOMB providesextensive (entity, feature, opinion) tuple annotations and a comprehensiveopinion-centric summary that highlights key opinion topics within each content,thereby enabling the evaluation of both the extractive and abstractivecapabilities of models. Through our proposed benchmark, we conduct acomprehensive analysis of which aspects remain challenging and where LLMsexhibit adaptability, to explore whether they can effectively serve as opinionminers in realistic online scenarios. This study lays the foundation forLLM-based opinion mining and discusses directions for future research in thisfield.</description><author>Ryang Heo, Yongsik Seo, Junseong Lee, Dongha Lee</author><pubDate>Mon, 01 Sep 2025 15:48:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.15695v2</guid></item><item><title>Mutual Information Surprise: Rethinking Unexpectedness in Autonomous Systems</title><link>http://arxiv.org/abs/2508.17403v2</link><description>Recent breakthroughs in autonomous experimentation have demonstratedremarkable physical capabilities, yet their cognitive control remainslimited--often relying on static heuristics or classical optimization. A corelimitation is the absence of a principled mechanism to detect and adapt to theunexpectedness. While traditional surprise measures--such as Shannon orBayesian Surprise--offer momentary detection of deviation, they fail to capturewhether a system is truly learning and adapting. In this work, we introduceMutual Information Surprise (MIS), a new framework that redefines surprise notas anomaly detection, but as a signal of epistemic growth. MIS quantifies theimpact of new observations on mutual information, enabling autonomous systemsto reflect on their learning progression. We develop a statistical testsequence to detect meaningful shifts in estimated mutual information andpropose a mutual information surprise reaction policy (MISRP) that dynamicallygoverns system behavior through sampling adjustment and process forking.Empirical evaluations--on both synthetic domains and a dynamic pollution mapestimation task--show that MISRP-governed strategies significantly outperformclassical surprise-based approaches in stability, responsiveness, andpredictive accuracy. By shifting surprise from reactive to reflective, MISoffers a path toward more self-aware and adaptive autonomous systems.</description><author>Yinsong Wang, Quan Zeng, Xiao Liu, Yu Ding</author><pubDate>Mon, 01 Sep 2025 15:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17403v2</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v3</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Junwei Liu, Jinjie Gu</author><pubDate>Mon, 01 Sep 2025 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v3</guid></item><item><title>Lemmanaid: Neuro-Symbolic Lemma Conjecturing</title><link>http://arxiv.org/abs/2504.04942v4</link><description>Automatically conjecturing useful, interesting and novel lemmas would greatlyimprove automated reasoning tools and lower the bar for formalizing mathematicsin proof assistants. It is however a very challenging task for both neural andsymbolic approaches. We present the first steps towards a practicalneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large LanguageModels (LLMs) and symbolic methods, and evaluate it on proof libraries for theIsabelle proof assistant. We train an LLM to generate lemma templates thatdescribe the shape of a lemma, and use symbolic methods to fill in the details.We compare Lemmanaid against an LLM trained to generate complete lemmastatements as well as previous fully symbolic conjecturing methods. Lemmanaidoutperforms both neural and symbolic methods on test sets from Isabelle's HOLlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% ofthe gold standard human written lemmas. This is 8-15% more lemmas than theneural-only method. By leveraging the best of both symbolic and neural methodswe can generate useful lemmas for a wide range of input domains, facilitatingcomputer-assisted theory development and formalization.</description><author>Yousef Alhessi, Sólrún Halla Einarsdóttir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone</author><pubDate>Mon, 01 Sep 2025 15:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.04942v4</guid></item><item><title>What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity</title><link>http://arxiv.org/abs/2506.16782v2</link><description>Fairness in machine learning (ML) has become a rapidly growing area ofresearch. But why, in the first place, is unfairness in ML wrong? And whyshould we care about improving fairness? Most fair-ML research implicitlyappeals to distributive equality: the idea that desirable benefits and goods,such as opportunities (e.g., Barocas et al., 2023), should be equallydistributed across society. Unfair ML models, then, are seen as wrong becausethey unequally distribute such benefits. This paper argues that this exclusivefocus on distributive equality offers an incomplete and potentially misleadingethical foundation. Grounding ML fairness in egalitarianism--the view thatequality is a fundamental moral and social ideal--requires challengingstructural inequality: systematic, institutional, and durable arrangements thatprivilege some groups while disadvantaging others. Structural inequalitymanifests through ML systems in two primary forms: allocative harms (e.g.,economic loss) and representational harms (e.g., stereotypes, erasure). Whiledistributive equality helps address allocative harms, it fails to explain whyrepresentational harms are wrong--why it is wrong for ML systems to reinforcesocial hierarchies that stratify people into superior and inferior groups--andwhy ML systems should aim to foster a society where people relate as equals(i.e., relational equality). To address these limitations, the paper proposes amultifaceted egalitarian framework for ML fairness that integrates bothdistributive and relational equality. Drawing on critical social and politicalphilosophy, this framework offers a more comprehensive ethical foundation fortackling the full spectrum of harms perpetuated by ML systems. The paper alsooutlines practical pathways for implementing the framework across the entire MLpipeline.</description><author>Youjin Kong</author><pubDate>Mon, 01 Sep 2025 15:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.16782v2</guid></item><item><title>Understanding and Scaling Collaborative Filtering Optimization from the Perspective of Matrix Rank</title><link>http://arxiv.org/abs/2410.23300v3</link><description>Collaborative Filtering (CF) methods dominate real-world recommender systemsgiven their ability to learn high-quality, sparse ID-embedding tables thateffectively capture user preferences. These tables scale linearly with thenumber of users and items, and are trained to ensure high similarity betweenembeddings of interacted user-item pairs, while maintaining low similarity fornon-interacted pairs. Despite their high performance, encouraging dispersionfor non-interacted pairs necessitates expensive regularization (e.g., negativesampling), hurting runtime and scalability. Existing research tends to addressthese challenges by simplifying the learning process, either by reducing modelcomplexity or sampling data, trading performance for runtime. In this work, wemove beyond model-level modifications and study the properties of the embeddingtables under different learning strategies. Through theoretical analysis, wefind that the singular values of the embedding tables are intrinsically linkedto different CF loss functions. These findings are empirically validated onreal-world datasets, demonstrating the practical benefits of higher stablerank, a continuous version of matrix rank which encodes the distribution ofsingular values. Based on these insights, we propose an efficient warm-startstrategy that regularizes the stable rank of the user and item embeddings. Weshow that stable rank regularization during early training phases can promotehigher-quality embeddings, resulting in training speed improvements of up to66%. Additionally, stable rank regularization can act as a proxy for negativesampling, allowing for performance gains of up to 21% over loss functions withsmall negative sampling ratios. Overall, our analysis unifies current CFmethods under a new perspective, their optimization of stable rank, motivatinga flexible regularization method.</description><author>Donald Loveland, Xinyi Wu, Tong Zhao, Danai Koutra, Neil Shah, Mingxuan Ju</author><pubDate>Mon, 01 Sep 2025 15:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.23300v3</guid></item><item><title>NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes</title><link>http://arxiv.org/abs/2503.16375v2</link><description>In this paper, we explore the task of generating expansive outdoor scenes,ranging from castles to high-rises. Unlike indoor scene generation, which hasbeen a primary focus of prior work, outdoor scene generation presents uniquechallenges, including wide variations in scene heights and the need for amethod capable of rapidly producing large landscapes. To address this, wepropose an efficient approach that encodes scene chunks as uniform vector sets,offering better compression and performance than the spatially structuredlatents used in prior methods. Furthermore, we train an explicit outpaintingmodel for unbounded generation, which improves coherence compared to priorresampling-based inpainting schemes while also speeding up generation byeliminating extra diffusion steps. To facilitate this task, we curateNuiScene43, a small but high-quality set of scenes, preprocessed for jointtraining. Notably, when trained on scenes of varying styles, our model canblend different environments, such as rural houses and city skyscrapers, withinthe same scene, highlighting the potential of our curation process to leverageheterogeneous scenes for joint training.</description><author>Han-Hung Lee, Qinghong Han, Angel X. Chang</author><pubDate>Mon, 01 Sep 2025 15:09:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.16375v2</guid></item><item><title>FL-CLEANER: byzantine and backdoor defense by CLustering Errors of Activation maps in Non-iid fedErated leaRning</title><link>http://arxiv.org/abs/2501.12123v2</link><description>Federated Learning (FL) enables clients to collaboratively train a globalmodel using their local datasets while reinforcing data privacy, but it isprone to poisoning attacks. Existing defense mechanisms assume that clients'data are independent and identically distributed (IID), making them ineffectivein real-world applications where data are non-IID. This paper presentsFL-CLEANER, the first defense capable of filtering both byzantine and backdoorattackers' model updates in a non-IID FL environment. The originality ofFL-CLEANER is twofold. First, it relies on a client confidence score derivedfrom the reconstruction errors of each client's model activation maps for agiven trigger set, with reconstruction errors obtained by means of aConditional Variational Autoencoder trained according to a novel server-sidestrategy. Second, it uses an original ad-hoc trust propagation algorithm wepropose. Based on previous client scores, it allows building a cluster ofbenign clients while flagging potential attackers. Experimental results on thedatasets MNIST and FashionMNIST demonstrate the efficiency of FL-CLEANERagainst Byzantine attackers as well as to some state-of-the-art backdoors innon-IID scenarios; it achieves a close-to-zero (&lt;1%) benign clientmisclassification rate, even in the absence of an attack, and achieves strongperformance compared to state of the art defenses.</description><author>Mehdi Ben Ghali, Gouenou Coatrieux, Reda Bellafqira</author><pubDate>Mon, 01 Sep 2025 15:07:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.12123v2</guid></item><item><title>Monocular Facial Appearance Capture in the Wild</title><link>http://arxiv.org/abs/2412.12765v2</link><description>We present a new method for reconstructing the appearance properties of humanfaces from a lightweight capture procedure in an unconstrained environment. Ourmethod recovers the surface geometry, diffuse albedo, specular intensity andspecular roughness from a monocular video containing a simple head rotationin-the-wild. Notably, we make no simplifying assumptions on the environmentlighting, and we explicitly take visibility and occlusions into account. As aresult, our method can produce facial appearance maps that approach thefidelity of studio-based multi-view captures, but with a far easier and cheaperprocedure.</description><author>Yingyan Xu, Kate Gadola, Prashanth Chandran, Sebastian Weiss, Markus Gross, Gaspard Zoss, Derek Bradley</author><pubDate>Mon, 01 Sep 2025 14:52:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.12765v2</guid></item><item><title>GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration</title><link>http://arxiv.org/abs/2507.14452v2</link><description>The accurate identification of high-quality correspondences is a prerequisitetask in feature-based point cloud registration. However, it is extremelychallenging to handle the fusion of local and global features due to featureredundancy and complex spatial relationships. Given that Gestalt principlesprovide key advantages in analyzing local and global relationships, we proposea novel Gestalt-guided Parallel Interaction Network via orthogonal geometricconsistency (GPI-Net) in this paper. It utilizes Gestalt principles tofacilitate complementary communication between local and global information.Specifically, we introduce an orthogonal integration strategy to optimallyreduce redundant information and generate a more compact global structure forhigh-quality correspondences. To capture geometric features in correspondences,we leverage a Gestalt Feature Attention (GFA) block through a hybridutilization of self-attention and cross-attention mechanisms. Furthermore, tofacilitate the integration of local detail information into the globalstructure, we design an innovative Dual-path Multi-Granularity parallelinteraction aggregation (DMG) block to promote information exchange acrossdifferent granularities. Extensive experiments on various challenging tasksdemonstrate the superior performance of our proposed GPI-Net in comparison toexisting methods. The code will be released athttps://github.com/gwk429/GPI-Net.</description><author>Weikang Gu, Mingyue Han, Li Xue, Heng Dong, Changcai Yang, Riqing Chen, Lifang Wei</author><pubDate>Mon, 01 Sep 2025 14:51:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14452v2</guid></item><item><title>Certifiably Optimal Anisotropic Rotation Averaging</title><link>http://arxiv.org/abs/2503.07353v2</link><description>Rotation averaging is a key subproblem in applications of computer vision androbotics. Many methods for solving this problem exist, and there are alsoseveral theoretical results analyzing difficulty and optimality. However, oneaspect that most of these have in common is a focus on the isotropic setting,where the intrinsic uncertainties in the measurements are not fullyincorporated into the resulting optimization task. Recent empirical resultssuggest that moving to an anisotropic framework, where these uncertainties areexplicitly included, can result in an improvement of solution quality. However,global optimization for rotation averaging has remained a challenge in thisscenario. In this work we show how anisotropic costs can be incorporated incertifiably optimal rotation averaging. We also demonstrate how existingsolvers, designed for isotropic situations, fail in the anisotropic setting.Finally, we propose a stronger relaxation and empirically show that it recoversglobal optima in all tested datasets and leads to more accurate reconstructionsin almost all scenes.</description><author>Carl Olsson, Yaroslava Lochman, Johan Malmport, Christopher Zach</author><pubDate>Mon, 01 Sep 2025 14:50:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.07353v2</guid></item><item><title>Multitask Learning with Stochastic Interpolants</title><link>http://arxiv.org/abs/2508.04605v3</link><description>We propose a framework for learning maps between probability distributionsthat broadly generalizes the time dynamics of flow and diffusion models. Toenable this, we generalize stochastic interpolants by replacing the scalar timevariable with vectors, matrices, or linear operators, allowing us to bridgeprobability distributions across multiple dimensional spaces. This approachenables the construction of versatile generative models capable of fulfillingmultiple tasks without task-specific training. Our operator-based interpolantsnot only provide a unifying theoretical perspective for existing generativemodels but also extend their capabilities. Through numerical experiments, wedemonstrate the zero-shot efficacy of our method on conditional generation andinpainting, fine-tuning and posterior sampling, and multiscale modeling,suggesting its potential as a generic task-agnostic alternative to specializedmodels.</description><author>Hugo Negrel, Florentin Coeurdoux, Michael S. Albergo, Eric Vanden-Eijnden</author><pubDate>Mon, 01 Sep 2025 14:49:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.04605v3</guid></item><item><title>Mask-PINNs: Mitigating Internal Covariate Shift in Physics-Informed Neural Networks</title><link>http://arxiv.org/abs/2505.06331v3</link><description>Physics-Informed Neural Networks (PINNs) have emerged as a powerful frameworkfor solving partial differential equations (PDEs) by embedding physical lawsdirectly into the loss function. However, as a fundamental optimization issue,internal covariate shift (ICS) hinders the stable and effective training ofPINNs by disrupting feature distributions and limiting model expressiveness.Unlike standard deep learning tasks, conventional remedies for ICS -- such asBatch Normalization and Layer Normalization -- are not directly applicable toPINNs, as they distort the physical consistency required for reliable PDEsolutions. To address this issue, we propose Mask-PINNs, a novel architecturethat introduces a learnable mask function to regulate feature distributionswhile preserving the underlying physical constraints of PINNs. We provide atheoretical analysis showing that the mask suppresses the expansion of featurerepresentations through a carefully designed modulation mechanism. Empirically,we validate the method on multiple PDE benchmarks -- including convection, wavepropagation, and Helmholtz equations -- across diverse activation functions.Our results show consistent improvements in prediction accuracy, convergencestability, and robustness. Furthermore, we demonstrate that Mask-PINNs enablethe effective use of wider networks, overcoming a key limitation in existingPINN frameworks.</description><author>Feilong Jiang, Xiaonan Hou, Jianqiao Ye, Min Xia</author><pubDate>Mon, 01 Sep 2025 14:38:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.06331v3</guid></item></channel></rss>