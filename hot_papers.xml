<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivhot papers</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 05 Oct 2025 13:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Online Process Reward Leanring for Agentic Reinforcement Learning</title><link>http://arxiv.org/abs/2509.19199v2</link><description>Large language models (LLMs) are increasingly trained with reinforcementlearning (RL) as autonomous agents that reason and act over long horizons ininteractive environments. However, sparse and sometimes unverifiable rewardsmake temporal credit assignment extremely challenging. Recent work attempts tointegrate process supervision into agent learning but suffers from biasedannotation, reward hacking, high-variance from overly fine-grained signals orfailtures when state overlap is rare. We therefore introduce Online ProcessReward Learning (OPRL), a general credit-assignment strategy for agentic RLthat integrates seamlessly with standard on-policy algorithms without relyingon additional rollouts or explicit step labels. In OPRL, we optimize animplicit process reward model (PRM) alternately with the agent's policy totransform trajectory preferences into implicit step rewards through atrajectory-based DPO objective. These step rewards are then used to computestep-level advantages, which are combined with episode-level advantages fromoutcome rewards for policy update, creating a self-reinforcing loop.Theoretical findings guarantee that the learned step rewards are consistentwith trajectory preferences and act as potential-based shaping rewards,providing bounded gradients to stabilize training. Empirically, we evaluateOPRL on three distinct agent benmarks, including WebShop and VisualSokoban, aswell as open-ended social interactions with unverfiable rewards in SOTOPIA.Crucially, OPRL shows superior performance over frontier LLMs and strong RLbaselines across domains, achieving state-of-the-art results with highersample-efficiency and lower variance during training. Further analysis alsodemonstrates the efficient exploration by OPRL using fewer actions,underscoring its potential for agentic learning in real-world scenarios.</description><author>Xiaoqian Liu, Ke Wang, Yuchuan Wu, Fei Huang, Yongbin Li, Junge Zhang, Jianbin Jiao</author><pubDate>Wed, 24 Sep 2025 01:27:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19199v2</guid></item><item><title>UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following</title><link>http://arxiv.org/abs/2509.25148v1</link><description>Shaping powerful LLMs to be beneficial and safe is central to AI alignment.We argue that post-training alignment is fundamentally a unified PreferenceLearning problem, involving two modalities: demonstrated preferences (e.g.,Supervised Fine-Tuning, SFT) and comparative preferences (e.g., ReinforcementLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed dueto a critical distributional mismatch: SFT uses static expert data, but as thepolicy evolves, its generation distribution drifts, making SFT knowledgebrittle. Subsequent RL then explores without direct access to the rich,ground-truth knowledge in expert demonstrations, leading to inefficient,ungrounded updates. This separation prevents mutual regularization between datasources. To address this, we reframe alignment as a constrained optimizationproblem and propose Unified Adversarial Preference Learning (UniAPL),a novelframework that dynamically aligns the policy's distribution with the expert's.UniAPL implements a single-stage unified training objective, jointly learningfrom mixed batches of SFT and preference data. In every gradient step, denseexpert demonstrations directly ground and regularize online exploration,inherently resolving distributional mismatch and maximizing data synergy.Weevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507as the teacher. Our models match or exceed strong GRPO baselines: +5.77% onQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming theteacher. Analyses of response length and log-probability distributions confirmthat UniAPL outputs closely mimic expert demonstrations, achieving bothstronger performance and better behavioral alignment.</description><author>FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, Yichao Wu</author><pubDate>Mon, 29 Sep 2025 17:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.25148v1</guid></item><item><title>Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark</title><link>http://arxiv.org/abs/2509.26574v2</link><description>While large language models (LLMs) with reasoning capabilities areprogressing rapidly on high-school math competitions and coding, can theyreason effectively through complex, open-ended challenges found in frontierphysics research? And crucially, what kinds of reasoning tasks do physicistswant LLMs to assist with? To address these questions, we present the CritPt(Complex Research using Integrated Thinking - Physics Test, pronounced"critical point"), the first benchmark designed to test LLMs on unpublished,research-level reasoning tasks that broadly covers modern physics researchareas, including condensed matter, quantum physics, atomic, molecular &amp; opticalphysics, astrophysics, high energy physics, mathematical physics, statisticalphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.CritPt consists of 71 composite research challenges designed to simulatefull-scale research projects at the entry level, which are also decomposed to190 simpler checkpoint tasks for more fine-grained insights. All problems arenewly created by 50+ active physics researchers based on their own research.Every problem is hand-curated to admit a guess-resistant and machine-verifiableanswer and is evaluated by an automated grading pipeline heavily customized foradvanced physics-specific output formats. We find that while currentstate-of-the-art LLMs show early promise on isolated checkpoints, they remainfar from being able to reliably solve full research-scale challenges: the bestaverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),moderately rising to around 10% when equipped with coding tools. Through therealistic yet standardized evaluation offered by CritPt, we highlight a largedisconnect between current model capabilities and realistic physics researchdemands, offering a foundation to guide the development of scientificallygrounded AI tools.</description><author>Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, Yaïr Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson, Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia</author><pubDate>Wed, 01 Oct 2025 02:12:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.26574v2</guid></item><item><title>VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection</title><link>http://arxiv.org/abs/2508.11167v2</link><description>Unsupervised domain adaptation methods have been widely explored to bridgedomain gaps. However, in real-world remote-sensing scenarios, privacy andtransmission constraints often preclude access to source domain data, whichlimits their practical applicability. Recently, Source-Free Object Detection(SFOD) has emerged as a promising alternative, aiming at cross-domainadaptation without relying on source data, primarily through a self-trainingparadigm. Despite its potential, SFOD frequently suffers from training collapsecaused by noisy pseudo-labels, especially in remote sensing imagery with denseobjects and complex backgrounds. Considering that limited target domainannotations are often feasible in practice, we propose a Visionfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervisedframework for SFOD in remote sensing images. VG-DETR integrates a VisionFoundation Model (VFM) into the training pipeline in a "free lunch" manner,leveraging a small amount of labeled target data to mitigate pseudo-label noisewhile improving the detector's feature-extraction capability. Specifically, weintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM'ssemantic priors to further assess the reliability of the generatedpseudo-labels. By recovering potentially correct predictions fromlow-confidence outputs, our strategy improves pseudo-label quality andquantity. In addition, a dual-level VFM-guided alignment method is proposed,which aligns detector features with VFM embeddings at both the instance andimage levels. Through contrastive learning among fine-grained prototypes andsimilarity matching between feature maps, this dual-level alignment furtherenhances the robustness of feature representations against domain gaps.Extensive experiments demonstrate that VG-DETR achieves superior performance insource-free remote sensing detection tasks.</description><author>Jianhong Han, Yupei Wang, Liang Chen</author><pubDate>Tue, 26 Aug 2025 08:55:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11167v2</guid></item><item><title>Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English</title><link>http://arxiv.org/abs/2508.15453v1</link><description>These pure languages understanding directly relates to translation knowledgewhere linguists and translators need to work and research to eradicatemisunderstanding. Misunderstandings mostly appear in non-equivalent wordsbecause there are different local and internal words like food, garment,cultural and traditional words and others in every notion. Truly, most of thesewords do not have equivalent in the target language and these words need to beworked and find their equivalent in the target language to fully understand theboth languages. The purpose of this research is to introduce the methods ofrendering non-equivalent words professionally from the source language to thetarget language and this research has been completed using library-basedresearch. However, some of these non-equivalent words are alreadyprofessionally rendered to the target language but still there many other wordsto be rendered. As a result, this research paper includes different ways andrules of rendering non-equivalent words from source language to the targetlanguage and 25 non-equvalent words have been rendered from Dar &amp; Uzbek intoEnglish and Russian languages.</description><author>Mohammad Ibrahim Qani</author><pubDate>Thu, 21 Aug 2025 11:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15453v1</guid></item><item><title>DoSReMC: Domain Shift Resilient Mammography Classification using Batch Normalization Adaptation</title><link>http://arxiv.org/abs/2508.15452v1</link><description>Numerous deep learning-based solutions have been developed for the automaticrecognition of breast cancer using mammography images. However, theirperformance often declines when applied to data from different domains,primarily due to domain shift -- the variation in data distributions betweensource and target domains. This performance drop limits the safe and equitabledeployment of AI in real-world clinical settings. In this study, we presentDoSReMC (Domain Shift Resilient Mammography Classification), a batchnormalization (BN) adaptation framework designed to enhance cross-domaingeneralization without retraining the entire model. Using three large-scalefull-field digital mammography (FFDM) datasets -- including HCTP, a newlyintroduced, pathologically confirmed in-house dataset -- we conduct asystematic cross-domain evaluation with convolutional neural networks (CNNs).Our results demonstrate that BN layers are a primary source of domaindependence: they perform effectively when training and testing occur within thesame domain, and they significantly impair model generalization under domainshift. DoSReMC addresses this limitation by fine-tuning only the BN and fullyconnected (FC) layers, while preserving pretrained convolutional filters. Wefurther integrate this targeted adaptation with an adversarial training scheme,yielding additional improvements in cross-domain generalizability. DoSReMC canbe readily incorporated into existing AI pipelines and applied across diverseclinical environments, providing a practical pathway toward more robust andgeneralizable mammography classification systems.</description><author>Uğurcan Akyüz, Deniz Katircioglu-Öztürk, Emre K. Süslü, Burhan Keleş, Mete C. Kaya, Gamze Durhan, Meltem G. Akpınar, Figen B. Demirkazık, Gözde B. Akar</author><pubDate>Thu, 21 Aug 2025 11:17:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15452v1</guid></item><item><title>EmbeddingGemma: Powerful and Lightweight Text Representations</title><link>http://arxiv.org/abs/2509.20354v1</link><description>We introduce EmbeddingGemma, a new lightweight, open text embedding modelbased on the Gemma 3 language model family. Our innovative training recipestrategically captures knowledge from larger models via encoder-decoderinitialization and geometric embedding distillation. We improve modelrobustness and expressiveness with a spread-out regularizer, and ensuregeneralizability by merging checkpoints from varied, optimized mixtures.Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,English, and code domains, EmbeddingGemma (300M) achieves state-of-the-artresults. Notably, it outperforms prior top models, both proprietary and open,with fewer than 500M parameters, and provides performance comparable to modelsdouble its size, offering an exceptional performance-to-cost ratio. Remarkably,this lead persists when quantizing model weights or truncating embeddingoutputs. This makes EmbeddingGemma particularly well-suited for low-latency andhigh-throughput use cases such as on-device applications. We provide ablationstudies exploring our key design choices. We release EmbeddingGemma to thecommunity to promote further research.</description><author>Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, Weiyi Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao Zheng, Jyotinder Singh, Abheesht Sharma, Divya Sreepat, Aashi Jain, Adham Elarabawy, AJ Co, Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca, Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo Hernández Ábrego, Hesen Zhang, Hui Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen, Madhuri Shanbhogue, Michael Boratko, Paul Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanf</author><pubDate>Wed, 24 Sep 2025 17:56:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.20354v1</guid></item><item><title>UltraUPConvNet: A UPerNet- and ConvNeXt-Based Multi-Task Network for Ultrasound Tissue Segmentation and Disease Prediction</title><link>http://arxiv.org/abs/2509.11108v2</link><description>Ultrasound imaging is widely used in clinical practice due to itscost-effectiveness, mobility, and safety. However, current AI research oftentreats disease prediction and tissue segmentation as two separate tasks andtheir model requires substantial computational overhead. In such a situation,we introduce UltraUPConvNet, a computationally efficient universal frameworkdesigned for both ultrasound image classification and segmentation. Trained ona large-scale dataset containing more than 9,700 annotations across sevendifferent anatomical regions, our model achieves state-of-the-art performanceon certain datasets with lower computational overhead. Our model weights andcodes are available at https://github.com/yyxl123/UltraUPConvNet</description><author>Zhi Chen, Le Zhang</author><pubDate>Thu, 02 Oct 2025 14:40:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.11108v2</guid></item><item><title>Large Language Model Aided QoS Prediction for Service Recommendation</title><link>http://arxiv.org/abs/2408.02223v3</link><description>Large language models (LLMs) have seen rapid improvement in the recent years,and have been used in a wider range of applications. After being trained onlarge text corpus, LLMs obtain the capability of extracting rich features fromtextual data. Such capability is potentially useful for the web servicerecommendation task, where the web users and services have intrinsic attributesthat can be described using natural language sentences and are useful forrecommendation. In this paper, we explore the possibility and practicality ofusing LLMs for web service recommendation. We propose the large language modelaided QoS prediction (llmQoS) model, which use LLMs to extract usefulinformation from attributes of web users and services via descriptivesentences. This information is then used in combination with the QoS values ofhistorical interactions of users and services, to predict QoS values for anygiven user-service pair. On the WSDream dataset, llmQoS is shown to overcomethe data sparsity issue inherent to the QoS prediction problem, and outperformscomparable baseline models consistently.</description><author>Huiying Liu, Zekun Zhang, Honghao Li, Qilin Wu, Yiwen Zhang</author><pubDate>Tue, 26 Aug 2025 13:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02223v3</guid></item><item><title>Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions</title><link>http://arxiv.org/abs/2508.19945v2</link><description>We present an inverse dynamic game-based algorithm to learn parametricconstraints from a given dataset of local generalized Nash equilibriuminteractions between multiple agents. Specifically, we introduce mixed-integerlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of theinteracting agents, which recover constraints consistent with the Nashstationarity of the interaction demonstrations. We establish theoreticalguarantees that our method learns inner approximations of the true safe andunsafe sets, as well as limitations of constraint learnability fromdemonstrations of Nash equilibrium interactions. We also use the interactionconstraints recovered by our method to design motion plans that robustlysatisfy the underlying constraints. Across simulations and hardwareexperiments, our methods proved capable of inferring constraints and designinginteractive motion plans for various classes of constraints, both convex andnon-convex, from interaction demonstrations of agents with nonlinear dynamics.</description><author>Zhouyu Zhang, Chih-Yuan Chiu, Glen Chou</author><pubDate>Thu, 28 Aug 2025 17:30:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19945v2</guid></item><item><title>LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa</title><link>http://arxiv.org/abs/2508.15110v1</link><description>In this work, we highlight the transformative potential of ArtificialIntelligence (AI), particularly Large Language Models (LLMs) and agentic AI, inthe insurance sector. We consider and emphasize the unique opportunities,challenges, and potential pathways in insurance amid rapid performanceimprovements, increased open-source access, decreasing deployment costs, andthe complexity of LLM or agentic AI frameworks. To bring it closer to home, weidentify critical gaps in the African insurance market and highlight key localefforts, players, and partnership opportunities. Finally, we call uponactuaries, insurers, regulators, and tech leaders to a collaborative effortaimed at creating inclusive, sustainable, and equitable AI strategies andsolutions: by and for Africans.</description><author>Graham Hill, JingYuan Gong, Thulani Babeli, Moseli Mots'oehli, James Gachomo Wanjiku</author><pubDate>Wed, 20 Aug 2025 22:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15110v1</guid></item><item><title>Quantum Verifiable Rewards for Post-Training Qiskit Code Assistant</title><link>http://arxiv.org/abs/2508.20907v1</link><description>Qiskit is an open-source quantum computing framework that allows users todesign, simulate, and run quantum circuits on real quantum hardware. We explorepost-training techniques for LLMs to assist in writing Qiskit code. Weintroduce quantum verification as an effective method for ensuring code qualityand executability on quantum hardware. To support this, we developed asynthetic data pipeline that generates quantum problem-unit test pairs and usedit to create preference data for aligning LLMs with DPO. Additionally, wetrained models using GRPO, leveraging quantum-verifiable rewards provided bythe quantum hardware. Our best-performing model, combining DPO and GRPO,surpasses the strongest open-source baselines on the challengingQiskit-HumanEval-hard benchmark.</description><author>Nicolas Dupuis, Adarsh Tiwari, Youssef Mroueh, David Kremer, Ismael Faro, Juan Cruz-Benito</author><pubDate>Thu, 28 Aug 2025 15:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20907v1</guid></item><item><title>A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center Asymptotic Analysis</title><link>http://arxiv.org/abs/2502.16331v2</link><description>Recent works have characterized the function-space inductive bias ofinfinite-width bounded-norm single-hidden-layer neural networks as a kind ofbounded-variation-type space. This novel neural network Banach spaceencompasses many classical multivariate function spaces, including certainSobolev spaces and the spectral Barron spaces. Notably, this Banach space alsoincludes functions that exhibit less classical regularity, such as those thatonly vary in a few directions. On bounded domains, it is well-established thatthe Gaussian reproducing kernel Hilbert space (RKHS) strictly embeds into thisBanach space, demonstrating a clear gap between the Gaussian RKHS and theneural network Banach space. It turns out that when investigating these spaceson unbounded domains, e.g., all of $\mathbb{R}^d$, the story is fundamentallydifferent. We establish the following fundamental result: Certain functionsthat lie in the Gaussian RKHS have infinite norm in the neural network Banachspace. This provides a nontrivial gap between kernel methods and neuralnetworks by exhibiting functions that kernel methods easily represent, whereasneural networks cannot.</description><author>Akash Kumar, Rahul Parhi, Mikhail Belkin</author><pubDate>Mon, 01 Sep 2025 04:46:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.16331v2</guid></item><item><title>Diffusion Beats Autoregressive in Data-Constrained Settings</title><link>http://arxiv.org/abs/2507.15857v6</link><description>Autoregressive (AR) models have long dominated the landscape of largelanguage models, driving progress across a wide range of tasks. Recently,diffusion-based language models have emerged as a promising alternative, thoughtheir advantages over AR models remain underexplored. In this paper, wesystematically study masked diffusion models in data-constrained settings-wheretraining involves repeated passes over limited data and find that theysignificantly outperform AR models when compute is abundant but data is scarce.Diffusion models make better use of repeated data, achieving lower validationloss and superior downstream performance. We find new scaling laws fordiffusion models and derive a closed-form expression for the critical computethreshold at which diffusion begins to outperform AR. Finally, we explain whydiffusion models excel in this regime: their randomized masking objectiveimplicitly trains over a rich distribution of token orderings, acting as animplicit data augmentation that AR's fixed left-to-right factorization lacks.Our results suggest that when data, not compute, is the bottleneck, diffusionmodels offer a compelling alternative to the standard AR paradigm. Our code isavailable at: https://diffusion-scaling.github.io.</description><author>Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak</author><pubDate>Fri, 15 Aug 2025 17:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.15857v6</guid></item><item><title>Is ChatGPT-5 Ready for Mammogram VQA?</title><link>http://arxiv.org/abs/2508.11628v1</link><description>Mammogram visual question answering (VQA) integrates image interpretationwith clinical reasoning and has potential to support breast cancer screening.We systematically evaluated the GPT-5 family and GPT-4o model on four publicmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,abnormality detection, and malignancy classification tasks. GPT-5 consistentlywas the best performing model but lagged behind both human experts anddomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scoresamong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),calcification (63.5%), and malignancy (52.8%) classification. On InBreast, itattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detectionand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADSaccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Comparedwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) andspecificity (52.3%). While GPT-5 exhibits promising capabilities for screeningtasks, its performance remains insufficient for high-stakes clinical imagingapplications without targeted domain adaptation and optimization. However, thetremendous improvements in performance from GPT-4o to GPT-5 show a promisingtrend in the potential for general large language models (LLMs) to assist withmammography VQA tasks.</description><author>Qiang Li, Shansong Wang, Mingzhe Hu, Mojtaba Safari, Zachary Eidex, Xiaofeng Yang</author><pubDate>Fri, 15 Aug 2025 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11628v1</guid></item><item><title>LoRAtorio: An intrinsic approach to LoRA Skill Composition</title><link>http://arxiv.org/abs/2508.11624v1</link><description>Low-Rank Adaptation (LoRA) has become a widely adopted technique intext-to-image diffusion models, enabling the personalisation of visual conceptssuch as characters, styles, and objects. However, existing approaches struggleto effectively compose multiple LoRA adapters, particularly in open-endedsettings where the number and nature of required skills are not known inadvance. In this work, we present LoRAtorio, a novel train-free framework formulti-LoRA composition that leverages intrinsic model behaviour. Our method ismotivated by two key observations: (1) LoRA adapters trained on narrow domainsproduce denoised outputs that diverge from the base model, and (2) whenoperating out-of-distribution, LoRA outputs show behaviour closer to the basemodel than when conditioned in distribution. The balance between these twoobservations allows for exceptional performance in the single LoRA scenario,which nevertheless deteriorates when multiple LoRAs are loaded. Our methodoperates in the latent space by dividing it into spatial patches and computingcosine similarity between each patch's predicted noise and that of the basemodel. These similarities are used to construct a spatially-aware weightmatrix, which guides a weighted aggregation of LoRA outputs. To address domaindrift, we further propose a modification to classifier-free guidance thatincorporates the base model's unconditional score into the composition. Weextend this formulation to a dynamic module selection setting, enablinginference-time selection of relevant LoRA adapters from a large pool. LoRAtorioachieves state-of-the-art performance, showing up to a 1.3% improvement inClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generaliseseffectively to multiple latent diffusion models.</description><author>Niki Foteinopoulou, Ignas Budvytis, Stephan Liwicki</author><pubDate>Fri, 15 Aug 2025 17:52:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11624v1</guid></item><item><title>Data Diversity as Implicit Regularization: How Does Diversity Shape the Weight Space of Deep Neural Networks?</title><link>http://arxiv.org/abs/2410.14602v2</link><description>Data augmentation that introduces diversity into the input data has long beenused in training deep learning models. It has demonstrated benefits inimproving robustness and generalization, practically aligning well with otherregularization strategies such as dropout and weight decay. However, theunderlying mechanism of how diverse training data contributes to modelimprovements remains unknown. In this paper, we investigate the impact of datadiversity on the weight space of deep neural networks using Random MatrixTheory. Through spectral analysis and comparing models trained with dataaugmentation, dropout, and weight decay, we reveal that increasing datadiversity alters the weight spectral distribution similarly to otherregularization techniques, while displaying a pattern more closely aligned withdropout than with weight decay. Building on these insights, we propose a metricto explain and compare the benefits of diversity introduced by traditional dataaugmentations and those achieved through synthetic data.</description><author>Yang Ba, Michelle V. Mancenido, Rong Pan</author><pubDate>Fri, 15 Aug 2025 17:36:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14602v2</guid></item><item><title>Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</title><link>http://arxiv.org/abs/2508.11618v1</link><description>Carbon capture and storage (CCS) projects typically involve a diverse arrayof stakeholders or players from public, private, and regulatory sectors, eachwith different objectives and responsibilities. Given the complexity, scale,and long-term nature of CCS operations, determining whether individualstakeholders can independently maximize their interests or whethercollaborative coalition agreements are needed remains a central question foreffective CCS project planning and management. CCS projects are oftenimplemented in geologically connected sites, where shared geological featuressuch as pressure space and reservoir pore capacity can lead to competitivebehavior among stakeholders. Furthermore, CO2 storage sites are often locatedin geologically mature basins that previously served as sites for hydrocarbonextraction or wastewater disposal in order to leverage existinginfrastructures, which makes unilateral optimization even more complicated andunrealistic. In this work, we propose a paradigm based on Markov games to quantitativelyinvestigate how different coalition structures affect the goals ofstakeholders. We frame this multi-stakeholder multi-site problem as amulti-agent reinforcement learning problem with safety constraints. Ourapproach enables agents to learn optimal strategies while compliant with safetyregulations. We present an example where multiple operators are injecting CO2into their respective project areas in a geologically connected basin. Toaddress the high computational cost of repeated simulations of high-fidelitymodels, a previously developed surrogate model based on the Embed-to-Control(E2C) framework is employed. Our results demonstrate the effectiveness of theproposed framework in addressing optimal management of CO2 storage whenmultiple stakeholders with various objectives and goals are involved.</description><author>Jungang Chen, Seyyed A. Hosseini</author><pubDate>Fri, 15 Aug 2025 17:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11618v1</guid></item><item><title>Once Upon an AI: Six Scaffolds for Child-AI Interaction Design, Inspired by Disney</title><link>http://arxiv.org/abs/2504.08670v3</link><description>To build AI that children can intuitively understand and benefit from,designers need a design grammar that serves their developmental needs. Thispaper bridges artificial intelligence design for children - an emerging fieldstill defining its best practices - and animation, a well established fieldwith decades of experience in engaging children through accessiblestorytelling. Pairing Piagetian developmental theory with design patternextraction from 52 works of animation, the paper presents a six scaffoldframework that integrates design insights transferable to child centred AIdesign: (1) signals for visual animacy and clarity, (2) sound for musical andauditory scaffolding, (3) synchrony in audiovisual cues, (4) sidekick stylepersonas, (5) storyplay that supports symbolic play and imaginativeexploration, and (6) structure in the form of predictable narratives. Thesestrategies, long refined in animation, function as multimodal scaffolds forattention, understanding, and attunement, supporting learning and comfort. Thisstructured design grammar is transferable to AI design. By reframing cinematicstorytelling and child development theory as design logic for AI, the paperoffers heuristics for AI that aligns with the cognitive stages and emotionalneeds of young users. The work contributes to design theory by showing howsensory, affective, and narrative techniques can inform developmentally attunedAI design. Future directions include empirical testing, cultural adaptation,and participatory co design.</description><author>Nomisha Kurian</author><pubDate>Fri, 15 Aug 2025 17:35:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.08670v3</guid></item><item><title>Controlling Multimodal LLMs via Reward-guided Decoding</title><link>http://arxiv.org/abs/2508.11616v1</link><description>As Multimodal Large Language Models (MLLMs) gain widespread applicability, itis becoming increasingly desirable to adapt them for diverse user needs. Inthis paper, we study the adaptation of MLLMs through controlled decoding. Toachieve this, we introduce the first method for reward-guided decoding of MLLMsand demonstrate its application in improving their visual grounding. Our methodinvolves building reward models for visual grounding and using them to guidethe MLLM's decoding process. Concretely, we build two separate reward models toindependently control the degree of object precision and recall in the model'soutput. Our approach enables on-the-fly controllability of an MLLM's inferenceprocess in two ways: first, by giving control over the relative importance ofeach reward function during decoding, allowing a user to dynamically trade offobject precision for recall in image captioning tasks; second, by givingcontrol over the breadth of the search during decoding, allowing the user tocontrol the trade-off between the amount of test-time compute and the degree ofvisual grounding. We evaluate our method on standard object hallucinationbenchmarks, showing that it provides significant controllability over MLLMinference, while consistently outperforming existing hallucination mitigationmethods.</description><author>Oscar Mañas, Pierluca D'Oro, Koustuv Sinha, Adriana Romero-Soriano, Michal Drozdzal, Aishwarya Agrawal</author><pubDate>Fri, 15 Aug 2025 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11616v1</guid></item><item><title>Pretrained Conformers for Audio Fingerprinting and Retrieval</title><link>http://arxiv.org/abs/2508.11609v1</link><description>Conformers have shown great results in speech processing due to their abilityto capture both local and global interactions. In this work, we utilize aself-supervised contrastive learning framework to train conformer-basedencoders that are capable of generating unique embeddings for small segments ofaudio, generalizing well to previously unseen data. We achieve state-of-the-artresults for audio retrieval tasks while using only 3 seconds of audio togenerate embeddings. Our models are almost completely immune to temporalmisalignments and achieve state-of-the-art results in cases of other audiodistortions such as noise, reverb or extreme temporal stretching. Code andmodels are made publicly available and the results are easy to reproduce as wetrain and test using popular and freely available datasets of different sizes.</description><author>Kemal Altwlkany, Elmedin Selmanovic, Sead Delalic</author><pubDate>Fri, 15 Aug 2025 17:19:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11609v1</guid></item><item><title>Lightweight Attribute Localizing Models for Pedestrian Attribute Recognition</title><link>http://arxiv.org/abs/2306.09822v2</link><description>Pedestrian Attribute Recognition (PAR) focuses on identifying variousattributes in pedestrian images, with key applications in person retrieval,suspect re-identification, and soft biometrics. However, Deep Neural Networks(DNNs) for PAR often suffer from over-parameterization and high computationalcomplexity, making them unsuitable for resource-constrained devices.Traditional tensor-based compression methods typically factorize layers withoutadequately preserving the gradient direction during compression, leading toinefficient compression and a significant accuracy loss. In this work, wepropose a novel approach for determining the optimal ranks of low-rank layers,ensuring that the gradient direction of the compressed model closely alignswith that of the original model. This means that the compressed modeleffectively preserves the update direction of the full model, enabling moreefficient compression for PAR tasks. The proposed procedure optimizes thecompression ranks for each layer within the ALM model, followed by compressionusing CPD-EPC or truncated SVD. This results in a reduction in model complexitywhile maintaining high performance.</description><author>Ashish Jha, Dimitrii Ermilov, Konstantin Sobolev, Anh Huy Phan, Salman Ahmadi-Asl, Naveed Ahmed, Imran Junejo, Zaher AL Aghbari, Thar Baker, Ahmed Mohamed Khedr, Andrzej Cichocki</author><pubDate>Fri, 15 Aug 2025 17:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09822v2</guid></item><item><title>TinyTim: A Family of Language Models for Divergent Generation</title><link>http://arxiv.org/abs/2508.11607v1</link><description>This work introduces TinyTim, a family of large language models fine-tuned onJames Joyce's `Finnegans Wake'. Through quantitative evaluation againstbaseline models, we demonstrate that TinyTim V1 produces a statisticallydistinct generative profile characterized by high lexical diversity and lowsemantic coherence. These findings are interpreted through theories ofcreativity and complex problem-solving, arguing that such specialized modelscan function as divergent knowledge sources within more extensive creativearchitectures, powering automated discovery mechanisms in diverse settings.</description><author>Christopher J. Agostino</author><pubDate>Fri, 15 Aug 2025 17:14:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11607v1</guid></item><item><title>Dataset Creation for Visual Entailment using Generative AI</title><link>http://arxiv.org/abs/2508.11605v1</link><description>In this paper we present and validate a new synthetic dataset for trainingvisual entailment models. Existing datasets for visual entailment are small andsparse compared to datasets for textual entailment. Manually creating datasetsis labor-intensive. We base our synthetic dataset on the SNLI dataset fortextual entailment. We take the premise text from SNLI as input prompts in agenerative image model, Stable Diffusion, creating an image to replace eachtextual premise. We evaluate our dataset both intrinsically and extrinsically.For extrinsic evaluation, we evaluate the validity of the generated images byusing them as training data for a visual entailment classifier based on CLIPfeature vectors. We find that synthetic training data only leads to a slightdrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 whentrained on real data. We also compare the quality of our generated trainingdata to original training data on another dataset: SICK-VTE. Again, there isonly a slight drop in F-score: from 0.400 to 0.384. These results indicate thatin settings with data sparsity, synthetic data can be a promising solution fortraining visual entailment models.</description><author>Rob Reijtenbach, Suzan Verberne, Gijs Wijnholds</author><pubDate>Fri, 15 Aug 2025 17:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11605v1</guid></item><item><title>CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion</title><link>http://arxiv.org/abs/2508.11603v1</link><description>Text-driven 3D editing seeks to modify 3D scenes according to textualdescriptions, and most existing approaches tackle this by adapting pre-trained2D image editors to multi-view inputs. However, without explicit control overmulti-view information exchange, they often fail to maintain cross-viewconsistency, leading to insufficient edits and blurry details. We introduceCoreEditor, a novel framework for consistent text-to-3D editing. The keyinnovation is a correspondence-constrained attention mechanism that enforcesprecise interactions between pixels expected to remain consistent throughoutthe diffusion denoising process. Beyond relying solely on geometric alignment,we further incorporate semantic similarity estimated during denoising, enablingmore reliable correspondence modeling and robust multi-view editing. Inaddition, we design a selective editing pipeline that allows users to choosepreferred results from multiple candidates, offering greater flexibility anduser control. Extensive experiments show that CoreEditor produces high-quality,3D-consistent edits with sharper details, significantly outperforming priormethods.</description><author>Zhe Zhu, Honghua Chen, Peng Li, Mingqiang Wei</author><pubDate>Fri, 15 Aug 2025 17:13:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11603v1</guid></item><item><title>A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability</title><link>http://arxiv.org/abs/2502.12052v2</link><description>In NLG meta-evaluation, evaluation metrics are typically assessed based ontheir consistency with humans. However, we identify some limitations intraditional NLG meta-evaluation approaches, such as issues in handling humanratings and ambiguous selections of correlation measures, which undermine theeffectiveness of meta-evaluation. In this work, we propose a dual-perspectiveNLG meta-evaluation framework that focuses on different evaluationcapabilities, thereby providing better interpretability. In addition, weintroduce a method of automatically constructing the corresponding benchmarkswithout requiring new human annotations. Furthermore, we conduct experimentswith 16 representative LLMs as the evaluators based on our proposed framework,comprehensively analyzing their evaluation performance from differentperspectives.</description><author>Xinyu Hu, Mingqi Gao, Li Lin, Zhenghan Yu, Xiaojun Wan</author><pubDate>Fri, 15 Aug 2025 17:10:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.12052v2</guid></item><item><title>CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection</title><link>http://arxiv.org/abs/2508.11599v1</link><description>Cryptographic algorithms are fundamental to modern security, yet theirimplementations frequently harbor subtle logic flaws that are hard to detect.We introduce CryptoScope, a novel framework for automated cryptographicvulnerability detection powered by Large Language Models (LLMs). CryptoScopecombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation(RAG), guided by a curated cryptographic knowledge base containing over 12,000entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarilyderived from real-world CVE vulnerabilities, complemented by cryptographicchallenges from major Capture The Flag (CTF) competitions and syntheticexamples across 11 programming languages. CryptoScope consistently improvesperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9previously undisclosed flaws in widely used open-source cryptographic projects.</description><author>Zhihao Li, Zimo Ji, Tao Zheng, Hao Ren, Xiao Lan</author><pubDate>Fri, 15 Aug 2025 17:07:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11599v1</guid></item><item><title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title><link>http://arxiv.org/abs/2508.11598v1</link><description>We introduce AuriStream, a biologically inspired model for encoding speechvia a two-stage framework inspired by the human auditory processing hierarchy.The first stage transforms raw audio into a time-frequency representation basedon the human cochlea, from which we extract discrete \textbf{cochlear tokens}.The second stage applies an autoregressive sequence model over the cochleartokens. AuriStream learns meaningful phoneme and word representations, andstate-of-the-art lexical semantics. AuriStream shows competitive performance ondiverse downstream SUPERB speech tasks. Complementing AuriStream's strongrepresentational capabilities, it generates continuations of audio which can bevisualized in a spectrogram space and decoded back into audio, providinginsights into the model's predictions. In summary, we present a two-stageframework for speech representation learning to advance the development of morehuman-like models that efficiently handle a range of speech-based tasks.</description><author>Greta Tuckute, Klemen Kotar, Evelina Fedorenko, Daniel L. K. Yamins</author><pubDate>Fri, 15 Aug 2025 17:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11598v1</guid></item><item><title>Nonparametric learning of stochastic differential equations from sparse and noisy data</title><link>http://arxiv.org/abs/2508.11597v1</link><description>The paper proposes a systematic framework for building data-driven stochasticdifferential equation (SDE) models from sparse, noisy observations. Unliketraditional parametric approaches, which assume a known functional form for thedrift, our goal here is to learn the entire drift function directly from datawithout strong structural assumptions, making it especially relevant inscientific disciplines where system dynamics are partially understood or highlycomplex. We cast the estimation problem as minimization of the penalizednegative log-likelihood functional over a reproducing kernel Hilbert space(RKHS). In the sparse observation regime, the presence of unobserved trajectorysegments makes the SDE likelihood intractable. To address this, we develop anExpectation-Maximization (EM) algorithm that employs a novel Sequential MonteCarlo (SMC) method to approximate the filtering distribution and generate MonteCarlo estimates of the E-step objective. The M-step then reduces to a penalizedempirical risk minimization problem in the RKHS, whose minimizer is given by afinite linear combination of kernel functions via a generalized representertheorem. To control model complexity across EM iterations, we also develop ahybrid Bayesian variant of the algorithm that uses shrinkage priors to identifysignificant coefficients in the kernel expansion. We establish importanttheoretical convergence results for both the exact and approximate EMsequences. The resulting EM-SMC-RKHS procedure enables accurate estimation ofthe drift function of stochastic dynamical systems in low-data regimes and isbroadly applicable across domains requiring continuous-time modeling underobservational constraints. We demonstrate the effectiveness of our methodthrough a series of numerical experiments.</description><author>Arnab Ganguly, Riten Mitra, Jinpu Zhou</author><pubDate>Fri, 15 Aug 2025 17:01:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11597v1</guid></item><item><title>DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring</title><link>http://arxiv.org/abs/2508.11591v1</link><description>Our study introduces a novel, low-cost, and reproducible framework forreal-time, object-level structural assessment and geolocation of roadsidevegetation and infrastructure with commonly available but underutilizeddashboard camera (dashcam) video data. We developed an end-to-end pipeline thatcombines monocular depth estimation, depth error correction, and geometrictriangulation to generate accurate spatial and structural data fromstreet-level video streams from vehicle-mounted dashcams. Depth maps were firstestimated using a state-of-the-art monocular depth model, then refined via agradient-boosted regression framework to correct underestimations, particularlyfor distant objects. The depth correction model achieved strong predictiveperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantlyreducing bias beyond 15 m. Further, object locations were estimated usingGPS-based triangulation, while object heights were calculated using pin holecamera geometry. Our method was evaluated under varying conditions of cameraplacement and vehicle speed. Low-speed vehicle with inside camera gave thehighest accuracy, with mean geolocation error of 2.83 m, and mean absoluteerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. Tothe best of our knowledge, it is the first framework to combine monocular depthmodeling, triangulated GPS-based geolocation, and real-time structuralassessment for urban vegetation and infrastructure using consumer-grade videodata. Our approach complements conventional RS methods, such as LiDAR and imageby offering a fast, real-time, and cost-effective solution for object-levelmonitoring of vegetation risks and infrastructure exposure, making itespecially valuable for utility companies, and urban planners aiming forscalable and frequent assessments in dynamic urban environments.</description><author>Durga Joshi, Chandi Witharana, Robert Fahey, Thomas Worthley, Zhe Zhu, Diego Cerrai</author><pubDate>Fri, 15 Aug 2025 16:55:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11591v1</guid></item><item><title>Convolutional Autoencoders for Data Compression and Anomaly Detection in Small Satellite Technologies</title><link>http://arxiv.org/abs/2505.00040v2</link><description>Small satellite technologies have enhanced the potential and feasibility ofgeodesic missions, through simplification of design and decreased costsallowing for more frequent launches. On-satellite data acquisition systems canbenefit from the implementation of machine learning (ML), for betterperformance and greater efficiency on tasks such as image processing or featureextraction. This work presents convolutional autoencoders for implementation onthe payload of small satellites, designed to achieve dual functionality of datacompression for more efficient off-satellite transmission, and at-sourceanomaly detection to inform satellite data-taking. This capability isdemonstrated for a use case of disaster monitoring using aerial image datasetsof the African continent, offering avenues for both novel ML-based approachesin small satellite applications along with the expansion of space technologyand artificial intelligence in Africa.</description><author>Dishanand Jayeprokash, Julia Gonski</author><pubDate>Fri, 15 Aug 2025 16:48:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.00040v2</guid></item><item><title>Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation</title><link>http://arxiv.org/abs/2508.11588v1</link><description>Effective and efficient agricultural manipulation and harvesting depend onaccurately understanding the current state of the grasp. The agriculturalenvironment presents unique challenges due to its complexity, clutter, andocclusion. Additionally, fruit is physically attached to the plant, requiringprecise separation during harvesting. Selecting appropriate sensors andmodeling techniques is critical for obtaining reliable feedback and correctlyidentifying grasp states. This work investigates a set of key sensors, namelyinertial measurement units (IMUs), infrared (IR) reflectance, tension, tactilesensors, and RGB cameras, integrated into a compliant gripper to classify graspstates. We evaluate the individual contribution of each sensor and compare theperformance of two widely used classification models: Random Forest and LongShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forestclassifier, trained in a controlled lab environment and tested on real cherrytomato plants, achieved 100% accuracy in identifying slip, grasp failure, andsuccessful picks, marking a substantial improvement over baseline performance.Furthermore, we identify a minimal viable sensor combination, namely IMU andtension sensors that effectively classifies grasp states. This classifierenables the planning of corrective actions based on real-time feedback, therebyenhancing the efficiency and reliability of fruit harvesting operations.</description><author>Benjamin Walt, Jordan Westphal, Girish Krishnan</author><pubDate>Fri, 15 Aug 2025 16:47:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11588v1</guid></item><item><title>Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs</title><link>http://arxiv.org/abs/2504.05147v2</link><description>The rise of large language models (LLMs) has introduced new privacychallenges, particularly during inference where sensitive information inprompts may be exposed to proprietary LLM APIs. In this paper, we address theproblem of formally protecting the sensitive information contained in a promptwhile maintaining response quality. To this end, first, we introduce acryptographically inspired notion of a prompt sanitizer which transforms aninput prompt to protect its sensitive tokens. Second, we proposePr$\epsilon\epsilon$mpt, a novel system that implements a prompt sanitizer.Pr$\epsilon\epsilon$mpt categorizes sensitive tokens into two types: (1) thosewhere the LLM's response depends solely on the format (such as SSNs, creditcard numbers), for which we use format-preserving encryption (FPE); and (2)those where the response depends on specific values, (such as age, salary) forwhich we apply metric differential privacy (mDP). Our evaluation demonstratesthat Pr$\epsilon\epsilon$mpt is a practical method to achieve meaningfulprivacy guarantees, while maintaining high utility compared to unsanitizedprompts, and outperforming prior methods</description><author>Amrita Roy Chowdhury, David Glukhov, Divyam Anshumaan, Prasad Chalasani, Nicolas Papernot, Somesh Jha, Mihir Bellare</author><pubDate>Fri, 15 Aug 2025 16:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05147v2</guid></item><item><title>Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</title><link>http://arxiv.org/abs/2508.11584v1</link><description>Deploying multiple machine learning models on resource-constrained roboticplatforms for different perception tasks often results in redundantcomputations, large memory footprints, and complex integration challenges. Inresponse, this work presents Visual Perception Engine (VPEngine), a modularframework designed to enable efficient GPU usage for visual multitasking whilemaintaining extensibility and developer accessibility. Our frameworkarchitecture leverages a shared foundation model backbone that extracts imagerepresentations, which are efficiently shared, without any unnecessary GPU-CPUmemory transfers, across multiple specialized task-specific model heads runningin parallel. This design eliminates the computational redundancy inherent infeature extraction component when deploying traditional sequential models whileenabling dynamic task prioritization based on application demands. Wedemonstrate our framework's capabilities through an example implementationusing DINOv2 as the foundation model with multiple task (depth, objectdetection and semantic segmentation) heads, achieving up to 3x speedup comparedto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngineoffers efficient GPU utilization and maintains a constant memory footprintwhile allowing per-task inference frequencies to be adjusted dynamically duringruntime. The framework is written in Python and is open source with ROS2 C++(Humble) bindings for ease of use by the robotics community across diverserobotic platforms. Our example implementation demonstrates end-to-end real-timeperformance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimizedmodels.</description><author>Jakub Łucki, Jonathan Becktor, Georgios Georgakis, Robert Royce, Shehryar Khattak</author><pubDate>Fri, 15 Aug 2025 16:42:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11584v1</guid></item><item><title>Emphasis Sensitivity in Speech Representations</title><link>http://arxiv.org/abs/2508.11566v1</link><description>This work investigates whether modern speech models are sensitive to prosodicemphasis - whether they encode emphasized and neutral words in systematicallydifferent ways. Prior work typically relies on isolated acoustic correlates(e.g., pitch, duration) or label prediction, both of which miss the relationalstructure of emphasis. This paper proposes a residual-based framework, definingemphasis as the difference between paired neutral and emphasized wordrepresentations. Analysis on self-supervised speech models shows that theseresiduals correlate strongly with duration changes and perform poorly at wordidentity prediction, indicating a structured, relational encoding of prosodicemphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% morecompact than in pre-trained models, further suggesting that emphasis is encodedas a consistent, low-dimensional transformation that becomes more structuredwith task-specific learning.</description><author>Shaun Cassini, Thomas Hain, Anton Ragni</author><pubDate>Fri, 15 Aug 2025 16:18:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11566v1</guid></item><item><title>Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models</title><link>http://arxiv.org/abs/2508.11582v1</link><description>Recent advancements in large language models (LLMs) have greatly improvedtheir capabilities on complex reasoning tasks through Long Chain-of-Thought(CoT). However, this approach often results in substantial redundancy,impairing computational efficiency and causing significant delays in real-timeapplications. To improve the efficiency, current methods often rely onhuman-defined difficulty priors, which do not align with the LLM's self-awareddifficulty, leading to inefficiencies. In this paper, we introduce the DynamicReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models todynamically assess and adjust their reasoning depth in response to problemcomplexity. DR. SAF integrates three key components: Boundary Self-AwarenessAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.These components allow models to optimize their reasoning processes, balancingefficiency and accuracy without compromising performance. Our experimentalresults demonstrate that DR. SAF achieves a 49.27% reduction in total responsetokens with minimal loss in accuracy. The framework also delivers a 6.59x gainin token efficiency and a 5x reduction in training time, making it well-suitedto resource-limited settings. During extreme training, DR. SAF can even surpasstraditional instruction-based models in token efficiency with more than 16%accuracy improvement.</description><author>Qiguang Chen, Dengyun Peng, Jinhao Liu, HuiKang Su, Jiannan Guan, Libo Qin, Wanxiang Che</author><pubDate>Fri, 15 Aug 2025 16:40:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11582v1</guid></item><item><title>Causality Matters: How Temporal Information Emerges in Video Language Models</title><link>http://arxiv.org/abs/2508.11576v1</link><description>Video language models (VideoLMs) have made significant progress in multimodalunderstanding. However, temporal understanding, which involves identifyingevent order, duration, and relationships across time, still remains a corechallenge. Prior works emphasize positional encodings (PEs) as a key mechanismfor encoding temporal structure. Surprisingly, we find that removing ormodifying PEs in video inputs yields minimal degradation in the performance oftemporal understanding. In contrast, reversing the frame sequence whilepreserving the original PEs causes a substantial drop. To explain thisbehavior, we conduct substantial analysis experiments to trace how temporalinformation is integrated within the model. We uncover a causal informationpathway: temporal cues are progressively synthesized through inter-frameattention, aggregated in the final frame, and subsequently integrated into thequery tokens. This emergent mechanism shows that temporal reasoning emergesfrom inter-visual token interactions under the constraints of causal attention,which implicitly encodes temporal structure. Based on these insights, wepropose two efficiency-oriented strategies: staged cross-modal attention and atemporal exit mechanism for early token truncation. Experiments on twobenchmarks validate the effectiveness of both approaches. To the best of ourknowledge, this is the first work to systematically investigate video temporalunderstanding in VideoLMs, offering insights for future model improvement.</description><author>Yumeng Shi, Quanyu Long, Yin Wu, Wenya Wang</author><pubDate>Fri, 15 Aug 2025 16:33:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11576v1</guid></item><item><title>TrajSV: A Trajectory-based Model for Sports Video Representations and Applications</title><link>http://arxiv.org/abs/2508.11569v1</link><description>Sports analytics has received significant attention from both academia andindustry in recent years. Despite the growing interest and efforts in thisfield, several issues remain unresolved, including (1) data unavailability, (2)lack of an effective trajectory-based framework, and (3) requirement forsufficient supervision labels. In this paper, we present TrajSV, atrajectory-based framework that addresses various issues in existing studies.TrajSV comprises three components: data preprocessing, Clip RepresentationNetwork (CRNet), and Video Representation Network (VRNet). The datapreprocessing module extracts player and ball trajectories from sportsbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module tolearn clip representations based on these trajectories. Additionally, VRNetlearns video representations by aggregating clip representations and visualfeatures with an encoder-decoder architecture. Finally, a triple contrastiveloss is introduced to optimize both video and clip representations in anunsupervised manner. The experiments are conducted on three broadcast videodatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,soccer, basketball, and volleyball) with three downstream applications (i.e.,sports video retrieval, action spotting, and video captioning). The resultsdemonstrate that TrajSV achieves state-of-the-art performance in sports videoretrieval, showcasing a nearly 70% improvement. It outperforms baselines inaction spotting, achieving state-of-the-art results in 9 out of 17 actioncategories, and demonstrates a nearly 20% improvement in video captioning.Additionally, we introduce a deployed system along with the three applicationsbased on TrajSV.</description><author>Zheng Wang, Shihao Xu, Wei Shi</author><pubDate>Fri, 15 Aug 2025 16:23:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11569v1</guid></item><item><title>AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment</title><link>http://arxiv.org/abs/2508.11567v1</link><description>Mental health assessment is crucial for early intervention and effectivetreatment, yet traditional clinician-based approaches are limited by theshortage of qualified professionals. Recent advances in artificial intelligencehave sparked growing interest in automated psychological assessment, yet mostexisting approaches are constrained by their reliance on static text analysis,limiting their ability to capture deeper and more informative insights thatemerge through dynamic interaction and iterative questioning. Therefore, inthis paper, we propose a multi-agent framework for mental health evaluationthat simulates clinical doctor-patient dialogues, with specialized agentsassigned to questioning, adequacy evaluation, scoring, and updating. Weintroduce an adaptive questioning mechanism in which an evaluation agentassesses the adequacy of user responses to determine the necessity ofgenerating targeted follow-up queries to address ambiguity and missinginformation. Additionally, we employ a tree-structured memory in which the rootnode encodes the user's basic information, while child nodes (e.g., topic andstatement) organize key information according to distinct symptom categoriesand interaction turns. This memory is dynamically updated throughout theinteraction to reduce redundant questioning and further enhance the informationextraction and contextual tracking capabilities. Experimental results on theDAIC-WOZ dataset illustrate the effectiveness of our proposed method, whichachieves better performance than existing approaches.</description><author>Jinpeng Hu, Ao Wang, Qianqian Xie, Hui Ma, Zhuo Li, Dan Guo</author><pubDate>Fri, 15 Aug 2025 16:20:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11567v1</guid></item><item><title>Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models</title><link>http://arxiv.org/abs/2508.11534v1</link><description>As large language models (LLMs) become more widely deployed, it is crucial toexamine their ethical tendencies. Building on research on fairness anddiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --discrimination based on species membership -- and how they value non-humananimals. We systematically examine this issue across three paradigms: (1)SpeciesismBench, a 1,003-item benchmark assessing recognition and moralevaluation of speciesist statements; (2) established psychological measurescomparing model responses with those of human participants; (3) text-generationtasks probing elaboration on, or resistance to, speciesist rationalizations. Inour benchmark, LLMs reliably detected speciesist statements but rarelycondemned them, often treating speciesist attitudes as morally acceptable. Onpsychological measures, results were mixed: LLMs expressed slightly lowerexplicit speciesism than people, yet in direct trade-offs they more often choseto save one human over multiple animals. A tentative interpretation is thatLLMs may weight cognitive capacity rather than species per se: when capacitieswere equal, they showed no species preference, and when an animal was describedas more capable, they tended to prioritize it over a less capable human. Inopen-ended text generation tasks, LLMs frequently normalized or rationalizedharm toward farmed animals while refusing to do so for non-farmed animals.These findings suggest that while LLMs reflect a mixture of progressive andmainstream human views, they nonetheless reproduce entrenched cultural normsaround animal exploitation. We argue that expanding AI fairness and alignmentframeworks to explicitly include non-human moral patients is essential forreducing these biases and preventing the entrenchment of speciesist attitudesin AI systems and the societies they influence.</description><author>Monika Jotautaitė, Lucius Caviola, David A. Brewster, Thilo Hagendorff</author><pubDate>Fri, 15 Aug 2025 15:22:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11534v1</guid></item><item><title>SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</title><link>http://arxiv.org/abs/2508.11553v1</link><description>We introduce SeamlessFlow, a server based reinforcement learning (RL)framework that addresses two core challenges in industrial scale RL: (1)decoupling RL training from the complex execution flow of agents; (2)maximizing GPU utilization with minimal idle time while preserving thestability and scalability required for large-scale deployments. First,SeamlessFlow introduces a data plane that decouples the RL trainer fromdiverse, complex agent implementations while sustaining high throughput. Acentral trajectory manager maintains complete interaction histories andsupports partial rollout, allowing rollout to pause for weight updates andresume seamlessly, keeping agents unaware of service interruptions. Second, wepropose a tag driven scheduling paradigm that abstracts hardware intocapability tagged resources, unifying colocated and disaggregatedarchitectures. Based on this, SeamlessFlow introduces a spatiotemporalmultiplexing pipeline that dynamically reassigns idle training nodes to rolloutin a train rollout separated setup, eliminating pipeline bubbles and fullyexploiting heterogeneous cluster resources. By combining these innovations,SeamlessFlow delivers both stability and high performance, making it wellsuited for multi agent, long horizon, and other complex RL tasks.</description><author>Jinghui Wang, Shaojie Wang, Yinghan Cui, Xuxing Chen, Chao Wang, Xiaojiang Zhang, Minglei Zhang, Jiarong Zhang, Wenhao Zhuang, Yuchen Cao, Wankang Bao, Haimo Li, Zheng Lin, Huiming Wang, Haoyang Huang, Zongxian Feng, Zizheng Zhan, Ken Deng, Wen Xiang, Huaixi Tang, Kun Wu, Mengtong Li, Mengfei Xie, Junyi Peng, Haotian Zhang, Bin Chen, Bing Yu</author><pubDate>Fri, 15 Aug 2025 15:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11553v1</guid></item><item><title>ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization</title><link>http://arxiv.org/abs/2508.11551v1</link><description>Determining the optimal data mixture for large language model trainingremains a challenging problem with an outsized impact on performance. Inpractice, language model developers continue to rely on heuristic explorationsince no learning-based approach has emerged as a reliable solution. In thiswork, we propose to view the selection of training data mixtures as a black-boxhyperparameter optimization problem, for which Bayesian Optimization is awell-established class of appropriate algorithms. Firstly, we cast data mixturelearning as a sequential decision-making problem, in which we aim to find asuitable trade-off between the computational cost of training exploratory(proxy-) models and final mixture performance. Secondly, we systematicallyexplore the properties of transferring mixtures learned at a small scale tolarger-scale experiments, providing insights and highlighting opportunities forresearch at a modest scale. By proposing Multi-fidelity Bayesian Optimizationas a suitable method in this common scenario, we introduce a natural frameworkto balance experiment cost with model fit, avoiding the risks of overfitting tosmaller scales while minimizing the number of experiments at high cost. Wepresent results for pre-training and instruction finetuning across modelsranging from 1 million to 7 billion parameters, varying from simplearchitectures to state-of-the-art models and benchmarks spanning dozens ofdatasets. We demonstrate consistently strong results relative to a wide rangeof benchmarks, showingspeed-ups of over 500% in determining the best datamixture on our largest experiments relative to recent baselines. In addition,we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 fulltraining &amp; evaluation runs across various model sizes worth over 13,000 GPUhours, greatly reducing the cost of conducting research in this area.</description><author>Shengzhuang Chen, Xu Ouyang, Michael Arthur Leopold Pearce, Thomas Hartvigsen, Jonathan Richard Schwarz</author><pubDate>Fri, 15 Aug 2025 15:53:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11551v1</guid></item><item><title>Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model</title><link>http://arxiv.org/abs/2508.11550v1</link><description>Industrial anomaly detection (AD) plays a significant role in manufacturingwhere a long-standing challenge is data scarcity. A growing body of works haveemerged to address insufficient anomaly data via anomaly generation. However,these anomaly generation methods suffer from lack of fidelity or need to betrained with extra data. To this end, we propose a training-free anomalygeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'sstrong generation ability for effective anomaly image generation. Given anormal image, mask and a simple text prompt, AAG can generate realistic andnatural anomalies in the specific regions and simultaneously keep contents inother regions unchanged. In particular, we propose Cross-Attention Enhancement(CAE) to re-engineer the cross-attention mechanism within Stable Diffusionbased on the given mask. CAE increases the similarity between visual tokens inspecific regions and text embeddings, which guides these generated visualtokens in accordance with the text description. Besides, generated anomaliesneed to be more natural and plausible with object in given image. We proposeSelf-Attention Enhancement (SAE) which improves similarity between each normalvisual token and anomaly visual tokens. SAE ensures that generated anomaliesare coherent with original pattern. Extensive experiments on MVTec AD and VisAdatasets demonstrate effectiveness of AAG in anomaly generation and itsutility. Furthermore, anomaly images generated by AAG can bolster performanceof various downstream anomaly inspection tasks.</description><author>Zuo Zuo, Jiahao Dong, Yanyun Qu, Zongze Wu</author><pubDate>Fri, 15 Aug 2025 15:52:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11550v1</guid></item><item><title>PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments</title><link>http://arxiv.org/abs/2506.06631v2</link><description>Visual parsing of images and videos is critical for a wide range ofreal-world applications. However, progress in this field is constrained bylimitations of existing datasets: (1) insufficient annotation granularity,which impedes fine-grained scene understanding and high-level reasoning; (2)limited coverage of domains, particularly a lack of datasets tailored foreducational scenarios; and (3) lack of explicit procedural guidance, withminimal logical rules and insufficient representation of structured taskprocess. To address these gaps, we introduce PhysLab, the first video datasetthat captures students conducting complex physics experiments. The datasetincludes four representative experiments that feature diverse scientificinstruments and rich human-object interaction (HOI) patterns. PhysLab comprises620 long-form videos and provides multilevel annotations that support a varietyof vision tasks, including action recognition, object detection, HOI analysis,etc. We establish strong baselines and perform extensive evaluations tohighlight key challenges in the parsing of procedural educational videos. Weexpect PhysLab to serve as a valuable resource for advancing fine-grainedvisual parsing, facilitating intelligent classroom systems, and fosteringcloser integration between computer vision and educational technologies. Thedataset and the evaluation toolkit are publicly available athttps://github.com/ZMH-SDUST/PhysLab.</description><author>Minghao Zou, Qingtian Zeng, Yongping Miao, Shangkun Liu, Zilong Wang, Hantao Liu, Wei Zhou</author><pubDate>Fri, 15 Aug 2025 15:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06631v2</guid></item><item><title>MetaAgents: Large Language Model Based Agents for Decision-Making on Teaming</title><link>http://arxiv.org/abs/2310.06500v2</link><description>Significant advancements have occurred in the application of Large LanguageModels (LLMs) for social simulations. Despite this, their abilities to performteaming in task-oriented social events are underexplored. Such capabilities arecrucial if LLMs are to effectively mimic human-like social behaviors and formefficient teams to solve tasks. To bridge this gap, we introduce MetaAgents, asocial simulation framework populated with LLM-based agents. MetaAgentsfacilitates agent engagement in conversations and a series of decision makingwithin social contexts, serving as an appropriate platform for investigatinginteractions and interpersonal decision-making of agents. In particular, weconstruct a job fair environment as a case study to scrutinize the teamassembly and skill-matching behaviors of LLM-based agents. We take advantage ofboth quantitative metrics evaluation and qualitative text analysis to assesstheir teaming abilities at the job fair. Our evaluation demonstrates thatLLM-based agents perform competently in making rational decisions to developefficient teams. However, we also identify limitations that hinder theireffectiveness in more complex team assembly tasks. Our work provides valuableinsights into the role and evolution of LLMs in task-oriented socialsimulations.</description><author>Yuan Li, Lichao Sun, Yixuan Zhang</author><pubDate>Fri, 15 Aug 2025 14:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.06500v2</guid></item><item><title>Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs</title><link>http://arxiv.org/abs/2506.10054v2</link><description>Direct Preference Optimization (DPO) has become a cornerstone ofreinforcement learning from human feedback (RLHF) due to its simplicity andefficiency. However, existing DPO-based approaches typically treat allpreference pairs uniformly, ignoring critical variations in their inherentquality and learning utility, leading to suboptimal data utilization andperformance. To address this challenge, we propose Omni-DPO, a dual-perspectiveoptimization framework that jointly accounts for (1) the inherent quality ofeach preference pair and (2) the model's evolving performance on those pairs.By adaptively weighting samples according to both data quality and the model'slearning dynamics during training, Omni-DPO enables more effective trainingdata utilization and achieves better performance. Experimental results onvarious models and benchmarks demonstrate the superiority and generalizationcapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-itfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significantmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoningtasks, Omni-DPO consistently outperforms the baseline methods across allbenchmarks, providing strong empirical evidence for the effectiveness androbustness of our approach. Code and models will be available athttps://github.com/pspdada/Omni-DPO.</description><author>Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang</author><pubDate>Fri, 15 Aug 2025 15:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.10054v2</guid></item><item><title>Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models</title><link>http://arxiv.org/abs/2508.11542v1</link><description>This paper presents a data-driven, nested Operator Inference (OpInf) approachfor learning physics-informed reduced-order models (ROMs) from snapshot data ofhigh-dimensional dynamical systems. The approach exploits the inherenthierarchy within the reduced space to iteratively construct initial guesses forthe OpInf learning problem that prioritize the interactions of the dominantmodes. The initial guess computed for any target reduced dimension correspondsto a ROM with provably smaller or equal snapshot reconstruction error than withstandard OpInf. Moreover, our nested OpInf algorithm can be warm-started frompreviously learned models, enabling versatile application scenarios involvingdynamic basis and model form updates. We demonstrate the performance of ouralgorithm on a cubic heat conduction problem, with nested OpInf achieving afour times smaller error than standard OpInf at a comparable offline time.Further, we apply nested OpInf to a large-scale, parameterized model of theGreenland ice sheet where, despite model form approximation errors, it learns aROM with, on average, 3% error and computational speed-up factor above 19,000.</description><author>Nicole Aretz, Karen Willcox</author><pubDate>Fri, 15 Aug 2025 15:38:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11542v1</guid></item><item><title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title><link>http://arxiv.org/abs/2508.11538v1</link><description>Video reasoning segmentation (VRS) endeavors to delineate referred objects invideos guided by implicit instructions that encapsulate human intent andtemporal logic. Previous approaches leverage large vision language models(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.However, this paradigm suffers from limited interpretability during inferenceand suboptimal performance due to inadequate spatiotemporal reasoning. Drawinginspiration from seminal breakthroughs in reinforcement learning, we introduceVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning insegmentation. Veason-R1 is trained through Group Relative Policy Optimization(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, wecurate high-quality CoT training data to instill structured reasoningtrajectories, bridging video-level semantics and frame-level spatial grounding,yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPOfine-tuning encourages efficient exploration of the reasoning space byoptimizing reasoning chains. To this end, we incorporate a holistic rewardmechanism that synergistically enhances spatial alignment and temporalconsistency, bolstering keyframe localization and fine-grained grounding.Comprehensive empirical evaluations demonstrate that Veason-R1 achievesstate-of-the-art performance on multiple benchmarks, surpassing prior art bysignificant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),while exhibiting robustness to hallucinations (+8.8 R). Our code and modelweights will be available at Veason-R1.</description><author>Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</author><pubDate>Fri, 15 Aug 2025 15:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11538v1</guid></item><item><title>Language models align with brain regions that represent concepts across modalities</title><link>http://arxiv.org/abs/2508.11536v1</link><description>Cognitive science and neuroscience have long faced the challenge ofdisentangling representations of language from representations of conceptualmeaning. As the same problem arises in today's language models (LMs), weinvestigate the relationship between LM--brain alignment and two neuralmetrics: (1) the level of brain activation during processing of sentences,targeting linguistic processing, and (2) a novel measure of meaning consistencyacross input modalities, which quantifies how consistently a brain regionresponds to the same concept across paradigms (sentence, word cloud, image)using an fMRI dataset (Pereira et al., 2018). Our experiments show that bothlanguage-only and language-vision models predict the signal better in moremeaning-consistent areas of the brain, even when these areas are not stronglysensitive to language processing, suggesting that LMs might internallyrepresent cross-modal conceptual meaning.</description><author>Maria Ryskina, Greta Tuckute, Alexander Fung, Ashley Malkin, Evelina Fedorenko</author><pubDate>Fri, 15 Aug 2025 15:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11536v1</guid></item><item><title>Random Walk Learning and the Pac-Man Attack</title><link>http://arxiv.org/abs/2508.05663v2</link><description>Random walk (RW)-based algorithms have long been popular in distributedsystems due to low overheads and scalability, with recent growing applicationsin decentralized learning. However, their reliance on local interactions makesthem inherently vulnerable to malicious behavior. In this work, we investigatean adversarial threat that we term the ``Pac-Man'' attack, in which a maliciousnode probabilistically terminates any RW that visits it. This stealthy behaviorgradually eliminates active RWs from the network, effectively halting thelearning process without triggering failure alarms. To counter this threat, wepropose the Average Crossing (AC) algorithm--a fully decentralized mechanismfor duplicating RWs to prevent RW extinction in the presence of Pac-Man. Ourtheoretical analysis establishes that (i) the RW population remains almostsurely bounded under AC and (ii) RW-based stochastic gradient descent remainsconvergent under AC, even in the presence of Pac-Man, with a quantifiabledeviation from the true optimum. Our extensive empirical results on bothsynthetic and real-world datasets corroborate our theoretical findings.Furthermore, they uncover a phase transition in the extinction probability as afunction of the duplication threshold. We offer theoretical insights byanalyzing a simplified variant of the AC, which sheds light on the observedphase transition.</description><author>Xingran Chen, Parimal Parag, Rohit Bhagat, Zonghong Liu, Salim El Rouayheb</author><pubDate>Fri, 15 Aug 2025 15:28:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05663v2</guid></item><item><title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title><link>http://arxiv.org/abs/2503.01307v2</link><description>Test-time inference has emerged as a powerful paradigm for enabling languagemodels to ``think'' longer and more carefully about complex challenges, muchlike skilled human experts. While reinforcement learning (RL) can driveself-improvement in language models on verifiable tasks, some models exhibitsubstantial gains while others quickly plateau. For instance, we find thatQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the gameof Countdown. This discrepancy raises a critical question: what intrinsicproperties enable effective self-improvement? We introduce a framework toinvestigate this question by analyzing four key cognitive behaviors --verification, backtracking, subgoal setting, and backward chaining -- that bothexpert human problem solvers and successful language models employ. Our studyreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llamainitially lacks them. In systematic experimentation with controlled behavioraldatasets, we find that priming Llama with examples containing these reasoningbehaviors enables substantial improvements during RL, matching or exceedingQwen's performance. Importantly, the presence of reasoning behaviors, ratherthan correctness of answers, proves to be the critical factor -- models primedwith incorrect solutions containing proper reasoning patterns achievecomparable performance to those trained on correct solutions. Finally,leveraging continued pretraining with OpenWebMath data, filtered to amplifyreasoning behaviors, enables the Llama model to match Qwen's self-improvementtrajectory. Our findings establish a fundamental relationship between initialreasoning behaviors and the capacity for improvement, explaining why somelanguage models effectively utilize additional computation while othersplateau.</description><author>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</author><pubDate>Fri, 15 Aug 2025 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.01307v2</guid></item><item><title>An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture</title><link>http://arxiv.org/abs/2508.11532v1</link><description>Intelligent analysis of medical imaging plays a crucial role in assistingclinical diagnosis. However, achieving efficient and high-accuracy imageclassification in resource-constrained computational environments remainschallenging. This study proposes a medical image classification method based onan improved ConvNeXt-Tiny architecture. Through structural optimization andloss function design, the proposed method enhances feature extractioncapability and classification performance while reducing computationalcomplexity. Specifically, the method introduces a dual global pooling (GlobalAverage Pooling and Global Max Pooling) feature fusion strategy into theConvNeXt-Tiny backbone to simultaneously preserve global statistical featuresand salient response information. A lightweight channel attention module,termed Squeeze-and-Excitation Vector (SEVector), is designed to improve theadaptive allocation of channel weights while minimizing parameter overhead.Additionally, a Feature Smoothing Loss is incorporated into the loss functionto enhance intra-class feature consistency and suppress intra-class variance.Under CPU-only conditions (8 threads), the method achieves a maximumclassification accuracy of 89.10% on the test set within 10 training epochs,exhibiting a stable convergence trend in loss values. Experimental resultsdemonstrate that the proposed method effectively improves medical imageclassification performance in resource-limited settings, providing a feasibleand efficient solution for the deployment and promotion of medical imaginganalysis models.</description><author>Jingsong Xia, Yue Yin, Xiuhan Li</author><pubDate>Fri, 15 Aug 2025 15:20:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11532v1</guid></item><item><title>Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction</title><link>http://arxiv.org/abs/2508.11531v1</link><description>Efficient trackers achieve faster runtime by reducing computationalcomplexity and model parameters. However, this efficiency often compromises theexpense of weakened feature representation capacity, thus limiting theirability to accurately capture target states using single-layer features. Toovercome this limitation, we propose Multi-State Tracker (MST), which utilizeshighly lightweight state-specific enhancement (SSE) to perform specializedenhancement on multi-state features produced by multi-state generation (MSG)and aggregates them in an interactive and adaptive manner using cross-stateinteraction (CSI). This design greatly enhances feature representation whileincurring minimal computational overhead, leading to improved trackingrobustness in complex environments. Specifically, the MSG generates multiplestate representations at multiple stages during feature extraction, while SSErefines them to highlight target-specific features. The CSI module facilitatesinformation exchange between these states and ensures the integration ofcomplementary features. Notably, the introduced SSE and CSI modules adopt ahighly lightweight hidden state adaptation-based state space duality (HSA-SSD)design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.Experimental results demonstrate that MST outperforms all previous efficienttrackers across multiple datasets, significantly improving tracking accuracyand robustness. In particular, it shows excellent runtime performance, with anAO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT onthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.</description><author>Shilei Wang, Gong Cheng, Pujian Lai, Dong Gao, Junwei Han</author><pubDate>Fri, 15 Aug 2025 15:19:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11531v1</guid></item><item><title>Incorporating Arbitrary Matrix Group Equivariance into KANs</title><link>http://arxiv.org/abs/2410.00435v4</link><description>Kolmogorov-Arnold Networks (KANs) have seen great success in scientificdomains thanks to spline activation functions, becoming an alternative toMulti-Layer Perceptrons (MLPs). However, spline functions may not respectsymmetry in tasks, which is crucial prior knowledge in machine learning. Inthis paper, we propose Equivariant Kolmogorov-Arnold Networks (EKAN), a methodfor incorporating arbitrary matrix group equivariance into KANs, aiming tobroaden their applicability to more fields. We first construct gated splinebasis functions, which form the EKAN layer together with equivariant linearweights, and then define a lift layer to align the input space of EKAN with thefeature space of the dataset, thereby building the entire EKAN architecture.Compared with baseline models, EKAN achieves higher accuracy with smallerdatasets or fewer parameters on symmetry-related tasks, such as particlescattering and the three-body problem, often reducing test MSE by severalorders of magnitude. Even in non-symbolic formula scenarios, such as top quarktagging with three jet constituents, EKAN achieves comparable results withstate-of-the-art equivariant architectures using fewer than 40% of theparameters, while KANs do not outperform MLPs as expected. Code and data areavailable at https://github.com/hulx2002/EKAN .</description><author>Lexiang Hu, Yisen Wang, Zhouchen Lin</author><pubDate>Fri, 15 Aug 2025 15:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.00435v4</guid></item><item><title>Landmark-Assisted Monte Carlo Planning</title><link>http://arxiv.org/abs/2508.11493v1</link><description>Landmarks$\unicode{x2013}$conditions that must be satisfied at some point inevery solution plan$\unicode{x2013}$have contributed to major advancements inclassical planning, but they have seldom been used in stochastic domains. Weformalize probabilistic landmarks and adapt the UCT algorithm to leverage themas subgoals to decompose MDPs; core to the adaptation is balancing betweengreedy landmark achievement and final goal achievement. Our results inbenchmark domains show that well-chosen landmarks can significantly improve theperformance of UCT in online probabilistic planning, while the best balance ofgreedy versus long-term goal achievement is problem-dependent. The resultssuggest that landmarks can provide helpful guidance for anytime algorithmssolving MDPs.</description><author>David H. Chan, Mark Roberts, Dana S. Nau</author><pubDate>Fri, 15 Aug 2025 14:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11493v1</guid></item><item><title>DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning</title><link>http://arxiv.org/abs/2508.11530v1</link><description>Decentralized Federated Learning (DFL) has emerged as a robust distributedparadigm that circumvents the single-point-of-failure and communicationbottleneck risks of centralized architectures. However, a significant challengearises as existing DFL optimization strategies, primarily designed for taskssuch as computer vision, fail to address the unique topological informationinherent in the local subgraph. Notably, while Federated Graph Learning (FGL)is tailored for graph data, it is predominantly implemented in a centralizedserver-client model, failing to leverage the benefits of decentralization.Tobridge this gap, we propose DFed-SST, a decentralized federated graph learningframework with adaptive communication. The core of our method is adual-topology adaptive communication mechanism that leverages the uniquetopological features of each client's local subgraph to dynamically constructand optimize the inter-client communication topology. This allows our frameworkto guide model aggregation efficiently in the face of heterogeneity. Extensiveexperiments on eight real-world datasets consistently demonstrate thesuperiority of DFed-SST, achieving 3.26% improvement in average accuracy overbaseline methods.</description><author>Lianshuai Guo, Zhongzheng Yuan, Xunkai Li, Yinlin Zhu, Meixia Qu, Wenyu Wang</author><pubDate>Fri, 15 Aug 2025 15:15:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11530v1</guid></item><item><title>A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</title><link>http://arxiv.org/abs/2508.11529v1</link><description>Artificial intelligence is reshaping science and industry, yet many usersstill regard its models as opaque "black boxes". Conventional explainableartificial-intelligence methods clarify individual predictions but overlook theupstream decisions and downstream quality checks that determine whetherinsights can be trusted. In this work, we present Holistic ExplainableArtificial Intelligence (HXAI), a user-centric framework that embedsexplanation into every stage of the data-analysis workflow and tailors thoseexplanations to users. HXAI unifies six components (data, analysis set-up,learning process, model output, model quality, communication channel) into asingle taxonomy and aligns each component with the needs of domain experts,data analysts and data scientists. A 112-item question bank covers these needs;our survey of contemporary tools highlights critical coverage gaps. Grounded intheories of human explanation, principles from human-computer interaction andfindings from empirical user studies, HXAI identifies the characteristics thatmake explanations clear, actionable and cognitively manageable. A comprehensivetaxonomy operationalises these insights, reducing terminological ambiguity andenabling rigorous coverage analysis of existing toolchains. We furtherdemonstrate how AI agents that embed large-language models can orchestratediverse explanation techniques, translating technical artifacts intostakeholder-specific narratives that bridge the gap between AI developers anddomain experts. Departing from traditional surveys or perspective articles,this work melds concepts from multiple disciplines, lessons from real-worldprojects and a critical synthesis of the literature to advance a novel,end-to-end viewpoint on transparency, trustworthiness and responsible AIdeployment.</description><author>George Paterakis, Andrea Castellani, George Papoutsoglou, Tobias Rodemann, Ioannis Tsamardinos</author><pubDate>Fri, 15 Aug 2025 15:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11529v1</guid></item><item><title>Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series</title><link>http://arxiv.org/abs/2508.11528v1</link><description>We propose an unsupervised anomaly detection approach based on aphysics-informed diffusion model for multivariate time series data. Over thepast years, diffusion model has demonstrated its effectiveness in forecasting,imputation, generation, and anomaly detection in the time series domain. Inthis paper, we present a new approach for learning the physics-dependenttemporal distribution of multivariate time series data using a weightedphysics-informed loss during diffusion model training. A weightedphysics-informed loss is constructed using a static weight schedule. Thisapproach enables a diffusion model to accurately approximate underlying datadistribution, which can influence the unsupervised anomaly detectionperformance. Our experiments on synthetic and real-world datasets show thatphysics-informed training improves the F1 score in anomaly detection; itgenerates better data diversity and log-likelihood. Our model outperformsbaseline approaches, additionally, it surpasses prior physics-informed work andpurely data-driven diffusion models on a synthetic dataset and one real-worlddataset while remaining competitive on others.</description><author>Juhi Soni, Markus Lange-Hegermann, Stefan Windmann</author><pubDate>Fri, 15 Aug 2025 15:13:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11528v1</guid></item><item><title>Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models</title><link>http://arxiv.org/abs/2508.11524v1</link><description>Addressing large-scale planning problems has become one of the centralchallenges in the planning community, deriving from the state-space explosioncaused by growing objects and actions. Recently, researchers have explored theeffectiveness of leveraging Large Language Models (LLMs) to generate helpfulactions and states to prune the search space. However, prior works have largelyoverlooked integrating LLMs with domain-specific knowledge to ensure validplans. In this paper, we propose a novel LLM-assisted planner integrated withproblem decomposition, which first decomposes large planning problems intomultiple simpler sub-tasks. Then we explore two novel paradigms to utilizeLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, whereLLM4Inspire provides heuristic guidance according to general knowledge andLLM4Predict employs domain-specific knowledge to infer intermediate conditions.We empirically validate the effectiveness of our planner across multipledomains, demonstrating the ability of search space partition when solvinglarge-scale planning problems. The experimental results show that LLMseffectively locate feasible solutions when pruning the search space, whereinfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holdsparticular promise compared with LLM4Inspire, which offers general knowledgewithin LLMs.</description><author>Wenkai Yu, Jianhang Tang, Yang Zhang, Shanjiang Tang, Kebing Jin, Hankz Hankui Zhuo</author><pubDate>Fri, 15 Aug 2025 15:08:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11524v1</guid></item><item><title>Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry</title><link>http://arxiv.org/abs/2508.09991v2</link><description>Automating data extraction from clinical documents offers significantpotential to improve efficiency in healthcare settings, yet deploying NaturalLanguage Processing (NLP) solutions presents practical challenges. Drawing uponour experience implementing various NLP models for information extraction andclassification tasks at the British Columbia Cancer Registry (BCCR), this papershares key lessons learned throughout the project lifecycle. We emphasize thecritical importance of defining problems based on clear business objectivesrather than solely technical accuracy, adopting an iterative approach todevelopment, and fostering deep interdisciplinary collaboration and co-designinvolving domain experts, end-users, and ML specialists from inception. Furtherinsights highlight the need for pragmatic model selection (including hybridapproaches and simpler methods where appropriate), rigorous attention to dataquality (representativeness, drift, annotation), robust error mitigationstrategies involving human-in-the-loop validation and ongoing audits, andbuilding organizational AI literacy. These practical considerations,generalizable beyond cancer registries, provide guidance for healthcareorganizations seeking to successfully implement AI/NLP solutions to enhancedata management processes and ultimately improve patient care and public healthoutcomes.</description><author>Lovedeep Gondara, Gregory Arbour, Raymond Ng, Jonathan Simkin, Shebnum Devji</author><pubDate>Fri, 15 Aug 2025 15:04:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09991v2</guid></item><item><title>Finite-Width Neural Tangent Kernels from Feynman Diagrams</title><link>http://arxiv.org/abs/2508.11522v1</link><description>Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,non-linear neural networks. In the infinite-width limit, NTKs can easily becomputed for most common architectures, yielding full analytic control over thetraining dynamics. However, at infinite width, important properties of trainingsuch as NTK evolution or feature learning are absent. Nevertheless, finitewidth effects can be included by computing corrections to the Gaussianstatistics at infinite width. We introduce Feynman diagrams for computingfinite-width corrections to NTK statistics. These dramatically simplify thenecessary algebraic manipulations and enable the computation of layer-wiserecursive relations for arbitrary statistics involving preactivations, NTKs andcertain higher-derivative tensors (dNTK and ddNTK) required to predict thetraining dynamics at leading order. We demonstrate the feasibility of ourframework by extending stability results for deep networks from preactivationsto NTKs and proving the absence of finite-width corrections for scale-invariantnonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. Wevalidate our results with numerical experiments.</description><author>Max Guillen, Philipp Misof, Jan E. Gerken</author><pubDate>Fri, 15 Aug 2025 15:02:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11522v1</guid></item><item><title>A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11</title><link>http://arxiv.org/abs/2508.11517v1</link><description>Accelerated aging of transportation infrastructure in the rapidly developingYangtze River Delta region necessitates efficient concrete crack detection, ascrack deterioration critically compromises structural integrity and regionaleconomic growth. To overcome the limitations of inefficient manual inspectionand the suboptimal performance of existing deep learning models, particularlyfor small-target crack detection within complex backgrounds, this paperproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection andsegmentation model based on the YOLOv11n architecture. The proposed modelintegrates a three-stage optimization framework: (1) Embedding dynamicKernelWarehouse convolution (KWConv) within the backbone network to enhancefeature representation through a dynamic kernel sharing mechanism; (2)Incorporating a triple attention mechanism (TA) into the feature pyramid tostrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoUloss function to facilitate adaptive bounding box regression penalization.Experimental validation demonstrates that the enhanced model achievessignificant performance improvements over the baseline, attaining 91.3%precision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm thesynergistic efficacy of the proposed modules. Furthermore, robustness testsindicate stable performance under conditions of data scarcity and noiseinterference. This research delivers an efficient computer vision solution forautomated infrastructure inspection, exhibiting substantial practicalengineering value.</description><author>Shaoze Huang, Qi Liu, Chao Chen, Yuhang Chen</author><pubDate>Fri, 15 Aug 2025 14:57:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11517v1</guid></item><item><title>Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations</title><link>http://arxiv.org/abs/2508.11515v1</link><description>The Weighted First-Order Model Counting Problem (WFOMC) asks to compute theweighted sum of models of a given first-order logic sentence over a givendomain. The boundary between fragments for which WFOMC can be computed inpolynomial time relative to the domain size lies between the two-variablefragment ($\text{FO}^2$) and the three-variable fragment ($\text{FO}^3$). It isknown that WFOMC for \FOthree{} is $\mathsf{\#P_1}$-hard while polynomial-timealgorithms exist for computing WFOMC for $\text{FO}^2$ and $\text{C}^2$,possibly extended by certain axioms such as the linear order axiom, theacyclicity axiom, and the connectedness axiom. All existing research hasconcentrated on extending the fragment with axioms on a single distinguishedrelation, leaving a gap in understanding the complexity boundary of axioms onmultiple relations. In this study, we explore the extension of the two-variablefragment by axioms on two relations, presenting both negative and positiveresults. We show that WFOMC for $\text{FO}^2$ with two linear order relationsand $\text{FO}^2$ with two acyclic relations are $\mathsf{\#P_1}$-hard.Conversely, we provide an algorithm in time polynomial in the domain size forWFOMC of $\text{C}^2$ with a linear order relation, its successor relation andanother successor relation.</description><author>Qipeng Kuang, Václav Kůla, Ondřej Kuželka, Yuanhong Wang, Yuyi Wang</author><pubDate>Fri, 15 Aug 2025 14:54:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11515v1</guid></item><item><title>DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality</title><link>http://arxiv.org/abs/2508.11514v1</link><description>The growing deployment of decision-making agents in dynamic environmentsincreases the demand for safety verification. While critical testing scenariogeneration has emerged as an appealing verification methodology, effectivelybalancing diversity and criticality remains a key challenge for existingmethods, particularly due to local optima entrapment in high-dimensionalscenario spaces. To address this limitation, we propose a dual-space guidedtesting framework that coordinates scenario parameter space and agent behaviorspace, aiming to generate testing scenarios considering diversity andcriticality. Specifically, in the scenario parameter space, a hierarchicalrepresentation framework combines dimensionality reduction andmulti-dimensional subspace evaluation to efficiently localize diverse andcritical subspaces. This guides dynamic coordination between two generationmodes: local perturbation and global exploration, optimizing critical scenarioquantity and diversity. Complementarily, in the agent behavior space,agent-environment interaction data are leveraged to quantify behavioralcriticality/diversity and adaptively support generation mode switching, forminga closed feedback loop that continuously enhances scenario characterization andexploration within the parameter space. Experiments show our framework improvescritical scenario generation by an average of 56.23\% and demonstrates greaterdiversity under novel parameter-behavior co-driven metrics when tested on fivedecision-making agents, outperforming state-of-the-art baselines.</description><author>Qitong Chu, Yufeng Yue, Danya Yao, Huaxin Pei</author><pubDate>Fri, 15 Aug 2025 14:51:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11514v1</guid></item><item><title>Human-AI Experience in Integrated Development Environments: A Systematic Literature Review</title><link>http://arxiv.org/abs/2503.06195v2</link><description>The integration of Artificial Intelligence (AI) into Integrated DevelopmentEnvironments (IDEs) is reshaping software development, fundamentally alteringhow developers interact with their tools. This shift marks the emergence ofHuman-AI Experience in Integrated Development Environment (in-IDE HAX), a fieldthat explores the evolving dynamics of Human-Computer Interaction inAI-assisted coding environments. Despite rapid adoption, research on in-IDE HAXremains fragmented, which highlights the need for a unified overview of currentpractices, challenges, and opportunities. To provide a structured overview ofexisting research, we conduct a systematic literature review of 90 studies,summarizing current findings and outlining areas for further investigation. We organize key insights from reviewed studies into three aspects: Impact,Design, and Quality of AI-based systems inside IDEs. Impact findings show thatAI-assisted coding enhances developer productivity but also introduceschallenges, such as verification overhead and over-reliance. Design studiesshow that effective interfaces surface context, provide explanations andtransparency of suggestion, and support user control. Quality studies documentrisks in correctness, maintainability, and security. For future research,priorities include productivity studies, design of assistance, and audit ofAI-generated code. The agenda calls for larger and longer evaluations, strongeraudit and verification assets, broader coverage across the software life cycle,and adaptive assistance under user control.</description><author>Agnia Sergeyuk, Ilya Zakharov, Ekaterina Koshchenko, Maliheh Izadi</author><pubDate>Fri, 15 Aug 2025 14:50:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.06195v2</guid></item><item><title>UI-Venus Technical Report: Building High-performance UI Agents with RFT</title><link>http://arxiv.org/abs/2508.10833v2</link><description>We present UI-Venus, a native UI agent that takes only screenshots as inputbased on a multimodal large language model. UI-Venus achieves SOTA performanceon both UI grounding and navigation tasks using only several hundred thousandhigh-quality training samples through reinforcement finetune (RFT) based onQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,Screenspot-V2 / Pro, surpassing the previous SOTA baselines includingopen-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary andplaning ability, we also evaluate it on the AndroidWorld, an online UInavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%success rate, also beating existing models. To achieve this, we introducecarefully designed reward functions for both UI grounding and navigation tasksand corresponding efficient data cleaning strategies. To further boostnavigation performance, we propose Self-Evolving Trajectory History Alignment &amp;Sparse Action Enhancement that refine historical reasoning traces and balancesthe distribution of sparse but critical actions, leading to more coherentplanning and better generalization in complex UI tasks. Our contributionsinclude the publish of SOTA open-source UI agents, comprehensive data cleaningprotocols and a novel self-evolving framework for improving navigationperformance, which encourage further research and development in the community.Code is available at https://github.com/inclusionAI/UI-Venus.</description><author>Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</author><pubDate>Fri, 15 Aug 2025 14:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10833v2</guid></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2507.01006v5</link><description>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models(VLMs) designed to advance general-purpose multimodal understanding andreasoning. In this report, we share our key findings in the development of thereasoning-centric training framework. We first develop a capable visionfoundation model with significant potential through large-scale pre-training,which arguably sets the upper bound for the final performance. We then proposeReinforcement Learning with Curriculum Sampling (RLCS) to unlock the fullpotential of the model, leading to comprehensive capability enhancement acrossa diverse range of tasks, including STEM problem solving, video understanding,content recognition, coding, grounding, GUI-based agents, and long documentinterpretation. In a comprehensive evaluation across 42 public benchmarks,GLM-4.5V achieves state-of-the-art performance on nearly all tasks amongopen-source models of similar size, and demonstrates competitive or evensuperior results compared to closed-source models such as Gemini-2.5-Flash onchallenging tasks including Coding and GUI Agents. Meanwhile, the smallerGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results tothe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source bothGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information arereleased at https://github.com/zai-org/GLM-V.</description><author>GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan</author><pubDate>Fri, 15 Aug 2025 13:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01006v5</guid></item><item><title>Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies</title><link>http://arxiv.org/abs/2508.11513v1</link><description>Enhancing the interpretability of graph neural networks (GNNs) is crucial toensure their safe and fair deployment. Recent work has introducedself-explainable GNNs that generate explanations as part of training, improvingboth faithfulness and efficiency. Some of these models, such as ProtGNN andPGIB, learn class-specific prototypes, offering a potential pathway towardclass-level explanations. However, their evaluations focus solely oninstance-level explanations, leaving open the question of whether theseprototypes meaningfully generalize across instances of the same class. In thispaper, we introduce GraphOracle, a novel self-explainable GNN frameworkdesigned to generate and evaluate class-level explanations for GNNs. Our modeljointly learns a GNN classifier and a set of structured, sparse subgraphs thatare discriminative for each class. We propose a novel integrated training thatcaptures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependenciesefficiently and faithfully, validated through a masking-based evaluationstrategy. This strategy enables us to retroactively assess whether priormethods like ProtGNN and PGIB deliver effective class-level explanations. Ourresults show that they do not. In contrast, GraphOracle achieves superiorfidelity, explainability, and scalability across a range of graphclassification tasks. We further demonstrate that GraphOracle avoids thecomputational bottlenecks of previous methods$\unicode{x2014}$like Monte CarloTree Search$\unicode{x2014}$by using entropy-regularized subgraph selection andlightweight random walk extraction, enabling faster and more scalable training.These findings position GraphOracle as a practical and principled solution forfaithful class-level self-explainability in GNNs.</description><author>Fanzhen Liu, Xiaoxiao Ma, Jian Yang, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Quan Z. Sheng, Jia Wu</author><pubDate>Fri, 15 Aug 2025 14:44:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11513v1</guid></item><item><title>Synthetic Data for Robust Stroke Segmentation</title><link>http://arxiv.org/abs/2404.01946v3</link><description>Current deep learning-based approaches to lesion segmentation in neuroimagingoften depend on high-resolution images and extensive annotated data, limitingclinical applicability. This paper introduces a novel synthetic data frameworktailored for stroke lesion segmentation, expanding the SynthSeg methodology toincorporate lesion-specific augmentations that simulate diverse pathologicalfeatures. Using a modified nnUNet architecture, our approach trains models withlabel maps from healthy and stroke datasets, facilitating segmentation acrossboth normal and pathological tissue without reliance on specific sequence-basedtraining. Evaluation across in-domain and out-of-domain (OOD) datasets revealsthat our method matches state-of-the-art performance within the training domainand significantly outperforms existing methods on OOD data. By minimizingdependence on large annotated datasets and allowing for cross-sequenceapplicability, our framework holds potential to improve clinical neuroimagingworkflows, particularly in stroke pathology. PyTorch training code and weightsare publicly available at https://github.com/liamchalcroft/SynthStroke, alongwith an SPM toolbox featuring a plug-and-play model athttps://github.com/liamchalcroft/SynthStrokeSPM.</description><author>Liam Chalcroft, Ioannis Pappas, Cathy J. Price, John Ashburner</author><pubDate>Fri, 15 Aug 2025 14:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01946v3</guid></item><item><title>Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification</title><link>http://arxiv.org/abs/2508.11511v1</link><description>Deep Learning has emerged as a promising approach for skin lesion analysis.However, existing methods mostly rely on fully supervised learning, requiringextensive labeled data, which is challenging and costly to obtain. To alleviatethis annotation burden, this study introduces a novel semi-supervised deeplearning approach that integrates ensemble learning with online knowledgedistillation for enhanced skin lesion classification. Our methodology involvestraining an ensemble of convolutional neural network models, using onlineknowledge distillation to transfer insights from the ensemble to its members.This process aims to enhance the performance of each model within the ensemble,thereby elevating the overall performance of the ensemble itself.Post-training, any individual model within the ensemble can be deployed at testtime, as each member is trained to deliver comparable performance to theensemble. This is particularly beneficial in resource-constrained environments.Experimental results demonstrate that the knowledge-distilled individual modelperforms better than independently trained models. Our approach demonstratessuperior performance on both the \emph{International Skin ImagingCollaboration} 2018 and 2019 public benchmark datasets, surpassing currentstate-of-the-art results. By leveraging ensemble learning and online knowledgedistillation, our method reduces the need for extensive labeled data whileproviding a more resource-efficient solution for skin lesion classification inreal-world scenarios.</description><author>Siyamalan Manivannan</author><pubDate>Fri, 15 Aug 2025 14:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11511v1</guid></item><item><title>Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection</title><link>http://arxiv.org/abs/2508.11504v1</link><description>Motor vehicle crashes remain a leading cause of injury and death worldwide,necessitating data-driven approaches to understand and mitigate crash severity.This study introduces a curated dataset of more than 3 million people involvedin accidents in Ohio over six years (2017-2022), aggregated to more than 2.3million vehicle-level records for predictive analysis. The primary contributionis a transparent and reproducible methodology that combines Automated MachineLearning (AutoML) and explainable artificial intelligence (AI) to identify andinterpret key risk factors associated with severe crashes. Using the JADBioAutoML platform, predictive models were constructed to distinguish betweensevere and non-severe crash outcomes. The models underwent rigorous featureselection across stratified training subsets, and their outputs wereinterpreted using SHapley Additive exPlanations (SHAP) to quantify thecontribution of individual features. A final Ridge Logistic Regression modelachieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out testset, with 17 features consistently identified as the most influentialpredictors. Key features spanned demographic, environmental, vehicle, human,and operational categories, including location type, posted speed, minimumoccupant age, and pre-crash action. Notably, certain traditionally emphasizedfactors, such as alcohol or drug impairment, were less influential in the finalmodel compared to environmental and contextual variables. Emphasizingmethodological rigor and interpretability over mere predictive performance,this study offers a scalable framework to support Vision Zero with alignedinterventions and advanced data-informed traffic safety policy.</description><author>Andrea Castellani, Zacharias Papadovasilakis, Giorgos Papoutsoglou, Mary Cole, Brian Bautsch, Tobias Rodemann, Ioannis Tsamardinos, Angela Harden</author><pubDate>Fri, 15 Aug 2025 14:31:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11504v1</guid></item><item><title>Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</title><link>http://arxiv.org/abs/2508.11503v1</link><description>Reliable autonomous navigation across the unstructured terrains of distantplanetary surfaces is a critical enabler for future space exploration. However,the deployment of learning-based controllers is hindered by the inherentsim-to-real gap, particularly for the complex dynamics of wheel interactionswith granular media. This work presents a complete sim-to-real framework fordeveloping and validating robust control policies for dynamic waypoint trackingon such challenging surfaces. We leverage massively parallel simulation totrain reinforcement learning agents across a vast distribution of procedurallygenerated environments with randomized physics. These policies are thentransferred zero-shot to a physical wheeled rover operating in a lunar-analoguefacility. Our experiments systematically compare multiple reinforcementlearning algorithms and action smoothing filters to identify the most effectivecombinations for real-world deployment. Crucially, we provide strong empiricalevidence that agents trained with procedural diversity achieve superiorzero-shot performance compared to those trained on static scenarios. We alsoanalyze the trade-offs of fine-tuning with high-fidelity particle physics,which offers minor gains in low-speed precision at a significant computationalcost. Together, these contributions establish a validated workflow for creatingreliable learning-based navigation systems, marking a critical step towardsdeploying autonomous robots in the final frontier.</description><author>Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez</author><pubDate>Fri, 15 Aug 2025 14:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11503v1</guid></item><item><title>G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration</title><link>http://arxiv.org/abs/2508.11379v1</link><description>We introduce G-CUT3R, a novel feed-forward approach for guided 3D scenereconstruction that enhances the CUT3R model by integrating prior information.Unlike existing feed-forward methods that rely solely on input images, ourmethod leverages auxiliary data, such as depth, camera calibrations, or camerapositions, commonly available in real-world scenarios. We propose a lightweightmodification to CUT3R, incorporating a dedicated encoder for each modality toextract features, which are fused with RGB image tokens via zero convolution.This flexible design enables seamless integration of any combination of priorinformation during inference. Evaluated across multiple benchmarks, including3D reconstruction and other multi-view tasks, our approach demonstratessignificant performance improvements, showing its ability to effectivelyutilize available priors while maintaining compatibility with varying inputmodalities.</description><author>Ramil Khafizov, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, Evgeny Burnaev</author><pubDate>Fri, 15 Aug 2025 10:25:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11379v1</guid></item><item><title>Image-to-Text for Medical Reports Using Adaptive Co-Attention and Triple-LSTM Module</title><link>http://arxiv.org/abs/2503.18297v3</link><description>Medical report generation requires specialized expertise that general largemodels often fail to accurately capture. Moreover, the inherent repetition andsimilarity in medical data make it difficult for models to extract meaningfulfeatures, resulting in a tendency to overfit. So in this paper, we propose amultimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learningmodel that combines transformer architectures with a Multi-LSTM network. ItsCo-Attention module synergistically links a vision transformer with a texttransformer to better differentiate medical images with similarities, augmentedby an adaptive weight operator to catch and amplify image labels with minorsimilarities. Furthermore, its Triple-LSTM module refines generated sentencesusing targeted image objects. Extensive evaluations over three public datasetshave demonstrated that CA-TriNet outperforms state-of-the-art models in termsof comprehensive ability, even pre-trained large language models on somemetrics.</description><author>Yishen Liu, Shengda Luo, Zishao Zhong, Hudan Pan</author><pubDate>Fri, 15 Aug 2025 14:30:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.18297v3</guid></item><item><title>AIM: Amending Inherent Interpretability via Self-Supervised Masking</title><link>http://arxiv.org/abs/2508.11502v1</link><description>It has been observed that deep neural networks (DNNs) often use both genuineas well as spurious features. In this work, we propose "Amending InherentInterpretability via Self-Supervised Masking" (AIM), a simple yet interestinglyeffective method that promotes the network's utilization of genuine featuresover spurious alternatives without requiring additional annotations. Inparticular, AIM uses features at multiple encoding stages to guide aself-supervised, sample-specific feature-masking process. As a result, AIMenables the training of well-performing and inherently interpretable modelsthat faithfully summarize the decision process. We validate AIM across adiverse range of challenging datasets that test both out-of-distributiongeneralization and fine-grained visual understanding. These includegeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,and ImageWoof, as well as fine-grained classification datasets such asWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dualbenefits: interpretability improvements, as measured by the Energy PointingGame (EPG) score, and accuracy gains over strong baselines. These consistentgains across domains and architectures provide compelling evidence that AIMpromotes the use of genuine and meaningful features that directly contribute toimproved generalization and human-aligned interpretability.</description><author>Eyad Alshami, Shashank Agnihotri, Bernt Schiele, Margret Keuper</author><pubDate>Fri, 15 Aug 2025 14:29:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11502v1</guid></item><item><title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title><link>http://arxiv.org/abs/2507.04996v3</link><description>Autonomy, from the Greek autos (self) and nomos (law), refers to the capacityto operate according to internal rules without external control. Accordingly,autonomous vehicles (AuVs) are viewed as vehicular systems capable ofperceiving their environment and executing pre-programmed tasks independentlyof external input. However, both research and real-world deploymentsincreasingly showcase vehicles that demonstrate behaviors beyond thisdefinition (including the SAE levels 0 to 5); Examples of this outpace includethe interaction with humans with natural language, goal adaptation, contextualreasoning, external tool use, and unseen ethical dilemma handling, largelyempowered by multi-modal large language models (LLMs). These developmentsreveal a conceptual gap between technical autonomy and the broader cognitiveand social capabilities needed for future human-centered mobility systems. Toaddress this gap, this paper introduces the concept of agentic vehicles (AgVs),referring to vehicles that integrate agentic AI systems to reason, adapt, andinteract within complex environments. This paper proposes the term AgVs andtheir distinguishing characteristics from conventional AuVs. It synthesizesrelevant advances in integrating LLMs and AuVs and highlights how AgVs mighttransform future mobility systems and ensure the systems are human-centered.The paper concludes by identifying key challenges in the development andgovernance of AgVs, and how they can play a significant role in future agentictransportation systems.</description><author>Jiangbo Yu</author><pubDate>Fri, 15 Aug 2025 14:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.04996v3</guid></item><item><title>Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models</title><link>http://arxiv.org/abs/2508.11499v1</link><description>Historical handwritten text recognition (HTR) is essential for unlocking thecultural and scholarly value of archival documents, yet digitization is oftenhindered by scarce transcriptions, linguistic variation, and highly diversehandwriting styles. In this study, we apply TrOCR, a state-of-the-arttransformer-based HTR model, to 16th-century Latin manuscripts authored byRudolf Gwalther. We investigate targeted image preprocessing and a broad suiteof data augmentation techniques, introducing four novel augmentation methodsdesigned specifically for historical handwriting characteristics. We alsoevaluate ensemble learning approaches to leverage the complementary strengthsof augmentation-trained models. On the Gwalther dataset, our best single-modelaugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while atop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relativeimprovement over the best reported TrOCR_BASE result and a 42% improvement overthe previous state of the art. These results highlight the impact ofdomain-specific augmentations and ensemble strategies in advancing HTRperformance for historical manuscripts.</description><author>Erez Meoded</author><pubDate>Fri, 15 Aug 2025 14:20:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11499v1</guid></item><item><title>Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition</title><link>http://arxiv.org/abs/2508.11497v1</link><description>Convolutional neural networks (CNNs) have demonstrated strong performance in visual recognition tasks, but their inherent reliance on regular grid structures limits their capacity to model complex topological relationships and non-local semantics within images. To address this limita tion, we proposethe hierarchical graph feature enhancement (HGFE), a novel framework that integrates graph-based rea soning into CNNs toenhance both structural awareness and feature representation. HGFE builds two complementary levels of graph structures: intra-window graph convolution to cap ture local spatialdependencies and inter-window supernode interactions to model global semantic relationships. Moreover, we introduce an adaptive frequency modulation module that dynamically balances low-frequency and high-frequency signal propagation, preserving critical edge and texture information while mitigating over-smoothing. The proposed HGFE module is lightweight, end-to-end trainable, and can be seamlessly integrated into standard CNN backbone networks. Extensive experiments on CIFAR-100 (classification), PASCAL VOC, and VisDrone (detection), as well as CrackSeg and CarParts (segmentation), validated the effectiveness of the HGFE in improving structural representation and enhancing overall recognition performance.</description><author>Feiyue Zhao, Zhichao Zhang</author><pubDate>Fri, 15 Aug 2025 14:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11497v1</guid></item><item><title>MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs</title><link>http://arxiv.org/abs/2508.08715v2</link><description>Generative speech models have demonstrated significant potential inpersonalizing teacher-student interactions, offering valuable real-worldapplications for language learning in children's education. However, achievinghigh-quality, child-friendly speech generation remains challenging,particularly for low-resource languages across diverse languages and culturalcontexts. In this paper, we propose MultiAiTutor, an educational multilingualgenerative AI tutor with child-friendly designs, leveraging LLM architecturefor speech generation tailored for educational purposes. We propose tointegrate age-appropriate multilingual speech generation using LLMarchitectures, facilitating young children's language learning throughculturally relevant image-description tasks in three low-resource languages:Singaporean-accent Mandarin, Malay, and Tamil. Experimental results from bothobjective metrics and subjective evaluations demonstrate the superiorperformance of the proposed MultiAiTutor compared to baseline methods.</description><author>Xiaoxue Gao, Huayun Zhang, Nancy F. Chen</author><pubDate>Fri, 15 Aug 2025 14:15:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.08715v2</guid></item><item><title>Relative Position Matters: Trajectory Prediction and Planning with Polar Representation</title><link>http://arxiv.org/abs/2508.11492v1</link><description>Trajectory prediction and planning in autonomous driving are highlychallenging due to the complexity of predicting surrounding agents' movementsand planning the ego agent's actions in dynamic environments. Existing methodsencode map and agent positions and decode future trajectories in Cartesiancoordinates. However, modeling the relationships between the ego vehicle andsurrounding traffic elements in Cartesian space can be suboptimal, as it doesnot naturally capture the varying influence of different elements based ontheir relative distances and directions. To address this limitation, we adoptthe Polar coordinate system, where positions are represented by radius andangle. This representation provides a more intuitive and effective way to modelspatial changes and relative relationships, especially in terms of distance anddirectional influence. Based on this insight, we propose Polaris, a novelmethod that operates entirely in Polar coordinates, distinguishing itself fromconventional Cartesian-based approaches. By leveraging the Polarrepresentation, this method explicitly models distance and direction variationsand captures relative relationships through dedicated encoding and refinementmodules, enabling more structured and spatially aware trajectory prediction andplanning. Extensive experiments on the challenging prediction (Argoverse 2) andplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-artperformance.</description><author>Bozhou Zhang, Nan Song, Bingzhao Gao, Li Zhang</author><pubDate>Fri, 15 Aug 2025 14:15:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11492v1</guid></item><item><title>Comparison of D-Wave Quantum Annealing and Markov Chain Monte Carlo for Sampling from a Probability Distribution of a Restricted Boltzmann Machine</title><link>http://arxiv.org/abs/2508.10228v2</link><description>A local-valley (LV) centered approach to assessing the quality of samplingfrom Restricted Boltzmann Machines (RBMs) was applied to the latest generationof the D-Wave quantum annealer. D-Wave and Gibbs samples from a classicallytrained RBM were obtained at conditions relevant to thecontrastive-divergence-based RBM learning. The samples were compared for thenumber of the LVs to which they belonged and the energy of the correspondinglocal minima. No significant (desirable) increase in the number of the LVs hasbeen achieved by decreasing the D-Wave annealing time. At any training epoch,the states sampled by the D-Wave belonged to a somewhat higher number of LVsthan in the Gibbs sampling. However, many of those LVs found by the twotechniques differed. For high-probability sampled states, the two techniqueswere (unfavorably) less complementary and more overlapping. Nevertheless, manypotentially "important" local minima, i.e., those having intermediate, even ifnot high, probability values, were found by only one of the two samplingtechniques while missed by the other. The two techniques overlapped less atlater than earlier training epochs, which is precisely the stage of thetraining when modest improvements to the sampling quality could make meaningfuldifferences for the RBM trainability. The results of this work may explain thefailure of previous investigations to achieve substantial (or any) improvementwhen using D-Wave-based sampling. However, the results reveal some potentialfor improvement, e.g., using a combined classical-quantum approach.</description><author>Abdelmoula El Yazizi, Samee U. Khan, Yaroslav Koshka</author><pubDate>Fri, 15 Aug 2025 14:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10228v2</guid></item><item><title>ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos</title><link>http://arxiv.org/abs/2508.06570v2</link><description>The existing research has primarily focused on text and image-based hatespeech detection, video-based approaches remain underexplored. In this work, weintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hatespeech detection in videos. ImpliHateVid consists of 2,009 videos comprising509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,making it one of the first large-scale video datasets dedicated to implicithate detection. We also propose a novel two-stage contrastive learningframework for hate speech detection in videos. In the first stage, we trainmodality-specific encoders for audio, text, and image using contrastive loss byconcatenating features from the three encoders. In the second stage, we traincross-encoders using contrastive learning to refine multimodal representations.Additionally, we incorporate sentiment, emotion, and caption-based features toenhance implicit hate detection. We evaluate our method on two datasets,ImpliHateVid for implicit hate speech detection and another dataset for generalhate speech detection in videos, HateMM dataset, demonstrating theeffectiveness of the proposed multimodal contrastive learning for hatefulcontent detection in videos and the significance of our dataset.</description><author>Mohammad Zia Ur Rehman, Anukriti Bhatnagar, Omkar Kabde, Shubhi Bansal, Nagendra Kumar</author><pubDate>Fri, 15 Aug 2025 14:09:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06570v2</guid></item><item><title>Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2508.11488v1</link><description>End-to-end autonomous driving has achieved remarkable advancements in recentyears. Existing methods primarily follow a perception-planning paradigm, whereperception and planning are executed sequentially within a fully differentiableframework for planning-oriented optimization. We further advance this paradigmthrough a perception-in-plan framework design, which integrates perception intothe planning process. This design facilitates targeted perception guided byevolving planning objectives over time, ultimately enhancing planningperformance. Building on this insight, we introduce VeteranAD, a coupledperception and planning framework for end-to-end autonomous driving. Byincorporating multi-mode anchored trajectories as planning priors, theperception module is specifically designed to gather traffic elements alongthese trajectories, enabling comprehensive and targeted perception. Planningtrajectories are then generated based on both the perception results and theplanning priors. To make perception fully serve planning, we adopt anautoregressive strategy that progressively predicts future trajectories whilefocusing on relevant regions for targeted perception at each step. With thissimple yet effective design, VeteranAD fully unleashes the potential ofplanning-oriented end-to-end methods, leading to more accurate and reliabledriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasetsdemonstrate that our VeteranAD achieves state-of-the-art performance.</description><author>Bozhou Zhang, Jingyu Li, Nan Song, Li Zhang</author><pubDate>Fri, 15 Aug 2025 14:05:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11488v1</guid></item><item><title>Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction</title><link>http://arxiv.org/abs/2503.06587v3</link><description>Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometryreconstruction quality than the popular 3DGS by using 2D surfels to approximatethin surfaces. However, it falls short when dealing with glossy surfaces,resulting in visible holes in these areas. We find that the reflectiondiscontinuity causes the issue. To fit the jump from diffuse to specularreflection at different viewing angles, depth bias is introduced in theoptimized Gaussian primitives. To address that, we first replace the depthdistortion loss in 2DGS with a novel depth convergence loss, which imposes astrong constraint on depth continuity. Then, we rectify the depth criterion indetermining the actual surface, which fully accounts for all the intersectingGaussians along the ray. Qualitative and quantitative evaluations acrossvarious datasets reveal that our method significantly improves reconstructionquality, with more complete and accurate surfaces than 2DGS. Code is availableat https://github.com/XiaoXinyyx/Unbiased_Surfel.</description><author>Yixin Yang, Yang Zhou, Hui Huang</author><pubDate>Fri, 15 Aug 2025 13:21:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.06587v3</guid></item><item><title>AFR-CLIP: Enhancing Zero-Shot Industrial Anomaly Detection with Stateless-to-Stateful Anomaly Feature Rectification</title><link>http://arxiv.org/abs/2503.12910v2</link><description>Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotalparadigm for industrial inspection and medical diagnostics, detecting defectsin novel objects without requiring any target-dataset samples during training.Existing CLIP-based ZSAD methods generate anomaly maps by measuring the cosinesimilarity between visual and textual features. However, CLIP's alignment withobject categories instead of their anomalous states limits its effectivenessfor anomaly detection. To address this limitation, we propose AFR-CLIP, aCLIP-based anomaly feature rectification framework. AFR-CLIP first performsimage-guided textual rectification, embedding the implicit defect informationfrom the image into a stateless prompt that describes the object categorywithout indicating any anomalous state. The enriched textual embeddings arethen compared with two pre-defined stateful (normal or abnormal) embeddings,and their text-on-text similarity yields the anomaly map that highlightsdefective regions. To further enhance perception to multi-scale features andcomplex anomalies, we introduce self prompting (SP) and multi-patch featureaggregation (MPFA) modules. Extensive experiments are conducted on elevenanomaly detection benchmarks across industrial and medical domains,demonstrating AFR-CLIP's superiority in ZSAD.</description><author>Jingyi Yuan, Chenqiang Gao, Pengyu Jie, Xuan Xia, Shangri Huang, Wanquan Liu</author><pubDate>Fri, 15 Aug 2025 14:03:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.12910v2</guid></item><item><title>Effective Stimulus Propagation in Neural Circuits: Driver Node Selection</title><link>http://arxiv.org/abs/2506.13615v4</link><description>Precise control of signal propagation in modular neural networks represents afundamental challenge in computational neuroscience. We establish a frameworkfor identifying optimal control nodes that maximize stimulus transmissionbetween weakly coupled neural populations. Using spiking stochastic block modelnetworks, we systematically compare driver node selection strategies -including random sampling and topology-based centrality measures (degree,betweenness, closeness, eigenvector, harmonic, and percolation centrality) - todetermine minimal control inputs for achieving inter-populationsynchronization. Targeted stimulation of just 10-20% of the most centralneurons in the source population significantly enhances spiking propagationfidelity compared to random selection. This approach yields a 64-fold increasein signal transfer efficiency at critical inter-module connection densities.These findings establish a theoretical foundation for precision neuromodulationin biological neural systems and neurotechnology applications.</description><author>Bulat Batuev, Arsenii Onuchin, Sergey Sukhov</author><pubDate>Fri, 15 Aug 2025 14:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.13615v4</guid></item><item><title>Automated Building Heritage Assessment Using Street-Level Imagery</title><link>http://arxiv.org/abs/2508.11486v1</link><description>Detailed data is required to quantify energy conservation measures inbuildings, such as envelop retrofits, without compromising cultural heritage.Novel artificial intelligence tools may improve efficiency in identifyingheritage values in buildings compared to costly and time-consuming traditionalinventories. In this study, the large language model GPT was used to detectvarious aspects of cultural heritage value in fa\c{c}ade images. Using thisdata and building register data as features, machine learning models weretrained to classify multi-family and non-residential buildings in Stockholm,Sweden. Validation against an expert-created inventory shows a macro F1-scoreof 0.71 using a combination of register data and features retrieved from GPT,and a score of 0.60 using only GPT-derived data. The presented methodology cancontribute to a higher-quality database and thus support careful energyefficiency measures and integrated consideration of heritage value inlarge-scale energetic refurbishment scenarios.</description><author>Kristina Dabrock, Tim Johansson, Anna Donarelli, Mikael Mangold, Noah Pflugradt, Jann Michael Weinand, Jochen Linßen</author><pubDate>Fri, 15 Aug 2025 13:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11486v1</guid></item><item><title>CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models</title><link>http://arxiv.org/abs/2508.11484v1</link><description>Despite significant advances in video synthesis, research into multi-shotvideo generation remains in its infancy. Even with scaled-up models and massivedatasets, the shot transition capabilities remain rudimentary and unstable,largely confining generated videos to single-shot sequences. In this work, weintroduce CineTrans, a novel framework for generating coherent multi-shotvideos with cinematic, film-style transitions. To facilitate insights into thefilm editing style, we construct a multi-shot video-text dataset Cine250K withdetailed shot annotations. Furthermore, our analysis of existing videodiffusion models uncovers a correspondence between attention maps in thediffusion model and shot boundaries, which we leverage to design a mask-basedcontrol mechanism that enables transitions at arbitrary positions and transferseffectively in a training-free setting. After fine-tuning on our dataset withthe mask mechanism, CineTrans produces cinematic multi-shot sequences whileadhering to the film editing style, avoiding unstable transitions or naiveconcatenations. Finally, we propose specialized evaluation metrics fortransition control, temporal consistency and overall quality, and demonstratethrough extensive experiments that CineTrans significantly outperforms existingbaselines across all criteria.</description><author>Xiaoxue Wu, Bingjie Gao, Yu Qiao, Yaohui Wang, Xinyuan Chen</author><pubDate>Fri, 15 Aug 2025 13:58:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11484v1</guid></item><item><title>OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring</title><link>http://arxiv.org/abs/2508.11482v1</link><description>The construction industry increasingly relies on visual data to supportArtificial Intelligence (AI) and Machine Learning (ML) applications for sitemonitoring. High-quality, domain-specific datasets, comprising images, videos,and point clouds, capture site geometry and spatiotemporal dynamics, includingthe location and interaction of objects, workers, and materials. However,despite growing interest in leveraging visual datasets, existing resources varywidely in sizes, data modalities, annotation quality, and representativeness ofreal-world construction conditions. A systematic review to categorize theirdata characteristics and application contexts is still lacking, limiting thecommunity's ability to fully understand the dataset landscape, identifycritical gaps, and guide future directions toward more effective, reliable, andscalable AI applications in construction. To address this gap, this studyconducts an extensive search of academic databases and open-data platforms,yielding 51 publicly available visual datasets that span the 2005-2024 period.These datasets are categorized using a structured data schema covering (i) datafundamentals (e.g., size and license), (ii) data modalities (e.g., RGB andpoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)downstream application domains (e.g., progress tracking). This studysynthesizes these findings into an open-source catalog, OpenConstruction,supporting data-driven method development. Furthermore, the study discussesseveral critical limitations in the existing construction dataset landscape andpresents a roadmap for future data infrastructure anchored in the Findability,Accessibility, Interoperability, and Reusability (FAIR) principles. Byreviewing the current landscape and outlining strategic priorities, this studysupports the advancement of data-centric solutions in the construction sector.</description><author>Ruoxin Xiong, Yanyu Wang, Jiannan Cai, Kaijian Liu, Yuansheng Zhu, Pingbo Tang, Nora El-Gohary</author><pubDate>Fri, 15 Aug 2025 13:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11482v1</guid></item><item><title>TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations</title><link>http://arxiv.org/abs/2508.11478v1</link><description>Abnormal Human Behavior Detection (AHBD) under special scenarios is becomingincreasingly crucial. While YOLO-based detection methods excel in real-timetasks, they remain hindered by challenges including small objects, taskconflicts, and multi-scale fusion in AHBD. To tackle them, we proposeTACR-YOLO, a new real-time framework for AHBD. We introduce a CoordinateAttention Module to enhance small object detection, a Task-Aware AttentionModule to deal with classification-regression conflicts, and a Strengthen NeckNetwork for refined multi-scale fusion, respectively. In addition, we optimizeAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improvebounding box regression. The Personnel Anomalous Behavior Detection (PABD)dataset, which includes 8,529 samples across four behavior categories, is alsopresented. Extensive experimental results indicate that TACR-YOLO achieves91.92% mAP on PABD, with competitive speed and robustness. Ablation studieshighlight the contribution of each improvement. This work provides new insightsfor abnormal behavior detection under special scenarios, advancing itsprogress.</description><author>Xinyi Yin, Wenbo Yuan, Xuecheng Wu, Liangyu Fu, Danlei Huang</author><pubDate>Fri, 15 Aug 2025 13:45:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11478v1</guid></item><item><title>SPG: Style-Prompting Guidance for Style-Specific Content Creation</title><link>http://arxiv.org/abs/2508.11476v1</link><description>Although recent text-to-image (T2I) diffusion models excel at aligninggenerated images with textual prompts, controlling the visual style of theoutput remains a challenging task. In this work, we propose Style-PromptingGuidance (SPG), a novel sampling strategy for style-specific image generation.SPG constructs a style noise vector and leverages its directional deviationfrom unconditional noise to guide the diffusion process toward the target styledistribution. By integrating SPG with Classifier-Free Guidance (CFG), ourmethod achieves both semantic fidelity and style consistency. SPG is simple,robust, and compatible with controllable frameworks like ControlNet andIPAdapter, making it practical and widely applicable. Extensive experimentsdemonstrate the effectiveness and generality of our approach compared tostate-of-the-art methods. Code is available athttps://github.com/Rumbling281441/SPG.</description><author>Qian Liang, Zichong Chen, Yang Zhou, Hui Huang</author><pubDate>Fri, 15 Aug 2025 13:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11476v1</guid></item><item><title>DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning</title><link>http://arxiv.org/abs/2508.06972v2</link><description>DSperse is a modular framework for distributed machine learning inferencewith strategic cryptographic verification. Operating within the emergingparadigm of distributed zero-knowledge machine learning, DSperse avoids thehigh cost and rigidity of full-model circuitization by enabling targetedverification of strategically chosen subcomputations. These verifiablesegments, or "slices", may cover part or all of the inference pipeline, withglobal consistency enforced through audit, replication, or economic incentives.This architecture supports a pragmatic form of trust minimization, localizingzero-knowledge proofs to the components where they provide the greatest value.We evaluate DSperse using multiple proving systems and report empirical resultson memory usage, runtime, and circuit behavior under sliced and unslicedconfigurations. By allowing proof boundaries to align flexibly with the model'slogical structure, DSperse supports scalable, targeted verification strategiessuited to diverse deployment needs.</description><author>Dan Ivanov, Tristan Freiberg, Shirin Shahabi, Jonathan Gold, Haruna Isah</author><pubDate>Fri, 15 Aug 2025 13:42:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06972v2</guid></item><item><title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title><link>http://arxiv.org/abs/2508.09736v2</link><description>We introduce M3-Agent, a novel multimodal agent framework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual andauditory inputs to build and update its long-term memory. Beyond episodicmemory, it also develops semantic memory, enabling it to accumulate worldknowledge over time. Its memory is organized in an entity-centric, multimodalformat, allowing deeper and more consistent understanding of the environment.Given an instruction, M3-Agent autonomously performs multi-turn, iterativereasoning and retrieves relevant information from memory to accomplish thetask. To evaluate memory effectiveness and memory-based reasoning in multimodalagents, we develop M3-Bench, a new long-video question answering benchmark.M3-Bench comprises 100 newly recorded real-world videos captured from a robot'sperspective (M3-Bench-robot) and 920 web-sourced videos across diversescenarios (M3-Bench-web). We annotate question-answer pairs designed to testkey capabilities essential for agent applications, such as human understanding,general knowledge extraction, and cross-modal reasoning. Experimental resultsshow that M3-Agent, trained via reinforcement learning, outperforms thestrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-weband VideoMME-long, respectively. Our work advances the multimodal agents towardmore human-like long-term memory and provides insights into their practicaldesign. Model, code and data are available athttps://github.com/bytedance-seed/m3-agent</description><author>Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</author><pubDate>Fri, 15 Aug 2025 13:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09736v2</guid></item><item><title>Discovering Invariant Neighborhood Patterns for Heterophilic Graphs</title><link>http://arxiv.org/abs/2403.10572v2</link><description>This paper studies the problem of distribution shifts on non-homophilousgraphs Mosting existing graph neural network methods rely on the homophilousassumption that nodes from the same class are more likely to be linked.However, such assumptions of homophily do not always hold in real-world graphs,which leads to more complex distribution shifts unaccounted for in previousmethods. The distribution shifts of neighborhood patterns are much more diverseon non-homophilous graphs. We propose a novel Invariant Neighborhood PatternLearning (INPL) to alleviate the distribution shifts problem on non-homophilousgraphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP)module to capture the adaptive neighborhood information, which could alleviatethe neighborhood pattern distribution shifts problem on non-homophilous graphs.We propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrainthe ANP and learn invariant graph representation on non-homophilous graphs.Extensive experimental results on real-world non-homophilous graphs show thatINPL could achieve state-of-the-art performance for learning on largenon-homophilous graphs.</description><author>Jinluan Yang, Ruihao Zhang, Zhengyu Chen, Teng Xiao, Yueyang Wang, Fei Wu, Kun Kuang</author><pubDate>Fri, 15 Aug 2025 13:36:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10572v2</guid></item><item><title>Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts</title><link>http://arxiv.org/abs/2509.05323v2</link><description>This paper presents an artistic and technical investigation into theattention mechanisms of video diffusion transformers. Inspired by early videoartists who manipulated analog video signals to create new visual aesthetics,this study proposes a method for extracting and visualizing cross-attentionmaps in generative video models. Built on the open-source Wan model, our toolprovides an interpretable window into the temporal and spatial behavior ofattention in text-to-video generation. Through exploratory probes and anartistic case study, we examine the potential of attention maps as bothanalytical tools and raw artistic material. This work contributes to thegrowing field of Explainable AI for the Arts (XAIxArts), inviting artists toreclaim the inner workings of AI as a creative medium.</description><author>Adam Cole, Mick Grierson</author><pubDate>Tue, 09 Sep 2025 12:40:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.05323v2</guid></item><item><title>RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning</title><link>http://arxiv.org/abs/2508.11472v1</link><description>Insider threat detection aims to identify malicious user behavior byanalyzing logs that record user interactions. Due to the lack of fine-grainedbehavior-level annotations, detecting specific behavior-level anomalies withinuser behavior sequences is challenging. Unsupervised methods face high falsepositive rates and miss rates due to the inherent ambiguity between normal andanomalous behaviors. In this work, we instead introduce weak labels of behaviorsequences, which have lower annotation costs, i.e., the training labels(anomalous or normal) are at sequence-level instead of behavior-level, toenhance the detection capability for behavior-level anomalies by learningdiscriminative features. To achieve this, we propose a novel framework calledRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres torepresent the normal patterns of behaviors. Initially, a one-class classifieris constructed as a good anomaly-supervision-free starting point. Building onthis, using multiple instance learning and adaptive behavior-levelself-training debiasing based on model prediction confidence, the frameworkfurther refines hyper-spheres and feature representations using weaksequence-level labels. This approach enhances the model's ability todistinguish between normal and anomalous behaviors. Extensive experimentsdemonstrate that RMSL significantly improves the performance of behavior-levelinsider threat detection.</description><author>Yang Wang, Yaxin Zhao, Xinyu Jiao, Sihan Xu, Xiangrui Cai, Ying Zhang, Xiaojie Yuan</author><pubDate>Fri, 15 Aug 2025 13:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11472v1</guid></item><item><title>CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation</title><link>http://arxiv.org/abs/2508.11469v1</link><description>Accurate segmentation of the glomerular basement membrane (GBM) in electronmicroscopy (EM) images is fundamental for quantifying membrane thickness andsupporting the diagnosis of various kidney diseases. While supervised deeplearning approaches achieve high segmentation accuracy, their reliance onextensive pixel-level annotation renders them impractical for clinicalworkflows. Few-shot learning can reduce this annotation burden but oftenstruggles to capture the fine structural details necessary for GBM analysis. Inthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shotsegmentation pipeline designed for GBM delineation in EM images. CoFi firsttrains a lightweight neural network using only three annotated images toproduce an initial coarse segmentation mask. This mask is then automaticallyprocessed to generate high-quality point prompts with morphology-aware pruning,which are subsequently used to guide SAM in refining the segmentation. Theproposed method achieved exceptional GBM segmentation performance, with a Dicecoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate thatCoFi not only alleviates the annotation and computational burdens associatedwith conventional methods, but also achieves accurate and reliable segmentationresults. The pipeline's speed and annotation efficiency make it well-suited forresearch and hold strong potential for clinical applications in renalpathology. The pipeline is publicly available at:https://github.com/ddrrnn123/CoFi.</description><author>Hongjin Fang, Daniel Reisenbüchler, Kenji Ikemura, Mert R. Sabuncu, Yihe Yang, Ruining Deng</author><pubDate>Fri, 15 Aug 2025 13:34:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11469v1</guid></item><item><title>Learning-based Sketches for Frequency Estimation in Data Streams without Ground Truth</title><link>http://arxiv.org/abs/2412.03611v3</link><description>Estimating the frequency of items on the high-volume, fast data stream hasbeen extensively studied in many areas, such as database and networkmeasurement. Traditional sketches provide only coarse estimates under strictmemory constraints. Although some learning-augmented methods have emergedrecently, they typically rely on offline training with real frequencies or/andlabels, which are often unavailable. Moreover, these methods suffer from slowupdate speeds, limiting their suitability for real-time processing despiteoffering only marginal accuracy improvements. To overcome these challenges, wepropose UCL-sketch, a practical learning-based paradigm for per-key frequencyestimation. Our design introduces two key innovations: (i) an online trainingmechanism based on equivalent learning that requires no ground truth (GT), and(ii) a highly scalable architecture leveraging logically structured estimationbuckets to scale to real-world data stream. The UCL-sketch, which utilizescompressive sensing (CS), converges to an estimator that provably yields aerror bound far lower than that of prior works, without sacrificing the speedof processing. Extensive experiments on both real-world and synthetic datasetsdemonstrate that our approach outperforms previously proposed approachesregarding per-key accuracy and distribution. Notably, under extremely tightmemory budgets, its quality almost matches that of an (infeasible) omniscientoracle. Moreover, compared to the existing equation-based sketch, UCL-sketchachieves an average decoding speedup of nearly 500 times. To help furtherresearch and development, our code is publicly available athttps://github.com/Y-debug-sys/UCL-sketch.</description><author>Xinyu Yuan, Yan Qiao, Meng Li, Zhenchun Wei, Cuiying Feng, Zonghui Wang, Wenzhi Chen</author><pubDate>Fri, 15 Aug 2025 13:27:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03611v3</guid></item><item><title>When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing</title><link>http://arxiv.org/abs/2508.10482v2</link><description>In the study of trustworthy Natural Language Processing (NLP), a number ofimportant research fields have emerged, including that of explainability andprivacy. While research interest in both explainable and privacy-preserving NLPhas increased considerably in recent years, there remains a lack ofinvestigation at the intersection of the two. This leaves a considerable gap inunderstanding of whether achieving both explainability and privacy is possible,or whether the two are at odds with each other. In this work, we conduct anempirical investigation into the privacy-explainability trade-off in thecontext of NLP, guided by the popular overarching methods of DifferentialPrivacy (DP) and Post-hoc Explainability. Our findings include a view into theintricate relationship between privacy and explainability, which is formed by anumber of factors, including the nature of the downstream task and choice ofthe text privatization and explainability method. In this, we highlight thepotential for privacy and explainability to co-exist, and we summarize ourfindings in a collection of practical recommendations for future work at thisimportant intersection.</description><author>Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci</author><pubDate>Fri, 15 Aug 2025 13:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10482v2</guid></item><item><title>Thyme: Think Beyond Images</title><link>http://arxiv.org/abs/2508.11630v1</link><description>Following OpenAI's introduction of the ``thinking with images'' concept,recent efforts have explored stimulating the use of visual information in thereasoning process to enhance model performance in perception and reasoningtasks. However, to the best of our knowledge, no open-source work currentlyoffers a feature set as rich as proprietary models (O3), which can performdiverse image manipulations and simultaneously enhance logical reasoningcapabilities through code. In this paper, we make a preliminary attempt in thisdirection by introducing Thyme (Think Beyond Images), a novel paradigm forenabling MLLMs to transcend existing ``think with images'' approaches byautonomously generating and executing diverse image processing andcomputational operations via executable code. This approach not onlyfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,rotation, contrast enhancement) but also allows for mathematical computations,all while maintaining high autonomy in deciding when and how to apply theseoperations. We activate this capability through a two-stage training strategy:an initial SFT on a curated dataset of 500K samples to teach code generation,followed by a RL phase to refine decision-making. For the RL stage, we manuallycollect and design high-resolution question-answer pairs to increase thelearning difficulty, and we propose GRPO-ATS (Group Relative PolicyOptimization with Adaptive Temperature Sampling), an algorithm that appliesdistinct temperatures to text and code generation to balance reasoningexploration with code execution precision. We conduct extensive experimentalanalysis and ablation studies. Comprehensive evaluations on nearly 20benchmarks show that Thyme yields significant and consistent performance gains,particularly in challenging high-resolution perception and complex reasoningtasks.</description><author>Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, Guorui Zhou</author><pubDate>Fri, 15 Aug 2025 17:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11630v1</guid></item></channel></rss>