<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 23 Apr 2024 11:08:01 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>In-Context Learning Dynamics with Random Binary Sequences</title><link>http://arxiv.org/abs/2310.17639v3</link><description>Large language models (LLMs) trained on huge corpora of text datasetsdemonstrate intriguing capabilities, achieving state-of-the-art performance ontasks they were not explicitly trained for. The precise nature of LLMcapabilities is often mysterious, and different prompts can elicit differentcapabilities through in-context learning. We propose a framework that enablesus to analyze in-context learning dynamics to understand latent conceptsunderlying LLMs' behavioral patterns. This provides a more nuancedunderstanding than success-or-failure evaluation benchmarks, but does notrequire observing internal activations as a mechanistic interpretation ofcircuits would. Inspired by the cognitive science of human randomnessperception, we use random binary sequences as context and study dynamics ofin-context learning by manipulating properties of context data, such assequence length. In the latest GPT-3.5+ models, we find emergent abilities togenerate seemingly random numbers and learn basic formal languages, withstriking in-context learning dynamics where model outputs transition sharplyfrom seemingly random behaviors to deterministic repetition.</description><author>Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tomer D. Ullman</author><pubDate>Tue, 16 Apr 2024 02:35:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.17639v3</guid></item><item><title>In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering</title><link>http://arxiv.org/abs/2311.06668v3</link><description>Large language models (LLMs) demonstrate emergent in-context learningcapabilities, where they adapt to new tasks based on example demonstrations.However, in-context learning has seen limited effectiveness in many settings,is difficult to quantitatively control and takes up context window space. Toovercome these limitations, we propose an alternative approach that recastsin-context learning as in-context vectors (ICV). Using ICV has two steps. Wefirst use a forward pass on demonstration examples to create the in-contextvector from the latent embedding of the LLM. This vector captures essentialinformation about the intended task. On a new query, instead of addingdemonstrations to the prompt, we shift the latent states of the LLM using theICV. The ICV approach has several benefits: 1) it enables the LLM to moreeffectively follow the demonstration examples; 2) it's easy to control byadjusting the magnitude of the ICV; 3) it reduces the length of the prompt byremoving the in-context demonstrations; 4) ICV is computationally much moreefficient than fine-tuning. We demonstrate that ICV achieves better performancecompared to standard in-context learning and fine-tuning on diverse tasksincluding safety, style transfer, role-playing and formatting. Moreover, weshow that we can flexibly teach LLM to simultaneously follow different types ofinstructions by simple vector arithmetics on the corresponding ICVs.</description><author>Sheng Liu, Haotian Ye, Lei Xing, James Zou</author><pubDate>Tue, 13 Feb 2024 22:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06668v3</guid></item><item><title>The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis</title><link>http://arxiv.org/abs/2402.12976v1</link><description>In-context learning is a popular inference strategy where large languagemodels solve a task using only a few labelled demonstrations without needingany parameter updates. Compared to work on monolingual (English) in-contextlearning, multilingual in-context learning is under-explored, and we lack anin-depth understanding of the role of demonstrations in this context. Toaddress this gap, we conduct a multidimensional analysis of multilingualin-context learning, experimenting with 5 models from different model families,9 datasets covering classification and generation tasks, and 56 typologicallydiverse languages. Our results reveal that the effectiveness of demonstrationsvaries significantly across models, tasks, and languages. We also find thatLlama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality ofdemonstrations. Instead, a carefully crafted template often eliminates thebenefits of demonstrations for some tasks and languages altogether. Thesefindings show that the importance of demonstrations might be overestimated. Ourwork highlights the need for granular evaluation across multiple axes towards abetter understanding of in-context learning.</description><author>Miaoran Zhang, Vagrant Gautam, Mingyang Wang, Jesujoba O. Alabi, Xiaoyu Shen, Dietrich Klakow, Marius Mosbach</author><pubDate>Tue, 20 Feb 2024 12:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12976v1</guid></item><item><title>Uncertainty Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v2</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:https://github.com/lingchen0331/UQ_ICL.</description><author>Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 28 Mar 2024 20:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v2</guid></item><item><title>Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models</title><link>http://arxiv.org/abs/2402.10189v1</link><description>In-context learning has emerged as a groundbreaking ability of Large LanguageModels (LLMs) and revolutionized various fields by providing a fewtask-relevant demonstrations in the prompt. However, trustworthy issues withLLM's response, such as hallucination, have also been actively discussed.Existing works have been devoted to quantifying the uncertainty in LLM'sresponse, but they often overlook the complex nature of LLMs and the uniquenessof in-context learning. In this work, we delve into the predictive uncertaintyof LLMs associated with in-context learning, highlighting that suchuncertainties may stem from both the provided demonstrations (aleatoricuncertainty) and ambiguities tied to the model's configurations (epistemicuncertainty). We propose a novel formulation and corresponding estimationmethod to quantify both types of uncertainties. The proposed method offers anunsupervised way to understand the prediction of in-context learning in aplug-and-play fashion. Extensive experiments are conducted to demonstrate theeffectiveness of the decomposition. The code and data are available at:\url{https://github.com/lingchen0331/UQ_ICL}.</description><author>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</author><pubDate>Thu, 15 Feb 2024 18:46:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10189v1</guid></item><item><title>Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability</title><link>http://arxiv.org/abs/2310.08049v3</link><description>What is the relationship between model architecture and the ability toperform in-context learning? In this empirical study, we take the first stepstoward answering this question. We evaluate thirteen model architecturescapable of causal language modeling across a suite of synthetic in-contextlearning tasks. These selected architectures represent a broad range ofparadigms, including recurrent and convolution-based neural networks,transformers, state space model inspired, and other emerging attentionalternatives. We discover that all the considered architectures can performin-context learning under a wider range of conditions than previouslydocumented. Additionally, we observe stark differences in statisticalefficiency and consistency by varying the number of in-context examples andtask difficulty. We also measure each architecture's predisposition towardsin-context learning when presented with the option to memorize rather thanleverage in-context examples. Finally, and somewhat surprisingly, we find thatseveral attention alternatives are sometimes competitive with or betterin-context learners than transformers. However, no single architecturedemonstrates consistency across all tasks, with performance either plateauingor declining when confronted with a significantly larger number of in-contextexamples than those encountered during gradient-based training.</description><author>Ivan Lee, Nan Jiang, Taylor Berg-Kirkpatrick</author><pubDate>Tue, 02 Apr 2024 02:54:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08049v3</guid></item><item><title>Understanding In-Context Learning from Repetitions</title><link>http://arxiv.org/abs/2310.00297v3</link><description>This paper explores the elusive mechanism underpinning in-context learning inLarge Language Models (LLMs). Our work provides a novel perspective byexamining in-context learning via the lens of surface repetitions. Wequantitatively investigate the role of surface features in text generation, andempirically establish the existence of \emph{token co-occurrencereinforcement}, a principle that strengthens the relationship between twotokens based on their contextual co-occurrences. By investigating the dualimpacts of these features, our research illuminates the internal workings ofin-context learning and expounds on the reasons for its failures. This paperprovides an essential contribution to the understanding of in-context learningand its potential limitations, providing a fresh perspective on this excitingcapability.</description><author>Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang</author><pubDate>Wed, 21 Feb 2024 09:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00297v3</guid></item><item><title>The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis</title><link>http://arxiv.org/abs/2311.00237v2</link><description>Understanding in-context learning (ICL) capability that enables largelanguage models (LLMs) to excel in proficiency through demonstration examplesis of utmost importance. This importance stems not only from the betterutilization of this capability across various tasks, but also from theproactive identification and mitigation of potential risks, including concernsregarding truthfulness, bias, and toxicity, that may arise alongside thecapability. In this paper, we present a thorough survey on the interpretationand analysis of in-context learning. First, we provide a concise introductionto the background and definition of in-context learning. Then, we give anoverview of advancements from two perspectives: 1) a theoretical perspective,emphasizing studies on mechanistic interpretability and delving into themathematical foundations behind ICL; and 2) an empirical perspective,concerning studies that empirically analyze factors associated with ICL. Weconclude by highlighting the challenges encountered and suggesting potentialavenues for future research. We believe that our work establishes the basis forfurther exploration into the interpretation of in-context learning.Additionally, we have created a repository containing the resources referencedin our survey.</description><author>Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He</author><pubDate>Fri, 16 Feb 2024 00:55:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.00237v2</guid></item><item><title>Supervised Knowledge Makes Large Language Models Better In-context Learners</title><link>http://arxiv.org/abs/2312.15918v2</link><description>Large Language Models (LLMs) exhibit emerging in-context learning abilitiesthrough prompt engineering. The recent progress in large-scale generativemodels has further expanded their use in real-world language applications.However, the critical challenge of improving the generalizability andfactuality of LLMs in natural language understanding and question answeringremains under-explored. While previous in-context learning research has focusedon enhancing models to adhere to users' specific instructions and qualityexpectations, and to avoid undesired outputs, little to no work has exploredthe use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs'in-context learning during the inference stage. Our primary contribution is theestablishment of a simple yet effective framework that enhances the reliabilityof LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMsbenefit from discriminative models, and 3) minimizes hallucinations ingenerative tasks. Using our proposed plug-in method, enhanced versions of Llama2 and ChatGPT surpass their original versions regarding generalizability andfactuality. We offer a comprehensive suite of resources, including 16 curateddatasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks.The code and data are released at:https://github.com/YangLinyi/Supervised-Knowledge-Makes-Large-Language-Models-Better-In-context-Learners.Our empirical analysis sheds light on the advantages of incorporatingdiscriminative models into LLMs and highlights the potential of our methodologyin fostering more reliable LLMs.</description><author>Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, Yue Zhang</author><pubDate>Thu, 11 Apr 2024 07:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15918v2</guid></item><item><title>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning</title><link>http://arxiv.org/abs/2309.07915v3</link><description>Since the resurgence of deep learning, vision-language models (VLMs) enhancedby large language models (LLMs) have grown exponentially in popularity.However, while LLMs can utilize extensive background knowledge and taskinformation with in-context learning, most VLMs still struggle withunderstanding complex multi-modal prompts with multiple images, making VLMsless effective in downstream vision-language tasks. In this paper, we addressthe limitation above by 1) introducing vision-language Model with Multi-ModalIn-Context Learning(MMICL), a new approach to allow the VLM to deal withmulti-modal inputs efficiently; 2) proposing a novel context scheme to augmentthe in-context learning ability of the VLM; 3) constructing the Multi-modalIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability tounderstand complex multi-modal prompts. Our experiments confirm that MMICLachieves new state-of-the-art zero-shot performance on a wide range of generalvision-language tasks, especially for complex benchmarks, including MME andMMBench. Our analysis demonstrates that MMICL effectively tackles the challengeof complex multi-modal prompt understanding and emerges the impressive ICLability. Furthermore, we observe that MMICL successfully alleviates languagebias in VLMs, a common issue for VLMs that often leads to hallucination whenfaced with extensive textual context. Our code, dataset, dataset tool, andmodel are available at https://github.com/PKUnlp-icler/MIC</description><author>Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang</author><pubDate>Wed, 20 Mar 2024 17:17:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07915v3</guid></item><item><title>Understanding In-Context Learning with a Pelican Soup Framework</title><link>http://arxiv.org/abs/2402.10424v1</link><description>Many existing theoretical analyses of in-context learning for naturallanguage processing are based on latent variable models that leaves gapsbetween theory and practice. We aim to close these gaps by proposing atheoretical framework, the Pelican Soup Framework. In this framework, weintroduce (1) the notion of a common sense knowledge base, (2) a generalformalism for natural language classification tasks, and the notion of (3)meaning association. Under this framework, we can establish a$\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the numberof example-label pairs in the demonstration. Compared with previous works, ourbound reflects the effect of the choice of verbalizers and the effect ofinstruction tuning. An additional notion of \textit{atom concepts} makes ourframework possible to explain the generalization to tasks unseen in thelanguage model training data. Finally, we propose a toy setup, Calcutec, and adigit addition task that mimics types of distribution shifts a model needs toovercome to perform in-context learning. We also experiment with GPT2-Large onreal-world NLP tasks. Our empirical results demonstrate the efficacy of ourframework to explain in-context learning.</description><author>Ting-Rui Chiang, Dani Yogatama</author><pubDate>Fri, 16 Feb 2024 03:20:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10424v1</guid></item><item><title>C-ICL: Contrastive In-context Learning for Information Extraction</title><link>http://arxiv.org/abs/2402.11254v1</link><description>Recently, there has been increasing interest in exploring the capabilities ofadvanced large language models (LLMs) in the field of information extraction(IE), specifically focusing on tasks related to named entity recognition (NER)and relation extraction (RE). Although researchers are exploring the use offew-shot information extraction through in-context learning with LLMs, theytend to focus only on using correct or positive examples for demonstration,neglecting the potential value of incorporating incorrect or negative examplesinto the learning process. In this paper, we present c-ICL, a novel few-shottechnique that leverages both correct and incorrect sample constructions tocreate in-context learning demonstrations. This approach enhances the abilityof LLMs to extract entities and relations by utilizing prompts that incorporatenot only the positive samples but also the reasoning behind them. This methodallows for the identification and correction of potential interface errors.Specifically, our proposed method taps into the inherent contextual informationand valuable information in hard negative samples and the nearest positiveneighbors to the test and then applies the in-context learning demonstrationsbased on LLMs. Our experiments on various datasets indicate that c-ICLoutperforms previous few-shot in-context learning methods, deliveringsubstantial enhancements in performance across a broad spectrum of relatedtasks. These improvements are noteworthy, showcasing the versatility of ourapproach in miscellaneous scenarios.</description><author>Ying Mo, Jian Yang, Jiahao Liu, Shun Zhang, Jingang Wang, Zhoujun Li</author><pubDate>Sat, 17 Feb 2024 11:28:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11254v1</guid></item><item><title>Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning</title><link>http://arxiv.org/abs/2401.05949v4</link><description>In-context learning, a paradigm bridging the gap between pre-training andfine-tuning, has demonstrated high efficacy in several NLP tasks, especially infew-shot settings. Despite being widely applied, in-context learning isvulnerable to malicious attacks. In this work, we raise security concernsregarding this paradigm. Our studies demonstrate that an attacker canmanipulate the behavior of large language models by poisoning the demonstrationcontext, without the need for fine-tuning the model. Specifically, we design anew backdoor attack method, named ICLAttack, to target large language modelsbased on in-context learning. Our method encompasses two types of attacks:poisoning demonstration examples and poisoning demonstration prompts, which canmake models behave in alignment with predefined intentions. ICLAttack does notrequire additional fine-tuning to implant a backdoor, thus preserving themodel's generality. Furthermore, the poisoned examples are correctly labeled,enhancing the natural stealth of our attack method. Extensive experimentalresults across several language models, ranging in size from 1.3B to 180Bparameters, demonstrate the effectiveness of our attack method, exemplified bya high average attack success rate of 95.0% across the three datasets on OPTmodels.</description><author>Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen</author><pubDate>Fri, 16 Feb 2024 13:45:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05949v4</guid></item><item><title>RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models</title><link>http://arxiv.org/abs/2308.07922v2</link><description>In this paper, we investigate the in-context learning ability ofretrieval-augmented encoder-decoder language models. We first conduct acomprehensive analysis of existing models and identify their limitations inin-context learning, primarily due to a mismatch between pretraining andinference, as well as a restricted context length. To address these issues, wepropose RAVEN, a model that combines retrieval-augmented masked languagemodeling and prefix language modeling. We further introduce Fusion-in-ContextLearning to enhance the few-shot performance by enabling the model to leveragemore in-context examples without requiring additional training. Throughextensive experiments, we demonstrate that our simple yet effective designsignificantly improves performance, achieving results comparable to the mostadvanced language models in certain scenarios, despite having substantiallyfewer parameters. Our work underscores the potential of retrieval-augmentedencoder-decoder language models for in-context learning and encourages furtherresearch in this direction.</description><author>Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, Bryan Catanzaro</author><pubDate>Mon, 01 Apr 2024 07:32:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07922v2</guid></item><item><title>SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging</title><link>http://arxiv.org/abs/2403.16578v1</link><description>Medical image segmentation models adapting to new tasks in a training-freemanner through in-context learning is an exciting advancement. Universalsegmentation models aim to generalize across the diverse modality of medicalimages, yet their effectiveness often diminishes when applied toout-of-distribution (OOD) data modalities and tasks, requiring intricatefine-tuning of model for optimal performance. For addressing this challenge, weintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) forimage segmentation. Unlike existing methods, SegICL has the capability toemploy text-guided segmentation and conduct in-context learning with a smallset of image-mask pairs, eliminating the need for training the model fromscratch or fine-tuning for OOD tasks (including OOD modality and dataset).Extensive experimental validation of SegICL demonstrates a positive correlationbetween the number of prompt samples and segmentation performance on OODmodalities and tasks. This indicates that SegICL effectively address newsegmentation tasks based on contextual information. Additionally, SegICL alsoexhibits comparable segmentation performance to mainstream models on OOD andin-distribution tasks. Our code will be released soon.</description><author>Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang</author><pubDate>Mon, 25 Mar 2024 10:43:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16578v1</guid></item><item><title>SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging</title><link>http://arxiv.org/abs/2403.16578v2</link><description>Medical image segmentation models adapting to new tasks in a training-freemanner through in-context learning is an exciting advancement. Universalsegmentation models aim to generalize across the diverse modality of medicalimages, yet their effectiveness often diminishes when applied toout-of-distribution (OOD) data modalities and tasks, requiring intricatefine-tuning of model for optimal performance. For addressing this challenge, weintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) forimage segmentation. Unlike existing methods, SegICL has the capability toemploy text-guided segmentation and conduct in-context learning with a smallset of image-mask pairs, eliminating the need for training the model fromscratch or fine-tuning for OOD tasks (including OOD modality and dataset).Extensive experimental validation of SegICL demonstrates a positive correlationbetween the number of prompt samples and segmentation performance on OODmodalities and tasks. This indicates that SegICL effectively address newsegmentation tasks based on contextual information. Additionally, SegICL alsoexhibits comparable segmentation performance to mainstream models on OOD andin-distribution tasks. Our code will be released soon.</description><author>Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shiming Xiang</author><pubDate>Tue, 02 Apr 2024 10:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16578v2</guid></item><item><title>How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?</title><link>http://arxiv.org/abs/2404.12866v1</link><description>The increase in parameter size of multimodal large language models (MLLMs)introduces significant capabilities, particularly in-context learning, whereMLLMs enhance task performance without updating pre-trained parameters. Thiseffectiveness, however, hinges on the appropriate selection of in-contextexamples, a process that is currently biased towards visual data, overlookingtextual information. Furthermore, the area of supervised retrievers for MLLMs,crucial for optimal in-context example selection, continues to beuninvestigated. Our study offers an in-depth evaluation of the impact oftextual information on the unsupervised selection of in-context examples inmultimodal contexts, uncovering a notable sensitivity of retriever performanceto the employed modalities. Responding to this, we introduce a novel supervisedMLLM-retriever MSIER that employs a neural network to select examples thatenhance multimodal in-context learning efficiency. This approach is validatedthrough extensive testing across three distinct tasks, demonstrating themethod's effectiveness. Additionally, we investigate the influence ofmodalities on our supervised retrieval method's training and pinpoint factorscontributing to our model's success. This exploration paves the way for futureadvancements, highlighting the potential for refined in-context learning inMLLMs through the strategic use of multimodal data.</description><author>Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You</author><pubDate>Fri, 19 Apr 2024 14:05:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12866v1</guid></item><item><title>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</title><link>http://arxiv.org/abs/2310.02954v5</link><description>Recent advances in natural language processing, primarily propelled by LargeLanguage Models (LLMs), have showcased their remarkable capabilities groundedin in-context learning. A promising avenue for guiding LLMs in intricatereasoning tasks involves the utilization of intermediate reasoning steps withinthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge liesin the effective selection of exemplars for facilitating in-context learning.In this study, we introduce a framework that leverages Dual Queries andLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplarsfor in-context learning. Dual Queries first query LLM to obtain LLM-generatedknowledge such as CoT, then query the retriever to obtain the final exemplarsvia both question and the knowledge. Moreover, for the second query, LoReemploys dimensionality reduction techniques to refine exemplar selection,ensuring close alignment with the input question's knowledge. Through extensiveexperiments, we demonstrate that DQ-LoRe significantly outperforms priorstate-of-the-art methods in the automatic selection of exemplars for GPT-4,enhancing performance from 92.5% to 94.2%. Our comprehensive analysis furtherreveals that DQ-LoRe consistently outperforms retrieval-based approaches interms of both performance and adaptability, especially in scenarioscharacterized by distribution shifts. DQ-LoRe pushes the boundary of in-contextlearning and opens up new avenues for addressing complex reasoning challenges.Our code is released athttps://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe.</description><author>Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</author><pubDate>Sat, 02 Mar 2024 14:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02954v5</guid></item><item><title>HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation</title><link>http://arxiv.org/abs/2402.09390v1</link><description>With the widespread adoption of large language models (LLMs) in numerousapplications, the challenge of factuality and the propensity for hallucinationsraises significant concerns. To address this issue, particularly inretrieval-augmented in-context learning, we introduce the hierarchical graph ofthoughts (HGOT), a structured, multi-layered graph approach designed to enhancethe retrieval of pertinent passages during in-context learning. The frameworkutilizes the emergent planning capabilities of LLMs, employing thedivide-and-conquer strategy to break down complex queries into manageablesub-queries. It refines self-consistency majority voting for answer selection,which incorporates the recently proposed citation recall and precision metricsto assess the quality of thoughts, linking an answer's credibilityintrinsically to the thought's quality. This methodology introduces a weightedsystem in majority voting, prioritizing answers based on the citation qualityof their thoughts. Additionally, we propose a scoring mechanism for evaluatingretrieved passages, considering factors such as citation frequency and quality,self-consistency confidence, and the retrieval module's ranking. Experimentsreveal that HGOT outperforms other retrieval-augmented in-context learningmethods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, andRetrieve-then-Read on different datasets by as much as $7\%$, demonstrating itsefficacy in enhancing the factuality of LLMs.</description><author>Yihao Fang, Stephen W. Thomas, Xiaodan Zhu</author><pubDate>Wed, 14 Feb 2024 18:41:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09390v1</guid></item><item><title>Identifying Semantic Induction Heads to Understand In-Context Learning</title><link>http://arxiv.org/abs/2402.13055v1</link><description>Although large language models (LLMs) have demonstrated remarkableperformance, the lack of transparency in their inference logic raises concernsabout their trustworthiness. To gain a better understanding of LLMs, we conducta detailed analysis of the operations of attention heads and aim to betterunderstand the in-context learning of LLMs. Specifically, we investigatewhether attention heads encode two types of relationships between tokenspresent in natural languages: the syntactic dependency parsed from sentencesand the relation within knowledge graphs. We find that certain attention headsexhibit a pattern where, when attending to head tokens, they recall tail tokensand increase the output logits of those tail tokens. More crucially, theformulation of such semantic induction heads has a close correlation with theemergence of the in-context learning ability of language models. The study ofsemantic attention heads advances our understanding of the intricate operationsof attention heads in transformers, and further provides new insights into thein-context learning of LLMs.</description><author>Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Xipeng Qiu, Dahua Lin</author><pubDate>Tue, 20 Feb 2024 14:43:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13055v1</guid></item><item><title>Dual Operating Modes of In-Context Learning</title><link>http://arxiv.org/abs/2402.18819v1</link><description>In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,acquiring a new skill from in-context samples, and task retrieval, i.e.,locating and activating a relevant pretrained skill. Recent theoretical workinvestigates various mathematical models to analyze ICL, but existing modelsexplain only one operating mode at a time. We introduce a probabilistic model,with which one can explain the dual operating modes of ICL simultaneously.Focusing on in-context learning of linear functions, we extend existing modelsfor pretraining data by introducing multiple task groups and task-dependentinput distributions. We then analyze the behavior of the optimally pretrainedmodel under the squared loss, i.e., the MMSE estimator of the label givenin-context examples. Regarding pretraining task distribution as prior andin-context examples as the observation, we derive the closed-form expression ofthe task posterior distribution. With the closed-form expression, we obtain aquantitative understanding of the two operating modes of ICL. Furthermore, weshed light on an unexplained phenomenon observed in practice: under certainsettings, the ICL risk initially increases and then decreases with morein-context examples. Our model offers a plausible explanation for this "earlyascent" phenomenon: a limited number of in-context samples may lead to theretrieval of an incorrect skill, thereby increasing the risk, which willeventually diminish as task learning takes effect with more in-context samples.We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,where in-context examples are assigned random labels. Lastly, we validate ourfindings and predictions via experiments involving Transformers and largelanguage models.</description><author>Ziqian Lin, Kangwook Lee</author><pubDate>Thu, 29 Feb 2024 03:06:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18819v1</guid></item><item><title>SQL-Encoder: Improving NL2SQL In-Context Learning Through a Context-Aware Encoder</title><link>http://arxiv.org/abs/2403.16204v1</link><description>Detecting structural similarity between queries is essential for selectingexamples in in-context learning models. However, assessing structuralsimilarity based solely on the natural language expressions of queries, withoutconsidering SQL queries, presents a significant challenge. This paper exploresthe significance of this similarity metric and proposes a model for accuratelyestimating it. To achieve this, we leverage a dataset comprising 170k questionpairs, meticulously curated to train a similarity prediction model. Ourcomprehensive evaluation demonstrates that the proposed model adeptly capturesthe structural similarity between questions, as evidenced by improvements inKendall-Tau distance and precision@k metrics. Notably, our model outperformsstrong competitive embedding models from OpenAI and Cohere. Furthermore,compared to these competitive models, our proposed encoder enhances thedownstream performance of NL2SQL models in 1-shot in-context learning scenariosby 1-2\% for GPT-3.5-turbo, 4-8\% for CodeLlama-7B, and 2-3\% forCodeLlama-13B.</description><author>Mohammadreza Pourreza, Davood Rafiei, Yuxi Feng, Raymond Li, Zhenan Fan, Weiwei Zhang</author><pubDate>Sun, 24 Mar 2024 16:57:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16204v1</guid></item><item><title>Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning</title><link>http://arxiv.org/abs/2402.15734v1</link><description>Recent years have witnessed the promise of coupling machine learning methodsand physical domain-specific insight for solving scientific problems based onpartial differential equations (PDEs). However, being data-intensive, thesemethods still require a large amount of PDE data. This reintroduces the needfor expensive numerical PDE solutions, partially undermining the original goalof avoiding these expensive simulations. In this work, seeking data efficiency,we design unsupervised pretraining and in-context learning methods for PDEoperator learning. To reduce the need for training data with simulatedsolutions, we pretrain neural operators on unlabeled PDE data usingreconstruction-based proxy tasks. To improve out-of-distribution performance,we further assist neural operators in flexibly leveraging in-context learningmethods, without incurring extra training costs or designs. Extensive empiricalevaluations on a diverse set of PDEs demonstrate that our method is highlydata-efficient, more generalizable, and even outperforms conventionalvision-pretrained models.</description><author>Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, Michael W. Mahoney</author><pubDate>Sat, 24 Feb 2024 06:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15734v1</guid></item><item><title>Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model</title><link>http://arxiv.org/abs/2404.09045v1</link><description>Timely identification is essential for the efficient handling of mentalhealth illnesses such as depression. However, the current research fails toadequately address the prediction of mental health conditions from social mediadata in low-resource African languages like Swahili. This study introduces twodistinct approaches utilising model-agnostic meta-learning and leveraging largelanguage models (LLMs) to address this gap. Experiments are conducted on threedatasets translated to low-resource language and applied to four mental healthtasks, which include stress, depression, depression severity and suicidalideation prediction. we first apply a meta-learning model withself-supervision, which results in improved model initialisation for rapidadaptation and cross-lingual transfer. The results show that our meta-trainedmodel performs significantly better than standard fine-tuning methods,outperforming the baseline fine-tuning in macro F1 score with 18\% and 0.8\%over XLM-R and mBERT. In parallel, we use LLMs' in-context learningcapabilities to assess their performance accuracy across the Swahili mentalhealth prediction tasks by analysing different cross-lingual promptingapproaches. Our analysis showed that Swahili prompts performed better thancross-lingual prompts but less than English prompts. Our findings show thatin-context learning can be achieved through cross-lingual transfer throughcarefully crafted prompt templates with examples and instructions.</description><author>Zita Lifelo, Huansheng Ning, Sahraoui Dhelim</author><pubDate>Sat, 13 Apr 2024 18:11:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.09045v1</guid></item><item><title>The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains</title><link>http://arxiv.org/abs/2402.11004v1</link><description>Large language models have the ability to generate text that mimics patternsin their inputs. We introduce a simple Markov Chain sequence modeling task inorder to study how this in-context learning (ICL) capability emerges. In oursetting, each example is sampled from a Markov chain drawn from a priordistribution over Markov chains. Transformers trained on this task form\emph{statistical induction heads} which compute accurate next-tokenprobabilities given the bigram statistics of the context. During the course oftraining, models pass through multiple phases: after an initial stage in whichpredictions are uniform, they learn to sub-optimally predict using in-contextsingle-token statistics (unigrams); then, there is a rapid phase transition tothe correct in-context bigram solution. We conduct an empirical and theoreticalinvestigation of this multi-phase process, showing how successful learningresults from the interaction between the transformer's layers, and uncoveringevidence that the presence of the simpler unigram solution may delay formationof the final bigram solution. We examine how learning is affected by varyingthe prior distribution over Markov chains, and consider the generalization ofour in-context learning of Markov chains (ICL-MC) task to $n$-grams for $n &gt;2$.</description><author>Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, Nikolaos Tsilivis</author><pubDate>Fri, 16 Feb 2024 18:28:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11004v1</guid></item><item><title>Prompt Optimization via Adversarial In-Context Learning</title><link>http://arxiv.org/abs/2312.02614v2</link><description>We propose a new method, Adversarial In-Context Learning (adv-ICL), tooptimize prompt for in-context learning (ICL) by employing one LLM as agenerator, another as a discriminator, and a third as a prompt modifier. As intraditional adversarial learning, adv-ICL is implemented as a two-player gamebetween the generator and discriminator, where the generator tries to generaterealistic enough output to fool the discriminator. In each round, given aninput prefixed by task instructions and several exemplars, the generatorproduces an output. The discriminator is then tasked with classifying thegenerator input-output pair as model-generated or real data. Based on thediscriminator loss, the prompt modifier proposes possible edits to thegenerator and discriminator prompts, and the edits that most improve theadversarial loss are selected. We show that adv-ICL results in significantimprovements over state-of-the-art prompt optimization techniques for both openand closed-source models on 11 generation and classification tasks includingsummarization, arithmetic reasoning, machine translation, data-to-textgeneration, and the MMLU and big-bench hard benchmarks. In addition, becauseour method uses pre-trained models and updates only prompts rather than modelparameters, it is computationally efficient, easy to extend to any LLM andtask, and effective in low-resource settings.</description><author>Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi, Michael Qizhe Xie, Junxian He</author><pubDate>Wed, 28 Feb 2024 04:42:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02614v2</guid></item><item><title>Can Whisper perform speech-based in-context learning?</title><link>http://arxiv.org/abs/2309.07081v2</link><description>This paper investigates the in-context learning abilities of the Whisperautomatic speech recognition (ASR) models released by OpenAI. A novelspeech-based in-context learning (SICL) approach is proposed for test-timeadaptation, which can reduce the word error rates (WERs) with only a smallnumber of labelled speech samples without gradient descent. Language-leveladaptation experiments using Chinese dialects showed that when applying SICL toisolated word ASR, consistent and considerable relative WER reductions can beachieved using Whisper models of any size on two dialects, which is on average32.3%. A k-nearest-neighbours-based in-context example selection technique canbe applied to further improve the efficiency of SICL, which can increase theaverage relative WER reduction to 36.4%. The findings are verified usingspeaker adaptation or continuous speech recognition tasks, and both achievedconsiderable relative WER reductions. Detailed quantitative analyses are alsoprovided to shed light on SICL's adaptability to phonological variances anddialect-specific lexical nuances.</description><author>Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang</author><pubDate>Wed, 20 Mar 2024 04:04:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07081v2</guid></item><item><title>What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation</title><link>http://arxiv.org/abs/2404.07129v1</link><description>In-context learning is a powerful emergent ability in transformer models.Prior work in mechanistic interpretability has identified a circuit elementthat may be critical for in-context learning -- the induction head (IH), whichperforms a match-and-copy operation. During training of large transformers onnatural language data, IHs emerge around the same time as a notable phasechange in the loss. Despite the robust evidence for IHs and this interestingcoincidence with the phase change, relatively little is known about thediversity and emergence dynamics of IHs. Why is there more than one IH, and howare they dependent on each other? Why do IHs appear all of a sudden, and whatare the subcircuits that enable them to emerge? We answer these questions bystudying IH emergence dynamics in a controlled setting by training on syntheticdata. In doing so, we develop and share a novel optogenetics-inspired causalframework for modifying activations throughout training. Using this framework,we delineate the diverse and additive nature of IHs. By clamping subsets ofactivations throughout training, we then identify three underlying subcircuitsthat interact to drive IH formation, yielding the phase change. Furthermore,these subcircuits shed light on data-dependent properties of formation, such asphase change timing, already showing the promise of this more in-depthunderstanding of subcircuits that need to "go right" for an induction head.</description><author>Aaditya K. Singh, Ted Moskovitz, Felix Hill, Stephanie C. Y. Chan, Andrew M. Saxe</author><pubDate>Wed, 10 Apr 2024 17:07:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07129v1</guid></item><item><title>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</title><link>http://arxiv.org/abs/2403.06914v2</link><description>Large Language models (LLMs) have demonstrated impressive in-context learning(ICL) capabilities, where a LLM makes predictions for a given test inputtogether with a few input-output pairs (demonstrations). Nevertheless, theinclusion of demonstrations leads to a quadratic increase in the computationaloverhead of the self-attention mechanism. Existing solutions attempt to distilllengthy demonstrations into compact vectors. However, they often requiretask-specific retraining or compromise LLM's in-context learning performance.To mitigate these challenges, we present Meta dEmonstratioN Distillation(MEND), where a language model learns to distill any lengthy demonstrationsinto vectors without retraining for a new downstream task. We exploit theknowledge distillation to enhance alignment between MEND and LLM, achievingboth efficiency and effectiveness simultaneously. MEND is endowed with themeta-knowledge of distilling demonstrations through a two-stage trainingprocess, which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL task partitions usingdecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It notonly matches but often outperforms the Vanilla ICL as well as otherstate-of-the-art distillation models, while significantly reducing thecomputational demands. This innovation promises enhanced scalability andefficiency for the practical deployment of large language models</description><author>Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo</author><pubDate>Tue, 12 Mar 2024 16:52:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06914v2</guid></item><item><title>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</title><link>http://arxiv.org/abs/2403.06914v1</link><description>Large Language models (LLMs) have demonstrated impressive in-context learning(ICL) capabilities, where a LLM makes predictions for a given test inputtogether with a few input-output pairs (demonstrations). Nevertheless, theinclusion of demonstrations leads to a quadratic increase in the computationaloverhead of the self-attention mechanism. Existing solutions attempt to distilllengthy demonstrations into compact vectors. However, they often requiretask-specific retraining or compromise LLM's in-context learning performance.To mitigate these challenges, we present Meta dEmonstratioN Distillation(MEND), where a language model learns to distill any lengthy demonstrationsinto vectors without retraining for a new downstream task. We exploit theknowledge distillation to enhance alignment between MEND and LLM, achievingboth efficiency and effectiveness simultaneously. MEND is endowed with themeta-knowledge of distilling demonstrations through a two-stage trainingprocess, which includes meta-distillation pretraining and fine-tuning.Comprehensive evaluations across seven diverse ICL task partitions usingdecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It notonly matches but often outperforms the Vanilla ICL as well as otherstate-of-the-art distillation models, while significantly reducing thecomputational demands. This innovation promises enhanced scalability andefficiency for the practical deployment of large language models</description><author>Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo</author><pubDate>Mon, 11 Mar 2024 18:03:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06914v1</guid></item><item><title>Point-In-Context: Understanding Point Cloud via In-Context Learning</title><link>http://arxiv.org/abs/2404.12352v1</link><description>With the emergence of large-scale models trained on diverse datasets,in-context learning has emerged as a promising paradigm for multitasking,notably in natural language processing and image processing. However, itsapplication in 3D point cloud tasks remains largely unexplored. In this work,we introduce Point-In-Context (PIC), a novel framework for 3D point cloudunderstanding via in-context learning. We address the technical challenge ofeffectively extending masked point modeling to 3D point clouds by introducing aJoint Sampling module and proposing a vanilla version of PIC calledPoint-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist modelfor various 3D point cloud tasks, with inputs and outputs modeled ascoordinates. In this paradigm, the challenging segmentation task is achieved byassigning label points with XYZ coordinates for each category; the finalprediction is then chosen based on the label point closest to the predictions.To break the limitation by the fixed label-coordinate assignment, which haspoor generalization upon novel classes, we propose two novel trainingstrategies, In-Context Labeling and In-Context Enhancing, forming an extendedversion of PIC named Point-In-Context-Segmenter (PIC-S), targeting improvingdynamic context labeling and model training. By utilizing dynamic in-contextlabels and extra in-context pairs, PIC-S achieves enhanced performance andgeneralization capability in and across part segmentation datasets. PIC is ageneral framework so that other tasks or datasets can be seamlessly introducedinto our PIC through a unified data format. We conduct extensive experiments tovalidate the versatility and adaptability of our proposed methods in handling awide range of tasks and segmenting multi-datasets. Our PIC-S is capable ofgeneralizing unseen datasets and performing novel part segmentation bycustomizing prompts.</description><author>Mengyuan Liu, Zhongbin Fang, Xia Li, Joachim M. Buhmann, Xiangtai Li, Chen Change Loy</author><pubDate>Thu, 18 Apr 2024 18:32:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12352v1</guid></item><item><title>VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning</title><link>http://arxiv.org/abs/2403.13164v1</link><description>Large language models (LLMs) famously exhibit emergent in-context learning(ICL) -- the ability to rapidly adapt to new tasks using few-shot examplesprovided as a prompt, without updating the model's weights. Built on top ofLLMs, vision large language models (VLLMs) have advanced significantly in areassuch as recognition, reasoning, and grounding. However, investigations into\emph{multimodal ICL} have predominantly focused on few-shot visual questionanswering (VQA), and image captioning, which we will show neither exploit thestrengths of ICL, nor test its limitations. The broader capabilities andlimitations of multimodal ICL remain under-explored. In this study, weintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-contextlearning, encompassing a broad spectrum of tasks that involve both images andtext as inputs and outputs, and different types of challenges, from {perceptionto reasoning and long context length}. We evaluate the abilities ofstate-of-the-art VLLMs against this benchmark suite, revealing their diversestrengths and weaknesses, and showing that even the most advanced models, suchas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,and the associated strengths and limitations of existing models, we hope thatour dataset will inspire future work on enhancing the in-context learningcapabilities of VLLMs, as well as inspire new applications that leverage VLLMICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.</description><author>Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</author><pubDate>Tue, 19 Mar 2024 22:31:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13164v1</guid></item><item><title>Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks</title><link>http://arxiv.org/abs/2311.01949v2</link><description>In-context learning (ICL) ability has emerged with the increasing scale oflarge language models (LLMs), enabling them to learn input-label mappings fromdemonstrations and perform well on downstream tasks. However, under thestandard ICL setting, LLMs may sometimes neglect query-related information indemonstrations, leading to incorrect predictions. To address this limitation,we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) toexplore the power of ICL in open-domain question answering, an important formin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extractquery-related knowledge from demonstrations, then concatenates the knowledge toprompt LLMs in a more explicit way. Furthermore, we track the source of thisknowledge to identify specific examples, and introduce a Hint-related ExampleRetriever (HER) to select informative examples for enhanced demonstrations. Weevaluate HICL with HER on 3 open-domain QA benchmarks, and observe averageperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EMscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.</description><author>Yifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, Yujiu Yang</author><pubDate>Thu, 18 Apr 2024 16:08:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01949v2</guid></item><item><title>Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction</title><link>http://arxiv.org/abs/2402.13741v1</link><description>The in-context learning (ICL) for relational triple extraction (RTE) hasachieved promising performance, but still encounters two key challenges: (1)how to design effective prompts and (2) how to select proper demonstrations.Existing methods, however, fail to address these challenges appropriately. Onthe one hand, they usually recast RTE task to text-to-text prompting formats,which is unnatural and results in a mismatch between the output format at thepre-training time and the inference time for large language models (LLMs). Onthe other hand, they only utilize surface natural language features and lackconsideration of triple semantics in sample selection. These issues areblocking improved performance in ICL for RTE, thus we aim to tackle promptdesigning and sample selection challenges simultaneously. To this end, wedevise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE taskinto a table generation task to incorporate explicit structured informationinto ICL, facilitating conversion of outputs to RTE structures. Then we proposeinstructive in-context learning (I$^2$CL) which only selects and annotates afew samples considering internal triple semantics in massive unlabeled samples.</description><author>Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji, Jiajun Liu, Ziyu Shang, Qiqing Luo</author><pubDate>Wed, 21 Feb 2024 12:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13741v1</guid></item><item><title>Visual In-Context Learning for Large Vision-Language Models</title><link>http://arxiv.org/abs/2402.11574v1</link><description>In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning(ICL) remains limited by challenges in cross-modal interactions andrepresentation disparities. To overcome these challenges, we introduce a novelVisual In-Context Learning (VICL) method comprising Visual DemonstrationRetrieval, Intent-Oriented Image Summarization, and Intent-OrientedDemonstration Composition. Our approach retrieves images via ''Retrieval &amp;Rerank'' paradigm, summarises images with task intent and task-specific visualparsing, and composes language-based demonstrations that reduce token count andalleviate cross-modal interaction problem. Experimental evaluations on fivevisual reasoning datasets demonstrate the effectiveness of our method.Moreover, our extensive experiments leverage information flow analysis toelucidate the effectiveness of our method, and investigate the impact of lengthand position of demonstrations for LVLM. The use of in-context unlearningfurther shows promise in resetting specific model knowledge without retraining.</description><author>Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen</author><pubDate>Sun, 18 Feb 2024 12:43:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11574v1</guid></item><item><title>Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models</title><link>http://arxiv.org/abs/2310.00313v4</link><description>Large language models (LLMs) exhibit remarkable performance improvementthrough in-context learning (ICL) by leveraging task-specific examples in theinput. However, the mechanisms behind this improvement remain elusive. In thiswork, we investigate how LLM embeddings and attention representations changefollowing in-context-learning, and how these changes mediate improvement inbehavior. We employ neuroscience-inspired techniques such as representationalsimilarity analysis (RSA) and propose novel methods for parameterized probingand measuring ratio of attention to relevant vs. irrelevant information inLlama-2 70B and Vicuna 13B. We designed two tasks with a priori relationshipsamong their conditions: linear regression and reading comprehension. We formedhypotheses about expected similarities in task representations and measuredhypothesis alignment of LLM representations before and after ICL as well aschanges in attention. Our analyses revealed a meaningful correlation betweenimprovements in behavior after ICL and changes in both embeddings and attentionweights across LLM layers. This empirical framework empowers a nuancedunderstanding of how latent representations shape LLM behavior, offeringvaluable tools and insights for future research and practical applications.</description><author>Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Raphal Millire, Ida Momennejad</author><pubDate>Wed, 21 Feb 2024 19:51:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.00313v4</guid></item><item><title>An Empirical Study of In-context Learning in LLMs for Machine Translation</title><link>http://arxiv.org/abs/2401.12097v2</link><description>Recent interest has surged in employing Large Language Models (LLMs) formachine translation (MT) via in-context learning (ICL) (Vilar et al., 2023).Most prior studies primarily focus on optimizing translation quality, withlimited attention to understanding the specific aspects of ICL that influencethe said quality. To this end, we perform the first of its kind, exhaustivestudy of in-context learning for machine translation. We first establish thatICL is primarily example-driven and not instruction-driven. Following this, weconduct an extensive exploration of various aspects of the examples tounderstand their influence on downstream performance. Our analysis includesfactors such as quality and quantity of demonstrations, spatial proximity, andsource versus target originality. Further, we also investigate challengingscenarios involving indirectness and misalignment of examples to understand thelimits of ICL. While we establish the significance of the quality of the targetdistribution over the source distribution of demonstrations, we further observethat perturbations sometimes act as regularizers, resulting in performanceimprovements. Surprisingly, ICL does not necessitate examples from the sametask, and a related task with the same target distribution proves sufficient.We hope that our study acts as a guiding resource for considerations inutilizing ICL for MT.</description><author>Pranjal A. Chitale, Jay Gala, Raj Dabre</author><pubDate>Sat, 17 Feb 2024 07:08:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.12097v2</guid></item><item><title>FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis</title><link>http://arxiv.org/abs/2403.01063v1</link><description>Multi-domain aspect-based sentiment analysis (ABSA) seeks to capturefine-grained sentiment across diverse domains. While existing research narrowlyfocuses on single-domain applications constrained by methodological limitationsand data scarcity, the reality is that sentiment naturally traverses multipledomains. Although large language models (LLMs) offer a promising solution forABSA, it is difficult to integrate effectively with established techniques,including graph-based models and linguistics, because modifying their internalarchitecture is not easy. To alleviate this problem, we propose a novelframework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). Thecore insight of FaiMA is to utilize in-context learning (ICL) as afeature-aware mechanism that facilitates adaptive learning in multi-domain ABSAtasks. Specifically, we employ a multi-head graph attention network as a textencoder optimized by heuristic rules for linguistic, domain, and sentimentfeatures. Through contrastive learning, we optimize sentence representations byfocusing on these diverse features. Additionally, we construct an efficientindexing mechanism, allowing FaiMA to stably retrieve highly relevant examplesacross multiple dimensions for any given input. To evaluate the efficacy ofFaiMA, we build the first multi-domain ABSA benchmark dataset. Extensiveexperimental results demonstrate that FaiMA achieves significant performanceimprovements in multiple domains compared to baselines, increasing F1 by 2.07%on average. Source code and data sets are anonymously available athttps://github.com/SupritYoung/FaiMA.</description><author>Songhua Yang, Xinke Jiang, Hanjie Zhao, Wenxuan Zeng, Hongde Liu, Yuxiang Jia</author><pubDate>Sat, 02 Mar 2024 02:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01063v1</guid></item><item><title>Self-Augmented In-Context Learning for Unsupervised Word Translation</title><link>http://arxiv.org/abs/2402.10024v1</link><description>Recent work has shown that, while large language models (LLMs) demonstratestrong word translation or bilingual lexicon induction (BLI) capabilities infew-shot setups, they still cannot match the performance of 'traditional'mapping-based approaches in the unsupervised scenario where no seed translationpairs are available, especially for lower-resource languages. To address thischallenge with LLMs, we propose self-augmented in-context learning (SAIL) forunsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces aset of high-confidence word translation pairs for in-context learning (ICL)from an LLM, which it then reapplies to the same LLM in the ICL fashion. Ourmethod shows substantial gains over zero-shot prompting of LLMs on twoestablished BLI benchmarks spanning a wide range of language pairs, alsooutperforming mapping-based baselines across the board. In addition toachieving state-of-the-art unsupervised BLI performance, we also conductcomprehensive analyses on SAIL and discuss its limitations.</description><author>Yaoyiran Li, Anna Korhonen, Ivan Vuli</author><pubDate>Thu, 15 Feb 2024 15:43:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10024v1</guid></item><item><title>OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples</title><link>http://arxiv.org/abs/2307.11729v3</link><description>Large Language Models (LLMs) have achieved human-level fluency in textgeneration, making it difficult to distinguish between human-written andLLM-generated texts. This poses a growing risk of misuse of LLMs and demandsthe development of detectors to identify LLM-generated texts. However, existingdetectors lack robustness against attacks: they degrade detection accuracy bysimply paraphrasing LLM-generated texts. Furthermore, a malicious user mightattempt to deliberately evade the detectors based on detection results, butthis has not been assumed in previous studies. In this paper, we proposeOUTFOX, a framework that improves the robustness of LLM-generated-textdetectors by allowing both the detector and the attacker to consider eachother's output. In this framework, the attacker uses the detector's predictionlabels as examples for in-context learning and adversarially generates essaysthat are harder to detect, while the detector uses the adversarially generatedessays as examples for in-context learning to learn to detect essays from astrong attacker. Experiments in the domain of student essays show that theproposed detector improves the detection performance on the attacker-generatedtexts by up to +41.3 points F1-score. Furthermore, the proposed detector showsa state-of-the-art detection performance: up to 96.9 points F1-score, beatingexisting detectors on non-attacked texts. Finally, the proposed attackerdrastically degrades the performance of detectors by up to -57.0 pointsF1-score, massively outperforming the baseline paraphrasing method for evadingdetection.</description><author>Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki</author><pubDate>Sun, 18 Feb 2024 12:25:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11729v3</guid></item><item><title>GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification</title><link>http://arxiv.org/abs/2404.03052v1</link><description>Harmful and offensive communication or content is detrimental to socialbonding and the mental state of users on social media platforms. Textdetoxification is a crucial task in natural language processing (NLP), wherethe goal is removing profanity and toxicity from text while preserving itscontent. Supervised and unsupervised learning are common approaches fordesigning text detoxification solutions. However, these methods necessitatefine-tuning, leading to computational overhead. In this paper, we proposeGPT-DETOX as a framework for prompt-based in-context learning for textdetoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot promptingtechniques for detoxifying input sentences. To generate few-shot prompts, wepropose two methods: word-matching example selection (WMES) andcontext-matching example selection (CMES). We additionally take into accountensemble in-context learning (EICL) where the ensemble is shaped by baseprompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIAas benchmark detoxification datasets. Our experimental results show that thezero-shot solution achieves promising performance, while our best few-shotsetting outperforms the state-of-the-art models on ParaDetox and showscomparable results on APPDIA. Our EICL solutions obtain the greatestperformance, adding at least 10% improvement, against both datasets.</description><author>Ali Pesaranghader, Nikhil Verma, Manasa Bharadwaj</author><pubDate>Wed, 03 Apr 2024 21:35:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03052v1</guid></item><item><title>EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning</title><link>http://arxiv.org/abs/2309.10687v3</link><description>Language models are achieving impressive performance on various tasks byaggressively adopting inference-time prompting techniques, such as zero-shotand few-shot prompting. In this work, we introduce EchoPrompt, a simple yeteffective approach that prompts the model to rephrase its queries beforeanswering them. EchoPrompt is adapted for both zero-shot and few-shotin-context learning with standard and chain-of-thought prompting. Experimentalresults show that EchoPrompt yields substantial improvements across all thesesettings for four families of causal language models. These improvements areobserved across various numerical reasoning (e.g. GSM8K, SVAMP), readingcomprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. Onaverage, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002by 5% in numerical tasks and 13% in reading comprehension tasks. We investigatethe factors contributing to EchoPrompt's effectiveness through ablationstudies, which reveal that both the original query and the model-generatedrephrased version are instrumental in its performance gains. Our empiricalresults indicate that EchoPrompt is an effective technique that enhancesin-context learning performance. We recommend incorporating EchoPrompt intovarious baseline prompting strategies to achieve performance boosts.</description><author>Rajasekhar Reddy Mekala, Yasaman Razeghi, Sameer Singh</author><pubDate>Tue, 20 Feb 2024 20:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.10687v3</guid></item><item><title>Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning</title><link>http://arxiv.org/abs/2404.07546v1</link><description>In-context Learning (ICL) has emerged as a powerful capability alongside thedevelopment of scaled-up large language models (LLMs). By instructing LLMsusing few-shot demonstrative examples, ICL enables them to perform a wide rangeof tasks without updating millions of parameters. However, the precisecontributions of demonstrations towards improving end-task performance have notbeen thoroughly investigated in recent analytical studies. In this paper, weempirically decompose the overall performance of ICL into three dimensions,label space, format, and discrimination, and we evaluate four general-purposeLLMs across a diverse range of tasks. Counter-intuitively, we find that thedemonstrations have a marginal impact on provoking discriminative knowledge oflanguage models. However, ICL exhibits significant efficacy in regulating thelabel space and format which helps LLMs to respond in desired label words. Wethen demonstrate this ability functions similar to detailed instructions forLLMs to follow. We additionally provide an in-depth analysis of the mechanismof retrieval helping with ICL and find that retrieving the most semanticallysimilar examples notably boosts model's discriminative capability.</description><author>Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan</author><pubDate>Thu, 11 Apr 2024 09:20:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07546v1</guid></item><item><title>NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning</title><link>http://arxiv.org/abs/2402.05515v2</link><description>In-Context Learning (ICL) is suffering from unsatisfactory performance andunder-calibration due to high prior bias and unfaithful confidence. Someprevious works fine-tuned language models for better ICL performance withenormous datasets and computing costs. In this paper, we propose NoisyICL,simply perturbing the model parameters by random noises to strive for betterperformance and calibration. Our experiments on two models and 12 downstreamdatasets show that NoisyICL can help ICL produce more accurate predictions. Ourfurther analysis indicates that NoisyICL enables the model to provide more fairpredictions, and also with more faithful confidence. Therefore, we believe thatNoisyICL is an effective calibration of ICL. Our experimental code is uploadedto Github.</description><author>Yufeng Zhao, Yoshihiro Sakai, Naoya Inoue</author><pubDate>Thu, 15 Feb 2024 15:25:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05515v2</guid></item><item><title>In-Context Learning Learns Label Relationships but Is Not Conventional Learning</title><link>http://arxiv.org/abs/2307.12375v4</link><description>The predictions of Large Language Models (LLMs) on downstream tasks oftenimprove significantly when including examples of the input--label relationshipin the context. However, there is currently no consensus about how thisin-context learning (ICL) ability of LLMs works. For example, while Xie et al.(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)argue ICL does not even learn label relationships from in-context examples. Inthis paper, we provide novel insights into how ICL leverages label information,revealing both capabilities and limitations. To ensure we obtain acomprehensive picture of ICL behavior, we study probabilistic aspects of ICLpredictions and thoroughly examine the dynamics of ICL as more examples areprovided. Our experiments show that ICL predictions almost always depend onin-context labels and that ICL can learn truly novel tasks in-context. However,we also find that ICL struggles to fully overcome prediction preferencesacquired from pre-training data and, further, that ICL does not consider allin-context information equally.</description><author>Jannik Kossen, Yarin Gal, Tom Rainforth</author><pubDate>Wed, 13 Mar 2024 16:00:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12375v4</guid></item><item><title>ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification</title><link>http://arxiv.org/abs/2311.09649v2</link><description>This paper focuses on the task of Extreme Multi-Label Classification (XMC)whose goal is to predict multiple labels for each instance from an extremelylarge label space. While existing research has primarily focused on fullysupervised XMC, real-world scenarios often lack supervision signals,highlighting the importance of zero-shot settings. Given the large label space,utilizing in-context learning approaches is not trivial. We address this issueby introducing In-Context Extreme Multilabel Learning (ICXML), a two-stageframework that cuts down the search space by generating a set of candidatelabels through incontext learning and then reranks them. Extensive experimentssuggest that ICXML advances the state of the art on two diverse publicbenchmarks.</description><author>Yaxin Zhu, Hamed Zamani</author><pubDate>Mon, 15 Apr 2024 14:16:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09649v2</guid></item><item><title>Stronger Random Baselines for In-Context Learning</title><link>http://arxiv.org/abs/2404.13020v1</link><description>Evaluating the in-context learning classification performance of languagemodels poses challenges due to small dataset sizes, extensive prompt-selectionusing the validation set, and intentionally difficult tasks that lead tonear-random performance. The standard random baseline -- the expected accuracyof guessing labels uniformly at random -- is stable when the evaluation set isused only once or when the dataset is large. We account for the common practiceof validation set reuse and existing small datasets with a stronger randombaseline: the expected maximum accuracy across multiple random classifiers.When choosing the best prompt demonstrations across six quantized languagemodels applied to 16 BIG-bench Lite tasks, more than 20\% of the few-shotresults that exceed the standard baseline do not exceed this stronger randombaseline. When held-out test sets are available, this stronger baseline is alsoa better predictor of held-out performance than the standard baseline, avoidingunnecessary test set evaluations. This maximum random baseline provides aneasily calculated drop-in replacement for the standard baseline.</description><author>Gregory Yauney, David Mimno</author><pubDate>Fri, 19 Apr 2024 18:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13020v1</guid></item><item><title>Contextrast: Contextual Contrastive Learning for Semantic Segmentation</title><link>http://arxiv.org/abs/2404.10633v1</link><description>Despite great improvements in semantic segmentation, challenges persistbecause of the lack of local/global contexts and the relationship between them.In this paper, we propose Contextrast, a contrastive learning-based semanticsegmentation method that allows to capture local/global contexts and comprehendtheir relationships. Our proposed method comprises two parts: a) contextualcontrastive learning (CCL) and b) boundary-aware negative (BANE) sampling.Contextual contrastive learning obtains local/global context from multi-scalefeature aggregation and inter/intra-relationship of features for betterdiscrimination capabilities. Meanwhile, BANE sampling selects embeddingfeatures along the boundaries of incorrectly predicted regions to employ themas harder negative samples on our contrastive learning, resolving segmentationissues along the boundary region by exploiting fine-grained details. Wedemonstrate that our Contextrast substantially enhances the performance ofsemantic segmentation networks, outperforming state-of-the-art contrastivelearning approaches on diverse public datasets, e.g. Cityscapes, CamVid,PASCAL-C, COCO-Stuff, and ADE20K, without an increase in computational costduring inference.</description><author>Changki Sung, Wanhee Kim, Jungho An, Wooju Lee, Hyungtae Lim, Hyun Myung</author><pubDate>Tue, 16 Apr 2024 16:04:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10633v1</guid></item><item><title>Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning</title><link>http://arxiv.org/abs/2309.14771v2</link><description>Large language models (LLMs) enable in-context learning (ICL) by conditioningon a few labeled training examples as a text-based prompt, eliminating the needfor parameter updates and achieving competitive performance. In this paper, wedemonstrate that factual knowledge is imperative for the performance of ICL inthree core facets: the inherent knowledge learned in LLMs, the factualknowledge derived from the selected in-context examples, and the knowledgebiases in LLMs for output generation. To unleash the power of LLMs in few-shotlearning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT)framework to further improve the performance of ICL: 1) injecting knowledgeinto LLMs during continual self-supervised pre-training, 2) judiciouslyselecting the examples for ICL with high knowledge relevance, and 3)calibrating the prediction results based on prior knowledge. We evaluate theproposed approaches on autoregressive models (e.g., GPT-style LLMs) overmultiple text classification and question-answering tasks. Experimental resultsdemonstrate that KICT substantially outperforms strong baselines and improvesby more than 13% and 7% on text classification and question-answering tasks,respectively.</description><author>Jianing Wang, Chengyu Wang, Chuanqi Tan, Jun Huang, Ming Gao</author><pubDate>Sun, 31 Mar 2024 14:55:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14771v2</guid></item><item><title>RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning</title><link>http://arxiv.org/abs/2305.14502v2</link><description>Recent developments in large pre-trained language models have enabledunprecedented performance on a variety of downstream tasks. Achieving bestperformance with these models often leverages in-context learning, where amodel performs a (possibly new) task given one or more examples. However,recent work has shown that the choice of examples can have a large impact ontask performance and that finding an optimal set of examples is non-trivial.While there are many existing methods for selecting in-context examples, theygenerally score examples independently, ignoring the dependency between themand the order in which they are provided to the model. In this work, we proposeRetrieval for In-Context Learning (RetICL), a learnable method for modeling andoptimally selecting examples sequentially for in-context learning. We frame theproblem of sequential example selection as a Markov decision process and trainan example retriever using reinforcement learning. We evaluate RetICL on mathword problem solving and scientific question answering tasks and show that itconsistently outperforms or matches heuristic and learnable baselines. We alsouse case studies to show that RetICL implicitly learns representations ofproblem solving strategies.</description><author>Alexander Scarlatos, Andrew Lan</author><pubDate>Tue, 16 Apr 2024 18:25:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14502v2</guid></item><item><title>Many-Shot In-Context Learning</title><link>http://arxiv.org/abs/2404.11018v1</link><description>Large language models (LLMs) excel at few-shot in-context learning (ICL) --learning from a few examples provided in context at inference, without anyweight updates. Newly expanded context windows allow us to investigate ICL withhundreds or thousands of examples -- the many-shot regime. Going from few-shotto many-shot, we observe significant performance gains across a wide variety ofgenerative and discriminative tasks. While promising, many-shot ICL can bebottlenecked by the available amount of human-generated examples. To mitigatethis limitation, we explore two new settings: Reinforced and Unsupervised ICL.Reinforced ICL uses model-generated chain-of-thought rationales in place ofhuman examples. Unsupervised ICL removes rationales from the prompt altogether,and prompts the model only with domain-specific questions. We find that bothReinforced and Unsupervised ICL can be quite effective in the many-shot regime,particularly on complex reasoning tasks. Finally, we demonstrate that, unlikefew-shot learning, many-shot learning is effective at overriding pretrainingbiases and can learn high-dimensional functions with numerical inputs. Ouranalysis also reveals the limitations of next-token prediction loss as anindicator of downstream ICL performance.</description><author>Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, Hugo Larochelle</author><pubDate>Wed, 17 Apr 2024 03:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11018v1</guid></item><item><title>Leveraging Code to Improve In-context Learning for Semantic Parsing</title><link>http://arxiv.org/abs/2311.09519v2</link><description>In-context learning (ICL) is an appealing approach for semantic parsing dueto its few-shot nature and improved generalization. However, learning to parseto rare domain-specific languages (DSLs) from just a few demonstrations ischallenging, limiting the performance of even the most capable LLMs. In thiswork, we improve the effectiveness of ICL for semantic parsing by (1) usinggeneral-purpose programming languages such as Python instead of DSLs, and (2)augmenting prompts with a structured domain description that includes, e.g.,the available classes and functions. We show that both these changessignificantly improve accuracy across three popular datasets. Combined, theylead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositionalsplit), nearly closing the performance gap between easier i.i.d.\ and hardercompositional splits when used with a strong model, and reducing the need for alarge number of demonstrations. We find that the resemblance of the targetparse language to general-purpose code is a more important factor than thelanguage's popularity in pre-training corpora. Our findings provide an improvedmethodology for building semantic parsers in the modern context of ICL withLLMs.</description><author>Ben Bogin, Shivanshu Gupta, Peter Clark, Ashish Sabharwal</author><pubDate>Wed, 27 Mar 2024 22:52:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09519v2</guid></item><item><title>In-context Exploration-Exploitation for Reinforcement Learning</title><link>http://arxiv.org/abs/2403.06826v1</link><description>In-context learning is a promising approach for online policy learning ofoffline reinforcement learning (RL) methods, which can be achieved at inferencetime without gradient optimization. However, this method is hindered bysignificant computational costs resulting from the gathering of large trainingtrajectory sets and the need to train large Transformer models. We address thischallenge by introducing an In-context Exploration-Exploitation (ICEE)algorithm, designed to optimize the efficiency of in-context policy learning.Unlike existing models, ICEE performs an exploration-exploitation trade-off atinference time within a Transformer model, without the need for explicitBayesian inference. Consequently, ICEE can solve Bayesian optimization problemsas efficiently as Gaussian process biased methods do, but in significantly lesstime. Through experiments in grid world environments, we demonstrate that ICEEcan learn to solve new RL tasks using only tens of episodes, marking asubstantial improvement over the hundreds of episodes needed by the previousin-context learning method.</description><author>Zhenwen Dai, Federico Tomasi, Sina Ghiassian</author><pubDate>Mon, 11 Mar 2024 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.06826v1</guid></item><item><title>Rectifying Demonstration Shortcut in In-Context Learning</title><link>http://arxiv.org/abs/2403.09488v1</link><description>Large language models (LLMs) are able to solve various tasks with only a fewdemonstrations utilizing their in-context learning (ICL) abilities. However,LLMs often rely on their pre-trained semantic priors of demonstrations ratherthan on the input-label relationships to proceed with ICL prediction. In thiswork, we term this phenomenon as the `Demonstration Shortcut'. While previousworks have primarily focused on improving ICL prediction results for predefinedtasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLMto effectively learn new input-label relationships from demonstrations. Toachieve this, we introduce In-Context Calibration, a demonstration-awarecalibration method. We evaluate the effectiveness of the proposed method in twosettings: (1) the Original ICL Task using the standard label space and (2) theTask Learning setting, where the label space is replaced with semanticallyunrelated tokens. In both settings, In-Context Calibration demonstratessubstantial improvements, with results generalized across three LLM families(OPT, GPT, and Llama2) under various configurations.</description><author>Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu</author><pubDate>Thu, 14 Mar 2024 16:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09488v1</guid></item><item><title>Rectifying Demonstration Shortcut in In-Context Learning</title><link>http://arxiv.org/abs/2403.09488v2</link><description>Large language models (LLMs) are able to solve various tasks with only a fewdemonstrations utilizing their in-context learning (ICL) abilities. However,LLMs often rely on their pre-trained semantic priors of demonstrations ratherthan on the input-label relationships to proceed with ICL prediction. In thiswork, we term this phenomenon as the 'Demonstration Shortcut'. While previousworks have primarily focused on improving ICL prediction results for predefinedtasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLMto effectively learn new input-label relationships from demonstrations. Toachieve this, we introduce In-Context Calibration, a demonstration-awarecalibration method. We evaluate the effectiveness of the proposed method in twosettings: (1) the Original ICL Task using the standard label space and (2) theTask Learning setting, where the label space is replaced with semanticallyunrelated tokens. In both settings, In-Context Calibration demonstratessubstantial improvements, with results generalized across three LLM families(OPT, GPT, and Llama2) under various configurations.</description><author>Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu</author><pubDate>Fri, 29 Mar 2024 06:51:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09488v2</guid></item><item><title>Rectifying Demonstration Shortcut in In-Context Learning</title><link>http://arxiv.org/abs/2403.09488v3</link><description>Large language models (LLMs) are able to solve various tasks with only a fewdemonstrations utilizing their in-context learning (ICL) abilities. However,LLMs often rely on their pre-trained semantic priors of demonstrations ratherthan on the input-label relationships to proceed with ICL prediction. In thiswork, we term this phenomenon as the 'Demonstration Shortcut'. While previousworks have primarily focused on improving ICL prediction results for predefinedtasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLMto effectively learn new input-label relationships from demonstrations. Toachieve this, we introduce In-Context Calibration, a demonstration-awarecalibration method. We evaluate the effectiveness of the proposed method in twosettings: (1) the Original ICL Task using the standard label space and (2) theTask Learning setting, where the label space is replaced with semanticallyunrelated tokens. In both settings, In-Context Calibration demonstratessubstantial improvements, with results generalized across three LLM families(OPT, GPT, and Llama2) under various configurations.</description><author>Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu</author><pubDate>Mon, 15 Apr 2024 05:29:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09488v3</guid></item><item><title>In-Context Learning through the Bayesian Prism</title><link>http://arxiv.org/abs/2306.04891v2</link><description>In-context learning (ICL) is one of the surprising and useful features oflarge language models and subject of intense research. Recently, stylizedmeta-learning-like ICL setups have been devised that train transformers onsequences of input-output pairs $(x, f(x))$. The function $f$ comes from afunction class and generalization is checked by evaluating on sequencesgenerated from unseen functions from the same class. One of the maindiscoveries in this line of research has been that for several functionclasses, such as linear regression, transformers successfully generalize to newfunctions in the class. However, the inductive biases of these models resultingin this behavior are not clearly understood. A model with unlimited trainingdata and compute is a Bayesian predictor: it learns the pretrainingdistribution. In this paper we empirically examine how far this Bayesianperspective can help us understand ICL. To this end, we generalize the previousmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multipletask families. We instantiate this setup on a diverse range of linear andnonlinear function families and find that transformers can do ICL in thissetting as well. Where Bayesian inference is tractable, we find evidence thathigh-capacity transformers mimic the Bayesian predictor. The Bayesianperspective provides insights into the inductive bias of ICL and howtransformers perform a particular task when they are trained on multiple tasks.We also find that transformers can learn to generalize to new function classesthat were not seen during pretraining. This involves deviation from theBayesian predictor. We examine these deviations in more depth offering newinsights and hypotheses.</description><author>Madhur Panwar, Kabir Ahuja, Navin Goyal</author><pubDate>Sun, 14 Apr 2024 06:12:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04891v2</guid></item><item><title>In-context Learning and Gradient Descent Revisited</title><link>http://arxiv.org/abs/2311.07772v4</link><description>In-context learning (ICL) has shown impressive results in few-shot learningtasks, yet its underlying mechanism is still not fully understood. A recentline of work suggests that ICL performs gradient descent (GD)-basedoptimization implicitly. While appealing, much of the research focuses onsimplified settings, where the parameters of a shallow model are optimized. Inthis work, we revisit evidence for ICL-GD correspondence on realistic NLP tasksand models. We find gaps in evaluation, both in terms of problematic metricsand insufficient baselines. We show that surprisingly, even untrained modelsachieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next,we explore a major discrepancy in the flow of information throughout the modelbetween ICL and GD, which we term Layer Causality. We propose a simple GD-basedoptimization procedure that respects layer causality, and show it improvessimilarity scores significantly.</description><author>Gilad Deutch, Nadav Magar, Tomer Bar Natan, Guy Dar</author><pubDate>Sun, 31 Mar 2024 20:33:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07772v4</guid></item><item><title>Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation</title><link>http://arxiv.org/abs/2402.09954v1</link><description>Previous in-context learning (ICL) research has focused on tasks such asclassification, machine translation, text2table, etc., while studies on whetherICL can improve human-like dialogue generation are scarce. Our work fills thisgap by systematically investigating the ICL capabilities of large languagemodels (LLMs) in persona-based dialogue generation, conducting extensiveexperiments on high-quality real human Chinese dialogue datasets. Fromexperimental results, we draw three conclusions: 1) adjusting promptinstructions is the most direct, effective, and economical way to improvegeneration quality; 2) randomly retrieving demonstrations (demos) achieves thebest results, possibly due to the greater diversity and the amount of effectiveinformation; counter-intuitively, retrieving demos with a context identical tothe query performs the worst; 3) even when we destroy the multi-turnassociations and single-turn semantics in the demos, increasing the number ofdemos still improves dialogue performance, proving that LLMs can learn fromcorrupted dialogue demos. Previous explanations of the ICL mechanism, such as$n$-gram induction head, cannot fully account for this phenomenon.</description><author>Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang</author><pubDate>Thu, 15 Feb 2024 14:03:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09954v1</guid></item><item><title>Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation</title><link>http://arxiv.org/abs/2402.09954v2</link><description>Previous in-context learning (ICL) research has focused on tasks such asclassification, machine translation, text2table, etc., while studies on whetherICL can improve human-like dialogue generation are scarce. Our work fills thisgap by systematically investigating the ICL capabilities of large languagemodels (LLMs) in persona-based dialogue generation, conducting extensiveexperiments on high-quality real human Chinese dialogue datasets. Fromexperimental results, we draw three conclusions: 1) adjusting promptinstructions is the most direct, effective, and economical way to improvegeneration quality; 2) randomly retrieving demonstrations (demos) achieves thebest results, possibly due to the greater diversity and the amount of effectiveinformation; counter-intuitively, retrieving demos with a context identical tothe query performs the worst; 3) even when we destroy the multi-turnassociations and single-turn semantics in the demos, increasing the number ofdemos still improves dialogue performance, proving that LLMs can learn fromcorrupted dialogue demos. Previous explanations of the ICL mechanism, such as$n$-gram induction head, cannot fully account for this phenomenon.</description><author>Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang</author><pubDate>Sat, 17 Feb 2024 06:11:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09954v2</guid></item><item><title>Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning</title><link>http://arxiv.org/abs/2401.06469v2</link><description>In this paper, by treating in-context learning (ICL) as a meta-optimizationprocess, we explain why LLMs are sensitive to the order of ICL examples. Thisunderstanding leads us to the development of Batch-ICL, an effective,efficient, and order-agnostic inference algorithm for ICL. Differing from thestandard N-shot learning approach, Batch-ICL employs $N$ separate 1-shotforward computations and aggregates the resulting meta-gradients. Theseaggregated meta-gradients are then applied to the forward computation of azero-shot query to generate the final prediction. This batch processingapproach renders the LLM agnostic to the order of ICL examples. Throughextensive experiments and analysis, we demonstrate that Batch-ICL consistentlyoutperforms most permutations of ICL examples. In some cases, it even exceedsthe performance of the best order for standard ICL, all while reducing thecomputational resources required. Furthermore, we develop a novel variant ofBatch-ICL featuring multiple "epochs" of meta-optimization. This variantimplicitly explores permutations of ICL examples, further enhancing ICLperformance.</description><author>Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, Rui Yan</author><pubDate>Fri, 16 Feb 2024 10:58:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.06469v2</guid></item><item><title>In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax</title><link>http://arxiv.org/abs/2311.07811v2</link><description>In-context learning (ICL) is now a common method for teaching large languagemodels (LLMs) new tasks: given labeled examples in the input context, the LLMlearns to perform the task without weight updates. Do models guided via ICLinfer the underlying structure of the task defined by the context, or do theyrely on superficial heuristics that only generalize to identically distributedexamples? We address this question using transformations tasks and an NLI taskthat assess sensitivity to syntax - a requirement for robust languageunderstanding. We further investigate whether out-of-distributiongeneralization can be improved via chain-of-thought prompting, where the modelis provided with a sequence of intermediate computation steps that illustratehow the task ought to be performed. In experiments with models from the GPT,PaLM, and Llama 2 families, we find large variance across LMs. The variance isexplained more by the composition of the pre-training corpus and supervisionmethods than by model size; in particular, models pre-trained on codegeneralize better, and benefit more from chain-of-thought prompting.</description><author>Aaron Mueller, Albert Webson, Jackson Petty, Tal Linzen</author><pubDate>Wed, 10 Apr 2024 16:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07811v2</guid></item><item><title>Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective</title><link>http://arxiv.org/abs/2306.06615v2</link><description>Molecule discovery plays a crucial role in various scientific fields,advancing the design of tailored materials and drugs. However, most of theexisting methods heavily rely on domain experts, require excessivecomputational cost, or suffer from sub-optimal performance. On the other hand,Large Language Models (LLMs), like ChatGPT, have shown remarkable performancein various cross-modal tasks due to their powerful capabilities in naturallanguage understanding, generalization, and in-context learning (ICL), whichprovides unprecedented opportunities to advance molecule discovery. Despiteseveral previous works trying to apply LLMs in this task, the lack ofdomain-specific corpus and difficulties in training specialized LLMs stillremain challenges. In this work, we propose a novel LLM-based framework(MolReGPT) for molecule-caption translation, where an In-Context Few-ShotMolecule Learning paradigm is introduced to empower molecule discovery withLLMs like ChatGPT to perform their in-context learning capability withoutdomain-specific pre-training and fine-tuning. MolReGPT leverages the principleof molecular similarity to retrieve similar molecules and their textdescriptions from a local database to enable LLMs to learn the task knowledgefrom context examples. We evaluate the effectiveness of MolReGPT onmolecule-caption translation, including molecule understanding and text-basedmolecule generation. Experimental results show that compared to fine-tunedmodels, MolReGPT outperforms MolT5-base and is comparable to MolT5-largewithout additional training. To the best of our knowledge, MolReGPT is thefirst work to leverage LLMs via in-context learning in molecule-captiontranslation for advancing molecule discovery. Our work expands the scope of LLMapplications, as well as providing a new paradigm for molecule discovery anddesign.</description><author>Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, Qing Li</author><pubDate>Mon, 22 Apr 2024 18:41:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06615v2</guid></item><item><title>Parallel Structures in Pre-training Data Yield In-Context Learning</title><link>http://arxiv.org/abs/2402.12530v1</link><description>Pre-trained language models (LMs) are capable of in-context learning (ICL):they can adapt to a task with only a few examples given in the prompt withoutany parameter update. However, it is unclear where this capability comes fromas there is a stark distribution shift between pre-training text and ICLprompts. In this work, we study what patterns of the pre-training datacontribute to ICL. We find that LMs' ICL ability depends on $\textit{parallelstructures}$ in the pre-training data -- pairs of phrases following similartemplates in the same context window. Specifically, we detect parallelstructures by checking whether training on one phrase improves prediction ofthe other, and conduct ablation experiments to study their effect on ICL. Weshow that removing parallel structures in the pre-training data reduces LMs'ICL accuracy by 51% (vs 2% from random ablation). This drop persists even whenexcluding common patterns such as n-gram repetitions and long-range dependency,showing the diversity and generality of parallel structures. A closer look atthe detected parallel structures indicates that they cover diverse linguistictasks and span long distances in the data.</description><author>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, He He</author><pubDate>Mon, 19 Feb 2024 20:40:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12530v1</guid></item><item><title>Cell-Free Multi-User MIMO Equalization via In-Context Learning</title><link>http://arxiv.org/abs/2404.05538v1</link><description>Large pre-trained sequence models, such as transformers, excel as few-shotlearners capable of in-context learning (ICL). In ICL, a model is trained toadapt its operation to a new task based on limited contextual information,typically in the form of a few training examples for the given task. Previouswork has explored the use of ICL for channel equalization in single-usermulti-input and multiple-output (MIMO) systems. In this work, we demonstratethat ICL can be also used to tackle the problem of multi-user equalization incell-free MIMO systems with limited fronthaul capacity. In this scenario, atask is defined by channel statistics, signal-to-noise ratio, and modulationschemes. The context encompasses the users' pilot sequences, the correspondingquantized received signals, and the current received data signal. Differentprompt design strategies are proposed and evaluated that encompass alsolarge-scale fading and modulation information. Experiments demonstrate thatICL-based equalization provides estimates with lower mean squared error ascompared to the linear minimum mean squared error equalizer, especially in thepresence of limited fronthaul capacity and pilot contamination.</description><author>Matteo Zecchin, Kai Zu, Osvaldo Simeone</author><pubDate>Mon, 08 Apr 2024 15:06:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05538v1</guid></item><item><title>Cell-Free Multi-User MIMO Equalization via In-Context Learning</title><link>http://arxiv.org/abs/2404.05538v2</link><description>Large pre-trained sequence models, such as transformers, excel as few-shotlearners capable of in-context learning (ICL). In ICL, a model is trained toadapt its operation to a new task based on limited contextual information,typically in the form of a few training examples for the given task. Previouswork has explored the use of ICL for channel equalization in single-usermulti-input and multiple-output (MIMO) systems. In this work, we demonstratethat ICL can be also used to tackle the problem of multi-user equalization incell-free MIMO systems with limited fronthaul capacity. In this scenario, atask is defined by channel statistics, signal-to-noise ratio, and modulationschemes. The context encompasses the users' pilot sequences, the correspondingquantized received signals, and the current received data signal. Differentprompt design strategies are proposed and evaluated that encompass alsolarge-scale fading and modulation information. Experiments demonstrate thatICL-based equalization provides estimates with lower mean squared error ascompared to the linear minimum mean squared error equalizer, especially in thepresence of limited fronthaul capacity and pilot contamination.</description><author>Matteo Zecchin, Kai Yu, Osvaldo Simeone</author><pubDate>Thu, 11 Apr 2024 10:45:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05538v2</guid></item><item><title>In-context Learning with Retrieved Demonstrations for Language Models: A Survey</title><link>http://arxiv.org/abs/2401.11624v3</link><description>Language models, especially pre-trained large language models, have showcasedremarkable abilities as few-shot in-context learners (ICL), adept at adaptingto new tasks with just a few demonstrations in the input context. However, themodel's ability to perform ICL is sensitive to the choice of the few-shotdemonstrations. Instead of using a fixed set of demonstrations, one recentdevelopment is to retrieve demonstrations tailored to each input query. Theimplementation of demonstration retrieval is relatively straightforward,leveraging existing databases and retrieval systems. This not only improves theefficiency and scalability of the learning process but also has been shown toreduce biases inherent in manual example selection. In light of the encouragingresults and growing research in ICL with retrieved demonstrations, we conductan extensive review of studies in this area. In this survey, we discuss andcompare different design choices for retrieval models, retrieval trainingprocedures, and inference algorithms.</description><author>Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi</author><pubDate>Tue, 13 Feb 2024 20:46:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.11624v3</guid></item><item><title>In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</title><link>http://arxiv.org/abs/2402.11639v1</link><description>A striking property of transformers is their ability to perform in-contextlearning (ICL), a machine learning framework in which the learner is presentedwith a novel context during inference implicitly through some data, and taskedwith making a prediction in that context. As such that learner must adapt tothe context without additional training. We explore the role of softmaxattention in an ICL setting where each context encodes a regression task. Weshow that an attention unit learns a window that it uses to implement anearest-neighbors predictor adapted to the landscape of the pretraining tasks.Specifically, we show that this window widens with decreasing Lipschitzness andincreasing label noise in the pretraining tasks. We also show that on low-rank,linear problems, the attention unit learns to project onto the appropriatesubspace before inference. Further, we show that this adaptivity reliescrucially on the softmax activation and thus cannot be replicated by the linearactivation often studied in prior theoretical analyses.</description><author>Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, Sanjay Shakkottai</author><pubDate>Sun, 18 Feb 2024 16:37:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11639v1</guid></item><item><title>Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer</title><link>http://arxiv.org/abs/2311.13120v3</link><description>Scene text recognition (STR) in the wild frequently encounters challengeswhen coping with domain variations, font diversity, shape deformations, etc. Astraightforward solution is performing model fine-tuning tailored to a specificscenario, but it is computationally intensive and requires multiple modelcopies for various scenarios. Recent studies indicate that large languagemodels (LLMs) can learn from a few demonstration examples in a training-freemanner, termed "In-Context Learning" (ICL). Nevertheless, applying LLMs as atext recognizer is unacceptably resource-consuming. Moreover, our pilotexperiments on LLMs show that ICL fails in STR, mainly attributed to theinsufficient incorporation of contextual information from diverse samples inthe training stage. To this end, we introduce E$^2$STR, a STR model trainedwith context-rich scene text sequences, where the sequences are generated viaour proposed in-context training strategy. E$^2$STR demonstrates that aregular-sized model is sufficient to achieve effective ICL capabilities in STR.Extensive experiments show that E$^2$STR exhibits remarkable training-freeadaptation in various scenarios and outperforms even the fine-tunedstate-of-the-art approaches on public benchmarks. The code is released athttps://github.com/bytedance/E2STR .</description><author>Zhen Zhao, Jingqun Tang, Chunhui Lin, Binghong Wu, Can Huang, Hao Liu, Xin Tan, Zhizhong Zhang, Yuan Xie</author><pubDate>Thu, 28 Mar 2024 09:30:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.13120v3</guid></item><item><title>Task-Oriented Dialogue with In-Context Learning</title><link>http://arxiv.org/abs/2402.12234v1</link><description>We describe a system for building task-oriented dialogue systems combiningthe in-context learning abilities of large language models (LLMs) with thedeterministic execution of business logic. LLMs are used to translate betweenthe surface form of the conversation and a domain-specific language (DSL) whichis used to progress the business logic. We compare our approach to theintent-based NLU approach predominantly used in industry today. Our experimentsshow that developing chatbots with our system requires significantly lesseffort than established approaches, that these chatbots can successfullynavigate complex dialogues which are extremely challenging for NLU-basedsystems, and that our system has desirable properties for scaling task-orienteddialogue systems to a large number of tasks. We make our implementationavailable for use and further study.</description><author>Tom Bocklisch, Thomas Werkmeister, Daksh Varshneya, Alan Nichol</author><pubDate>Mon, 19 Feb 2024 15:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12234v1</guid></item><item><title>In-Context Learning Demonstration Selection via Influence Analysis</title><link>http://arxiv.org/abs/2402.11750v1</link><description>Large Language Models (LLMs) have demonstrated their In-Context Learning(ICL) capabilities which provides an opportunity to perform few shot learningwithout any gradient update. Despite its multiple benefits, ICL generalizationperformance is sensitive to the selected demonstrations. Selecting effectivedemonstrations for ICL is still an open research challenge. To address thischallenge, we propose a demonstration selection method called InfICL whichanalyzes influences of training samples through influence functions.Identifying highly influential training samples can potentially aid inuplifting the ICL generalization performance. To limit the running cost ofInfICL, we only employ the LLM to generate sample embeddings, and don't performany costly fine tuning. We perform empirical study on multiple real-worlddatasets and show merits of our InfICL against state-of-the-art baselines.</description><author>Vinay M. S., Minh-Hao Van, Xintao Wu</author><pubDate>Mon, 19 Feb 2024 00:39:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11750v1</guid></item><item><title>ParaICL: Towards Robust Parallel In-Context Learning</title><link>http://arxiv.org/abs/2404.00570v1</link><description>Large language models (LLMs) have become the norm in natural languageprocessing (NLP), excelling in few-shot in-context learning (ICL) with theirremarkable abilities. Nonetheless, the success of ICL largely hinges on thechoice of few-shot demonstration examples, making the selection processincreasingly crucial. Existing methods have delved into optimizing the quantityand semantic similarity of these examples to improve ICL performances. However,our preliminary experiments indicate that the effectiveness of ICL is limitedby the length of the input context. Moreover, varying combinations of few-shotdemonstration examples can significantly boost accuracy across different testsamples. To address this, we propose a novel method named parallel in-contextlearning (ParaICL) that effectively utilizes all demonstration examples withoutexceeding the manageable input context length. ParaICL employs parallelbatching to distribute demonstration examples into different batches accordingto the semantic similarities of the questions in the demonstrations to the testquestion. It then computes normalized batch semantic scores for each batch. Aweighted average semantic objective, constrained by adaptive plausibility, isapplied to select the most appropriate tokens. Through extensive experiments,we validate the effectiveness of ParaICL and conduct ablation studies tounderscore its design rationale. We further demonstrate that ParaICL canseamlessly integrate with existing methods.</description><author>Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, Lidong Bing</author><pubDate>Sun, 31 Mar 2024 06:56:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00570v1</guid></item><item><title>Universal Link Predictor By In-Context Learning on Graphs</title><link>http://arxiv.org/abs/2402.07738v2</link><description>Link prediction is a crucial task in graph machine learning, where the goalis to infer missing or future links within a graph. Traditional approachesleverage heuristic methods based on widely observed connectivity patterns,offering broad applicability and generalizability without the need for modeltraining. Despite their utility, these methods are limited by their reliance onhuman-derived heuristics and lack the adaptability of data-driven approaches.Conversely, parametric link predictors excel in automatically learning theconnectivity patterns from data and achieving state-of-the-art but fail shortto directly transfer across different graphs. Instead, it requires the cost ofextensive training and hyperparameter optimization to adapt to the targetgraph. In this work, we introduce the Universal Link Predictor (UniLP), a novelmodel that combines the generalizability of heuristic approaches with thepattern learning capabilities of parametric models. UniLP is designed toautonomously identify connectivity patterns across diverse graphs, ready forimmediate application to any unseen graph dataset without targeted training. Weaddress the challenge of conflicting connectivity patterns-arising from theunique distributions of different graphs-through the implementation ofIn-context Learning (ICL). This approach allows UniLP to dynamically adjust tovarious target graphs based on contextual demonstrations, thereby avoidingnegative transfer. Through rigorous experimentation, we demonstrate UniLP'seffectiveness in adapting to new, unseen graphs at test time, showcasing itsability to perform comparably or even outperform parametric models that havebeen finetuned for specific datasets. Our findings highlight UniLP's potentialto set a new standard in link prediction, combining the strengths of heuristicand parametric methods in a single, versatile framework.</description><author>Kaiwen Dong, Haitao Mao, Zhichun Guo, Nitesh V. Chawla</author><pubDate>Thu, 15 Feb 2024 15:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07738v2</guid></item><item><title>Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models</title><link>http://arxiv.org/abs/2402.17671v1</link><description>As foundation models (FMs) continue to shape the landscape of AI, thein-context learning (ICL) paradigm thrives but also encounters issues such astoxicity, hallucination, disparity, adversarial vulnerability, andinconsistency. Ensuring the reliability and responsibility of FMs is crucialfor the sustainable development of the AI ecosystem. In this concise overview,we investigate recent advancements in enhancing the reliability andtrustworthiness of FMs within ICL frameworks, focusing on four keymethodologies, each with its corresponding subgoals. We sincerely hope thispaper can provide valuable insights for researchers and practitionersendeavoring to build safe and dependable FMs and foster a stable and consistentICL environment, thereby unlocking their vast potential.</description><author>Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, Xiaoxing Ma</author><pubDate>Tue, 27 Feb 2024 16:44:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17671v1</guid></item><item><title>Are Human-generated Demonstrations Necessary for In-context Learning?</title><link>http://arxiv.org/abs/2309.14681v4</link><description>Despite the promising few-shot ability of large language models (LLMs), thestandard paradigm of In-context Learning (ICL) suffers the disadvantages ofsusceptibility to selected demonstrations and the intricacy to generate thesedemonstrations. In this paper, we raise the fundamental question that whetherhuman-generated demonstrations are necessary for ICL. To answer this question,we propose self-contemplation prompting strategy (SEC), a paradigm free fromhuman-crafted demonstrations. The key point of SEC is that, instead of usinghand-crafted examples as demonstrations in ICL, SEC asks LLMs to first createdemonstrations on their own, based on which the final output is generated. SECis a flexible framework and can be adapted to both the vanilla ICL and thechain-of-thought (CoT), but with greater ease: as the manual-generation processof both examples and rationale can be saved. Extensive experiments inarithmetic reasoning, commonsense reasoning, multi-task language understanding,and code generation benchmarks, show that SEC, which does not requirehand-crafted demonstrations, significantly outperforms the zero-shot learningstrategy, and achieves comparable results to ICL with hand-crafteddemonstrations. This demonstrates that, for many tasks, contemporary LLMspossess a sufficient level of competence to exclusively depend on their owncapacity for decision making, removing the need for external training data.Code is available at https://github.com/ruili33/SEC.</description><author>Rui Li, Guoyin Wang, Jiwei Li</author><pubDate>Wed, 21 Feb 2024 05:49:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14681v4</guid></item><item><title>DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning</title><link>http://arxiv.org/abs/2403.04233v1</link><description>It has long been assumed that the sheer number of parameters in largelanguage models (LLMs) drives in-context learning (ICL) capabilities, enablingremarkable performance improvements by leveraging task-specific demonstrations.Challenging this hypothesis, we introduce DEEP-ICL, a novel task DefinitionEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extractstask definitions from given demonstrations and generates responses throughlearning task-specific examples. We argue that improvement from ICL does notdirectly rely on model size, but essentially stems from understanding taskdefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two3B models with distinct roles (one for concluding task definitions and theother for learning task demonstrations) and achieves comparable performance toLLaMA2-13B. Furthermore, our framework outperforms conventional ICL byovercoming pretraining sequence length limitations, by supporting unlimiteddemonstrations. We contend that DEEP-ICL presents a novel alternative forachieving efficient few-shot learning, extending beyond the conventional ICL.</description><author>Xingwei Qu, Yiming Liang, Yucheng Wang, Tianyu Zheng, Tommy Yue, Lei Ma, Stephen W. Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Ge Zhang</author><pubDate>Thu, 07 Mar 2024 05:26:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04233v1</guid></item><item><title>In-Context Learning State Vector with Inner and Momentum Optimization</title><link>http://arxiv.org/abs/2404.11225v1</link><description>Large Language Models (LLMs) have exhibited an impressive ability to performIn-Context Learning (ICL) from only a few examples. Recent works have indicatedthat the functions learned by ICL can be represented through compressed vectorsderived from the transformer. However, the working mechanisms and optimizationof these vectors are yet to be thoroughly explored. In this paper, we addressthis gap by presenting a comprehensive analysis of these compressed vectors,drawing parallels to the parameters trained with gradient descent, andintroduce the concept of state vector. Inspired by the works on model soup andmomentum-based gradient descent, we propose inner and momentum optimizationmethods that are applied to refine the state vector progressively as test-timeadaptation. Moreover, we simulate state vector aggregation in the multipleexample setting, where demonstrations comprising numerous examples are usuallytoo lengthy for regular ICL, and further propose a divide-and-conqueraggregation method to address this challenge. We conduct extensive experimentsusing Llama-2 and GPT-J in both zero-shot setting and few-shot setting. Theexperimental results show that our optimization method effectively enhances thestate vector and achieves the state-of-the-art performance on diverse tasks.Code is available at https://github.com/HITsz-TMG/ICL-State-Vector</description><author>Dongfang Li, Zhenyu Liu, Xinshuo Hu, Zetian Sun, Baotian Hu, Min Zhang</author><pubDate>Wed, 17 Apr 2024 11:19:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.11225v1</guid></item><item><title>Can MLLMs Perform Text-to-Image In-Context Learning?</title><link>http://arxiv.org/abs/2402.01293v2</link><description>The evolution from Large Language Models (LLMs) to Multimodal Large LanguageModels (MLLMs) has spurred research into extending In-Context Learning (ICL) toits multimodal counterpart. Existing such studies have primarily concentratedon image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its uniquecharacteristics and potential applications, remains underexplored. To addressthis gap, we formally define the task of T2I-ICL and present CoBSAT, the firstT2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset tobenchmark six state-of-the-art MLLMs, we uncover considerable difficultiesMLLMs encounter in solving T2I-ICL. We identify the primary challenges as theinherent complexity of multimodality and image generation, and show thatstrategies such as fine-tuning and Chain-of-Thought prompting help to mitigatethese difficulties, leading to notable improvements in performance. Our codeand dataset are available at https://github.com/UW-Madison-Lee-Lab/CoBSAT.</description><author>Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, Kangwook Lee</author><pubDate>Mon, 15 Apr 2024 22:30:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.01293v2</guid></item><item><title>Towards Multimodal In-Context Learning for Vision &amp; Language Models</title><link>http://arxiv.org/abs/2403.12736v1</link><description>Inspired by the emergence of Large Language Models (LLMs) that can trulyunderstand human language, significant progress has been made in aligningother, non-language, modalities to be `understandable' by an LLM, primarily viaconverting their samples into a sequence of embedded language-like tokensdirectly fed into the LLM (decoder) input stream. However, so far limitedattention has been given to transferring (and evaluating) one of the core LLMcapabilities to the emerging VLMs, namely the In-Context Learning (ICL)ability, or in other words to guide VLMs to desired target downstream tasks oroutput structure using in-context image+text demonstrations. In this work, wedive deeper into analyzing the capabilities of some of the state-of-the-artVLMs to follow ICL instructions, discovering them to be somewhat lacking. Wediscover that even models that underwent large-scale mixed modalitypre-training and were implicitly guided to make use of interleaved image andtext information (intended to consume helpful context from multiple images)under-perform when prompted with few-shot (ICL) demonstrations, likely due totheir lack of `direct' ICL instruction tuning. To test this conjecture, wepropose a simple, yet surprisingly effective, strategy of extending a commonVLM alignment framework with ICL support, methodology, and curriculum. Weexplore, analyze, and provide insights into effective data mixes, leading up toa significant 21.03% (and 11.3% on average) ICL performance boost over thestrongest VLM baselines and a variety of ICL benchmarks. We also contribute newbenchmarks for ICL evaluation in VLMs and discuss their advantages over theprior art.</description><author>Sivan Doveh, Shaked Perek, M. Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, Leonid Karlinsky</author><pubDate>Tue, 19 Mar 2024 14:53:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12736v1</guid></item><item><title>In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization</title><link>http://arxiv.org/abs/2402.14951v1</link><description>We study the \emph{in-context learning} (ICL) ability of a \emph{LinearTransformer Block} (LTB) that combines a linear attention component and alinear multi-layer perceptron (MLP) component. For ICL of linear regressionwith a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achievenearly Bayes optimal ICL risk. In contrast, using only linear attention mustincur an irreducible additive approximation error. Furthermore, we establish acorrespondence between LTB and one-step gradient descent estimators withlearnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sensethat every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented byan LTB estimator and every optimal LTB estimator that minimizes the in-classICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator.Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can beefficiently optimized with gradient flow, despite a non-convex trainingobjective. Our results reveal that LTB achieves ICL by implementing$\mathsf{GD}\text{-}\mathbf{\beta}$, and they highlight the role of MLP layersin reducing approximation error.</description><author>Ruiqi Zhang, Jingfeng Wu, Peter L. Bartlett</author><pubDate>Thu, 22 Feb 2024 20:26:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14951v1</guid></item><item><title>Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality</title><link>http://arxiv.org/abs/2402.19442v1</link><description>We study the dynamics of gradient flow for training a multi-head softmaxattention model for in-context learning of multi-task linear regression. Weestablish the global convergence of gradient flow under suitable choices ofinitialization. In addition, we prove that an interesting "task allocation"phenomenon emerges during the gradient flow dynamics, where each attention headfocuses on solving a single task of the multi-task model. Specifically, weprove that the gradient flow dynamics can be split into three phases -- awarm-up phase where the loss decreases rather slowly and the attention headsgradually build up their inclination towards individual tasks, an emergencephase where each head selects a single task and the loss rapidly decreases, anda convergence phase where the attention parameters converge to a limit.Furthermore, we prove the optimality of gradient flow in the sense that thelimiting model learned by gradient flow is on par with the best possiblemulti-head softmax attention model up to a constant factor. Our analysis alsodelineates a strict separation in terms of the prediction accuracy of ICLbetween single-head and multi-head attention models. The key technique for ourconvergence analysis is to map the gradient flow dynamics in the parameterspace to a set of ordinary differential equations in the spectral domain, wherethe relative magnitudes of the semi-singular values of the attention weightsdetermines task allocation. To our best knowledge, our work provides the firstconvergence result for the multi-head softmax attention model.</description><author>Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang</author><pubDate>Thu, 29 Feb 2024 18:43:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19442v1</guid></item><item><title>Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning</title><link>http://arxiv.org/abs/2402.10738v1</link><description>Demonstration ordering, which is an important strategy for in-contextlearning (ICL), can significantly affects the performance of large languagemodels (LLMs). However, most of the current approaches of ordering requireadditional knowledge and similarity calculation. We advocate the few-shotin-context curriculum learning (ICCL), a simple but effective demonstrationordering method for ICL, which implies gradually increasing the complexity ofprompt demonstrations during the inference process. Then we design threeexperiments to discuss the effectiveness of ICCL, the formation mechanism ofLLM's ICCL capability, and the impact of ordering subjects. Experimentalresults demonstrate that ICCL, developed during the instruction-tuning stage,is effective for open-source LLMs. Moreover, LLMs exhibit a weaker capacitycompared to humans in discerning the difficulty levels of demonstrations. Werelease our code at https://github.com/61peng/curri_learning.</description><author>Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, Wei Lu</author><pubDate>Fri, 16 Feb 2024 14:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10738v1</guid></item><item><title>Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction</title><link>http://arxiv.org/abs/2404.12957v1</link><description>We propose an approach for estimating the latent knowledge embedded insidelarge language models (LLMs). We leverage the in-context learning (ICL)abilities of LLMs to estimate the extent to which an LLM knows the facts storedin a knowledge base. Our knowledge estimator avoids reliability concerns withprevious prompting-based methods, is both conceptually simpler and easier toapply, and we demonstrate that it can surface more of the latent knowledgeembedded in LLMs. We also investigate how different design choices affect theperformance of ICL-based knowledge estimation. Using the proposed estimator, weperform a large-scale evaluation of the factual knowledge of a variety of opensource LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large setof relations and facts from the Wikidata knowledge base. We observe differencesin the factual knowledge between different model families and models ofdifferent sizes, that some relations are consistently better known than othersbut that models differ in the precise facts they know, and differences in theknowledge of base models and their finetuned counterparts.</description><author>Qinyuan Wu, Mohammad Aflah Khan, Soumi Das, Vedant Nanda, Bishwamittra Ghosh, Camila Kolling, Till Speicher, Laurent Bindschaedler, Krishna P. Gummadi, Evimaria Terzi</author><pubDate>Fri, 19 Apr 2024 16:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.12957v1</guid></item><item><title>A Study on the Calibration of In-context Learning</title><link>http://arxiv.org/abs/2312.04021v4</link><description>Accurate uncertainty quantification is crucial for the safe deployment ofmachine learning models, and prior research has demonstrated improvements inthe calibration of modern language models (LMs). We study in-context learning(ICL), a prevalent method for adapting static LMs through tailored prompts, andexamine the balance between performance and calibration across a broad spectrumof natural language understanding and reasoning tasks. Through comprehensiveexperiments, we observe that, with an increasing number of ICL examples, modelsinitially exhibit increased miscalibration before achieving better calibrationand miscalibration tends to arise in low-shot settings. Moreover, we find thatmethods aimed at improving usability, such as fine-tuning and chain-of-thought(CoT) prompting, can lead to miscalibration and unreliable natural languageexplanations. Furthermore, we explore recalibration techniques and find that ascaling-binning calibrator can reduce calibration errors consistently.</description><author>Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Himabindu Lakkaraju, Sham Kakade</author><pubDate>Thu, 28 Mar 2024 04:01:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04021v4</guid></item><item><title>Long-context LLMs Struggle with Long In-context Learning</title><link>http://arxiv.org/abs/2404.02060v1</link><description>Large Language Models (LLMs) have made significant strides in handling longsequences exceeding 32K tokens. However, their performance evaluation haslargely been confined to metrics like perplexity and synthetic tasks, which maynot fully capture their abilities in more nuanced, real-world scenarios. Thisstudy introduces a specialized benchmark (LIConBench) focusing on longin-context learning within the realm of extreme-label classification. Wemeticulously selected six datasets with a label range spanning 28 to 174classes covering different input (few-shot demonstration) length from 2K to50K. Our benchmark requires LLMs to comprehend the entire input to recognizethe massive label spaces to make correct prediction. We evaluate 13long-context LLMs on our benchmarks. We find that the long-context LLMs performrelatively well under the token length of 20K and the performance benefits fromutilizing the long context window. However, after the context window exceeds20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gapin current LLM capabilities for processing and understanding long, context-richsequences. Further analysis revealed a tendency among models to favorpredictions for labels presented towards the end at the sequence. Their abilityto reason over multiple pieces in the long sequence is yet to be improved. Ourstudy reveals that long context understanding and reasoning is still achallenging task for the existing LLMs. We believe LIConBench could serve as amore realistic evaluation for the future long context LLMs.</description><author>Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen</author><pubDate>Tue, 02 Apr 2024 16:59:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02060v1</guid></item><item><title>Long-context LLMs Struggle with Long In-context Learning</title><link>http://arxiv.org/abs/2404.02060v2</link><description>Large Language Models (LLMs) have made significant strides in handling longsequences exceeding 32K tokens. However, their performance evaluation haslargely been confined to metrics like perplexity and synthetic tasks, which maynot fully capture their abilities in more nuanced, real-world scenarios. Thisstudy introduces a specialized benchmark (LongICLBench) focusing on longin-context learning within the realm of extreme-label classification. Wemeticulously selected six datasets with a label range spanning 28 to 174classes covering different input (few-shot demonstration) lengths from 2K to50K tokens. Our benchmark requires LLMs to comprehend the entire input torecognize the massive label spaces to make correct predictions. We evaluate 13long-context LLMs on our benchmarks. We find that the long-context LLMs performrelatively well on less challenging tasks with shorter demonstration lengths byeffectively utilizing the long context window. However, on the most challengingtask Discovery with 174 labels, all the LLMs struggle to understand the taskdefinition, thus reaching a performance close to zero. This suggests a notablegap in current LLM capabilities for processing and understanding long,context-rich sequences. Further analysis revealed a tendency among models tofavor predictions for labels presented toward the end of the sequence. Theirability to reason over multiple pieces in the long sequence is yet to beimproved. Our study reveals that long context understanding and reasoning isstill a challenging task for the existing LLMs. We believe LongICLBench couldserve as a more realistic evaluation for the future long-context LLMs.</description><author>Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen</author><pubDate>Thu, 04 Apr 2024 01:01:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02060v2</guid></item><item><title>WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents</title><link>http://arxiv.org/abs/2404.05902v1</link><description>In the realm of web agent research, achieving both generalization andaccuracy remains a challenging problem. Due to high variance in websitestructure, existing approaches often fail. Moreover, existing fine-tuning andin-context learning techniques fail to generalize across multiple websites. Weintroduce Wilbur, an approach that uses a differentiable ranking model and anovel instruction synthesis technique to optimally populate a black-box largelanguage model's prompt with task demonstrations from previous runs. Tomaximize end-to-end success rates, we also propose an intelligent backtrackingmechanism that learns and recovers from its mistakes. Finally, we show that ourranking model can be trained on data from a generative auto-curriculum whichsamples representative goals from an LLM, runs the agent, and automaticallyevaluates it, with no manual annotation. Wilbur achieves state-of-the-artresults on the WebVoyager benchmark, beating text-only models by 8% overall,and up to 36% on certain websites. On the same benchmark, Wilbur is within 5%of a strong multi-modal model despite only receiving textual inputs, andfurther analysis reveals a substantial number of failures are due toengineering challenges of operating the web.</description><author>Michael Lutz, Arth Bohra, Manvel Saroyan, Artem Harutyunyan, Giovanni Campagna</author><pubDate>Tue, 09 Apr 2024 00:10:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05902v1</guid></item><item><title>Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving</title><link>http://arxiv.org/abs/2404.03938v1</link><description>Math Word Problem (MWP) solving presents a challenging task in NaturalLanguage Processing (NLP). This study aims to provide MWP solvers with a morediverse training set, ultimately improving their ability to solve various mathproblems. We propose several methods for data augmentation by modifying theproblem texts and equations, such as synonym replacement, rule-based: questionreplacement, and rule based: reversing question methodologies over two EnglishMWP datasets. This study extends by introducing a new in-context learningaugmentation method, employing the Llama-7b language model. This approachinvolves instruction-based prompting for rephrasing the math problem texts.Performance evaluations are conducted on 9 baseline models, revealing thataugmentation methods outperform baseline models. Moreover, concatenatingexamples generated by various augmentation methods further improvesperformance.</description><author>Gulsum Yigit, Mehmet Fatih Amasyali</author><pubDate>Fri, 05 Apr 2024 08:57:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03938v1</guid></item><item><title>Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?</title><link>http://arxiv.org/abs/2402.12819v1</link><description>When solving a task with limited labelled data, researchers can either use ageneral large language model without further update, or use the few examples totune a specialised smaller model. When enough labels are available, thespecialised models outperform the general ones on many NLP tasks. In this work,we aim to investigate how many labelled samples are required for thespecialised models to achieve this superior performance, while taking theresults variance into consideration. Observing the behaviour of prompting,in-context learning, fine-tuning and instruction-tuning, identifying theirbreak-even points when increasing number of labelled training samples acrossthree tasks of varying complexity, we find that the specialised models oftenneed only few samples ($100-1000$) to be on par or better than the generalones. At the same time, the amount of required labelled data strongly dependson the task complexity and results variance.</description><author>Branislav Pecher, Ivan Srba, Maria Bielikova</author><pubDate>Tue, 20 Feb 2024 08:38:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12819v1</guid></item><item><title>Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity</title><link>http://arxiv.org/abs/2403.09428v1</link><description>Multimodal machine learning with missing modalities is an increasinglyrelevant challenge arising in various applications such as healthcare. Thispaper extends the current research into missing modalities to the low-dataregime, i.e., a downstream task has both missing modalities and limited samplesize issues. This problem setting is particularly challenging and alsopractical as it is often expensive to get full-modality data and sufficientannotated training samples. We propose to use retrieval-augmented in-contextlearning to address these two crucial issues by unleashing the potential of atransformer's in-context learning ability. Diverging from existing methods,which primarily belong to the parametric paradigm and often require sufficienttraining samples, our work exploits the value of the available full-modalitydata, offering a novel perspective on resolving the challenge. The proposeddata-dependent framework exhibits a higher degree of sample efficiency and isempirically demonstrated to enhance the classification model's performance onboth full- and missing-modality data in the low-data regime across variousmultimodal learning tasks. When only 1% of the training data are available, ourproposed method demonstrates an average improvement of 6.1% over a recentstrong baseline across various datasets and missing states. Notably, our methodalso reduces the performance gap between full-modality and missing-modalitydata compared with the baseline.</description><author>Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues</author><pubDate>Thu, 14 Mar 2024 15:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09428v1</guid></item><item><title>Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity</title><link>http://arxiv.org/abs/2403.09428v2</link><description>Multimodal machine learning with missing modalities is an increasinglyrelevant challenge arising in various applications such as healthcare. Thispaper extends the current research into missing modalities to the low-dataregime, i.e., a downstream task has both missing modalities and limited samplesize issues. This problem setting is particularly challenging and alsopractical as it is often expensive to get full-modality data and sufficientannotated training samples. We propose to use retrieval-augmented in-contextlearning to address these two crucial issues by unleashing the potential of atransformer's in-context learning ability. Diverging from existing methods,which primarily belong to the parametric paradigm and often require sufficienttraining samples, our work exploits the value of the available full-modalitydata, offering a novel perspective on resolving the challenge. The proposeddata-dependent framework exhibits a higher degree of sample efficiency and isempirically demonstrated to enhance the classification model's performance onboth full- and missing-modality data in the low-data regime across variousmultimodal learning tasks. When only 1% of the training data are available, ourproposed method demonstrates an average improvement of 6.1% over a recentstrong baseline across various datasets and missing states. Notably, our methodalso reduces the performance gap between full-modality and missing-modalitydata compared with the baseline.</description><author>Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues</author><pubDate>Tue, 26 Mar 2024 18:38:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.09428v2</guid></item><item><title>JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using in-context learning with GPT and instruction-tuned Llama models</title><link>http://arxiv.org/abs/2403.04798v2</link><description>This paper presents our system development for SemEval-2024 Task 3: "TheCompetition of Multimodal Emotion Cause Analysis in Conversations". Effectivelycapturing emotions in human conversations requires integrating multiplemodalities such as text, audio, and video. However, the complexities of thesediverse modalities pose challenges for developing an efficient multimodalemotion cause analysis (ECA) system. Our proposed approach addresses thesechallenges by a two-step framework. We adopt two different approaches in ourimplementation. In Approach 1, we employ instruction-tuning with two separateLlama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4Vfor conversation-level video description and employ in-context learning withannotated conversation using GPT 3.5. Our system wins rank 4, and systemablation experiments demonstrate that our proposed solutions achievesignificant performance gains. All the experimental codes are available onGithub.</description><author>Arefa, Mohammed Abbas Ansari, Chandni Saxena, Tanvir Ahmad</author><pubDate>Tue, 02 Apr 2024 15:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04798v2</guid></item><item><title>RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model</title><link>http://arxiv.org/abs/2402.10828v1</link><description>Robots powered by 'blackbox' models need to provide human-understandableexplanations which we can trust. Hence, explainability plays a critical role intrustworthy autonomous decision-making to foster transparency and acceptanceamong end users, especially in complex autonomous driving. Recent advancementsin Multi-Modal Large Language models (MLLMs) have shown promising potential inenhancing the explainability as a driving agent by producing controlpredictions along with natural language explanations. However, severe datascarcity due to expensive annotation costs and significant domain gaps betweendifferent datasets makes the development of a robust and generalisable systeman extremely challenging task. Moreover, the prohibitively expensive trainingrequirements of MLLM and the unsolved problem of catastrophic forgettingfurther limit their generalisability post-deployment. To address thesechallenges, we present RAG-Driver, a novel retrieval-augmented multi-modallarge language model that leverages in-context learning for high-performance,explainable, and generalisable autonomous driving. By grounding in retrievedexpert demonstration, we empirically validate that RAG-Driver achievesstate-of-the-art performance in producing driving action explanations,justifications, and control signal prediction. More importantly, it exhibitsexceptional zero-shot generalisation capabilities to unseen environmentswithout further training endeavours.</description><author>Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd</author><pubDate>Fri, 16 Feb 2024 16:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10828v1</guid></item><item><title>LLoCO: Learning Long Contexts Offline</title><link>http://arxiv.org/abs/2404.07979v1</link><description>Processing long contexts remains a challenge for large language models (LLMs)due to the quadratic computational and memory overhead of the self-attentionmechanism and the substantial KV cache sizes during generation. We propose anovel approach to address this problem by learning contexts offline throughcontext compression and in-domain parameter-efficient finetuning. Our methodenables an LLM to create a concise representation of the original context andefficiently retrieve relevant information to answer questions accurately. Weintroduce LLoCO, a technique that combines context compression, retrieval, andparameter-efficient finetuning using LoRA. Our approach extends the effectivecontext window of a 4k token LLaMA2-7B model to handle up to 128k tokens. Weevaluate our approach on several long-context question-answering datasets,demonstrating that LLoCO significantly outperforms in-context learning whileusing $30\times$ fewer tokens during inference. LLoCO achieves up to$7.62\times$ speed-up and substantially reduces the cost of long documentquestion answering, making it a promising solution for efficient long contextprocessing. Our code is publicly available athttps://github.com/jeffreysijuntan/lloco.</description><author>Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca Ada Popa</author><pubDate>Thu, 11 Apr 2024 18:57:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07979v1</guid></item><item><title>Modularized Networks for Few-shot Hateful Meme Detection</title><link>http://arxiv.org/abs/2402.11845v1</link><description>In this paper, we address the challenge of detecting hateful memes in thelow-resource setting where only a few labeled examples are available. Ourapproach leverages the compositionality of Low-rank adaptation (LoRA), a widelyused parameter-efficient tuning technique. We commence by fine-tuning largelanguage models (LLMs) with LoRA on selected tasks pertinent to hateful memedetection, thereby generating a suite of LoRA modules. These modules arecapable of essential reasoning skills for hateful meme detection. We then usethe few available annotated samples to train a module composer, which assignsweights to the LoRA modules based on their relevance. The model's learnableparameters are directly proportional to the number of LoRA modules. Thismodularized network, underpinned by LLMs and augmented with LoRA modules,exhibits enhanced generalization in the context of hateful meme detection. Ourevaluation spans three datasets designed for hateful meme detection in afew-shot learning context. The proposed method demonstrates superiorperformance to traditional in-context learning, which is also morecomputationally intensive during inference.We then use the few availableannotated samples to train a module composer, which assigns weights to the LoRAmodules based on their relevance. The model's learnable parameters are directlyproportional to the number of LoRA modules. This modularized network,underpinned by LLMs and augmented with LoRA modules, exhibits enhancedgeneralization in the context of hateful meme detection. Our evaluation spansthree datasets designed for hateful meme detection in a few-shot learningcontext. The proposed method demonstrates superior performance to traditionalin-context learning, which is also more computationally intensive duringinference.</description><author>Rui Cao, Roy Ka-Wei Lee, Jing Jiang</author><pubDate>Mon, 19 Feb 2024 05:15:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11845v1</guid></item><item><title>MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation</title><link>http://arxiv.org/abs/2402.17725v1</link><description>Volumetric medical segmentation is a critical component of 3D medical imageanalysis that delineates different semantic regions. Deep neural networks havesignificantly improved volumetric medical segmentation, but they generallyrequire large-scale annotated data to achieve better performance, which can beexpensive and prohibitive to obtain. To address this limitation, existing workstypically perform transfer learning or design dedicated pretraining-finetuningstages to learn representative features. However, the mismatch between thesource and target domain can make it challenging to learn optimalrepresentation for volumetric data, while the multi-stage training demandshigher compute as well as careful selection of stage-specific design choices.In contrast, we propose a universal training framework called MedContext thatis architecture-agnostic and can be incorporated into any existing trainingframework for 3D medical segmentation. Our approach effectively learns selfsupervised contextual cues jointly with the supervised voxel segmentation taskwithout requiring large-scale annotated volumetric medical data or dedicatedpretraining-finetuning stages. The proposed approach induces contextualknowledge in the network by learning to reconstruct the missing organ or partsof an organ in the output segmentation space. The effectiveness of MedContextis validated across multiple 3D medical datasets and four state-of-the-artmodel architectures. Our approach demonstrates consistent gains in segmentationperformance across datasets and different architectures even in few-shot datascenarios. Our code and pretrained models are available athttps://github.com/hananshafi/MedContext</description><author>Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan</author><pubDate>Tue, 27 Feb 2024 17:58:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17725v1</guid></item><item><title>Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization</title><link>http://arxiv.org/abs/2403.10967v1</link><description>Zero-shot generalization (ZSG) to unseen dynamics is a major challenge forcreating generally capable embodied agents. To address the broader challenge,we start with the simpler setting of contextual reinforcement learning (cRL),assuming observability of the context values that parameterize the variation inthe system's dynamics, such as the mass or dimensions of a robot, withoutmaking further simplifying assumptions about the observability of the Markovianstate. Toward the goal of ZSG to unseen variation in context, we propose thecontextual recurrent state-space model (cRSSM), which introduces changes to theworld model of the Dreamer (v3) (Hafner et al., 2023). This allows the worldmodel to incorporate context for inferring latent Markovian states from theobservations and modeling the latent dynamics. Our experiments show that suchsystematic incorporation of the context improves the ZSG of the policiestrained on the ``dreams'' of the world model. We further find qualitativelythat our approach allows Dreamer to disentangle the latent state from context,allowing it to extrapolate its dreams to the many worlds of unseen contexts.The code for all our experiments is available at\url{https://github.com/sai-prasanna/dreaming_of_many_worlds}.</description><author>Sai Prasanna, Karim Farid, Raghu Rajan, Andr Biedenkapp</author><pubDate>Sat, 16 Mar 2024 17:29:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10967v1</guid></item><item><title>Towards More Unified In-context Visual Understanding</title><link>http://arxiv.org/abs/2312.02520v2</link><description>The rapid advancement of large language models (LLMs) has accelerated theemergence of in-context learning (ICL) as a cutting-edge approach in thenatural language processing domain. Recently, ICL has been employed in visualunderstanding tasks, such as semantic segmentation and image captioning,yielding promising results. However, existing visual ICL framework can notenable producing content across multiple modalities, which limits theirpotential usage scenarios. To address this issue, we present a new ICLframework for visual understanding with multi-modal output enabled. First, wequantize and embed both text and visual prompt into a unified representationalspace, structured as interleaved in-context sequences. Then a decoder-onlysparse transformer architecture is employed to perform generative modeling onthem, facilitating in-context learning. Thanks to this design, the model iscapable of handling in-context vision understanding tasks with multimodaloutput in a unified pipeline.Experimental results demonstrate that our modelachieves competitive performance compared with specialized models and previousICL baselines. Overall, our research takes a further step toward unifiedmultimodal in-context learning.</description><author>Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Tao Gong, Bin Liu, Shengwei Xu, Nenghai Yu</author><pubDate>Sat, 16 Mar 2024 17:07:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02520v2</guid></item><item><title>Deconstructing In-Context Learning: Understanding Prompts via Corruption</title><link>http://arxiv.org/abs/2404.02054v1</link><description>The ability of large language models (LLMs) to "learn in context" based onthe provided prompt has led to an explosive growth in their use, culminating inthe proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AIassistants are known to be robust to minor prompt modifications, mostly due toalignment techniques that use human feedback. In contrast, the underlyingpre-trained LLMs they use as a backbone are known to be brittle in thisrespect. Building high-quality backbone models remains a core challenge, and acommon approach to assessing their quality is to conduct few-shot evaluation.Such evaluation is notorious for being highly sensitive to minor promptmodifications, as well as the choice of specific in-context examples. Priorwork has examined how modifying different elements of the prompt can affectmodel performance. However, these earlier studies tended to concentrate on alimited number of specific prompt attributes and often produced contradictoryresults. Additionally, previous research either focused on models with fewerthan 15 billion parameters or exclusively examined black-box models like GPT-3or PaLM, making replication challenging. In the present study, we decompose theentire prompt into four components: task description, demonstration inputs,labels, and inline instructions provided for each demonstration. We investigatethe effects of structural and semantic corruptions of these elements on modelperformance. We study models ranging from 1.5B to 70B in size, using tendatasets covering classification and generation tasks. We find that repeatingtext within the prompt boosts model performance, and bigger models ($\geq$30B)are more sensitive to the semantics of the prompt. Finally, we observe thatadding task and inline instructions to the demonstrations enhances modelperformance even when the instructions are semantically corrupted.</description><author>Namrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky</author><pubDate>Tue, 02 Apr 2024 16:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02054v1</guid></item></channel></rss>