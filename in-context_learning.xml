<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 14 Feb 2025 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Chain-of-Dictionary Prompting Elicits Translation in Large Language Models</title><link>http://arxiv.org/abs/2305.06575v5</link><description>Large language models (LLMs) have shown surprisingly good performance inmultilingual neural machine translation (MNMT) even when trained withoutparallel data. Yet, despite the fact that the amount of training data isgigantic, they still struggle with translating rare words, particularly forlow-resource languages. Even worse, it is usually unrealistic to retrieverelevant demonstrations for in-context learning with low-resource languages onLLMs, which restricts the practical use of LLMs for translation -- how shouldwe mitigate this problem? To this end, we present a novel method, CoD, whichaugments LLMs with prior knowledge with the chains of multilingual dictionariesfor a subset of input words to elicit translation abilities for LLMs. Extensiveexperiments indicate that augmenting ChatGPT with CoD elicits large gains by upto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written inCyrillic script) on FLORES-200 full devtest set. We further demonstrate theimportance of chaining the multilingual dictionaries, as well as thesuperiority of CoD to few-shot demonstration for low-resource languages.</description><author>Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, Furu Wei</author><pubDate>Wed, 10 Jul 2024 09:53:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06575v5</guid></item><item><title>Video In-context Learning</title><link>http://arxiv.org/abs/2407.07356v1</link><description>In-context learning for vision data has been underexplored compared with thatin natural language. Previous works studied image in-context learning, urgingmodels to generate a single image guided by demonstrations. In this paper, wepropose and study video in-context learning, where the model starts from anexisting video clip and generates diverse potential future sequences, eachsemantically guided by the prompted video demonstrations. To achieve this, weprovide a clear definition of the task, and train an autoregressive Transformeron video datasets. We thoroughly analyze the effect of different datasets andrepresent frames as discrete tokens, and then model them by next tokenpredictions. We design various evaluation metrics, including both objective andsubjective measures, to demonstrate the visual quality and semantic accuracy ofgeneration results. Our model follows the scaling law and generateshigh-quality video clips that accurately align with the semantic guidanceprovided by in-context examples.</description><author>Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, Jiang Bian</author><pubDate>Wed, 10 Jul 2024 04:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07356v1</guid></item><item><title>From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models</title><link>http://arxiv.org/abs/2310.07338v4</link><description>Tabular data is foundational to predictive modeling in various crucialindustries, including healthcare, finance, retail, sustainability, etc. Despitethe progress made in specialized models, there is an increasing demand foruniversal models that can transfer knowledge, generalize from limited data, andfollow human instructions. These are challenges that current tabular deeplearning approaches have not fully tackled. Here we introduce GenerativeTabular Learning (GTL), a novel framework that integrates the advancedfunctionalities of large language models (LLMs)-such as prompt-based zero-shotgeneralization and in-context learning-into tabular deep learning. GTLcapitalizes on the pre-training of LLMs on diverse tabular data, enhancingtheir understanding of domain-specific knowledge, numerical sequences, andstatistical dependencies critical for accurate predictions. Our empirical studyspans 384 public datasets, rigorously analyzing GTL's convergence and scalingbehaviors and assessing the impact of varied data templates. The GTL-enhancedLLaMA-2 model demonstrates superior zero-shot and in-context learningcapabilities across numerous classification and regression tasks. Notably, itachieves this without fine-tuning, outperforming traditional methods andrivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, wenot only foster a deeper integration of LLMs' sophisticated abilities intotabular data comprehension and application but also offer a new trainingresource and a test bed for LLMs to enhance their ability to comprehend tabulardata. To facilitate reproducible research, we release our code, data, and modelcheckpoints at https://github.com/microsoft/Industrial-Foundation-Models.</description><author>Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian</author><pubDate>Thu, 11 Jul 2024 04:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07338v4</guid></item><item><title>HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context</title><link>http://arxiv.org/abs/2407.09375v1</link><description>This work explores the in-context learning capabilities of State Space Models(SSMs) and presents, to the best of our knowledge, the first theoreticalexplanation of a possible underlying mechanism. We introduce a novel weightconstruction for SSMs, enabling them to predict the next state of any dynamicalsystem after observing previous states without parameter fine-tuning. This isaccomplished by extending the HiPPO framework to demonstrate that continuousSSMs can approximate the derivative of any input signal. Specifically, we findan explicit weight construction for continuous SSMs and provide an asymptoticerror bound on the derivative approximation. The discretization of thiscontinuous SSM subsequently yields a discrete SSM that predicts the next state.Finally, we demonstrate the effectiveness of our parameterization empirically.This work should be an initial step toward understanding how sequence modelsbased on SSMs learn in context.</description><author>Federico Arangath Joseph, Kilian Haefeli, Noah Liniger, Caglar Gulcehre</author><pubDate>Fri, 12 Jul 2024 15:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09375v1</guid></item><item><title>Can large language models explore in-context?</title><link>http://arxiv.org/abs/2403.15371v2</link><description>We investigate the extent to which contemporary Large Language Models (LLMs)can engage in exploration, a core capability in reinforcement learning anddecision making. We focus on native performance of existing LLMs, withouttraining interventions. We deploy LLMs as agents in simple multi-armed banditenvironments, specifying the environment description and interaction historyentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,GPT-4, and Llama2, using a variety of prompt designs, and find that the modelsdo not robustly engage in exploration without substantial interventions: i)Across all of our experiments, only one configuration resulted in satisfactoryexploratory behavior: GPT-4 with chain-of-thought reasoning and an externallysummarized interaction history, presented as sufficient statistics; ii) Allother configurations did not result in robust exploratory behavior, includingthose with chain-of-thought reasoning but unsummarized history. Although thesefindings can be interpreted positively, they suggest that externalsummarization -- which may not be possible in more complex settings -- isimportant for obtaining desirable behavior from LLM agents. We conclude thatnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,may be required to empower LLM-based decision making agents in complexsettings.</description><author>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</author><pubDate>Fri, 12 Jul 2024 14:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15371v2</guid></item><item><title>OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text</title><link>http://arxiv.org/abs/2406.08418v3</link><description>Image-text interleaved data, consisting of multiple images and texts arrangedin a natural document format, aligns with the presentation paradigm of internetdata and closely resembles human reading habits. Recent studies have shown thatsuch data aids multimodal in-context learning and maintains the capabilities oflarge language models during multimodal fine-tuning. However, the limited scaleand diversity of current image-text interleaved data restrict the developmentof multimodal large language models. In this paper, we introduce OmniCorpus, a10 billion-scale image-text interleaved dataset. Using an efficient dataengine, we filter and extract large-scale high-quality documents, which contain8.6 billion images and 1,696 billion text tokens. Compared to counterparts(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales whilemaintaining good data quality; 2) features more diverse sources, including bothEnglish and non-English websites as well as video-centric websites; 3) is moreflexible, easily degradable from an image-text interleaved format to pure textcorpus and image-text pairs. Through comprehensive analysis and experiments, wevalidate the quality, usability, and effectiveness of the proposed dataset. Wehope this could provide a solid data foundation for future multimodal modelresearch. Code and data are released athttps://github.com/OpenGVLab/OmniCorpus.</description><author>Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Zhenxiang Li, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Zhongying Tu, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai</author><pubDate>Fri, 12 Jul 2024 08:54:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08418v3</guid></item><item><title>SpreadsheetLLM: Encoding Spreadsheets for Large Language Models</title><link>http://arxiv.org/abs/2407.09025v1</link><description>Spreadsheets, with their extensive two-dimensional grids, various layouts,and diverse formatting options, present notable challenges for large languagemodels (LLMs). In response, we introduce SpreadsheetLLM, pioneering anefficient encoding method designed to unleash and optimize LLMs' powerfulunderstanding and reasoning capability on spreadsheets. Initially, we propose avanilla serialization approach that incorporates cell addresses, values, andformats. However, this approach was limited by LLMs' token constraints, makingit impractical for most applications. To tackle this challenge, we developSheetCompressor, an innovative encoding framework that compresses spreadsheetseffectively for LLMs. It comprises three modules: structural-anchor-basedcompression, inverse index translation, and data-format-aware aggregation. Itsignificantly improves performance in spreadsheet table detection task,outperforming the vanilla approach by 25.6% in GPT4's in-context learningsetting. Moreover, fine-tuned LLM with SheetCompressor has an averagecompression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score,surpassing the best existing models by 12.3%. Finally, we propose Chain ofSpreadsheet for downstream tasks of spreadsheet understanding and validate in anew and demanding spreadsheet QA task. We methodically leverage the inherentlayout and structure of spreadsheets, demonstrating that SpreadsheetLLM ishighly effective across a variety of spreadsheet tasks.</description><author>Yuzhang Tian, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, José Cambronero, Yeye He, Shi Han, Dongmei Zhang</author><pubDate>Fri, 12 Jul 2024 06:34:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09025v1</guid></item><item><title>RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL</title><link>http://arxiv.org/abs/2407.08273v2</link><description>Large language models (LLMs) with in-context learning have significantlyimproved the performance of text-to-SQL task. Previous works generally focus onusing exclusive SQL generation prompt to improve the LLMs' reasoning ability.However, they are mostly hard to handle large databases with numerous tablesand columns, and usually ignore the significance of pre-processing database andextracting valuable information for more efficient prompt engineering. Based onabove analysis, we propose RB-SQL, a novel retrieval-based LLM framework forin-context prompt engineering, which consists of three modules that retrieveconcise tables and columns as schema, and targeted examples for in-contextlearning. Experiment results demonstrate that our model achieves betterperformance than several competitive baselines on public datasets BIRD andSpider.</description><author>Zhenhe Wu, Zhongqiu Li, Jie Zhang, Mengxiang Li, Yu Zhao, Ruiyu Fang, Zhongjiang He, Xuelong Li, Zhoujun Li, Shuangyong Song</author><pubDate>Fri, 12 Jul 2024 06:24:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08273v2</guid></item><item><title>Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection</title><link>http://arxiv.org/abs/2407.08952v1</link><description>Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate newsfrom real ones in extremely low-resource scenarios. This task has garneredincreased attention due to the widespread dissemination and harmful impact offake news on social media. Large Language Models (LLMs) have demonstratedcompetitive performance with the help of their rich prior knowledge andexcellent in-context learning abilities. However, existing methods facesignificant limitations, such as the Understanding Ambiguity and InformationScarcity, which significantly undermine the potential of LLMs. To address theseshortcomings, we propose a Dual-perspective Augmented Fake News Detection(DAFND) model, designed to enhance LLMs from both inside and outsideperspectives. Specifically, DAFND first identifies the keywords of each newsarticle through a Detection Module. Subsequently, DAFND creatively designs anInvestigation Module to retrieve inside and outside valuable informationconcerning to the current news, followed by another Judge Module to derive itsrespective two prediction results. Finally, a Determination Module furtherintegrates these two predictions and derives the final result. Extensiveexperiments on two publicly available datasets show the efficacy of ourproposed method, particularly in low-resource settings.</description><author>Ye Liu, Jiajun Zhu, Kai Zhang, Haoyu Tang, Yanghai Zhang, Xukai Liu, Qi Liu, Enhong Chen</author><pubDate>Fri, 12 Jul 2024 03:15:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08952v1</guid></item><item><title>DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding</title><link>http://arxiv.org/abs/2407.08801v1</link><description>Recent point cloud understanding research suffers from performance drops onunseen data, due to the distribution shifts across different domains. Whilerecent studies use Domain Generalization (DG) techniques to mitigate this bylearning domain-invariant features, most are designed for a single task andneglect the potential of testing data. Despite In-Context Learning (ICL)showcasing multi-task learning capability, it usually relies on high-qualitycontext-rich data and considers a single dataset, and has rarely been studiedin point cloud understanding. In this paper, we introduce a novel, practical,multi-domain multi-task setting, handling multiple domains and multiple taskswithin one unified model for domain generalized point cloud understanding. Tothis end, we propose Domain Generalized Point-In-Context Learning (DG-PIC) thatboosts the generalizability across various tasks and domains at testing time.In particular, we develop dual-level source prototype estimation that considersboth global-level shape contextual and local-level geometrical structures forrepresenting source domains and a dual-level test-time feature shiftingmechanism that leverages both macro-level domain semantic information andmicro-level patch positional relationships to pull the target data closer tothe source ones during the testing. Our DG-PIC does not require any modelupdates during the testing and can handle unseen domains and multiple tasks,\textit{i.e.,} point cloud reconstruction, denoising, and registration, withinone unified model. We also introduce a benchmark for this new setting.Comprehensive experiments demonstrate that DG-PIC outperforms state-of-the-arttechniques significantly.</description><author>Jincen Jiang, Qianyu Zhou, Yuhang Li, Xuequan Lu, Meili Wang, Lizhuang Ma, Jian Chang, Jian Jun Zhang</author><pubDate>Thu, 11 Jul 2024 18:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08801v1</guid></item><item><title>GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM</title><link>http://arxiv.org/abs/2407.10870v1</link><description>Large vision-language models (LVLMs), such as the Generative Pre-trainedTransformer 4-omni (GPT-4o), are emerging multi-modal foundation models whichhave great potential as powerful artificial-intelligence (AI) assistance toolsfor a myriad of applications, including healthcare, industrial, and academicsectors. Although such foundation models perform well in a wide range ofgeneral tasks, their capability without fine-tuning is often limited inspecialized tasks. However, full fine-tuning of large foundation models ischallenging due to enormous computation/memory/dataset requirements. We showthat GPT-4o can decode hand gestures from forearm ultrasound data even with nofine-tuning, and improves with few-shot, in-context learning.</description><author>Keshav Bimbraw, Ye Wang, Jing Liu, Toshiaki Koike-Akino</author><pubDate>Mon, 15 Jul 2024 16:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10870v1</guid></item><item><title>Learning to Generate Answers with Citations via Factual Consistency Models</title><link>http://arxiv.org/abs/2406.13124v2</link><description>Large Language Models (LLMs) frequently hallucinate, impeding theirreliability in mission-critical situations. One approach to address this issueis to provide citations to relevant sources alongside generated content,enhancing the verifiability of generations. However, citing passages accuratelyin answers remains a substantial challenge. This paper proposes aweakly-supervised fine-tuning method leveraging factual consistency models(FCMs). Our approach alternates between generating texts with citations andsupervised fine-tuning with FCM-filtered citation data. Focused learning isintegrated into the objective, directing the fine-tuning process to emphasisethe factual unit tokens, as measured by an FCM. Results on the ALCE few-shotcitation benchmark with various instruction-tuned LLMs demonstrate superiorperformance compared to in-context learning, vanilla supervised fine-tuning,and state-of-the-art methods, with an average improvement of $34.1$, $15.5$,and $10.5$ citation F$_1$ points, respectively. Moreover, in a domain transfersetting we show that the obtained citation generation ability robustlytransfers to unseen datasets. Notably, our citation improvements contribute tothe lowest factual error rate across baselines.</description><author>Rami Aly, Zhiqiang Tang, Samson Tan, George Karypis</author><pubDate>Mon, 15 Jul 2024 16:04:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13124v2</guid></item><item><title>Are Emergent Abilities in Large Language Models just In-Context Learning?</title><link>http://arxiv.org/abs/2309.01809v2</link><description>Large language models, comprising billions of parameters and pre-trained onextensive web-scale corpora, have been claimed to acquire certain capabilitieswithout having been specifically trained on them. These capabilities, referredto as "emergent abilities," have been a driving force in discussions regardingthe potentials and risks of language models. A key challenge in evaluatingemergent abilities is that they are confounded by model competencies that arisethrough alternative prompting techniques, including in-context learning, whichis the ability of models to complete a task based on a few examples. We presenta novel theory that explains emergent abilities, taking into account theirpotential confounding factors, and rigorously substantiate this theory throughover 1000 experiments. Our findings suggest that purported emergent abilitiesare not truly emergent, but result from a combination of in-context learning,model memory, and linguistic knowledge. Our work is a foundational step inexplaining language model performance, providing a template for their efficientuse and clarifying the paradox of their ability to excel in some instanceswhile faltering in others. Thus, we demonstrate that their capabilities shouldnot be overestimated.</description><author>Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych</author><pubDate>Mon, 15 Jul 2024 12:21:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01809v2</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v1</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny</author><pubDate>Tue, 16 Jul 2024 10:50:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v1</guid></item><item><title>Reasoning with Large Language Models, a Survey</title><link>http://arxiv.org/abs/2407.11511v1</link><description>Scaling up language models to billions of parameters has opened uppossibilities for in-context learning, allowing instruction tuning and few-shotlearning on tasks that the model was not specifically trained for. This hasachieved breakthrough performance on language tasks such as translation,summarization, and question-answering. Furthermore, in addition to theseassociative "System 1" tasks, recent advances in Chain-of-thought promptlearning have demonstrated strong "System 2" reasoning abilities, answering aquestion in the field of artificial general intelligence whether LLMs canreason. The field started with the question whether LLMs can solve grade schoolmath word problems. This paper reviews the rapidly expanding field ofprompt-based reasoning with LLMs. Our taxonomy identifies different ways togenerate, evaluate, and control multi-step reasoning. We provide an in-depthcoverage of core approaches and open problems, and we propose a research agendafor the near future. Finally, we highlight the relation between reasoning andprompt-based learning, and we discuss the relation between reasoning,sequential decision processes, and reinforcement learning. We find thatself-improvement, self-reflection, and some metacognitive abilities of thereasoning processes are possible through the judicious use of prompts. Trueself-improvement and self-reasoning, to go from reasoning with LLMs toreasoning by LLMs, remains future work.</description><author>Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</author><pubDate>Tue, 16 Jul 2024 08:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11511v1</guid></item><item><title>Cross-lingual QA: A Key to Unlocking In-context Cross-lingual Performance</title><link>http://arxiv.org/abs/2305.15233v3</link><description>Multilingual large language models (MLLMs) have demonstrated significantcross-lingual capabilities through in-context learning. Existing approachestypically construct monolingual in-context examples, either in the source ortarget language. However, translating entire in-context examples into thetarget language might compromise contextual integrity and be costly in the caseof long-context passages. To address this, we introduce Cross-lingual QA, across-lingual prompting method that translates only the question and answerparts, thus reducing translation costs. Experiments on four typologicallydiverse multilingual benchmarks show that Cross-lingual QA promptingeffectively stimulates models to elicit their cross-lingual knowledge,outperforming prior monolingual prompting approaches. Furthermore, we show thatprompting open-source MLLMs with cross-lingual in-context examples enhancesperformance as the model scale increases.</description><author>Sunkyoung Kim, Dayeon Ki, Yireun Kim, Jinsik Lee</author><pubDate>Tue, 16 Jul 2024 08:18:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15233v3</guid></item><item><title>Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of Few-Shot Learning</title><link>http://arxiv.org/abs/2407.12498v1</link><description>The linguistic capabilities of Multimodal Large Language Models (MLLMs) arecritical for their effective application across diverse tasks. This study aimsto evaluate the performance of MLLMs on the VALSE benchmark, focusing on theefficacy of few-shot In-Context Learning (ICL), and Chain-of-Thought (CoT)prompting. We conducted a comprehensive assessment of state-of-the-art MLLMs,varying in model size and pretraining datasets. The experimental results revealthat ICL and CoT prompting significantly boost model performance, particularlyin tasks requiring complex reasoning and contextual understanding. Modelspretrained on captioning datasets show superior zero-shot performance, whilethose trained on interleaved image-text data benefit from few-shot learning.Our findings provide valuable insights into optimizing MLLMs for bettergrounding of language in visual contexts, highlighting the importance of thecomposition of pretraining data and the potential of few-shot learningstrategies to improve the reasoning abilities of MLLMs.</description><author>Mustafa Dogan, Ilker Kesen, Iacer Calixto, Aykut Erdem, Erkut Erdem</author><pubDate>Wed, 17 Jul 2024 11:26:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12498v1</guid></item><item><title>Towards Multimodal In-Context Learning for Vision &amp; Language Models</title><link>http://arxiv.org/abs/2403.12736v2</link><description>State-of-the-art Vision-Language Models (VLMs) ground the vision and thelanguage modality primarily via projecting the vision tokens from the encoderto language-like tokens, which are directly fed to the Large Language Model(LLM) decoder. While these models have shown unprecedented performance in manydownstream zero-shot tasks (eg image captioning, question answers, etc), stilllittle emphasis has been put on transferring one of the core LLM capability ofIn-Context Learning (ICL). ICL is the ability of a model to reason about adownstream task with a few examples demonstrations embedded in the prompt. Inthis work, through extensive evaluations, we find that the state-of-the-artVLMs somewhat lack the ability to follow ICL instructions. In particular, wediscover that even models that underwent large-scale mixed modalitypre-training and were implicitly guided to make use of interleaved image andtext information (intended to consume helpful context from multiple images)under-perform when prompted with few-shot demonstrations (in an ICL way),likely due to their lack of direct ICL instruction tuning. To enhance the ICLabilities of the present VLM, we propose a simple yet surprisingly effectivemulti-turn curriculum-based learning methodology with effective data mixes,leading up to a significant 21.03% (and 11.3% on average) ICL performance boostover the strongest VLM baselines and a variety of ICL benchmarks. Furthermore,we also contribute new benchmarks for ICL evaluation in VLMs and discuss theiradvantages over the prior art.</description><author>Sivan Doveh, Shaked Perek, M. Jehanzeb Mirza, Wei Lin, Amit Alfassy, Assaf Arbelle, Shimon Ullman, Leonid Karlinsky</author><pubDate>Wed, 17 Jul 2024 08:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12736v2</guid></item><item><title>When can transformers compositionally generalize in-context?</title><link>http://arxiv.org/abs/2407.12275v1</link><description>Many tasks can be composed from a few independent components. This gives riseto a combinatorial explosion of possible tasks, only some of which might beencountered during training. Under what circumstances can transformerscompositionally generalize from a subset of tasks to all possible combinationsof tasks that share similar components? Here we study a modular multitasksetting that allows us to precisely control compositional structure in the datageneration process. We present evidence that transformers learning in-contextstruggle to generalize compositionally on this task despite being in principleexpressive enough to do so. Compositional generalization becomes possible onlywhen introducing a bottleneck that enforces an explicit separation between taskinference and task execution.</description><author>Seijin Kobayashi, Simon Schug, Yassir Akram, Florian Redhardt, Johannes von Oswald, Razvan Pascanu, Guillaume Lajoie, João Sacramento</author><pubDate>Wed, 17 Jul 2024 02:49:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12275v1</guid></item><item><title>Large Language Models as Reliable Knowledge Bases?</title><link>http://arxiv.org/abs/2407.13578v1</link><description>The NLP community has recently shown a growing interest in leveraging LargeLanguage Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potentialknowledge bases (KBs). However, the reliability and extent to which LLMs canfunction as KBs remain underexplored. While previous studies suggest LLMs canencode knowledge within their parameters, the amount of parametric knowledgealone is not sufficient to evaluate their effectiveness as KBs. This studydefines criteria that a reliable LLM-as-KB should meet, focusing on factualityand consistency, and covering both seen and unseen knowledge. We developseveral metrics based on these criteria and use them to evaluate 26 popularLLMs, while providing a comprehensive analysis of the effects of model size,instruction tuning, and in-context learning (ICL). Our results paint a worryingpicture. Even a high-performant model like GPT-3.5-turbo is not factual orconsistent, and strategies like ICL and fine-tuning are unsuccessful at makingLLMs better KBs.</description><author>Danna Zheng, Mirella Lapata, Jeff Z. Pan</author><pubDate>Thu, 18 Jul 2024 15:20:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13578v1</guid></item><item><title>Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks</title><link>http://arxiv.org/abs/2407.13511v1</link><description>Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPTand Anthropic's Claude 3 Opus, have dominated natural language processing (NLP)benchmarks across different domains. New competing Open-Source alternativeslike Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap whileoften offering higher throughput and being less costly to use. Open-Source LLMscan also be self-hosted, which makes them interesting for enterprise andclinical use cases where sensitive data should not be processed by thirdparties. We participated in the 12th BioASQ challenge, which is a retrievalaugmented generation (RAG) setting, and explored the performance of current GPTmodels Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning(zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additionalrelevant knowledge from Wikipedia added to the context-window of the LLM mightimprove their performance. Mixtral 8x7b was competitive in the 10-shot setting,both with and without fine-tuning, but failed to produce usable results in thezero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead tomeasurable performance gains. Our results indicate that the performance gapbetween commercial and open-source models in RAG setups exists mainly in thezero-shot setting and can be closed by simply collecting few-shot examples fordomain-specific use cases. The code needed to rerun these experiments isavailable through GitHub.</description><author>Samy Ateia, Udo Kruschwitz</author><pubDate>Thu, 18 Jul 2024 13:43:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13511v1</guid></item><item><title>PALM: Predicting Actions through Language Models</title><link>http://arxiv.org/abs/2311.17944v2</link><description>Understanding human activity is a crucial yet intricate task in egocentricvision, a field that focuses on capturing visual perspectives from the camerawearer's viewpoint. Traditional methods heavily rely on representation learningthat is trained on a large amount of video data. However, a major challengearises from the difficulty of obtaining effective video representation. Thisdifficulty stems from the complex and variable nature of human activities,which contrasts with the limited availability of data. In this study, weintroduce PALM, an approach that tackles the task of long-term actionanticipation, which aims to forecast forthcoming sequences of actions over anextended period. Our method PALM incorporates an action recognition model totrack previous action sequences and a vision-language model to articulaterelevant environmental details. By leveraging the context provided by thesepast events, we devise a prompting strategy for action anticipation using largelanguage models (LLMs). Moreover, we implement maximal marginal relevance forexample selection to facilitate in-context learning of the LLMs. Ourexperimental results demonstrate that PALM surpasses the state-of-the-artmethods in the task of long-term action anticipation on the Ego4D benchmark. Wefurther validate PALM on two additional benchmarks, affirming its capacity forgeneralization across intricate activities with different sets of taxonomies.</description><author>Sanghwan Kim, Daoji Huang, Yongqin Xian, Otmar Hilliges, Luc Van Gool, Xi Wang</author><pubDate>Thu, 18 Jul 2024 10:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17944v2</guid></item><item><title>Learning-From-Mistakes Prompting for Indigenous Language Translation</title><link>http://arxiv.org/abs/2407.13343v1</link><description>Using large language models, this paper presents techniques to improveextremely low-resourced indigenous language translations. Our approaches aregrounded in the use of (1) the presence of a datastore consisting of a limitednumber of parallel translation examples, (2) the inherent capabilities of LLMslike GPT-3.5, and (3) a word-level translation dictionary. We harness thepotential of LLMs and in-context learning techniques in such a setting forusing LLMs as universal translators for extremely low-resourced languages. Ourmethodology hinges on utilizing LLMs as language compilers for selectedlanguage pairs, hypothesizing that they could internalize syntactic structuresto facilitate accurate translation. We introduce three techniques: KNNPromptingwith Retrieved Prompting Context, Chain-of-Thought Prompting andLearningfrom-Mistakes Prompting, with the last method addressing past errors.The evaluation results suggest that, even with limited corpora, LLMs caneffectively translate extremely low-resource languages when paired with properprompting.</description><author>You-Cheng Liao, Chen-Jui Yu, Chi-Yi Lin, He-Feng Yun, Yen-Hsiang Wang, Hsiao-Min Li, Yao-Chung Fan</author><pubDate>Thu, 18 Jul 2024 09:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13343v1</guid></item><item><title>SpeciaLex: A Benchmark for In-Context Specialized Lexicon Learning</title><link>http://arxiv.org/abs/2407.13297v1</link><description>Specialized lexicons are collections of words with associated constraintssuch as special definitions, specific roles, and intended target audiences.These constraints are necessary for content generation and documentation tasks(e.g., writing technical manuals or children's books), where the goal is toreduce the ambiguity of text content and increase its overall readability for aspecific group of audience. Understanding how large language models can capturethese constraints can help researchers build better, more impactful tools forwider use beyond the NLP community. Towards this end, we introduce SpeciaLex, abenchmark for evaluating a language model's ability to follow specializedlexicon-based constraints across 18 diverse subtasks with 1,285 test instancescovering core tasks of Checking, Identification, Rewriting, and OpenGeneration. We present an empirical evaluation of 15 open and closed-sourceLLMs and discuss insights on how factors such as model scale, openness, setup,and recency affect performance upon evaluating with the benchmark.</description><author>Joseph Marvin Imperial, Harish Tayyar Madabushi</author><pubDate>Thu, 18 Jul 2024 08:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13297v1</guid></item><item><title>QuRating: Selecting High-Quality Data for Training Language Models</title><link>http://arxiv.org/abs/2402.09739v3</link><description>Selecting high-quality pre-training data is important for creating capablelanguage models, but existing methods rely on simple heuristics. We introduceQuRating, a method for selecting pre-training data that can capture humanintuitions about data quality. In this paper, we investigate four qualities -writing style, required expertise, facts &amp; trivia, and educational value - andfind that LLMs are able to discern these qualities, especially when makingpairwise judgments of texts. We train a QuRater model to learn scalar ratingsfrom pairwise judgments, and use it to annotate a 260B training corpus withquality ratings for each of the four criteria. In our experiments, we select30B tokens according to the different quality ratings and train 1.3B-parameterlanguage models on the selected data. We find that it is important to balancequality and diversity. When we sample using quality ratings as logits overdocuments, our models obtain lower perplexity and stronger in-context learningperformance than baselines. Our best model is based on educational value andperforms similarly to a model trained with uniform sampling for 50% more steps.Beyond data selection, we use the quality ratings to construct a trainingcurriculum which improves performance without changing the training dataset. Weextensively analyze the quality ratings and discuss their characteristics,biases, and wider implications.</description><author>Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen</author><pubDate>Wed, 17 Jul 2024 20:50:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09739v3</guid></item><item><title>R+X: Retrieval and Execution from Everyday Human Videos</title><link>http://arxiv.org/abs/2407.12957v1</link><description>We present R+X, a framework which enables robots to learn skills from long,unlabelled, first-person videos of humans performing everyday tasks. Given alanguage command from a human, R+X first retrieves short video clips containingrelevant behaviour, and then executes the skill by conditioning an in-contextimitation learning method on this behaviour. By leveraging a Vision LanguageModel (VLM) for retrieval, R+X does not require any manual annotation of thevideos, and by leveraging in-context learning for execution, robots can performcommanded skills immediately, without requiring a period of training on theretrieved videos. Experiments studying a range of everyday household tasks showthat R+X succeeds at translating unlabelled human videos into robust robotskills, and that R+X outperforms several recent alternative methods. Videos areavailable at https://www.robot-learning.uk/r-plus-x.</description><author>Georgios Papagiannis, Norman Di Palo, Pietro Vitiello, Edward Johns</author><pubDate>Wed, 17 Jul 2024 18:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12957v1</guid></item><item><title>HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems in Context</title><link>http://arxiv.org/abs/2407.09375v2</link><description>This work explores the in-context learning capabilities of State Space Models(SSMs) and presents, to the best of our knowledge, the first theoreticalexplanation of a possible underlying mechanism. We introduce a novel weightconstruction for SSMs, enabling them to predict the next state of any dynamicalsystem after observing previous states without parameter fine-tuning. This isaccomplished by extending the HiPPO framework to demonstrate that continuousSSMs can approximate the derivative of any input signal. Specifically, we findan explicit weight construction for continuous SSMs and provide an asymptoticerror bound on the derivative approximation. The discretization of thiscontinuous SSM subsequently yields a discrete SSM that predicts the next state.Finally, we demonstrate the effectiveness of our parameterization empirically.This work should be an initial step toward understanding how sequence modelsbased on SSMs learn in context.</description><author>Federico Arangath Joseph, Kilian Konstantin Haefeli, Noah Liniger, Caglar Gulcehre</author><pubDate>Fri, 19 Jul 2024 15:34:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09375v2</guid></item><item><title>Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer</title><link>http://arxiv.org/abs/2309.07566v2</link><description>Direct speech-to-speech translation (S2ST) with discrete self-supervisedrepresentations has achieved remarkable accuracy, but is unable to preserve thespeaker timbre of the source speech. Meanwhile, the scarcity of high-qualityspeaker-parallel data poses a challenge for learning style transfer duringtranslation. We design an S2ST pipeline with style-transfer capability on thebasis of discrete self-supervised speech representations and codec units. Theacoustic language model we introduce for style transfer leveragesself-supervised in-context learning, acquiring style transfer ability withoutrelying on any speaker-parallel data, thereby overcoming data scarcity. Byusing extensive training data, our model achieves zero-shot cross-lingual styletransfer on previously unseen source languages. Experiments show that our modelgenerates translated speeches with high fidelity and speaker similarity. Audiosamples are available at http://stylelm.github.io/ .</description><author>Yongqi Wang, Jionghao Bai, Rongjie Huang, Ruiqi Li, Zhiqing Hong, Zhou Zhao</author><pubDate>Fri, 19 Jul 2024 12:11:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07566v2</guid></item><item><title>Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability</title><link>http://arxiv.org/abs/2407.15720v1</link><description>Large language models (LLMs) have emerged as powerful tools for many AIproblems and exhibit remarkable in-context learning (ICL) capabilities.Compositional ability, solving unseen complex tasks that combine two or moresimple tasks, is an essential reasoning ability for Artificial GeneralIntelligence. Despite LLM's tremendous success, how they approach compositetasks, especially those not encountered during the pretraining phase, remainsan open question and largely ununderstood. In this study, we delve into the ICLcapabilities of LLMs on composite tasks, with only simple tasks as in-contextexamples. We develop a test suite of composite tasks that include linguisticand logical challenges and perform empirical studies across different LLMfamilies. We observe that models exhibit divergent behaviors: (1) For simplercomposite tasks that apply distinct mapping mechanisms to different inputsegments, the models demonstrate decent compositional ability, while scaling upthe model enhances this ability; (2) for more complex composite tasks thatinvolving reasoning multiple steps, where each step represent one task, modelstypically underperform, and scaling up generally provide no improvements. Weoffer theoretical analysis in a simplified setting, explaining that modelsexhibit compositional capability when the task handles different input partsseparately. We believe our work sheds new light on the capabilities of LLMs insolving composite tasks regarding the nature of the tasks and model scale. Ourdataset and code are available at{\url{https://github.com/OliverXUZY/LLM_Compose}}.</description><author>Zhuoyan Xu, Zhenmei Shi, Yingyu Liang</author><pubDate>Mon, 22 Jul 2024 15:22:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15720v1</guid></item><item><title>SEGIC: Unleashing the Emergent Correspondence for In-Context Segmentation</title><link>http://arxiv.org/abs/2311.14671v3</link><description>In-context segmentation aims at segmenting novel images using a few labeledexample images, termed as "in-context examples", exploring content similaritiesbetween examples and the target. The resulting models can be generalizedseamlessly to novel segmentation tasks, significantly reducing the labeling andtraining costs compared with conventional pipelines. However, in-contextsegmentation is more challenging than classic ones requiring the model to learnsegmentation rules conditioned on a few samples. Unlike previous work withad-hoc or non-end-to-end designs, we propose SEGIC, an end-to-endsegment-in-context framework built upon a single vision foundation model (VFM).In particular, SEGIC leverages the emergent correspondence within VFM tocapture dense relationships between target images and in-context samples. Assuch, information from in-context samples is then extracted into three types ofinstructions, i.e. geometric, visual, and meta instructions, serving asexplicit conditions for the final mask prediction. SEGIC is a straightforwardyet effective approach that yields state-of-the-art performance on one-shotsegmentation benchmarks. Notably, SEGIC can be easily generalized to diversetasks, including video object segmentation and open-vocabulary segmentation.Code will be available at https://github.com/MengLcool/SEGIC.</description><author>Lingchen Meng, Shiyi Lan, Hengduo Li, Jose M. Alvarez, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Mon, 22 Jul 2024 15:16:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.14671v3</guid></item><item><title>AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization</title><link>http://arxiv.org/abs/2407.11591v2</link><description>Despite the advances in the abstractive summarization task using LargeLanguage Models (LLM), there is a lack of research that asses their abilitiesto easily adapt to different domains. We evaluate the domain adaptationabilities of a wide range of LLMs on the summarization task across variousdomains in both fine-tuning and in-context learning settings. We also presentAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes adomain benchmark and a set of metrics to facilitate the analysis of domainadaptation. Our results demonstrate that LLMs exhibit comparable performance inthe in-context learning setting, regardless of their parameter scale.</description><author>Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell</author><pubDate>Mon, 22 Jul 2024 13:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11591v2</guid></item><item><title>Can Large Language Models Automatically Jailbreak GPT-4V?</title><link>http://arxiv.org/abs/2407.16686v1</link><description>GPT-4V has attracted considerable attention due to its extraordinary capacityfor integrating and processing multimodal information. At the same time, itsability of face recognition raises new safety concerns of privacy leakage.Despite researchers' efforts in safety alignment through RLHF or preprocessingfilters, vulnerabilities might still be exploited. In our study, we introduceAutoJailbreak, an innovative automatic jailbreak technique inspired by promptoptimization. We leverage Large Language Models (LLMs) for red-teaming torefine the jailbreak prompt and employ weak-to-strong in-context learningprompts to boost efficiency. Furthermore, we present an effective search methodthat incorporates early stopping to minimize optimization time and tokenexpenditure. Our experiments demonstrate that AutoJailbreak significantlysurpasses conventional methods, achieving an Attack Success Rate (ASR)exceeding 95.3\%. This research sheds light on strengthening GPT-4V security,underscoring the potential for LLMs to be exploited in compromising GPT-4Vintegrity.</description><author>Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun</author><pubDate>Tue, 23 Jul 2024 17:50:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16686v1</guid></item><item><title>Interpretable Machine Learning for TabPFN</title><link>http://arxiv.org/abs/2403.10923v2</link><description>The recently developed Prior-Data Fitted Networks (PFNs) have shown verypromising results for applications in low-data regimes. The TabPFN model, aspecial case of PFNs for tabular data, is able to achieve state-of-the-artperformance on a variety of classification tasks while producing posteriorpredictive distributions in mere seconds by in-context learning without theneed for learning parameters or hyperparameter tuning. This makes TabPFN a veryattractive option for a wide range of domain applications. However, a majordrawback of the method is its lack of interpretability. Therefore, we proposeseveral adaptations of popular interpretability methods that we specificallydesign for TabPFN. By taking advantage of the unique properties of the model,our adaptations allow for more efficient computations than existingimplementations. In particular, we show how in-context learning facilitates theestimation of Shapley values by avoiding approximate retraining and enables theuse of Leave-One-Covariate-Out (LOCO) even when working with large-scaleTransformers. In addition, we demonstrate how data valuation methods can beused to address scalability challenges of TabPFN. Our proposed methods areimplemented in a package tabpfn_iml and made available athttps://github.com/david-rundel/tabpfn_iml.</description><author>David Rundel, Julius Kobialka, Constantin von Crailsheim, Matthias Feurer, Thomas Nagler, David Rügamer</author><pubDate>Tue, 23 Jul 2024 16:10:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10923v2</guid></item><item><title>Assessing In-context Learning and Fine-tuning for Topic Classification of German Web Data</title><link>http://arxiv.org/abs/2407.16516v1</link><description>Researchers in the political and social sciences often rely on classificationmodels to analyze trends in information consumption by examining browsinghistories of millions of webpages. Automated scalable methods are necessary dueto the impracticality of manual labeling. In this paper, we model the detectionof topic-related content as a binary classification task and compare theaccuracy of fine-tuned pre-trained encoder models against in-context learningstrategies. Using only a few hundred annotated data points per topic, we detectcontent related to three German policies in a database of scraped webpages. Wecompare multilingual and monolingual models, as well as zero and few-shotapproaches, and investigate the impact of negative sampling strategies and thecombination of URL &amp; content-based features. Our results show that a smallsample of annotated data is sufficient to train an effective classifier.Fine-tuning encoder-based models yields better results than in-contextlearning. Classifiers using both URL &amp; content-based features perform best,while using URLs alone provides adequate results when content is unavailable.</description><author>Julian Schelb, Roberto Ulloa, Andreas Spitz</author><pubDate>Tue, 23 Jul 2024 14:31:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16516v1</guid></item><item><title>Does In-Context Learning Really Learn? Rethinking How Large Language Models Respond and Solve Tasks via In-Context Learning</title><link>http://arxiv.org/abs/2404.07546v2</link><description>In-context Learning (ICL) has emerged as a powerful capability alongside thedevelopment of scaled-up large language models (LLMs). By instructing LLMsusing few-shot demonstrative examples, ICL enables them to perform a wide rangeof tasks without updating millions of parameters. However, the precisecontributions of demonstrations towards improving end-task performance have notbeen thoroughly investigated in recent analytical studies. In this paper, weempirically decompose the overall performance of ICL into three dimensions,label space, format, and discrimination, and we evaluate four general-purposeLLMs across a diverse range of tasks. Counter-intuitively, we find that thedemonstrations have a marginal impact on provoking discriminative knowledge oflanguage models. However, ICL exhibits significant efficacy in regulating thelabel space and format, which helps LLMs respond to desired label words. Wethen demonstrate that this ability functions similar to detailed instructionsfor LLMs to follow. We additionally provide an in-depth analysis of themechanism of retrieval helping with ICL. Our findings demonstrate thatretrieving the semantically similar examples notably boosts the model'sdiscriminative capability. However, we also observe a trade-off in selectinggood in-context examples regarding label diversity.</description><author>Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan</author><pubDate>Tue, 23 Jul 2024 12:28:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07546v2</guid></item><item><title>Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction</title><link>http://arxiv.org/abs/2407.16370v1</link><description>Building upon the strength of modern large language models (LLMs), generativeerror correction (GEC) has emerged as a promising paradigm that can elevate theperformance of modern automatic speech recognition (ASR) systems. Onerepresentative approach is to leverage in-context learning to prompt LLMs sothat a better hypothesis can be generated by the LLMs based on acarefully-designed prompt and an $N$-best list of hypotheses produced by ASRsystems. However, it is yet unknown whether the existing prompts are the mosteffective ones for the task of post-ASR error correction. In this context, thispaper first explores alternative prompts to identify an initial set ofeffective prompts, and then proposes to employ an evolutionary promptoptimization algorithm to refine the initial prompts. Evaluations results onthe CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show theeffectiveness and potential of the proposed algorithms.</description><author>Rithik Sachdev, Zhong-Qiu Wang, Chao-Han Huck Yang</author><pubDate>Tue, 23 Jul 2024 10:38:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16370v1</guid></item><item><title>PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing</title><link>http://arxiv.org/abs/2407.16318v1</link><description>Deploying language models (LMs) necessitates outputs to be both high-qualityand compliant with safety guidelines. Although Inference-Time Guardrails (ITG)offer solutions that shift model output distributions towards compliance, wefind that current methods struggle in balancing safety with helpfulness. ITGMethods that safely address non-compliant queries exhibit lower helpfulnesswhile those that prioritize helpfulness compromise on safety. We refer to thistrade-off as the guardrail tax, analogous to the alignment tax. To addressthis, we propose PrimeGuard, a novel ITG method that utilizes structuredcontrol flow. PrimeGuard routes requests to different self-instantiations of the LM withvarying instructions, leveraging its inherent instruction-followingcapabilities and in-context learning. Our tuning-free approach dynamicallycompiles system-designer guidelines for each query. We construct and releasesafe-eval, a diverse red-team safety benchmark. Extensive evaluationsdemonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail taxby (1) significantly increasing resistance to iterative jailbreak attacks and(2) achieving state-of-the-art results in safety guardrailing while (3)matching helpfulness scores of alignment-tuned models. Extensive evaluationsdemonstrate that PrimeGuard, without fine-tuning, outperforms all competingbaselines and overcomes the guardrail tax by improving the fraction of saferesponses from 61% to 97% and increasing average helpfulness scores from 4.17to 4.29 on the largest models, while reducing attack success rate from 100% to8%. PrimeGuard implementation is available athttps://github.com/dynamofl/PrimeGuard and safe-eval dataset is available athttps://huggingface.co/datasets/dynamoai/safe_eval.</description><author>Blazej Manczak, Eliott Zemour, Eric Lin, Vaikkunth Mugunthan</author><pubDate>Tue, 23 Jul 2024 09:14:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16318v1</guid></item><item><title>Grammar-based Game Description Generation using Large Language Models</title><link>http://arxiv.org/abs/2407.17404v1</link><description>To lower the barriers to game design development, automated game design,which generates game designs through computational processes, has beenexplored. In automated game design, machine learning-based techniques such asevolutionary algorithms have achieved success. Benefiting from the remarkableadvancements in deep learning, applications in computer vision and naturallanguage processing have progressed in level generation. However, due to thelimited amount of data in game design, the application of deep learning hasbeen insufficient for tasks such as game description generation. To pioneer anew approach for handling limited data in automated game design, we focus onthe in-context learning of large language models (LLMs). LLMs can capture thefeatures of a task from a few demonstration examples and apply the capabilitiesacquired during pre-training. We introduce the grammar of game descriptions,which effectively structures the game design space, into the LLMs' reasoningprocess. Grammar helps LLMs capture the characteristics of the complex task ofgame description generation. Furthermore, we propose a decoding method thatiteratively improves the generated output by leveraging the grammar. Ourexperiments demonstrate that this approach performs well in generating gamedescriptions.</description><author>Tsunehiko Tanaka, Edgar Simo-Serra</author><pubDate>Wed, 24 Jul 2024 16:36:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17404v1</guid></item><item><title>Identifying Semantic Induction Heads to Understand In-Context Learning</title><link>http://arxiv.org/abs/2402.13055v2</link><description>Although large language models (LLMs) have demonstrated remarkableperformance, the lack of transparency in their inference logic raises concernsabout their trustworthiness. To gain a better understanding of LLMs, we conducta detailed analysis of the operations of attention heads and aim to betterunderstand the in-context learning of LLMs. Specifically, we investigatewhether attention heads encode two types of relationships between tokenspresent in natural languages: the syntactic dependency parsed from sentencesand the relation within knowledge graphs. We find that certain attention headsexhibit a pattern where, when attending to head tokens, they recall tail tokensand increase the output logits of those tail tokens. More crucially, theformulation of such semantic induction heads has a close correlation with theemergence of the in-context learning ability of language models. The study ofsemantic attention heads advances our understanding of the intricate operationsof attention heads in transformers, and further provides new insights into thein-context learning of LLMs.</description><author>Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Quanshi Zhang, Xipeng Qiu, Dahua Lin</author><pubDate>Thu, 25 Jul 2024 08:07:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13055v2</guid></item><item><title>Unified Lexical Representation for Interpretable Visual-Language Alignment</title><link>http://arxiv.org/abs/2407.17827v1</link><description>Visual-Language Alignment (VLA) has gained a lot of attention since CLIP'sgroundbreaking work. Although CLIP performs well, the typical direct latentfeature alignment lacks clarity in its representation and similarity scores. Onthe other hand, lexical representation, a vector whose element represents thesimilarity between the sample and a word from the vocabulary, is a naturalsparse representation and interpretable, providing exact matches for individualwords. However, lexical representations is difficult to learn due to noground-truth supervision and false-discovery issues, and thus requires complexdesign to train effectively. In this paper, we introduce LexVLA, a moreinterpretable VLA framework by learning a unified lexical representation forboth modalities without complex design. We use DINOv2 as our visual model forits local-inclined features and Llama 2, a generative language model, toleverage its in-context lexical prediction ability. To avoid the falsediscovery, we propose an overuse penalty to refrain the lexical representationfrom falsely frequently activating meaningless words. We demonstrate that thesetwo pre-trained uni-modal models can be well-aligned by fine-tuning on modestmulti-modal dataset and avoid intricate training configurations. On cross-modalretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and thosetrained from scratch on even bigger datasets (e.g., 1.1B data, includingCC-12M). We conduct extensive experiments to analyze LexVLA.</description><author>Yifan Li, Yikai Wang, Yanwei Fu, Dongyu Ru, Zheng Zhang, Tong He</author><pubDate>Thu, 25 Jul 2024 07:35:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17827v1</guid></item><item><title>Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples</title><link>http://arxiv.org/abs/2402.07386v2</link><description>Automatic taxonomy induction is crucial for web search, recommendationsystems, and question answering. Manual curation of taxonomies is expensive interms of human effort, making automatic taxonomy construction highly desirable.In this work, we introduce Chain-of-Layer which is an in-context learningframework designed to induct taxonomies from a given set of entities.Chain-of-Layer breaks down the task into selecting relevant candidate entitiesin each layer and gradually building the taxonomy from top to bottom. Tominimize errors, we introduce the Ensemble-based Ranking Filter to reduce thehallucinated content generated at each iteration. Through extensiveexperiments, we demonstrate that Chain-of-Layer achieves state-of-the-artperformance on four real-world benchmarks.</description><author>Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang, Zhihan Zhang, Meng Jiang</author><pubDate>Thu, 25 Jul 2024 02:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07386v2</guid></item><item><title>Transformers on Markov Data: Constant Depth Suffices</title><link>http://arxiv.org/abs/2407.17686v1</link><description>Attention-based transformers have been remarkably successful at modelinggenerative processes across various domains and modalities. In this paper, westudy the behavior of transformers on data drawn from \kth Markov processes,where the conditional distribution of the next symbol in a sequence depends onthe previous $k$ symbols observed. We observe a surprising phenomenonempirically which contradicts previous findings: when trained for sufficientlylong, a transformer with a fixed depth and $1$ head per layer is able toachieve low test loss on sequences drawn from \kth Markov sources, even as $k$grows. Furthermore, this low test loss is achieved by the transformer's abilityto represent and learn the in-context conditional empirical distribution. Onthe theoretical side, our main result is that a transformer with a single headand three layers can represent the in-context conditional empiricaldistribution for \kth Markov sources, concurring with our empiricalobservations. Along the way, we prove that \textit{attention-only} transformerswith $O(\log_2(k))$ layers can represent the in-context conditional empiricaldistribution by composing induction heads to track the previous $k$ symbols inthe sequence. These results provide more insight into our current understandingof the mechanisms by which transformers learn to capture context, byunderstanding their behavior on Markov sources.</description><author>Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar, Ashok Vardhan Makkuva</author><pubDate>Thu, 25 Jul 2024 01:07:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17686v1</guid></item><item><title>Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements</title><link>http://arxiv.org/abs/2310.05140v4</link><description>Empathetic dialogue is an indispensable part of building harmonious socialrelationships and contributes to the development of a helpful AI. Previousapproaches are mainly based on fine small-scale language models. With theadvent of ChatGPT, the application effect of large language models (LLMs) inthis field has attracted great attention. This work empirically investigatesthe performance of LLMs in generating empathetic responses and proposes threeimprovement methods of semantically similar in-context learning, two-stageinteractive generation, and combination with the knowledge base. Extensiveexperiments show that LLMs can significantly benefit from our proposed methodsand is able to achieve state-of-the-art performance in both automatic and humanevaluations. Additionally, we explore the possibility of GPT-4 simulating humanevaluators.</description><author>Yushan Qian, Wei-Nan Zhang, Ting Liu</author><pubDate>Fri, 26 Jul 2024 15:07:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05140v4</guid></item><item><title>TabMDA: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting</title><link>http://arxiv.org/abs/2406.01805v2</link><description>Tabular data is prevalent in many critical domains, yet it is oftenchallenging to acquire in large quantities. This scarcity usually results inpoor performance of machine learning models on such data. Data augmentation, acommon strategy for performance improvement in vision and language tasks,typically underperforms for tabular data due to the lack of explicit symmetriesin the input space. To overcome this challenge, we introduce TabMDA, a novelmethod for manifold data augmentation on tabular data. This method utilises apre-trained in-context model, such as TabPFN, to map the data into an embeddingspace. TabMDA performs label-invariant transformations by encoding the datamultiple times with varied contexts. This process explores the learnedembedding space of the underlying in-context models, thereby enlarging thetraining dataset. TabMDA is a training-free method, making it applicable to anyclassifier. We evaluate TabMDA on five standard classifiers and observesignificant performance improvements across various tabular datasets. Ourresults demonstrate that TabMDA provides an effective way to leverageinformation from pre-trained in-context models to enhance the performance ofdownstream classifiers. Code is available athttps://github.com/AdrianBZG/TabMDA.</description><author>Andrei Margeloiu, Adrián Bazaga, Nikola Simidjievski, Pietro Liò, Mateja Jamnik</author><pubDate>Mon, 29 Jul 2024 15:08:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.01805v2</guid></item><item><title>MICL: Improving In-Context Learning through Multiple-Label Words in Demonstration</title><link>http://arxiv.org/abs/2406.10908v2</link><description>In-context learning (ICL) enables large language models (LLMs) to perform newtasks by using sample-label pairs as demonstrations. However, variations indemonstrations can lead to significantly different performances. Currentresearch mainly focuses on selecting demonstration samples, preassuming theclass name to be the label word when creating sample-label pairs. However, thechoice of label words is crucial for ICL performance. In addition, we observethat using a single class name in demonstration may not yield optimal results.In this paper, we propose to use multiple label words in one sample-label pairto enhance ICL performance. Further, we select and order sample-label pairsbased on LLM's output distribution, aiming to optimize the demonstrationexamples from both the samples' and labels' perspectives. Evaluation results onseven classification datasets show that the use of multiple label words,strategically organized by their selection, order and quantity, improves ICLperformance through diverse label information.</description><author>Zhu Zixiao, Feng Zijian, Zhou Hanzhang, Qian Junlang, Mao Kezhi</author><pubDate>Mon, 29 Jul 2024 13:05:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10908v2</guid></item><item><title>AgEval: A Benchmark for Zero-Shot and Few-Shot Plant Stress Phenotyping with Multimodal LLMs</title><link>http://arxiv.org/abs/2407.19617v1</link><description>Plant stress phenotyping traditionally relies on expert assessments andspecialized models, limiting scalability in agriculture. Recent advances inmultimodal large language models (LLMs) offer potential solutions to thischallenge. We present AgEval, a benchmark comprising 12 diverse plant stressphenotyping tasks, to evaluate these models' capabilities. Our study assesseszero-shot and few-shot in-context learning performance of state-of-the-artmodels, including Claude, GPT, Gemini, and LLaVA. Results show significantperformance improvements with few-shot learning, with F1 scores increasing from46.24% to 73.37% in 8-shot identification for the best-performing model.Few-shot examples from other classes in the dataset have negligible or negativeimpacts, although having the exact category example helps to increaseperformance by 15.38%. We also quantify the consistency of model performanceacross different classes within each task, finding that the coefficient ofvariance (CV) ranges from 26.02% to 58.03% across models, implying that subjectmatter expertise is needed - of 'difficult' classes - to achieve reliability inperformance. AgEval establishes baseline metrics for multimodal LLMs inagricultural applications, offering insights into their promise for enhancingplant stress phenotyping at scale. Benchmark and code can be accessed at:https://anonymous.4open.science/r/AgEval/</description><author>Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar</author><pubDate>Mon, 29 Jul 2024 00:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19617v1</guid></item><item><title>Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment</title><link>http://arxiv.org/abs/2407.19346v1</link><description>Simple function classes have emerged as toy problems to better understandin-context-learning in transformer-based architectures used for large languagemodels. But previously proposed simple function classes like linear regressionor multi-layer-perceptrons lack the structure required to explore things likeprompting and alignment within models capable of in-context-learning. Wepropose univariate polynomial regression as a function class that is just richenough to study prompting and alignment, while allowing us to visualize andunderstand what is going on clearly.</description><author>Max Wilcoxson, Morten Svendgård, Ria Doshi, Dylan Davis, Reya Vir, Anant Sahai</author><pubDate>Sat, 27 Jul 2024 22:00:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19346v1</guid></item><item><title>Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications</title><link>http://arxiv.org/abs/2407.19262v1</link><description>Understanding whether and to what extent large language models (LLMs) havememorised training data has important implications for the reliability of theiroutput and the privacy of their training data. In order to cleanly measure anddisentangle memorisation from other phenomena (e.g. in-context learning), wecreate an experimental framework that is based on repeatedly exposing LLMs torandom strings. Our framework allows us to better understand the dynamics,i.e., the behaviour of the model, when repeatedly exposing it to randomstrings. Using our framework, we make several striking observations: (a) wefind consistent phases of the dynamics across families of models (Pythia, Phiand Llama2), (b) we identify factors that make some strings easier to memorisethan others, and (c) we identify the role of local prefixes and global contextin memorisation. We also show that sequential exposition to different randomstrings has a significant effect on memorisation. Our results, oftensurprising, have significant downstream implications in the study and usage ofLLMs.</description><author>Till Speicher, Mohammad Aflah Khan, Qinyuan Wu, Vedant Nanda, Soumi Das, Bishwamittra Ghosh, Krishna P. Gummadi, Evimaria Terzi</author><pubDate>Sat, 27 Jul 2024 14:00:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19262v1</guid></item><item><title>SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition</title><link>http://arxiv.org/abs/2407.20920v1</link><description>Multi-label image recognition is a fundamental task in computer vision.Recently, Vision-Language Models (VLMs) have made notable advancements in thisarea. However, previous methods fail to effectively leverage the rich knowledgein language models and often incorporate label semantics into visual featuresunidirectionally. To overcome these problems, we propose a Split-and-SynthesizePrompting with Gated Alignments (SSPA) framework to amplify the potential ofVLMs. Specifically, we develop an in-context learning approach to associate theinherent knowledge from LLMs. Then we propose a novel Split-and-SynthesizePrompting (SSP) strategy to first model the generic knowledge and downstreamlabel semantics individually and then aggregate them carefully through thequaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) tobidirectionally interact visual and linguistic modalities while eliminatingredundant cross-modal information, enabling more efficient region-levelalignments. Rather than making the final prediction by a sharp manner inprevious works, we propose a soft aggregator to jointly consider results fromall image regions. With the help of flexible prompting and gated alignments,SSPA is generalizable to specific domains. Extensive experiments on ninedatasets from three domains (i.e., natural, pedestrian attributes and remotesensing) demonstrate the state-of-the-art performance of SSPA. Further analysesverify the effectiveness of SSP and the interpretability of GDMA. The code willbe made public.</description><author>Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei, Stan Z. Li</author><pubDate>Tue, 30 Jul 2024 15:58:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20920v1</guid></item><item><title>SceneTeller: Language-to-3D Scene Generation</title><link>http://arxiv.org/abs/2407.20727v1</link><description>Designing high-quality indoor 3D scenes is important in many practicalapplications, such as room planning or game development. Conventionally, thishas been a time-consuming process which requires both artistic skill andfamiliarity with professional software, making it hardly accessible for laymanusers. However, recent advances in generative AI have established solidfoundation for democratizing 3D design. In this paper, we propose a pioneeringapproach for text-based 3D room design. Given a prompt in natural languagedescribing the object placement in the room, our method produces a high-quality3D scene corresponding to it. With an additional text prompt the users canchange the appearance of the entire scene or of individual objects in it. Builtusing in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-basedstylization, our turnkey pipeline produces state-of-the-art 3D scenes, whilebeing easy to use even for novices. Our project page is available athttps://sceneteller.github.io/.</description><author>Başak Melis Öcal, Maxim Tatarchenko, Sezer Karaoglu, Theo Gevers</author><pubDate>Tue, 30 Jul 2024 10:45:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20727v1</guid></item><item><title>Chat2Layout: Interactive 3D Furniture Layout with a Multimodal LLM</title><link>http://arxiv.org/abs/2407.21333v1</link><description>Automatic furniture layout is long desired for convenient interior design.Leveraging the remarkable visual reasoning capabilities of multimodal largelanguage models (MLLMs), recent methods address layout generation in a staticmanner, lacking the feedback-driven refinement essential for interactive userengagement. We introduce Chat2Layout, a novel interactive furniture layoutgeneration system that extends the functionality of MLLMs into the realm ofinteractive layout design. To achieve this, we establish a unifiedvision-question paradigm for in-context learning, enabling seamlesscommunication with MLLMs to steer their behavior without altering modelweights. Within this framework, we present a novel training-free visualprompting mechanism. This involves a visual-text prompting technique thatassist MLLMs in reasoning about plausible layout plans, followed by anOffline-to-Online search (O2O-Search) method, which automatically identifiesthe minimal set of informative references to provide exemplars for visual-textprompting. By employing an agent system with MLLMs as the core controller, weenable bidirectional interaction. The agent not only comprehends the 3Denvironment and user requirements through linguistic and visual perception butalso plans tasks and reasons about actions to generate and arrange furniturewithin the virtual space. Furthermore, the agent iteratively updates based onvisual feedback from execution results. Experimental results demonstrate thatour approach facilitates language-interactive generation and arrangement fordiverse and complex 3D furniture.</description><author>Can Wang, Hongliang Zhong, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao</author><pubDate>Wed, 31 Jul 2024 04:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21333v1</guid></item><item><title>LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks</title><link>http://arxiv.org/abs/2310.11409v4</link><description>Penetration testing, an essential component of software security testing,allows organizations to identify and remediate vulnerabilities in theirsystems, thus bolstering their defense mechanisms against cyberattacks. Onerecent advancement in the realm of penetration testing is the utilization ofLanguage Models (LLMs). We explore the intersection of LLMs and penetrationtesting to gain insight into their capabilities and challenges in the contextof privilege escalation. We introduce a fully automated privilege-escalationtool designed for evaluating the efficacy of LLMs for (ethical) hacking,executing benchmarks using multiple LLMs, and investigating their respectiveresults. Our results show that GPT-4-turbo is well suited to exploit vulnerabilities(33-83% of vulnerabilities). GPT-3.5-turbo can abuse 16-50% of vulnerabilities,while local models, such as Llama3, can only exploit between 0 and 33% of thevulnerabilities. We analyze the impact of different context sizes, in-context learning,optional high-level guidance mechanisms, and memory management techniques. Wediscuss challenging areas for LLMs, including maintaining focus during testing,coping with errors, and finally comparing LLMs with human hackers. The current version of the LLM-guided privilege-escalation prototype can befound at https://github.com/ipa-labs/hackingBuddyGPT.</description><author>Andreas Happe, Aaron Kaplan, Juergen Cito</author><pubDate>Thu, 01 Aug 2024 06:42:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.11409v4</guid></item><item><title>Intermittent Semi-working Mask: A New Masking Paradigm for LLMs</title><link>http://arxiv.org/abs/2408.00539v1</link><description>Multi-turn dialogues are a key interaction method between humans and LargeLanguage Models (LLMs), as conversations extend over multiple rounds, keepingLLMs' high generation quality and low latency is a challenge. Mainstream LLMscan be grouped into two categories based on masking strategy: causal LLM andprefix LLM. Several works have demonstrated that prefix LLMs tend to outperformcausal ones in scenarios that heavily depend on historical context such asmulti-turn dialogues or in-context learning, thanks to their bidirectionalattention on prefix sequences. However, prefix LLMs have an inherentinefficient training problem in multi-turn dialogue datasets. In addition, theattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KVCache) across dialogue rounds to reduce generation latency. In this paper, wepropose a novel masking scheme called Intermittent Semi-working Mask (ISM) toaddress these problems. Specifically, we apply alternate bidirectional andunidirectional attention on queries and answers in the dialogue history. Inthis way, ISM is able to maintain the high quality of prefix LLM and lowgeneration latency of causal LLM, simultaneously. Extensive experimentsillustrate that our ISM achieves significant performance.</description><author>Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu</author><pubDate>Thu, 01 Aug 2024 13:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00539v1</guid></item><item><title>Jailbreaking Text-to-Image Models with LLM-Based Agents</title><link>http://arxiv.org/abs/2408.00523v1</link><description>Recent advancements have significantly improved automated task-solvingcapabilities using autonomous agents powered by large language models (LLMs).However, most LLM-based agents focus on dialogue, programming, or specializeddomains, leaving gaps in addressing generative AI safety tasks. These gaps areprimarily due to the challenges posed by LLM hallucinations and the lack ofclear guidelines. In this paper, we propose Atlas, an advanced LLM-basedmulti-agent framework that integrates an efficient fuzzing workflow to targetgenerative AI models, specifically focusing on jailbreak attacks againsttext-to-image (T2I) models with safety filters. Atlas utilizes avision-language model (VLM) to assess whether a prompt triggers the T2I model'ssafety filter. It then iteratively collaborates with both LLM and VLM togenerate an alternative prompt that bypasses the filter. Atlas also enhancesthe reasoning abilities of LLMs in attack scenarios by leveraging multi-agentcommunication, in-context learning (ICL) memory mechanisms, and thechain-of-thought (COT) approach. Our evaluation demonstrates that Atlassuccessfully jailbreaks several state-of-the-art T2I models in a black-boxsetting, which are equipped with multi-modal safety filters. In addition, Atlasoutperforms existing methods in both query efficiency and the quality of thegenerated images.</description><author>Yingkai Dong, Zheng Li, Xiangtao Meng, Ning Yu, Shanqing Guo</author><pubDate>Thu, 01 Aug 2024 12:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00523v1</guid></item><item><title>Adversarial Text Rewriting for Text-aware Recommender Systems</title><link>http://arxiv.org/abs/2408.00312v1</link><description>Text-aware recommender systems incorporate rich textual features, such astitles and descriptions, to generate item recommendations for users. The use oftextual features helps mitigate cold-start problems, and thus, such recommendersystems have attracted increased attention. However, we argue that thedependency on item descriptions makes the recommender system vulnerable tomanipulation by adversarial sellers on e-commerce platforms. In this paper, weexplore the possibility of such manipulation by proposing a new text rewritingframework to attack text-aware recommender systems. We show that the rewritingattack can be exploited by sellers to unfairly uprank their products, eventhough the adversarially rewritten descriptions are perceived as realistic byhuman evaluators. Methodologically, we investigate two different variations tocarry out text rewriting attacks: (1) two-phase fine-tuning for greater attackperformance, and (2) in-context learning for higher text rewriting quality.Experiments spanning 3 different datasets and 4 existing approaches demonstratethat recommender systems exhibit vulnerability against the proposed textrewriting attack. Our work adds to the existing literature around therobustness of recommender systems, while highlighting a new dimension ofvulnerability in the age of large-scale automated text generation.</description><author>Sejoon Oh, Gaurav Verma, Srijan Kumar</author><pubDate>Thu, 01 Aug 2024 06:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00312v1</guid></item><item><title>QUITO: Accelerating Long-Context Reasoning through Query-Guided Context Compression</title><link>http://arxiv.org/abs/2408.00274v1</link><description>In-context learning (ICL) capabilities are foundational to the success oflarge language models (LLMs). Recently, context compression has attractedgrowing interest since it can largely reduce reasoning complexities andcomputation costs of LLMs. In this paper, we introduce a novel Query-gUIdedaTtention cOmpression (QUITO) method, which leverages attention of the questionover the contexts to filter useless information. Specifically, we take atrigger token to calculate the attention distribution of the context inresponse to the question. Based on the distribution, we propose three differentfiltering methods to satisfy the budget constraints of the context length. Weevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions andASQA. Experimental results demonstrate that QUITO significantly outperformsestablished baselines across various datasets and downstream LLMs, underscoringits effectiveness. Our code is available athttps://github.com/Wenshansilvia/attention_compressor.</description><author>Wenshan Wang, Yihang Wang, Yixing Fan, Huaming Liao, Jiafeng Guo</author><pubDate>Thu, 01 Aug 2024 04:28:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00274v1</guid></item><item><title>Distributed In-Context Learning under Non-IID Among Clients</title><link>http://arxiv.org/abs/2408.00144v1</link><description>Advancements in large language models (LLMs) have shown their effectivenessin multiple complicated natural language reasoning tasks. A key challengeremains in adapting these models efficiently to new or unfamiliar tasks.In-context learning (ICL) provides a promising solution for few-shot adaptationby retrieving a set of data points relevant to a query, called in-contextexamples (ICE), from a training dataset and providing them during the inferenceas context. Most existing studies utilize a centralized training dataset, yetmany real-world datasets may be distributed among multiple clients, and remotedata retrieval can be associated with costs. Especially when the client dataare non-identical independent distributions (non-IID), retrieving from clientsa proper set of ICEs needed for a test query presents critical challenges. Inthis paper, we first show that in this challenging setting, test queries willhave different preferences among clients because of non-IIDness, and equalcontribution often leads to suboptimal performance. We then introduce a novelapproach to tackle the distributed non-IID ICL problem when a data usage budgetis present. The principle is that each client's proper contribution (budget)should be designed according to the preference of each query for that client.Our approach uses a data-driven manner to allocate a budget for each client,tailored to each test query. Through extensive empirical studies on diversedatasets, our framework demonstrates superior performance relative to competingbaselines.</description><author>Siqi Liang, Sumyeong Ahn, Jiayu Zhou</author><pubDate>Wed, 31 Jul 2024 20:06:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00144v1</guid></item><item><title>Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs</title><link>http://arxiv.org/abs/2408.00114v1</link><description>Reasoning encompasses two typical types: deductive reasoning and inductivereasoning. Despite extensive research into the reasoning capabilities of LargeLanguage Models (LLMs), most studies have failed to rigorously differentiatebetween inductive and deductive reasoning, leading to a blending of the two.This raises an essential question: In LLM reasoning, which poses a greaterchallenge - deductive or inductive reasoning? While the deductive reasoningcapabilities of LLMs, (i.e. their capacity to follow instructions in reasoningtasks), have received considerable attention, their abilities in true inductivereasoning remain largely unexplored. To delve into the true inductive reasoningcapabilities of LLMs, we propose a novel framework, SolverLearner. Thisframework enables LLMs to learn the underlying function (i.e., $y = f_w(x)$),that maps input data points $(x)$ to their corresponding output values $(y)$,using only in-context examples. By focusing on inductive reasoning andseparating it from LLM-based deductive reasoning, we can isolate andinvestigate inductive reasoning of LLMs in its pure form via SolverLearner. Ourobservations reveal that LLMs demonstrate remarkable inductive reasoningcapabilities through SolverLearner, achieving near-perfect performance with ACCof 1 in most cases. Surprisingly, despite their strong inductive reasoningabilities, LLMs tend to relatively lack deductive reasoning capabilities,particularly in tasks involving ``counterfactual'' reasoning.</description><author>Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, Bing Yin, Yizhou Sun</author><pubDate>Wed, 31 Jul 2024 18:47:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00114v1</guid></item><item><title>Transformers Learn Temporal Difference Methods for In-Context Reinforcement Learning</title><link>http://arxiv.org/abs/2405.13861v3</link><description>In-context learning refers to the learning ability of a model duringinference time without adapting its parameters. The input (i.e., prompt) to themodel (e.g., transformers) consists of both a context (i.e., instance-labelpairs) and a query instance. The model is then able to output a label for thequery instance according to the context during inference. A possibleexplanation for in-context learning is that the forward pass of (linear)transformers implements iterations of gradient descent on the instance-labelpairs in the context. In this paper, we prove by construction that transformerscan also implement temporal difference (TD) learning in the forward pass, aphenomenon we refer to as in-context TD. We demonstrate the emergence ofin-context TD after training the transformer with a multi-task TD algorithm,accompanied by theoretical analysis. Furthermore, we prove that transformersare expressive enough to implement many other policy evaluation algorithms inthe forward pass, including residual gradient, TD with eligibility trace, andaverage-reward TD.</description><author>Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, Shangtong Zhang</author><pubDate>Wed, 31 Jul 2024 15:10:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.13861v3</guid></item><item><title>NOLO: Navigate Only Look Once</title><link>http://arxiv.org/abs/2408.01384v1</link><description>The in-context learning ability of Transformer models has brought newpossibilities to visual navigation. In this paper, we focus on the videonavigation setting, where an in-context navigation policy needs to be learnedpurely from videos in an offline manner, without access to the actualenvironment. For this setting, we propose Navigate Only Look Once (NOLO), amethod for learning a navigation policy that possesses the in-context abilityand adapts to new scenes by taking corresponding context videos as inputwithout finetuning or re-training. To enable learning from videos, we firstpropose a pseudo action labeling procedure using optical flow to recover theaction label from egocentric videos. Then, offline reinforcement learning isapplied to learn the navigation policy. Through extensive experiments ondifferent scenes, we show that our algorithm outperforms baselines by a largemargin, which demonstrates the in-context learning ability of the learnedpolicy.</description><author>Bohan Zhou, Jiangxing Wang, Zongqing Lu</author><pubDate>Fri, 02 Aug 2024 16:41:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01384v1</guid></item><item><title>Transformers are Universal In-context Learners</title><link>http://arxiv.org/abs/2408.01367v1</link><description>Transformers are deep architectures that define "in-context mappings" whichenable predicting new tokens based on a given set of tokens (such as a promptin NLP applications or a set of patches for vision transformers). This workstudies in particular the ability of these architectures to handle anarbitrarily large number of context tokens. To mathematically and uniformlyaddress the expressivity of these architectures, we consider the case that themappings are conditioned on a context represented by a probability distributionof tokens (discrete for a finite number of tokens). The related notion ofsmoothness corresponds to continuity in terms of the Wasserstein distancebetween these contexts. We demonstrate that deep transformers are universal andcan approximate continuous in-context mappings to arbitrary precision,uniformly over compact token domains. A key aspect of our results, compared toexisting findings, is that for a fixed precision, a single transformer canoperate on an arbitrary (even infinite) number of tokens. Additionally, itoperates with a fixed embedding dimension of tokens (this dimension does notincrease with precision) and a fixed number of heads (proportional to thedimension). The use of MLP layers between multi-head attention layers is alsoexplicitly controlled.</description><author>Takashi Furuya, Maarten V. de Hoop, Gabriel Peyré</author><pubDate>Fri, 02 Aug 2024 16:21:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01367v1</guid></item><item><title>Dual Operating Modes of In-Context Learning</title><link>http://arxiv.org/abs/2402.18819v2</link><description>In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,acquiring a new skill from in-context samples, and task retrieval, i.e.,locating and activating a relevant pretrained skill. Recent theoretical workinvestigates various mathematical models to analyze ICL, but existing modelsexplain only one operating mode at a time. We introduce a probabilistic model,with which one can explain the dual operating modes of ICL simultaneously.Focusing on in-context learning of linear functions, we extend existing modelsfor pretraining data by introducing multiple task groups and task-dependentinput distributions. We then analyze the behavior of the optimally pretrainedmodel under the squared loss, i.e., the MMSE estimator of the label givenin-context examples. Regarding pretraining task distribution as prior andin-context examples as the observation, we derive the closed-form expression ofthe task posterior distribution. With the closed-form expression, we obtain aquantitative understanding of the two operating modes of ICL. Furthermore, weshed light on an unexplained phenomenon observed in practice: under certainsettings, the ICL risk initially increases and then decreases with morein-context examples. Our model offers a plausible explanation for this "earlyascent" phenomenon: a limited number of in-context samples may lead to theretrieval of an incorrect skill, thereby increasing the risk, which willeventually diminish as task learning takes effect with more in-context samples.We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,where in-context examples are assigned random labels. Lastly, we validate ourfindings and predictions via experiments involving Transformers and largelanguage models.</description><author>Ziqian Lin, Kangwook Lee</author><pubDate>Fri, 02 Aug 2024 08:22:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18819v2</guid></item><item><title>Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations</title><link>http://arxiv.org/abs/2310.01651v3</link><description>Large language and vision-language models are rapidly being deployed inpractice thanks to their impressive capabilities in instruction following,in-context learning, and so on. This raises an urgent need to carefully analysetheir robustness so that stakeholders can understand if and when such modelsare trustworthy enough to be relied upon in any given application. In thispaper, we highlight a specific vulnerability in popular models, namelypermutation sensitivity in multiple-choice question answering (MCQA).Specifically, we show empirically that popular models are vulnerable toadversarial permutation in answer sets for multiple-choice prompting, which issurprising as models should ideally be as invariant to prompt permutation ashumans are. These vulnerabilities persist across various model sizes, and existin very recent language and vision-language models. Code is available athttps://github.com/ys-zong/FoolyourVLLMs.</description><author>Yongshuo Zong, Tingyang Yu, Ruchika Chavhan, Bingchen Zhao, Timothy Hospedales</author><pubDate>Thu, 01 Aug 2024 21:41:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01651v3</guid></item><item><title>What Do Language Models Learn in Context? The Structured Task Hypothesis</title><link>http://arxiv.org/abs/2406.04216v3</link><description>Large language models (LLMs) exhibit an intriguing ability to learn a noveltask from in-context examples presented in a demonstration, termed in-contextlearning (ICL). Understandably, a swath of research has been dedicated touncovering the theories underpinning ICL. One popular hypothesis explains ICLby task selection. LLMs identify the task based on the demonstration andgeneralize it to the prompt. Another popular hypothesis is that ICL is a formof meta-learning, i.e., the models learn a learning algorithm at pre-trainingtime and apply it to the demonstration. Finally, a third hypothesis argues thatLLMs use the demonstration to select a composition of tasks learned duringpre-training to perform ICL. In this paper, we empirically explore these threehypotheses that explain LLMs' ability to learn in context with a suite ofexperiments derived from common text classification tasks. We invalidate thefirst two hypotheses with counterexamples and provide evidence in support ofthe last hypothesis. Our results suggest an LLM could learn a novel task incontext via composing tasks learned during pre-training.</description><author>Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell</author><pubDate>Mon, 05 Aug 2024 15:08:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.04216v3</guid></item><item><title>OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar</title><link>http://arxiv.org/abs/2408.02520v1</link><description>The FIFA World Cup in Qatar was discussed extensively in the news and onsocial media. Due to news reports with allegations of human rights violations,there were calls to boycott it. Wearing a OneLove armband was part of a plannedprotest activity. Controversy around the armband arose when FIFA threatened tosanction captains who wear it. To understand what topics Twitter users Tweetedabout and what the opinion of German Twitter users was towards the OneLovearmband, we performed an analysis of German Tweets published during the WorldCup using in-context learning with LLMs. We validated the labels on humanannotations. We found that Twitter users initially discussed the armband'simpact, LGBT rights, and politics; after the ban, the conversation shiftedtowards politics in sports in general, accompanied by a subtle shift insentiment towards neutrality. Our evaluation serves as a framework for futureresearch to explore the impact of sports activism and evolving publicsentiment. This is especially useful in settings where labeling datasets forspecific opinions is unfeasible, such as when events are unfolding.</description><author>Christoph Rauchegger, Sonja Mei Wang, Pieter Delobelle</author><pubDate>Mon, 05 Aug 2024 14:40:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02520v1</guid></item><item><title>Affect Recognition in Conversations Using Large Language Models</title><link>http://arxiv.org/abs/2309.12881v2</link><description>Affect recognition, encompassing emotions, moods, and feelings, plays apivotal role in human communication. In the realm of conversational artificialintelligence, the ability to discern and respond to human affective cues is acritical factor for creating engaging and empathetic interactions. This studyinvestigates the capacity of large language models (LLMs) to recognise humanaffect in conversations, with a focus on both open-domain chit-chat dialoguesand task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP(Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al.,2014), covering a spectrum of dialogues from casual conversations to clinicalinterviews, we evaluate and compare LLMs' performance in affect recognition.Our investigation explores the zero-shot and few-shot capabilities of LLMsthrough in-context learning as well as their model capacities throughtask-specific fine-tuning. Additionally, this study takes into account thepotential impact of automatic speech recognition errors on LLM predictions.With this work, we aim to shed light on the extent to which LLMs can replicatehuman-like affect recognition capabilities in conversations.</description><author>Shutong Feng, Guangzhi Sun, Nurul Lubis, Wen Wu, Chao Zhang, Milica Gašić</author><pubDate>Mon, 05 Aug 2024 12:13:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12881v2</guid></item><item><title>A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models</title><link>http://arxiv.org/abs/2408.02377v1</link><description>Knowledge graphs (KGs) have been successfully applied to the analysis ofcomplex scientific and technological domains, with automatic KG generationmethods typically building upon relation extraction models capturingfine-grained relations between domain entities in text. While these relationsare fully applicable across scientific areas, existing models are trained onfew domain-specific datasets such as SciERC and do not perform well on newtarget domains. In this paper, we experiment with leveraging in-contextlearning capabilities of Large Language Models to perform schema-constraineddata annotation, collecting in-domain training instances for aTransformer-based relation extraction model deployed on titles and abstracts ofresearch papers in the Architecture, Construction, Engineering and Operations(AECO) domain. By assessing the performance gain with respect to a baselineDeep Learning architecture trained on off-domain data, we show that by using afew-shot learning strategy with structured prompts and only minimal expertannotation the presented approach can potentially support domain adaptation ofa science KG generation model.</description><author>Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli</author><pubDate>Mon, 05 Aug 2024 11:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02377v1</guid></item><item><title>Spin glass model of in-context learning</title><link>http://arxiv.org/abs/2408.02288v1</link><description>Large language models show a surprising in-context learning ability -- beingable to use a prompt to form a prediction for a query, yet without additionaltraining, in stark contrast to old-fashioned supervised learning. Providing amechanistic interpretation and linking the empirical phenomenon to physics arethus challenging and remain unsolved. We study a simple yet expressivetransformer with linear attention, and map this structure to a spin glass modelwith real-valued spins, where the couplings and fields explain the intrinsicdisorder in data. The spin glass model explains how the weight parametersinteract with each other during pre-training, and most importantly why anunseen function can be predicted by providing only a prompt yet withouttraining. Our theory reveals that for single instance learning, increasing thetask diversity leads to the emergence of the in-context learning, by allowingthe Boltzmann distribution to converge to a unique correct solution of weightparameters. Therefore the pre-trained transformer displays a prediction powerin a novel prompt setting. The proposed spin glass model thus establishes afoundation to understand the empirical success of large language models.</description><author>Yuhao Li, Ruoran Bai, Haiping Huang</author><pubDate>Mon, 05 Aug 2024 07:54:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02288v1</guid></item><item><title>Improving Black-box Robustness with In-Context Rewriting</title><link>http://arxiv.org/abs/2402.08225v3</link><description>Machine learning models for text classification often excel onin-distribution (ID) data but struggle with unseen out-of-distribution (OOD)inputs. Most techniques for improving OOD robustness are not applicable tosettings where the model is effectively a black box, such as when the weightsare frozen, retraining is costly, or the model is leveraged via an API.Test-time augmentation (TTA) is a simple post-hoc technique for improvingrobustness that sidesteps black-box constraints by aggregating predictionsacross multiple augmentations of the test input. TTA has seen limited use inNLP due to the challenge of generating effective natural languageaugmentations. In this work, we propose LLM-TTA, which uses LLM-generatedaugmentations as TTA's augmentation function. LLM-TTA outperforms conventionalaugmentation functions across sentiment, toxicity, and news classificationtasks for BERT and T5 models, with BERT's OOD robustness improving by anaverage of 4.48 percentage points without regressing average ID performance. Weexplore selectively augmenting inputs based on prediction entropy to reduce therate of expensive LLM augmentations, allowing us to maintain performance gainswhile reducing the average number of generated augmentations by 57.74\%.LLM-TTA is agnostic to the task model architecture, does not require OODlabels, and is effective across low and high-resource settings. We share ourdata, models, and code for reproducibility.</description><author>Kyle O'Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, Thomas Hartvigsen</author><pubDate>Mon, 05 Aug 2024 00:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08225v3</guid></item><item><title>Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process</title><link>http://arxiv.org/abs/2408.02103v1</link><description>In-context learning (ICL) is a few-shot learning paradigm that involveslearning mappings through input-output pairs and appropriately applying them tonew instances. Despite the remarkable ICL capabilities demonstrated by LargeLanguage Models (LLMs), existing works are highly dependent on large-scalelabeled support sets, not always feasible in practical scenarios. To refinethis approach, we focus primarily on an innovative selective annotationmechanism, which precedes the standard demonstration retrieval. We introducethe Language Model-based Determinant Point Process (LM-DPP) that simultaneouslyconsiders the uncertainty and diversity of unlabeled instances for optimalselection. Consequently, this yields a subset for annotation that strikes atrade-off between the two factors. We apply LM-DPP to various language models,including GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2Generation datasets demonstrate that LM-DPP can effectively select canonicalexamples. Further analysis reveals that LLMs benefit most significantly fromsubsets that are both low uncertainty and high diversity.</description><author>Peng Wang, Xiaobin Wang, Chao Lou, Shengyu Mao, Pengjun Xie, Yong Jiang</author><pubDate>Sun, 04 Aug 2024 18:08:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02103v1</guid></item><item><title>Fine-tuning multilingual language models in Twitter/X sentiment analysis: a study on Eastern-European V4 languages</title><link>http://arxiv.org/abs/2408.02044v1</link><description>The aspect-based sentiment analysis (ABSA) is a standard NLP task withnumerous approaches and benchmarks, where large language models (LLM) representthe current state-of-the-art. We focus on ABSA subtasks based on Twitter/X datain underrepresented languages. On such narrow tasks, small tuned languagemodels can often outperform universal large ones, providing available and cheapsolutions. We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) forclassification of sentiment towards Russia and Ukraine in the context of theongoing military conflict. The training/testing dataset was obtained from theacademic API from Twitter/X during 2023, narrowed to the languages of the V4countries (Czech Republic, Slovakia, Poland, Hungary). Then we measure theirperformance under a variety of settings including translations, sentimenttargets, in-context learning and more, using GPT4 as a reference model. Wedocument several interesting phenomena demonstrating, among others, that somemodels are much better fine-tunable on multilingual Twitter tasks than others,and that they can reach the SOTA level with a very small training set. Finallywe identify combinations of settings providing the best results.</description><author>Tomáš Filip, Martin Pavlíček, Petr Sosík</author><pubDate>Sun, 04 Aug 2024 14:35:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02044v1</guid></item><item><title>Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue</title><link>http://arxiv.org/abs/2406.06399v3</link><description>We study the limitations of Large Language Models (LLMs) for the task ofresponse generation in human-machine dialogue. Several techniques have beenproposed in the literature for different dialogue types (e.g., Open-Domain).However, the evaluations of these techniques have been limited in terms of baseLLMs, dialogue types and evaluation metrics. In this work, we extensivelyanalyze different LLM adaptation techniques when applied to different dialoguetypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialoguetypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.We evaluate the performance of in-context learning and fine-tuning techniquesacross datasets selected for each dialogue type. We assess the impact ofincorporating external knowledge to ground the generation in both scenarios ofRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistentevaluation and explainability criteria for automatic metrics and humanevaluation protocols. Our analysis shows that there is no universalbest-technique for adapting large language models as the efficacy of eachtechnique depends on both the base LLM and the specific type of dialogue. Lastbut not least, the assessment of the best adaptation technique should includehuman evaluation to avoid false expectations and outcomes derived fromautomatic metrics.</description><author>Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi</author><pubDate>Sat, 03 Aug 2024 15:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06399v3</guid></item><item><title>OpenLogParser: Unsupervised Parsing with Open-Source Large Language Models</title><link>http://arxiv.org/abs/2408.01585v1</link><description>Log parsing is a critical step that transforms unstructured log data intostructured formats, facilitating subsequent log-based analysis. Traditionalsyntax-based log parsers are efficient and effective, but they often experiencedecreased accuracy when processing logs that deviate from the predefined rules.Recently, large language models (LLM) based log parsers have shown superiorparsing accuracy. However, existing LLM-based parsers face three mainchallenges: 1)time-consuming and labor-intensive manual labeling forfine-tuning or in-context learning, 2)increased parsing costs due to the vastvolume of log data and limited context size of LLMs, and 3)privacy risks fromusing commercial models like ChatGPT with sensitive log information. Toovercome these limitations, this paper introduces OpenLogParser, anunsupervised log parsing approach that leverages open-source LLMs (i.e.,Llama3-8B) to enhance privacy and reduce operational costs while achievingstate-of-the-art parsing accuracy. OpenLogParser first groups logs with similarstatic text but varying dynamic variables using a fixed-depth grouping tree. Itthen parses logs within these groups using three components: i)similarityscoring-based retrieval augmented generation: selects diverse logs within eachgroup based on Jaccard similarity, helping the LLM distinguish between statictext and dynamic variables; ii)self-reflection: iteratively query LLMs torefine log templates to improve parsing accuracy; and iii) log template memory:stores parsed templates to reduce LLM queries for improved parsing efficiency.Our evaluation on LogHub-2.0 shows that OpenLogParser achieves 25% higherparsing accuracy and processes logs 2.7 times faster compared tostate-of-the-art LLM-based parsers. In short, OpenLogParser addresses privacyand cost concerns of using commercial LLMs while achieving state-of-the-artsparsing efficiency and accuracy.</description><author>Zeyang Ma, Dong Jae Kim, Tse-Hsun Chen</author><pubDate>Fri, 02 Aug 2024 21:54:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01585v1</guid></item><item><title>Dynamic In-context Learning with Conversational Models for Data Extraction and Materials Property Prediction</title><link>http://arxiv.org/abs/2405.10448v2</link><description>The advent of natural language processing and large language models (LLMs)has revolutionized the extraction of data from unstructured scholarly papers.However, ensuring data trustworthiness remains a significant challenge. In thispaper, we introduce PropertyExtractor, an open-source tool that leveragesadvanced conversational LLMs like Google gemini-pro and OpenAI gpt-4, blendszero-shot with few-shot in-context learning, and employs engineered prompts forthe dynamic refinement of structured information hierarchies - enablingautonomous, efficient, scalable, and accurate identification, extraction, andverification of material property data. Our tests on material data demonstrateprecision and recall that exceed 95\% with an error rate of approximately 9%,highlighting the effectiveness and versatility of the toolkit. Finally,databases for 2D material thicknesses, a critical parameter for deviceintegration, and energy bandgap values are developed using PropertyExtractor.Specifically for the thickness database, the rapid evolution of the field hasoutpaced both experimental measurements and computational methods, creating asignificant data gap. Our work addresses this gap and showcases the potentialof PropertyExtractor as a reliable and efficient tool for the autonomousgeneration of various material property databases, advancing the field.</description><author>Chinedu Ekuma</author><pubDate>Fri, 02 Aug 2024 20:07:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.10448v2</guid></item><item><title>Pre-training and in-context learning IS Bayesian inference a la De Finetti</title><link>http://arxiv.org/abs/2408.03307v1</link><description>Accurately gauging uncertainty on the underlying environment is alongstanding goal of intelligent systems. We characterize which latent conceptspre-trained sequence models are naturally able to reason with. We go back to DeFinetti's predictive view of Bayesian reasoning: instead of modeling latentparameters through priors and likelihoods like topic models do, De Finetti haslong advocated for modeling exchangeable (permutation invariant) sequences ofobservables. According to this view, pre-training autoregressive modelsformulates informed beliefs based on prior observations ("empirical Bayes"),and forward generation is a simulated instantiation of an environment("posterior inference"). This connection allows extending in-context learning(ICL) beyond predictive settings, highlighting sequence models' ability toperform explicit statistical inference. In particular, we show the sequenceprediction loss over exchangeable documents controls performance on downstreamtasks where uncertainty quantification is key. Empirically, we propose anddemonstrate several approaches for encoding exchangeability in sequence modelarchitectures: data augmentation, regularization, and causal masking.</description><author>Naimeng Ye, Hanming Yang, Andrew Siah, Hongseok Namkoong</author><pubDate>Tue, 06 Aug 2024 17:16:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03307v1</guid></item><item><title>Enhancing Complex Causality Extraction via Improved Subtask Interaction and Knowledge Fusion</title><link>http://arxiv.org/abs/2408.03079v1</link><description>Event Causality Extraction (ECE) aims at extracting causal event pairs fromtexts. Despite ChatGPT's recent success, fine-tuning small models remains thebest approach for the ECE task. However, existing fine-tuning based ECE methodscannot address all three key challenges in ECE simultaneously: 1) ComplexCausality Extraction, where multiple causal-effect pairs occur within a singlesentence; 2) Subtask~ Interaction, which involves modeling the mutualdependence between the two subtasks of ECE, i.e., extracting events andidentifying the causal relationship between extracted events; and 3) KnowledgeFusion, which requires effectively fusing the knowledge in two modalities,i.e., the expressive pretrained language models and the structured knowledgegraphs. In this paper, we propose a unified ECE framework (UniCE to address allthree issues in ECE simultaneously. Specifically, we design a subtaskinteraction mechanism to enable mutual interaction between the two ECEsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge inthe two modalities. Furthermore, we employ separate decoders for each subtaskto facilitate complex causality extraction. Experiments on three benchmarkdatasets demonstrate that our method achieves state-of-the-art performance andoutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,our model can also be used to effectively improve the ECE performance ofChatGPT via in-context learning.</description><author>Jinglong Gao, Chen Lu, Xiao Ding, Zhongyang Li, Ting Liu, Bing Qin</author><pubDate>Tue, 06 Aug 2024 10:15:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03079v1</guid></item><item><title>Hide and Seek: Fingerprinting Large Language Models with Evolutionary Learning</title><link>http://arxiv.org/abs/2408.02871v1</link><description>As content generated by Large Language Model (LLM) has grown exponentially,the ability to accurately identify and fingerprint such text has becomeincreasingly crucial. In this work, we introduce a novel black-box approach forfingerprinting LLMs, achieving an impressive 72% accuracy in identifying thecorrect family of models (Such as Llama, Mistral, Gemma, etc) among a lineup ofLLMs. We present an evolutionary strategy that leverages the capabilities ofone LLM to discover the most salient features for identifying other LLMs. Ourmethod employs a unique "Hide and Seek" algorithm, where an Auditor LLMgenerates discriminative prompts, and a Detective LLM analyzes the responses tofingerprint the target models. This approach not only demonstrates thefeasibility of LLM-driven model identification but also reveals insights intothe semantic manifolds of different LLM families. By iteratively refiningprompts through in-context learning, our system uncovers subtle distinctionsbetween model outputs, providing a powerful tool for LLM analysis andverification. This research opens new avenues for understanding LLM behaviorand has significant implications for model attribution, security, and thebroader field of AI transparency.</description><author>Dmitri Iourovitski, Sanat Sharma, Rakshak Talwar</author><pubDate>Tue, 06 Aug 2024 00:13:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02871v1</guid></item><item><title>Soft Prompting for Unlearning in Large Language Models</title><link>http://arxiv.org/abs/2406.12038v2</link><description>The widespread popularity of Large Language Models (LLMs), partly due totheir unique ability to perform in-context learning, has also brought to lightthe importance of ethical and safety considerations when deploying thesepre-trained models. In this work, we focus on investigating machine unlearningfor LLMs motivated by data protection regulations. In contrast to the growingliterature on fine-tuning methods to achieve unlearning, we focus on acomparatively lightweight alternative called soft prompting to realize theunlearning of a subset of training data. With losses designed to enforceforgetting as well as utility preservation, our framework \textbf{S}oft\textbf{P}rompting for \textbf{U}n\textbf{l}earning (SPUL) learns prompt tokensthat can be appended to an arbitrary query to induce unlearning of specificexamples at inference time without updating LLM parameters. We conduct arigorous evaluation of the proposed method and our results indicate that SPULcan significantly improve the trade-off between utility and forgetting in thecontext of text classification and question answering with LLMs. We furthervalidate our method using multiple LLMs to highlight the scalability of ourframework and provide detailed insights into the choice of hyperparameters andthe influence of the size of unlearning data. Our implementation is availableat \url{https://github.com/karuna-bhaila/llm_unlearning}.</description><author>Karuna Bhaila, Minh-Hao Van, Xintao Wu</author><pubDate>Mon, 05 Aug 2024 21:48:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12038v2</guid></item><item><title>Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</title><link>http://arxiv.org/abs/2408.03615v1</link><description>Building a general-purpose agent is a long-standing vision in the field ofartificial intelligence. Existing agents have made remarkable progress in manydomains, yet they still struggle to complete long-horizon tasks in an openworld. We attribute this to the lack of necessary world knowledge andmultimodal experience that can guide agents through a variety of long-horizontasks. In this paper, we propose a Hybrid Multimodal Memory module to addressthe above challenges. It 1) transforms knowledge into Hierarchical DirectedKnowledge Graph that allows agents to explicitly represent and learn worldknowledge, and 2) summarises historical information into Abstracted MultimodalExperience Pool that provide agents with rich references for in-contextlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,Optimus-1, is constructed with dedicated Knowledge-guided Planner andExperience-Driven Reflector, contributing to a better planning and reflectionin the face of long-horizon tasks in Minecraft. Extensive experimental resultsshow that Optimus-1 significantly outperforms all existing agents onchallenging long-horizon task benchmarks, and exhibits near human-levelperformance on many tasks. In addition, we introduce various Multimodal LargeLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results showthat Optimus-1 exhibits strong generalization with the help of the HybridMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.</description><author>Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie</author><pubDate>Wed, 07 Aug 2024 08:16:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03615v1</guid></item><item><title>Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs</title><link>http://arxiv.org/abs/2408.00114v2</link><description>Reasoning encompasses two typical types: deductive reasoning and inductivereasoning. Despite extensive research into the reasoning capabilities of LargeLanguage Models (LLMs), most studies have failed to rigorously differentiatebetween inductive and deductive reasoning, leading to a blending of the two.This raises an essential question: In LLM reasoning, which poses a greaterchallenge - deductive or inductive reasoning? While the deductive reasoningcapabilities of LLMs, (i.e. their capacity to follow instructions in reasoningtasks), have received considerable attention, their abilities in true inductivereasoning remain largely unexplored. To investigate into the true inductivereasoning capabilities of LLMs, we propose a novel framework, SolverLearner.This framework enables LLMs to learn the underlying function (i.e., $y =f_w(x)$), that maps input data points $(x)$ to their corresponding outputvalues $(y)$, using only in-context examples. By focusing on inductivereasoning and separating it from LLM-based deductive reasoning, we can isolateand investigate inductive reasoning of LLMs in its pure form via SolverLearner.Our observations reveal that LLMs demonstrate remarkable inductive reasoningcapabilities through SolverLearner, achieving near-perfect performance with ACCof 1 in most cases. Surprisingly, despite their strong inductive reasoningabilities, LLMs tend to relatively lack deductive reasoning capabilities,particularly in tasks involving ``counterfactual'' reasoning.</description><author>Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, Bing Yin, Yizhou Sun</author><pubDate>Wed, 07 Aug 2024 00:52:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00114v2</guid></item><item><title>Can LLMs Serve As Time Series Anomaly Detectors?</title><link>http://arxiv.org/abs/2408.03475v1</link><description>An emerging topic in large language models (LLMs) is their application totime series forecasting, characterizing mainstream and patternablecharacteristics of time series. A relevant but rarely explored and morechallenging question is whether LLMs can detect and explain time seriesanomalies, a critical task across various real-world applications. In thispaper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3,in detecting and explaining anomalies in time series. Our studies reveal that:1) LLMs cannot be directly used for time series anomaly detection. 2) Bydesigning prompt strategies such as in-context learning and chain-of-thoughtprompting, GPT-4 can detect time series anomalies with results competitive tobaseline methods. 3) We propose a synthesized dataset to automatically generatetime series anomalies with corresponding explanations. By applying instructionfine-tuning on this dataset, LLaMA3 demonstrates improved performance in timeseries anomaly detection tasks. In summary, our exploration shows the promisingpotential of LLMs as time series anomaly detectors.</description><author>Manqing Dong, Hao Huang, Longbing Cao</author><pubDate>Tue, 06 Aug 2024 23:14:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03475v1</guid></item><item><title>How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression</title><link>http://arxiv.org/abs/2408.04532v1</link><description>Despite the remarkable success of transformer-based models in variousreal-world tasks, their underlying mechanisms remain poorly understood. Recentstudies have suggested that transformers can implement gradient descent as anin-context learner for linear regression problems and have developed varioustheoretical analyses accordingly. However, these works mostly focus on theexpressive power of transformers by designing specific parameter constructions,lacking a comprehensive understanding of their inherent working mechanismspost-training. In this study, we consider a sparse linear regression problemand investigate how a trained multi-head transformer performs in-contextlearning. We experimentally discover that the utilization of multi-headsexhibits different patterns across layers: multiple heads are utilized andessential in the first layer, while usually only a single head is sufficientfor subsequent layers. We provide a theoretical explanation for thisobservation: the first layer preprocesses the context data, and the followinglayers execute simple optimization steps based on the preprocessed context.Moreover, we demonstrate that such a preprocess-then-optimize algorithm cansignificantly outperform naive gradient descent and ridge regressionalgorithms. Further experimental results support our explanations. Our findingsoffer insights into the benefits of multi-head attention and contribute tounderstanding the more intricate mechanisms hidden within trained transformers.</description><author>Xingwu Chen, Lei Zhao, Difan Zou</author><pubDate>Thu, 08 Aug 2024 15:33:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04532v1</guid></item><item><title>Enhancing Robustness of Retrieval-Augmented Language Models with In-Context Learning</title><link>http://arxiv.org/abs/2408.04414v1</link><description>Retrieval-Augmented Language Models (RALMs) have significantly improvedperformance in open-domain question answering (QA) by leveraging externalknowledge. However, RALMs still struggle with unanswerable queries, where theretrieved contexts do not contain the correct answer, and with conflictinginformation, where different sources provide contradictory answers due toimperfect retrieval. This study introduces an in-context learning-basedapproach to enhance the reasoning capabilities of RALMs, making them morerobust in imperfect retrieval scenarios. Our method incorporates MachineReading Comprehension (MRC) demonstrations, referred to as cases, to boost themodel's capabilities to identify unanswerabilities and conflicts among theretrieved contexts. Experiments on two open-domain QA datasets show that ourapproach increases accuracy in identifying unanswerable and conflictingscenarios without requiring additional fine-tuning. This work demonstrates thatin-context learning can effectively enhance the robustness of RALMs inopen-domain QA tasks.</description><author>Seong-Il Park, Seung-Woo Choi, Na-Hyun Kim, Jay-Yoon Lee</author><pubDate>Thu, 08 Aug 2024 12:42:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04414v1</guid></item><item><title>A Survey on Mixture of Experts</title><link>http://arxiv.org/abs/2407.06204v2</link><description>Large language models (LLMs) have garnered unprecedented advancements acrossdiverse fields, ranging from natural language processing to computer vision andbeyond. The prowess of LLMs is underpinned by their substantial model size,extensive and diverse datasets, and the vast computational power harnessedduring training, all of which contribute to the emergent abilities of LLMs(e.g., in-context learning) that are not present in small models. Within thiscontext, the mixture of experts (MoE) has emerged as an effective method forsubstantially scaling up model capacity with minimal computation overhead,gaining significant attention from academia and industry. Despite its growingprevalence, there lacks a systematic and comprehensive review of the literatureon MoE. This survey seeks to bridge that gap, serving as an essential resourcefor researchers delving into the intricacies of MoE. We first briefly introducethe structure of the MoE layer, followed by proposing a new taxonomy of MoE.Next, we overview the core designs for various MoE models including bothalgorithmic and systemic aspects, alongside collections of availableopen-source implementations, hyperparameter configurations and empiricalevaluations. Furthermore, we delineate the multifaceted applications of MoE inpractice, and outline some potential directions for future research. Tofacilitate ongoing updates and the sharing of cutting-edge developments in MoEresearch, we have established a resource repository accessible athttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts.</description><author>Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, Jiayi Huang</author><pubDate>Thu, 08 Aug 2024 07:13:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06204v2</guid></item><item><title>An In-Context Learning Agent for Formal Theorem-Proving</title><link>http://arxiv.org/abs/2310.04353v5</link><description>We present an in-context learning agent for formal theorem-proving inenvironments like Lean and Coq. Current state-of-the-art models for the problemare finetuned on environment-specific proof data. By contrast, our approach,called COPRA, repeatedly asks a high-capacity, general-purpose large languagemodel (GPT-4) to propose tactic applications from within a statefulbacktracking search. Proposed tactics are executed in the underlying proofenvironment. Feedback from the execution is used to build the prompt for thenext model query, along with selected information from the search history andlemmas retrieved from an external database. We evaluate our implementation ofCOPRA on the miniF2F benchmark for Lean and a set of Coq tasks from theCompCert project. On these benchmarks, COPRA significantly outperforms few-shotinvocations of GPT-4. It also compares favorably against finetuning-basedapproaches, outperforming ReProver, a state-of-the-art finetuned approach forLean, in terms of the pass@1 metric. Our code and data are available athttps://github.com/trishullab/copra.</description><author>Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, Swarat Chaudhuri</author><pubDate>Thu, 08 Aug 2024 04:15:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04353v5</guid></item><item><title>LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven Prompting Strategy for Document-Level Event Argument Extraction</title><link>http://arxiv.org/abs/2311.06555v3</link><description>In this study, we investigate in-context learning (ICL) in document-levelevent argument extraction (EAE) to alleviate the dependency on large-scalelabeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy(HD-LoA) prompting to address the challenge of example selection and to developa prompting strategy tailored for EAE. Specifically, we hypothesize andvalidate that LLMs learn task-specific heuristics from demonstrations via ICL.Building upon this hypothesis, we introduce an explicit heuristic-drivendemonstration construction approach, which transforms the haphazard exampleselection process into a methodical method that emphasizes task heuristics.Additionally, inspired by the analogical reasoning of human, we propose thelink-of-analogy prompting, which enables LLMs to process new situations bydrawing analogies to known situations, enhancing their performance on unseenclasses beyond limited ICL examples. Experiments show that our methodoutperforms existing prompting methods and few-shot supervised learning methodson document-level EAE datasets. Additionally, the HD-LoA prompting showseffectiveness in diverse tasks like sentiment analysis and natural languageinference, demonstrating its broad adaptability.</description><author>Hanzhang Zhou, Junlang Qian, Zijian Feng, Hui Lu, Zixiao Zhu, Kezhi Mao</author><pubDate>Thu, 08 Aug 2024 03:20:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.06555v3</guid></item><item><title>Retrieval Augmented Thought Process for Private Data Handling in Healthcare</title><link>http://arxiv.org/abs/2402.07812v2</link><description>Large Language Models (LLMs) have demonstrated the strong potential to assistboth clinicians and the general public with their extensive medical knowledge.However, their application in healthcare is constrained due to concerns aboutthe privacy of data used in training, which prevents the integration of privateand personal information because of security and ethical issues. Moreover, iftheir capabilities can be enhanced with information retrieval to accessup-to-date knowledge, the current integration of LLMs with Informationretrieval lacks robustness to imperfect retrieval, which can hinder theireffectiveness and even reduce overall performance. In this work, we addressthis challenge by introducing the Retrieval-Augmented Thought Process (RATP).Given access to external knowledge, RATP formulates the thought generation ofLLMs as a multiple-step decision process. To optimise such a thought process,RATP leverages Monte-Carlo Tree Search and learns a proxy reward function thatpermits cost-efficient inference. On a private dataset of electronic medicalrecords, deliberately excluded from any LLM training set, RATP achieves 35%additional accuracy compared to in-context retrieval-augmented generation forthe question-answering task.</description><author>Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar</author><pubDate>Wed, 07 Aug 2024 18:27:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07812v2</guid></item><item><title>SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation</title><link>http://arxiv.org/abs/2408.04872v1</link><description>In-context learning (ICL) greatly improves the performance of large languagemodels (LLMs) on various down-stream tasks, where the improvement highlydepends on the quality of demonstrations. In this work, we introduce syntacticknowledge to select better in-context examples for machine translation (MT). Wepropose a new strategy, namely Syntax-augmented COverage-based In-contextexample selection (SCOI), leveraging the deep syntactic structure beyondconventional word matching. Specifically, we measure the set-level syntacticcoverage by computing the coverage of polynomial terms with the help of asimplified tree-to-polynomial algorithm, and lexical coverage using wordoverlap. Furthermore, we devise an alternate selection approach to combine bothcoverage measures, taking advantage of syntactic and lexical information. Weconduct experiments with two multi-lingual LLMs on six translation directions.Empirical results show that our proposed SCOI obtains the highest average COMETscore among all learning-free methods, indicating that combining syntactic andlexical coverage successfully helps to select better in-context examples forMT.</description><author>Chenming Tang, Zhixiang Wang, Yunfang Wu</author><pubDate>Fri, 09 Aug 2024 05:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04872v1</guid></item><item><title>Benchmarking Cognitive Biases in Large Language Models as Evaluators</title><link>http://arxiv.org/abs/2309.17012v2</link><description>Large Language Models (LLMs) have recently been shown to be effective asautomatic evaluators with simple prompting and in-context learning. In thiswork, we assemble 15 LLMs of four different size ranges and evaluate theiroutput responses by preference ranking from the other LLMs as evaluators, suchas System Star is better than System Square. We then evaluate the quality ofranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators(CoBBLEr), a benchmark to measure six different cognitive biases in LLMevaluation outputs, such as the Egocentric bias where a model prefers to rankits own outputs highly in evaluation. We find that LLMs are biased text qualityevaluators, exhibiting strong indications on our bias benchmark (average of 40%of comparisons across all models) within each of their evaluations thatquestion their robustness as evaluators. Furthermore, we examine thecorrelation between human and machine preferences and calculate the averageRank-Biased Overlap (RBO) score to be 49.6%, indicating that machinepreferences are misaligned with humans. According to our findings, LLMs maystill be unable to be utilized for automatic annotation aligned with humanpreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.</description><author>Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang</author><pubDate>Mon, 12 Aug 2024 17:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17012v2</guid></item><item><title>MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples</title><link>http://arxiv.org/abs/2312.06363v3</link><description>Although In-Context Learning (ICL) brings remarkable performance gains toLarge Language Models (LLMs), the improvements remain lower than fine-tuning ondownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),a novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning byfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). Wepropose the Multi-Modal Hub (M-Hub), a unified module that captures variousmulti-modal features according to different inputs and objectives. Based onM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textualfeatures and subsequently generate outputs conditioned on the textual-guidedvisual features. Moreover, leveraging the flexibility of M-Hub, we design avariety of in-context demonstrations. Extensive experiments on a diverse rangeof downstream multi-modal tasks demonstrate that MMICT significantlyoutperforms traditional fine-tuning strategy and the vanilla ICT method thatdirectly takes the concatenation of all information from different modalitiesas input. Our implementation is available at:https://github.com/KDEGroup/MMICT.</description><author>Tao Chen, Enwei Zhang, Yuting Gao, Ke Li, Xing Sun, Yan Zhang, Hui Li, Rongrong Ji</author><pubDate>Mon, 12 Aug 2024 06:17:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06363v3</guid></item><item><title>Universal Approximation Theory: The basic theory for large language models</title><link>http://arxiv.org/abs/2407.00958v2</link><description>Language models have emerged as a critical area of focus in artificialintelligence, particularly with the introduction of groundbreaking innovationslike ChatGPT. Large-scale Transformer networks have quickly become the leadingapproach for advancing natural language processing algorithms. Built on theTransformer architecture, these models enable interactions that closely mimichuman communication and, equipped with extensive knowledge, can even assist inguiding human tasks. Despite their impressive capabilities and growingcomplexity, a key question remains-the theoretical foundations of largelanguage models (LLMs). What makes Transformer so effective for poweringintelligent language applications, such as translation and coding? Whatunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA schemeenhance the fine-tuning of LLMs? And what supports the practicality of pruningLLMs? To address these critical questions and explore the technologicalstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) tooffer a theoretical backdrop, shedding light on the mechanisms that underpinthese advancements.</description><author>Wei Wang, Qing Li</author><pubDate>Mon, 12 Aug 2024 02:08:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00958v2</guid></item><item><title>LLM-Based Robust Product Classification in Commerce and Compliance</title><link>http://arxiv.org/abs/2408.05874v1</link><description>Product classification is a crucial task in international trade, ascompliance regulations are verified and taxes and duties are applied based onproduct categories. Manual classification of products is time-consuming anderror-prone, and the sheer volume of products imported and exported renders themanual process infeasible. Consequently, e-commerce platforms and enterprisesinvolved in international trade have turned to automatic product classificationusing machine learning. However, current approaches do not consider thereal-world challenges associated with product classification, such as veryabbreviated and incomplete product descriptions. In addition, recentadvancements in generative Large Language Models (LLMs) and their reasoningcapabilities are mainly untapped in product classification and e-commerce. Inthis research, we explore the real-life challenges of industrial classificationand we propose data perturbations that allow for realistic data simulation.Furthermore, we employ LLM-based product classification to improve therobustness of the prediction in presence of incomplete data. Our research showsthat LLMs with in-context learning outperform the supervised approaches in theclean-data scenario. Additionally, we illustrate that LLMs are significantlymore robust than the supervised approaches when data attacks are present.</description><author>Sina Gholamian, Gianfranco Romani, Bartosz Rudnikowicz, Laura Skylaki</author><pubDate>Sun, 11 Aug 2024 22:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05874v1</guid></item><item><title>Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability</title><link>http://arxiv.org/abs/2407.15720v2</link><description>Large language models (LLMs) have emerged as powerful tools for many AIproblems and exhibit remarkable in-context learning (ICL) capabilities.Compositional ability, solving unseen complex tasks that combine two or moresimple tasks, is an essential reasoning ability for Artificial GeneralIntelligence. Despite the tremendous success of LLMs, how they approachcomposite tasks, especially those not encountered during the pretraining phase,remains an open and largely underexplored question. In this study, we delveinto the ICL capabilities of LLMs on composite tasks, with only simple tasks asin-context examples. We develop a test suite of composite tasks includinglinguistic and logical challenges and perform empirical studies acrossdifferent LLM families. We observe that models exhibit divergent behaviors: (1)For simpler composite tasks that apply distinct mapping mechanisms to differentinput segments, the models demonstrate decent compositional ability, whilescaling up the model enhances this ability; (2) for more complex compositetasks involving reasoning multiple steps, where each step represents one task,models typically underperform, and scaling up generally provides noimprovements. We offer theoretical analysis in a simplified setting, explainingthat models exhibit compositional capability when the task handles differentinput parts separately. We believe our work sheds new light on the capabilitiesof LLMs in solving composite tasks regarding the nature of the tasks and modelscale. Our dataset and code are available at{\url{https://github.com/OliverXUZY/LLM_Compose}}.</description><author>Zhuoyan Xu, Zhenmei Shi, Yingyu Liang</author><pubDate>Sun, 11 Aug 2024 04:39:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15720v2</guid></item><item><title>In-Context Exploiter for Extensive-Form Games</title><link>http://arxiv.org/abs/2408.05575v1</link><description>Nash equilibrium (NE) is a widely adopted solution concept in game theory dueto its stability property. However, we observe that the NE strategy might notalways yield the best results, especially against opponents who do not adhereto NE strategies. Based on this observation, we pose a new game-solvingquestion: Can we learn a model that can exploit any, even NE, opponent tomaximize their own utility? In this work, we make the first attempt toinvestigate this problem through in-context learning. Specifically, weintroduce a novel method, In-Context Exploiter (ICE), to train a single modelthat can act as any player in the game and adaptively exploit opponentsentirely by in-context learning. Our ICE algorithm involves generating diverseopponent strategies, collecting interactive history training data by areinforcement learning algorithm, and training a transformer-based agent withina well-designed curriculum learning framework. Finally, comprehensiveexperimental results validate the effectiveness of our ICE algorithm,showcasing its in-context learning ability to exploit any unknown opponent,thereby positively answering our initial game-solving question.</description><author>Shuxin Li, Chang Yang, Youzhi Zhang, Pengdeng Li, Xinrun Wang, Xiao Huang, Hau Chan, Bo An</author><pubDate>Sat, 10 Aug 2024 14:59:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05575v1</guid></item><item><title>LaiDA: Linguistics-aware In-context Learning with Data Augmentation for Metaphor Components Identification</title><link>http://arxiv.org/abs/2408.05404v1</link><description>Metaphor Components Identification (MCI) contributes to enhancing machineunderstanding of metaphors, thereby advancing downstream natural languageprocessing tasks. However, the complexity, diversity, and dependency on contextand background knowledge pose significant challenges for MCI. Large languagemodels (LLMs) offer new avenues for accurate comprehension of complex naturallanguage texts due to their strong semantic analysis and extensive commonsenseknowledge. In this research, a new LLM-based framework is proposed, namedLinguistics-aware In-context Learning with Data Augmentation (LaiDA).Specifically, ChatGPT and supervised fine-tuning are utilized to tailor ahigh-quality dataset. LaiDA incorporates a simile dataset for pre-training. Agraph attention network encoder generates linguistically rich featurerepresentations to retrieve similar examples. Subsequently, LLM is fine-tunedwith prompts that integrate linguistically similar examples. LaiDA ranked 2ndin Subtask 2 of NLPCC2024 Shared Task 9, demonstrating its effectiveness. Codeand data are available at https://github.com/WXLJZ/LaiDA.</description><author>Hongde Liu, Chenyuan He, Feiyang Meng, Changyong Niu, Yuxiang Jia</author><pubDate>Sat, 10 Aug 2024 02:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05404v1</guid></item><item><title>RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios</title><link>http://arxiv.org/abs/2312.13303v2</link><description>Simulation plays a crucial role in the development of autonomous vehicles(AVs) due to the potential risks associated with real-world testing. Althoughsignificant progress has been made in the visual aspects of simulators,generating complex behavior among agents remains a formidable challenge. It isnot only imperative to ensure realism in the scenarios generated but alsoessential to incorporate preferences and conditions to facilitate controllablegeneration for AV training and evaluation. Traditional methods, mainly relyingon memorizing the distribution of training datasets, often fall short ingenerating unseen scenarios. Inspired by the success of retrieval augmentedgeneration in large language models, we present RealGen, a novelretrieval-based in-context learning framework for traffic scenario generation.RealGen synthesizes new scenarios by combining behaviors from multipleretrieved examples in a gradient-free way, which may originate from templatesor tagged scenarios. This in-context learning framework endows versatilegenerative capabilities, including the ability to edit scenarios, composevarious behaviors, and produce critical scenarios. Evaluations show thatRealGen offers considerable flexibility and controllability, marking a newdirection in the field of controllable traffic scenario generation. Check ourproject website for more information: https://realgen.github.io.</description><author>Wenhao Ding, Yulong Cao, Ding Zhao, Chaowei Xiao, Marco Pavone</author><pubDate>Tue, 13 Aug 2024 17:20:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.13303v2</guid></item><item><title>How Transformers Learn Causal Structure with Gradient Descent</title><link>http://arxiv.org/abs/2402.14735v2</link><description>The incredible success of transformers on sequence modeling tasks can belargely attributed to the self-attention mechanism, which allows information tobe transferred between different parts of a sequence. Self-attention allowstransformers to encode causal structure which makes them particularly suitablefor sequence modeling. However, the process by which transformers learn suchcausal structure via gradient-based training algorithms remains poorlyunderstood. To better understand this process, we introduce an in-contextlearning task that requires learning latent causal structure. We prove thatgradient descent on a simplified two-layer transformer learns to solve thistask by encoding the latent causal graph in the first attention layer. The keyinsight of our proof is that the gradient of the attention matrix encodes themutual information between tokens. As a consequence of the data processinginequality, the largest entries of this gradient correspond to edges in thelatent causal graph. As a special case, when the sequences are generated fromin-context Markov chains, we prove that transformers learn an induction head(Olsson et al., 2022). We confirm our theoretical findings by showing thattransformers trained on our in-context learning task are able to recover a widevariety of causal structures.</description><author>Eshaan Nichani, Alex Damian, Jason D. Lee</author><pubDate>Tue, 13 Aug 2024 15:45:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14735v2</guid></item><item><title>MICL: Improving In-Context Learning through Multiple-Label Words in Demonstration</title><link>http://arxiv.org/abs/2406.10908v3</link><description>In-context learning (ICL) enables large language models (LLMs) to perform newtasks by using sample-label pairs as demonstrations. However, variations indemonstrations can lead to significantly different performances. Currentresearch mainly focuses on selecting demonstration samples, preassuming theclass name to be the label word when creating sample-label pairs. However, thechoice of label words is crucial for ICL performance. Besides, we observe thatusing a single class name in demonstration may not yield optimal results whileusing multiple label words in one sample-label pair can enhance ICLperformance. In this paper, we propose a comprehensive approach that organizesboth samples and labels in demonstrations based on LLMs' output spacedistribution. This approach uses multiple label words in one sample-label pairto enhance label instruction. Evaluation results from seven classificationdatasets show that this demonstration organization method, which incorporatesmultiple label words to provide diverse label information, improves ICLperformance.</description><author>Zhu Zixiao, Feng Zijian, Zhou Hanzhang, Qian Junlang, Mao Kezhi</author><pubDate>Tue, 13 Aug 2024 11:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10908v3</guid></item><item><title>Transformers Can Do Bayesian Inference</title><link>http://arxiv.org/abs/2112.10510v7</link><description>Currently, it is hard to reap the benefits of deep learning for Bayesianmethods, which allow the explicit specification of prior knowledge andaccurately capture model uncertainty. We present Prior-Data Fitted Networks(PFNs). PFNs leverage in-context learning in large-scale machine learningtechniques to approximate a large set of posteriors. The only requirement forPFNs to work is the ability to sample from a prior distribution over supervisedlearning tasks (or functions). Our method restates the objective of posteriorapproximation as a supervised classification problem with a set-valued input:it repeatedly draws a task (or function) from the prior, draws a set of datapoints and their labels from it, masks one of the labels and learns to makeprobabilistic predictions for it based on the set-valued input of the rest ofthe data points. Presented with a set of samples from a new supervised learningtask as input, PFNs make probabilistic predictions for arbitrary other datapoints in a single forward propagation, having learned to approximate Bayesianinference. We demonstrate that PFNs can near-perfectly mimic Gaussian processesand also enable efficient Bayesian inference for intractable problems, withover 200-fold speedups in multiple setups compared to current methods. Weobtain strong results in very diverse areas such as Gaussian processregression, Bayesian neural networks, classification for small tabular datasets, and few-shot image classification, demonstrating the generality of PFNs.Code and trained PFNs are released athttps://github.com/automl/TransformersCanDoBayesianInference.</description><author>Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, Frank Hutter</author><pubDate>Tue, 13 Aug 2024 09:58:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2112.10510v7</guid></item><item><title>Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models</title><link>http://arxiv.org/abs/2405.15984v2</link><description>With the emergence of large language models, such as LLaMA and OpenAI GPT-3,In-Context Learning (ICL) gained significant attention due to its effectivenessand efficiency. However, ICL is very sensitive to the choice, order, andverbaliser used to encode the demonstrations in the prompt. Retrieval-AugmentedICL methods try to address this problem by leveraging retrievers to extractsemantically related examples as demonstrations. While this approach yieldsmore accurate results, its robustness against various types of adversarialattacks, including perturbations on test samples, demonstrations, and retrieveddata, remains under-explored. Our study reveals that retrieval-augmented modelscan enhance robustness against test sample attacks, outperforming vanilla ICLwith a 4.87% reduction in Attack Success Rate (ASR); however, they exhibitoverconfidence in the demonstrations, leading to a 2% increase in ASR fordemonstration attacks. Adversarial training can help improve the robustness ofICL methods to adversarial attacks; however, such a training scheme can be toocostly in the context of LLMs. As an alternative, we introduce an effectivetraining-free adversarial defence method, DARD, which enriches the example poolwith those attacked samples. We show that DARD yields improvements inperformance and robustness, achieving a 15% reduction in ASR over thebaselines. Code and data are released to encourage further research:https://github.com/simonucl/adv-retreival-icl</description><author>Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan</author><pubDate>Wed, 10 Jul 2024 11:08:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.15984v2</guid></item></channel></rss>