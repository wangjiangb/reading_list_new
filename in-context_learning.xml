<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 26 Aug 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</title><link>http://arxiv.org/abs/2508.11258v1</link><description>Instruction fine-tuned large language models (LLMs) enable a simple zero-shotor few-shot prompting paradigm, also known as in-context learning, for buildingprediction models. This convenience, combined with continued advances in LLMcapability, has the potential to drive their adoption across a broad range ofdomains, including high-stakes applications where group fairness -- preventingdisparate impacts across demographic groups -- is essential. The majority ofexisting approaches to enforcing group fairness on LLM-based classifiers relyon traditional fair algorithms applied via model fine-tuning or head-tuning onfinal-layer embeddings, but they are no longer applicable to closed-weight LLMsunder the in-context learning setting, which include some of the most capablecommercial models today, such as GPT-4, Gemini, and Claude. In this paper, wepropose a framework for deriving fair classifiers from closed-weight LLMs viaprompting: the LLM is treated as a feature extractor, and features are elicitedfrom its probabilistic predictions (e.g., token log probabilities) usingprompts strategically designed for the specified fairness criterion to obtainsufficient statistics for fair classification; a fair algorithm is then appliedto these features to train a lightweight fair classifier in a post-hoc manner.Experiments on five datasets, including three tabular ones, demonstrate strongaccuracy-fairness tradeoffs for the classifiers derived by our framework fromboth open-weight and closed-weight LLMs; in particular, our framework isdata-efficient and outperforms fair classifiers trained on LLM embeddings(i.e., head-tuning) or from scratch on raw tabular features.</description><author>Ruicheng Xian, Yuxuan Wan, Han Zhao</author><pubDate>Fri, 15 Aug 2025 06:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11258v1</guid></item><item><title>TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation</title><link>http://arxiv.org/abs/2406.10450v3</link><description>There is a growing interest in utilizing large-scale language models (LLMs)to advance next-generation Recommender Systems (RecSys), driven by theiroutstanding language understanding and in-context learning capabilities. Inthis scenario, tokenizing (i.e., indexing) users and items becomes essentialfor ensuring a seamless alignment of LLMs with recommendations. While severalstudies have made progress in representing users and items through textualcontents or latent representations, challenges remain in efficiently capturinghigh-order collaborative knowledge into discrete tokens that are compatiblewith LLMs. Additionally, the majority of existing tokenization approaches oftenface difficulties in generalizing effectively to new/unseen users or items thatwere not in the training corpus. To address these challenges, we propose anovel framework called TokenRec, which introduces not only an effective IDtokenization strategy but also an efficient retrieval paradigm for LLM-basedrecommendations. Specifically, our tokenization strategy, MaskedVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/itemrepresentations learned from collaborative filtering into discrete tokens, thusachieving a smooth incorporation of high-order collaborative knowledge and ageneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,our generative retrieval paradigm is designed to efficiently recommend top-$K$items for users to eliminate the need for the time-consuming auto-regressivedecoding and beam search processes used by LLMs, thus significantly reducinginference time. Comprehensive experiments validate the effectiveness of theproposed methods, demonstrating that TokenRec outperforms competitivebenchmarks, including both traditional recommender systems and emergingLLM-based recommender systems.</description><author>Haohao Qu, Wenqi Fan, Zihuai Zhao, Qing Li</author><pubDate>Fri, 15 Aug 2025 05:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10450v3</guid></item><item><title>LLMs Are In-Context Bandit Reinforcement Learners</title><link>http://arxiv.org/abs/2410.05362v3</link><description>Large Language Models (LLMs) excel at in-context learning (ICL), a supervisedlearning technique that relies on adding annotated examples to the modelcontext. We investigate a contextual bandit version of in-context reinforcementlearning (ICRL), where models learn in-context, online, from external reward,instead of supervised data. We show that LLMs effectively demonstrate suchlearning, and provide a detailed study of the phenomena, experimenting withchallenging classification tasks and models of sizes from 500M to 70Bparameters. This includes identifying and addressing the instability of theprocess, demonstrating learning with both semantic and abstract labels, andshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, whilealso underscoring fundamental limitations in their implicit reasoning abouterrors.</description><author>Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi</author><pubDate>Mon, 18 Aug 2025 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05362v3</guid></item><item><title>When can in-context learning generalize out of task distribution?</title><link>http://arxiv.org/abs/2506.05574v2</link><description>In-context learning (ICL) is a remarkable capability of pretrainedtransformers that allows models to generalize to unseen tasks after seeing onlya few examples. We investigate empirically the conditions necessary on thepretraining distribution for ICL to emerge and generalize\emph{out-of-distribution}. Previous work has focused on the number of distincttasks necessary in the pretraining dataset. Here, we use a different notion oftask diversity to study the emergence of ICL in transformers trained on linearfunctions. We find that as task diversity increases, transformers undergo atransition from a specialized solution, which exhibits ICL only within thepretraining task distribution, to a solution which generalizes out ofdistribution to the entire task space. We also investigate the nature of thesolutions learned by the transformer on both sides of the transition, andobserve similar transitions in nonlinear regression problems. We construct aphase diagram to characterize how our concept of task diversity interacts withthe number of pretraining tasks. In addition, we explore how factors such asthe depth of the model and the dimensionality of the regression probleminfluence the transition.</description><author>Chase Goddard, Lindsay M. Smith, Vudtiwat Ngampruetikorn, David J. Schwab</author><pubDate>Mon, 18 Aug 2025 16:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05574v2</guid></item><item><title>Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming</title><link>http://arxiv.org/abs/2409.11041v3</link><description>While there has been a lot of research recently on robots in householdenvironments, at the present time, most robots in existence can be found onshop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assemblylines traditionally require expert programming, limiting ability to makechanges, or manual guidance, limiting expressivity of the resulting programs.To address these limitations, we explore using Large Language Models (LLMs),and in particular, their abilities of doing in-context learning, forconversational code generation. As a first step, we define RATS, the``Repetitive Assembly Task'', a 2D building task designed to lay the foundationfor simulating industry assembly scenarios. In this task, a `programmer'instructs a cobot, using natural language, on how a certain assembly is to bebuilt; that is, the programmer induces a program, through natural language. Wecreate a dataset that pairs target structures with various example instructions(human-authored, template-based, and model-generated) and example code. Withthis, we systematically evaluate the capabilities of state-of-the-art LLMs forsynthesising this kind of code, given in-context examples. Evaluating in asimulated environment, we find that LLMs are capable of generating accurate`first order code' (instruction sequences), but have problems producing`higher-order code' (abstractions such as functions, or use of loops).</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11041v3</guid></item><item><title>Adversarial Attacks against Neural Ranking Models via In-Context Learning</title><link>http://arxiv.org/abs/2508.15283v1</link><description>While neural ranking models (NRMs) have shown high effectiveness, they remainsusceptible to adversarial manipulation. In this work, we introduce Few-ShotAdversarial Prompting (FSAP), a novel black-box attack framework that leveragesthe in-context learning capabilities of Large Language Models (LLMs) togenerate high-ranking adversarial documents. Unlike previous approaches thatrely on token-level perturbations or manual rewriting of existing documents,FSAP formulates adversarial attacks entirely through few-shot prompting,requiring no gradient access or internal model instrumentation. By conditioningthe LLM on a small support set of previously observed harmful examples, FSAPsynthesizes grammatically fluent and topically coherent documents that subtlyembed false or misleading information and rank competitively against authenticcontent. We instantiate FSAP in two modes: FSAP-IntraQ, which leverages harmfulexamples from the same query to enhance topic fidelity, and FSAP-InterQ, whichenables broader generalization by transferring adversarial patterns acrossunrelated queries. Our experiments on the TREC 2020 and 2021 HealthMisinformation Tracks, using four diverse neural ranking models, reveal thatFSAP-generated documents consistently outrank credible, factually accuratedocuments. Furthermore, our analysis demonstrates that these adversarialoutputs exhibit strong stance alignment and low detectability, posing arealistic and scalable threat to neural retrieval systems. FSAP alsoeffectively generalizes across both proprietary and open-source LLMs.</description><author>Amin Bigdeli, Negar Arabzadeh, Ebrahim Bagheri, Charles L. A. Clarke</author><pubDate>Thu, 21 Aug 2025 06:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15283v1</guid></item><item><title>Retrieval-Augmented Review Generation for Poisoning Recommender Systems</title><link>http://arxiv.org/abs/2508.15252v1</link><description>Recent studies have shown that recommender systems (RSs) are highlyvulnerable to data poisoning attacks, where malicious actors inject fake userprofiles, including a group of well-designed fake ratings, to manipulaterecommendations. Due to security and privacy constraints in practice, attackerstypically possess limited knowledge of the victim system and thus need to craftprofiles that have transferability across black-box RSs. To maximize the attackimpact, the profiles often remains imperceptible. However, generating suchhigh-quality profiles with the restricted resources is challenging. Some workssuggest incorporating fake textual reviews to strengthen the profiles; yet, thepoor quality of the reviews largely undermines the attack effectiveness andimperceptibility under the practical setting. To tackle the above challenges, in this paper, we propose to enhance thequality of the review text by harnessing in-context learning (ICL) capabilitiesof multimodal foundation models. To this end, we introduce a demonstrationretrieval algorithm and a text style transfer strategy to augment the navieICL. Specifically, we propose a novel practical attack framework named RAGAN togenerate high-quality fake user profiles, which can gain insights into therobustness of RSs. The profiles are generated by a jailbreaker andcollaboratively optimized on an instructional agent and a guardian to improvethe attack transferability and imperceptibility. Comprehensive experiments onvarious real-world datasets demonstrate that RAGAN achieves thestate-of-the-art poisoning attack performance.</description><author>Shiyi Yang, Xinshu Li, Guanglin Zhou, Chen Wang, Xiwei Xu, Liming Zhu, Lina Yao</author><pubDate>Thu, 21 Aug 2025 05:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15252v1</guid></item><item><title>Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent</title><link>http://arxiv.org/abs/2508.15243v1</link><description>We present Comp-X, the first intelligently interactive image compressionparadigm empowered by the impressive reasoning capability of large languagemodel (LLM) agent. Notably, commonly used image codecs usually suffer fromlimited coding modes and rely on manual mode selection by engineers, makingthem unfriendly for unprofessional users. To overcome this, we advance theevolution of image coding paradigm by introducing three key innovations: (i)multi-functional coding framework, which unifies different coding modes ofvarious objective/requirements, including human-machine perception, variablecoding, and spatial bit allocation, into one framework. (ii) interactive codingagent, where we propose an augmented in-context learning method with codingexpert feedback to teach the LLM agent how to understand the coding request,mode selection, and the use of the coding tools. (iii) IIC-bench, the firstdedicated benchmark comprising diverse user requests and the correspondingannotations from coding experts, which is systematically designed forintelligently interactive image compression evaluation. Extensive experimentalresults demonstrate that our proposed Comp-X can understand the coding requestsefficiently and achieve impressive textual interaction capability. Meanwhile,it can maintain comparable compression performance even with a single codingframework, providing a promising avenue for artificial general intelligence(AGI) in image compression.</description><author>Yixin Gao, Xin Li, Xiaohan Pan, Runsen Feng, Bingchen Li, Yunpeng Qi, Yiting Lu, Zhengxue Cheng, Zhibo Chen, Jörn Ostermann</author><pubDate>Thu, 21 Aug 2025 05:09:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15243v1</guid></item><item><title>Improving in-context learning with a better scoring function</title><link>http://arxiv.org/abs/2508.14685v1</link><description>Large language models (LLMs) exhibit a remarkable capacity to learn byanalogy, known as in-context learning (ICL). However, recent studies haverevealed limitations in this ability. In this paper, we examine theselimitations on tasks involving first-order quantifiers such as {\em all} and{\em some}, as well as on ICL with linear functions. We identify Softmax, thescoring function in attention mechanism, as a contributing factor to theseconstraints. To address this, we propose \textbf{scaled signed averaging(SSA)}, a novel alternative to Softmax. Empirical results show that SSAdramatically improves performance on our target tasks. Furthermore, we evaluateboth encoder-only and decoder-only transformers models with SSA, demonstratingthat they match or exceed their Softmax-based counterparts across a variety oflinguistic probing tasks.</description><author>Omar Naim, Swarnadeep Bhar, Jérôme Bolte, Nicholas Asher</author><pubDate>Wed, 20 Aug 2025 13:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14685v1</guid></item><item><title>ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities</title><link>http://arxiv.org/abs/2508.14377v1</link><description>Large language models (LLMs) have demonstrated potential in educationalapplications, yet their capacity to accurately assess the cognitive alignmentof reading materials with students' developmental stages remains insufficientlyexplored. This gap is particularly critical given the foundational educationalprinciple of the Zone of Proximal Development (ZPD), which emphasizes the needto match learning resources with Students' Cognitive Abilities (SCA). Despitethe importance of this alignment, there is a notable absence of comprehensivestudies investigating LLMs' ability to evaluate reading comprehensiondifficulty across different student age groups, especially in the context ofChinese language education. To fill this gap, we introduce ZPD-SCA, a novelbenchmark specifically designed to assess stage-level Chinese readingcomprehension difficulty. The benchmark is annotated by 60 Special Gradeteachers, a group that represents the top 0.15% of all in-service teachersnationwide. Experimental results reveal that LLMs perform poorly in zero-shotlearning scenarios, with Qwen-max and GLM even falling below the probability ofrandom guessing. When provided with in-context examples, LLMs performanceimproves substantially, with some models achieving nearly double the accuracyof their zero-shot baselines. These results reveal that LLMs possess emergingabilities to assess reading difficulty, while also exposing limitations intheir current training for educationally aligned judgment. Notably, even thebest-performing models display systematic directional biases, suggestingdifficulties in accurately aligning material difficulty with SCA. Furthermore,significant variations in model performance across different genres underscorethe complexity of task. We envision that ZPD-SCA can provide a foundation forevaluating and improving LLMs in cognitively aligned educational applications.</description><author>Wenhan Dong, Zhen Sun, Yuemeng Zhao, Zifan Peng, Jun Wu, Jingyi Zheng, Yule Liu, Xinlei He, Yu Wang, Ruiming Wang, Xinyi Huang, Lei Mo</author><pubDate>Wed, 20 Aug 2025 03:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14377v1</guid></item><item><title>One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks</title><link>http://arxiv.org/abs/2505.15009v2</link><description>We study the approximation capabilities and on-convergence behaviors ofone-layer transformers on the noiseless and noisy in-context reasoning ofnext-token prediction. Existing theoretical results focus on understanding thein-context reasoning behaviors for either the first gradient step or when thenumber of samples is infinite. Furthermore, no convergence rates norgeneralization abilities were known. Our work addresses these gaps by showingthat there exists a class of one-layer transformers that are provablyBayes-optimal with both linear and ReLU attention. When being trained withgradient descent, we show via a finite-sample analysis that the expected lossof these transformers converges at linear rate to the Bayes risk. Moreover, weprove that the trained models generalize to unseen samples as well as exhibitlearning behaviors that were empirically observed in previous works. Ourtheoretical findings are further supported by extensive empirical validations.</description><author>Quan Nguyen, Thanh Nguyen-Tang</author><pubDate>Wed, 20 Aug 2025 03:05:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.15009v2</guid></item><item><title>Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices</title><link>http://arxiv.org/abs/2503.10652v3</link><description>Stated preference (SP) surveys are a key method to research how individualsmake trade-offs in hypothetical, also futuristic, scenarios. In energy contextthis includes key decarbonisation enablement contexts, such as low-carbontechnologies, distributed renewable energy generation, and demand-side response[1,2]. However, they tend to be costly, time-consuming, and can be affected byrespondent fatigue and ethical constraints. Large language models (LLMs) havedemonstrated remarkable capabilities in generating human-like textualresponses, prompting growing interest in their application to survey research.This study investigates the use of LLMs to simulate consumer choices inenergy-related SP surveys and explores their integration into data analysisworkflows. A series of test scenarios were designed to systematically assessthe simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 andDeepSeek-R1) at both individual and aggregated levels, considering contextsfactors such as prompt design, in-context learning (ICL), chain-of-thought(CoT) reasoning, LLM types, integration with traditional choice models, andpotential biases. Cloud-based LLMs do not consistently outperform smaller localmodels. In this study, the reasoning model DeepSeek-R1 achieves the highestaverage accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factoridentification, and choice distribution alignment. Across models, systematicbiases are observed against the gas boiler and no-retrofit options, with apreference for more energy-efficient alternatives. The findings suggest thatprevious SP choices are the most effective input factor, while longer promptswith additional factors and varied formats can cause LLMs to lose focus,reducing accuracy.</description><author>Han Wang, Jacek Pawlak, Aruna Sivakumar</author><pubDate>Fri, 22 Aug 2025 17:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.10652v3</guid></item><item><title>CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention</title><link>http://arxiv.org/abs/2505.17097v2</link><description>Multimodal in-context learning (ICL) is emerging as a key capability thatenables large vision-language models (LVLMs) to adapt to novel tasks withoutparameter updates, expanding their utility across various real-worldapplications. However, ICL remains unstable, even with well-matched in-contextdemonstrations (ICDs), suggesting that LVLMs struggle to fully utilize theprovided context. While existing efforts focus on prompt engineering orpost-hoc logit calibration, we instead investigate the underlying attentiondynamics to overcome LVLMs' inherent limitations. We identify two criticaldeficits in their self-attention that impair effective ICL. To bridge the gap,we propose \textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-playand training-free method that dynamically modulates LVLM's attention logitsbased on the input in-context sequence. CAMA employs a two-stage attentionmodulation to address both identified deficits, enhancing the focus onsemantically significant tokens, particularly visual ones. Across four LVLMsand seven benchmarks, CAMA consistently outperforms vanilla models andbaselines, demonstrating great effectiveness and generalization. It can alsoactivate the desired effects of prompt engineering methods and remains robustunder diverse sequence configurations. Thus, CAMA paves the way for deeperexplorations of attention dynamics to advance multimodal reasoning.</description><author>Yanshu Li, Jianjiang Yang, Ziteng Yang, Bozheng Li, Hongyang He, Zhengtao Yao, Ligong Han, Yingjie Victor Chen, Songlin Fei, Dongfang Liu, Ruixiang Tang</author><pubDate>Fri, 22 Aug 2025 14:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17097v2</guid></item></channel></rss>