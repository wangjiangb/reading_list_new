<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 19 Aug 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</title><link>http://arxiv.org/abs/2508.11258v1</link><description>Instruction fine-tuned large language models (LLMs) enable a simple zero-shotor few-shot prompting paradigm, also known as in-context learning, for buildingprediction models. This convenience, combined with continued advances in LLMcapability, has the potential to drive their adoption across a broad range ofdomains, including high-stakes applications where group fairness -- preventingdisparate impacts across demographic groups -- is essential. The majority ofexisting approaches to enforcing group fairness on LLM-based classifiers relyon traditional fair algorithms applied via model fine-tuning or head-tuning onfinal-layer embeddings, but they are no longer applicable to closed-weight LLMsunder the in-context learning setting, which include some of the most capablecommercial models today, such as GPT-4, Gemini, and Claude. In this paper, wepropose a framework for deriving fair classifiers from closed-weight LLMs viaprompting: the LLM is treated as a feature extractor, and features are elicitedfrom its probabilistic predictions (e.g., token log probabilities) usingprompts strategically designed for the specified fairness criterion to obtainsufficient statistics for fair classification; a fair algorithm is then appliedto these features to train a lightweight fair classifier in a post-hoc manner.Experiments on five datasets, including three tabular ones, demonstrate strongaccuracy-fairness tradeoffs for the classifiers derived by our framework fromboth open-weight and closed-weight LLMs; in particular, our framework isdata-efficient and outperforms fair classifiers trained on LLM embeddings(i.e., head-tuning) or from scratch on raw tabular features.</description><author>Ruicheng Xian, Yuxuan Wan, Han Zhao</author><pubDate>Fri, 15 Aug 2025 06:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11258v1</guid></item><item><title>TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation</title><link>http://arxiv.org/abs/2406.10450v3</link><description>There is a growing interest in utilizing large-scale language models (LLMs)to advance next-generation Recommender Systems (RecSys), driven by theiroutstanding language understanding and in-context learning capabilities. Inthis scenario, tokenizing (i.e., indexing) users and items becomes essentialfor ensuring a seamless alignment of LLMs with recommendations. While severalstudies have made progress in representing users and items through textualcontents or latent representations, challenges remain in efficiently capturinghigh-order collaborative knowledge into discrete tokens that are compatiblewith LLMs. Additionally, the majority of existing tokenization approaches oftenface difficulties in generalizing effectively to new/unseen users or items thatwere not in the training corpus. To address these challenges, we propose anovel framework called TokenRec, which introduces not only an effective IDtokenization strategy but also an efficient retrieval paradigm for LLM-basedrecommendations. Specifically, our tokenization strategy, MaskedVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/itemrepresentations learned from collaborative filtering into discrete tokens, thusachieving a smooth incorporation of high-order collaborative knowledge and ageneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,our generative retrieval paradigm is designed to efficiently recommend top-$K$items for users to eliminate the need for the time-consuming auto-regressivedecoding and beam search processes used by LLMs, thus significantly reducinginference time. Comprehensive experiments validate the effectiveness of theproposed methods, demonstrating that TokenRec outperforms competitivebenchmarks, including both traditional recommender systems and emergingLLM-based recommender systems.</description><author>Haohao Qu, Wenqi Fan, Zihuai Zhao, Qing Li</author><pubDate>Fri, 15 Aug 2025 05:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10450v3</guid></item></channel></rss>