<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Aug 2025 17:37:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</title><link>http://arxiv.org/abs/2508.11258v1</link><description>Instruction fine-tuned large language models (LLMs) enable a simple zero-shotor few-shot prompting paradigm, also known as in-context learning, for buildingprediction models. This convenience, combined with continued advances in LLMcapability, has the potential to drive their adoption across a broad range ofdomains, including high-stakes applications where group fairness -- preventingdisparate impacts across demographic groups -- is essential. The majority ofexisting approaches to enforcing group fairness on LLM-based classifiers relyon traditional fair algorithms applied via model fine-tuning or head-tuning onfinal-layer embeddings, but they are no longer applicable to closed-weight LLMsunder the in-context learning setting, which include some of the most capablecommercial models today, such as GPT-4, Gemini, and Claude. In this paper, wepropose a framework for deriving fair classifiers from closed-weight LLMs viaprompting: the LLM is treated as a feature extractor, and features are elicitedfrom its probabilistic predictions (e.g., token log probabilities) usingprompts strategically designed for the specified fairness criterion to obtainsufficient statistics for fair classification; a fair algorithm is then appliedto these features to train a lightweight fair classifier in a post-hoc manner.Experiments on five datasets, including three tabular ones, demonstrate strongaccuracy-fairness tradeoffs for the classifiers derived by our framework fromboth open-weight and closed-weight LLMs; in particular, our framework isdata-efficient and outperforms fair classifiers trained on LLM embeddings(i.e., head-tuning) or from scratch on raw tabular features.</description><author>Ruicheng Xian, Yuxuan Wan, Han Zhao</author><pubDate>Fri, 15 Aug 2025 06:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11258v1</guid></item><item><title>TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation</title><link>http://arxiv.org/abs/2406.10450v3</link><description>There is a growing interest in utilizing large-scale language models (LLMs)to advance next-generation Recommender Systems (RecSys), driven by theiroutstanding language understanding and in-context learning capabilities. Inthis scenario, tokenizing (i.e., indexing) users and items becomes essentialfor ensuring a seamless alignment of LLMs with recommendations. While severalstudies have made progress in representing users and items through textualcontents or latent representations, challenges remain in efficiently capturinghigh-order collaborative knowledge into discrete tokens that are compatiblewith LLMs. Additionally, the majority of existing tokenization approaches oftenface difficulties in generalizing effectively to new/unseen users or items thatwere not in the training corpus. To address these challenges, we propose anovel framework called TokenRec, which introduces not only an effective IDtokenization strategy but also an efficient retrieval paradigm for LLM-basedrecommendations. Specifically, our tokenization strategy, MaskedVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/itemrepresentations learned from collaborative filtering into discrete tokens, thusachieving a smooth incorporation of high-order collaborative knowledge and ageneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,our generative retrieval paradigm is designed to efficiently recommend top-$K$items for users to eliminate the need for the time-consuming auto-regressivedecoding and beam search processes used by LLMs, thus significantly reducinginference time. Comprehensive experiments validate the effectiveness of theproposed methods, demonstrating that TokenRec outperforms competitivebenchmarks, including both traditional recommender systems and emergingLLM-based recommender systems.</description><author>Haohao Qu, Wenqi Fan, Zihuai Zhao, Qing Li</author><pubDate>Fri, 15 Aug 2025 05:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10450v3</guid></item><item><title>LLMs Are In-Context Bandit Reinforcement Learners</title><link>http://arxiv.org/abs/2410.05362v3</link><description>Large Language Models (LLMs) excel at in-context learning (ICL), a supervisedlearning technique that relies on adding annotated examples to the modelcontext. We investigate a contextual bandit version of in-context reinforcementlearning (ICRL), where models learn in-context, online, from external reward,instead of supervised data. We show that LLMs effectively demonstrate suchlearning, and provide a detailed study of the phenomena, experimenting withchallenging classification tasks and models of sizes from 500M to 70Bparameters. This includes identifying and addressing the instability of theprocess, demonstrating learning with both semantic and abstract labels, andshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, whilealso underscoring fundamental limitations in their implicit reasoning abouterrors.</description><author>Giovanni Monea, Antoine Bosselut, Kiant√© Brantley, Yoav Artzi</author><pubDate>Mon, 18 Aug 2025 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05362v3</guid></item><item><title>When can in-context learning generalize out of task distribution?</title><link>http://arxiv.org/abs/2506.05574v2</link><description>In-context learning (ICL) is a remarkable capability of pretrainedtransformers that allows models to generalize to unseen tasks after seeing onlya few examples. We investigate empirically the conditions necessary on thepretraining distribution for ICL to emerge and generalize\emph{out-of-distribution}. Previous work has focused on the number of distincttasks necessary in the pretraining dataset. Here, we use a different notion oftask diversity to study the emergence of ICL in transformers trained on linearfunctions. We find that as task diversity increases, transformers undergo atransition from a specialized solution, which exhibits ICL only within thepretraining task distribution, to a solution which generalizes out ofdistribution to the entire task space. We also investigate the nature of thesolutions learned by the transformer on both sides of the transition, andobserve similar transitions in nonlinear regression problems. We construct aphase diagram to characterize how our concept of task diversity interacts withthe number of pretraining tasks. In addition, we explore how factors such asthe depth of the model and the dimensionality of the regression probleminfluence the transition.</description><author>Chase Goddard, Lindsay M. Smith, Vudtiwat Ngampruetikorn, David J. Schwab</author><pubDate>Mon, 18 Aug 2025 16:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05574v2</guid></item><item><title>Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming</title><link>http://arxiv.org/abs/2409.11041v3</link><description>While there has been a lot of research recently on robots in householdenvironments, at the present time, most robots in existence can be found onshop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assemblylines traditionally require expert programming, limiting ability to makechanges, or manual guidance, limiting expressivity of the resulting programs.To address these limitations, we explore using Large Language Models (LLMs),and in particular, their abilities of doing in-context learning, forconversational code generation. As a first step, we define RATS, the``Repetitive Assembly Task'', a 2D building task designed to lay the foundationfor simulating industry assembly scenarios. In this task, a `programmer'instructs a cobot, using natural language, on how a certain assembly is to bebuilt; that is, the programmer induces a program, through natural language. Wecreate a dataset that pairs target structures with various example instructions(human-authored, template-based, and model-generated) and example code. Withthis, we systematically evaluate the capabilities of state-of-the-art LLMs forsynthesising this kind of code, given in-context examples. Evaluating in asimulated environment, we find that LLMs are capable of generating accurate`first order code' (instruction sequences), but have problems producing`higher-order code' (abstractions such as functions, or use of loops).</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11041v3</guid></item></channel></rss>