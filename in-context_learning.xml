<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivin-context learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 31 Aug 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</title><link>http://arxiv.org/abs/2508.11258v1</link><description>Instruction fine-tuned large language models (LLMs) enable a simple zero-shotor few-shot prompting paradigm, also known as in-context learning, for buildingprediction models. This convenience, combined with continued advances in LLMcapability, has the potential to drive their adoption across a broad range ofdomains, including high-stakes applications where group fairness -- preventingdisparate impacts across demographic groups -- is essential. The majority ofexisting approaches to enforcing group fairness on LLM-based classifiers relyon traditional fair algorithms applied via model fine-tuning or head-tuning onfinal-layer embeddings, but they are no longer applicable to closed-weight LLMsunder the in-context learning setting, which include some of the most capablecommercial models today, such as GPT-4, Gemini, and Claude. In this paper, wepropose a framework for deriving fair classifiers from closed-weight LLMs viaprompting: the LLM is treated as a feature extractor, and features are elicitedfrom its probabilistic predictions (e.g., token log probabilities) usingprompts strategically designed for the specified fairness criterion to obtainsufficient statistics for fair classification; a fair algorithm is then appliedto these features to train a lightweight fair classifier in a post-hoc manner.Experiments on five datasets, including three tabular ones, demonstrate strongaccuracy-fairness tradeoffs for the classifiers derived by our framework fromboth open-weight and closed-weight LLMs; in particular, our framework isdata-efficient and outperforms fair classifiers trained on LLM embeddings(i.e., head-tuning) or from scratch on raw tabular features.</description><author>Ruicheng Xian, Yuxuan Wan, Han Zhao</author><pubDate>Fri, 15 Aug 2025 06:50:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11258v1</guid></item><item><title>TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation</title><link>http://arxiv.org/abs/2406.10450v3</link><description>There is a growing interest in utilizing large-scale language models (LLMs)to advance next-generation Recommender Systems (RecSys), driven by theiroutstanding language understanding and in-context learning capabilities. Inthis scenario, tokenizing (i.e., indexing) users and items becomes essentialfor ensuring a seamless alignment of LLMs with recommendations. While severalstudies have made progress in representing users and items through textualcontents or latent representations, challenges remain in efficiently capturinghigh-order collaborative knowledge into discrete tokens that are compatiblewith LLMs. Additionally, the majority of existing tokenization approaches oftenface difficulties in generalizing effectively to new/unseen users or items thatwere not in the training corpus. To address these challenges, we propose anovel framework called TokenRec, which introduces not only an effective IDtokenization strategy but also an efficient retrieval paradigm for LLM-basedrecommendations. Specifically, our tokenization strategy, MaskedVector-Quantized (MQ) Tokenizer, involves quantizing the masked user/itemrepresentations learned from collaborative filtering into discrete tokens, thusachieving a smooth incorporation of high-order collaborative knowledge and ageneralizable tokenization of users and items for LLM-based RecSys. Meanwhile,our generative retrieval paradigm is designed to efficiently recommend top-$K$items for users to eliminate the need for the time-consuming auto-regressivedecoding and beam search processes used by LLMs, thus significantly reducinginference time. Comprehensive experiments validate the effectiveness of theproposed methods, demonstrating that TokenRec outperforms competitivebenchmarks, including both traditional recommender systems and emergingLLM-based recommender systems.</description><author>Haohao Qu, Wenqi Fan, Zihuai Zhao, Qing Li</author><pubDate>Fri, 15 Aug 2025 05:34:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10450v3</guid></item><item><title>LLMs Are In-Context Bandit Reinforcement Learners</title><link>http://arxiv.org/abs/2410.05362v3</link><description>Large Language Models (LLMs) excel at in-context learning (ICL), a supervisedlearning technique that relies on adding annotated examples to the modelcontext. We investigate a contextual bandit version of in-context reinforcementlearning (ICRL), where models learn in-context, online, from external reward,instead of supervised data. We show that LLMs effectively demonstrate suchlearning, and provide a detailed study of the phenomena, experimenting withchallenging classification tasks and models of sizes from 500M to 70Bparameters. This includes identifying and addressing the instability of theprocess, demonstrating learning with both semantic and abstract labels, andshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, whilealso underscoring fundamental limitations in their implicit reasoning abouterrors.</description><author>Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi</author><pubDate>Mon, 18 Aug 2025 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05362v3</guid></item><item><title>When can in-context learning generalize out of task distribution?</title><link>http://arxiv.org/abs/2506.05574v2</link><description>In-context learning (ICL) is a remarkable capability of pretrainedtransformers that allows models to generalize to unseen tasks after seeing onlya few examples. We investigate empirically the conditions necessary on thepretraining distribution for ICL to emerge and generalize\emph{out-of-distribution}. Previous work has focused on the number of distincttasks necessary in the pretraining dataset. Here, we use a different notion oftask diversity to study the emergence of ICL in transformers trained on linearfunctions. We find that as task diversity increases, transformers undergo atransition from a specialized solution, which exhibits ICL only within thepretraining task distribution, to a solution which generalizes out ofdistribution to the entire task space. We also investigate the nature of thesolutions learned by the transformer on both sides of the transition, andobserve similar transitions in nonlinear regression problems. We construct aphase diagram to characterize how our concept of task diversity interacts withthe number of pretraining tasks. In addition, we explore how factors such asthe depth of the model and the dimensionality of the regression probleminfluence the transition.</description><author>Chase Goddard, Lindsay M. Smith, Vudtiwat Ngampruetikorn, David J. Schwab</author><pubDate>Mon, 18 Aug 2025 16:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05574v2</guid></item><item><title>Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming</title><link>http://arxiv.org/abs/2409.11041v3</link><description>While there has been a lot of research recently on robots in householdenvironments, at the present time, most robots in existence can be found onshop floors, and most interactions between humans and robots happen there.``Collaborative robots'' (cobots) designed to work alongside humans on assemblylines traditionally require expert programming, limiting ability to makechanges, or manual guidance, limiting expressivity of the resulting programs.To address these limitations, we explore using Large Language Models (LLMs),and in particular, their abilities of doing in-context learning, forconversational code generation. As a first step, we define RATS, the``Repetitive Assembly Task'', a 2D building task designed to lay the foundationfor simulating industry assembly scenarios. In this task, a `programmer'instructs a cobot, using natural language, on how a certain assembly is to bebuilt; that is, the programmer induces a program, through natural language. Wecreate a dataset that pairs target structures with various example instructions(human-authored, template-based, and model-generated) and example code. Withthis, we systematically evaluate the capabilities of state-of-the-art LLMs forsynthesising this kind of code, given in-context examples. Evaluating in asimulated environment, we find that LLMs are capable of generating accurate`first order code' (instruction sequences), but have problems producing`higher-order code' (abstractions such as functions, or use of loops).</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:35:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11041v3</guid></item><item><title>Adversarial Attacks against Neural Ranking Models via In-Context Learning</title><link>http://arxiv.org/abs/2508.15283v1</link><description>While neural ranking models (NRMs) have shown high effectiveness, they remainsusceptible to adversarial manipulation. In this work, we introduce Few-ShotAdversarial Prompting (FSAP), a novel black-box attack framework that leveragesthe in-context learning capabilities of Large Language Models (LLMs) togenerate high-ranking adversarial documents. Unlike previous approaches thatrely on token-level perturbations or manual rewriting of existing documents,FSAP formulates adversarial attacks entirely through few-shot prompting,requiring no gradient access or internal model instrumentation. By conditioningthe LLM on a small support set of previously observed harmful examples, FSAPsynthesizes grammatically fluent and topically coherent documents that subtlyembed false or misleading information and rank competitively against authenticcontent. We instantiate FSAP in two modes: FSAP-IntraQ, which leverages harmfulexamples from the same query to enhance topic fidelity, and FSAP-InterQ, whichenables broader generalization by transferring adversarial patterns acrossunrelated queries. Our experiments on the TREC 2020 and 2021 HealthMisinformation Tracks, using four diverse neural ranking models, reveal thatFSAP-generated documents consistently outrank credible, factually accuratedocuments. Furthermore, our analysis demonstrates that these adversarialoutputs exhibit strong stance alignment and low detectability, posing arealistic and scalable threat to neural retrieval systems. FSAP alsoeffectively generalizes across both proprietary and open-source LLMs.</description><author>Amin Bigdeli, Negar Arabzadeh, Ebrahim Bagheri, Charles L. A. Clarke</author><pubDate>Thu, 21 Aug 2025 06:19:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15283v1</guid></item><item><title>Retrieval-Augmented Review Generation for Poisoning Recommender Systems</title><link>http://arxiv.org/abs/2508.15252v1</link><description>Recent studies have shown that recommender systems (RSs) are highlyvulnerable to data poisoning attacks, where malicious actors inject fake userprofiles, including a group of well-designed fake ratings, to manipulaterecommendations. Due to security and privacy constraints in practice, attackerstypically possess limited knowledge of the victim system and thus need to craftprofiles that have transferability across black-box RSs. To maximize the attackimpact, the profiles often remains imperceptible. However, generating suchhigh-quality profiles with the restricted resources is challenging. Some workssuggest incorporating fake textual reviews to strengthen the profiles; yet, thepoor quality of the reviews largely undermines the attack effectiveness andimperceptibility under the practical setting. To tackle the above challenges, in this paper, we propose to enhance thequality of the review text by harnessing in-context learning (ICL) capabilitiesof multimodal foundation models. To this end, we introduce a demonstrationretrieval algorithm and a text style transfer strategy to augment the navieICL. Specifically, we propose a novel practical attack framework named RAGAN togenerate high-quality fake user profiles, which can gain insights into therobustness of RSs. The profiles are generated by a jailbreaker andcollaboratively optimized on an instructional agent and a guardian to improvethe attack transferability and imperceptibility. Comprehensive experiments onvarious real-world datasets demonstrate that RAGAN achieves thestate-of-the-art poisoning attack performance.</description><author>Shiyi Yang, Xinshu Li, Guanglin Zhou, Chen Wang, Xiwei Xu, Liming Zhu, Lina Yao</author><pubDate>Thu, 21 Aug 2025 05:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15252v1</guid></item><item><title>Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent</title><link>http://arxiv.org/abs/2508.15243v1</link><description>We present Comp-X, the first intelligently interactive image compressionparadigm empowered by the impressive reasoning capability of large languagemodel (LLM) agent. Notably, commonly used image codecs usually suffer fromlimited coding modes and rely on manual mode selection by engineers, makingthem unfriendly for unprofessional users. To overcome this, we advance theevolution of image coding paradigm by introducing three key innovations: (i)multi-functional coding framework, which unifies different coding modes ofvarious objective/requirements, including human-machine perception, variablecoding, and spatial bit allocation, into one framework. (ii) interactive codingagent, where we propose an augmented in-context learning method with codingexpert feedback to teach the LLM agent how to understand the coding request,mode selection, and the use of the coding tools. (iii) IIC-bench, the firstdedicated benchmark comprising diverse user requests and the correspondingannotations from coding experts, which is systematically designed forintelligently interactive image compression evaluation. Extensive experimentalresults demonstrate that our proposed Comp-X can understand the coding requestsefficiently and achieve impressive textual interaction capability. Meanwhile,it can maintain comparable compression performance even with a single codingframework, providing a promising avenue for artificial general intelligence(AGI) in image compression.</description><author>Yixin Gao, Xin Li, Xiaohan Pan, Runsen Feng, Bingchen Li, Yunpeng Qi, Yiting Lu, Zhengxue Cheng, Zhibo Chen, Jörn Ostermann</author><pubDate>Thu, 21 Aug 2025 05:09:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15243v1</guid></item><item><title>Improving in-context learning with a better scoring function</title><link>http://arxiv.org/abs/2508.14685v1</link><description>Large language models (LLMs) exhibit a remarkable capacity to learn byanalogy, known as in-context learning (ICL). However, recent studies haverevealed limitations in this ability. In this paper, we examine theselimitations on tasks involving first-order quantifiers such as {\em all} and{\em some}, as well as on ICL with linear functions. We identify Softmax, thescoring function in attention mechanism, as a contributing factor to theseconstraints. To address this, we propose \textbf{scaled signed averaging(SSA)}, a novel alternative to Softmax. Empirical results show that SSAdramatically improves performance on our target tasks. Furthermore, we evaluateboth encoder-only and decoder-only transformers models with SSA, demonstratingthat they match or exceed their Softmax-based counterparts across a variety oflinguistic probing tasks.</description><author>Omar Naim, Swarnadeep Bhar, Jérôme Bolte, Nicholas Asher</author><pubDate>Wed, 20 Aug 2025 13:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14685v1</guid></item><item><title>ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities</title><link>http://arxiv.org/abs/2508.14377v1</link><description>Large language models (LLMs) have demonstrated potential in educationalapplications, yet their capacity to accurately assess the cognitive alignmentof reading materials with students' developmental stages remains insufficientlyexplored. This gap is particularly critical given the foundational educationalprinciple of the Zone of Proximal Development (ZPD), which emphasizes the needto match learning resources with Students' Cognitive Abilities (SCA). Despitethe importance of this alignment, there is a notable absence of comprehensivestudies investigating LLMs' ability to evaluate reading comprehensiondifficulty across different student age groups, especially in the context ofChinese language education. To fill this gap, we introduce ZPD-SCA, a novelbenchmark specifically designed to assess stage-level Chinese readingcomprehension difficulty. The benchmark is annotated by 60 Special Gradeteachers, a group that represents the top 0.15% of all in-service teachersnationwide. Experimental results reveal that LLMs perform poorly in zero-shotlearning scenarios, with Qwen-max and GLM even falling below the probability ofrandom guessing. When provided with in-context examples, LLMs performanceimproves substantially, with some models achieving nearly double the accuracyof their zero-shot baselines. These results reveal that LLMs possess emergingabilities to assess reading difficulty, while also exposing limitations intheir current training for educationally aligned judgment. Notably, even thebest-performing models display systematic directional biases, suggestingdifficulties in accurately aligning material difficulty with SCA. Furthermore,significant variations in model performance across different genres underscorethe complexity of task. We envision that ZPD-SCA can provide a foundation forevaluating and improving LLMs in cognitively aligned educational applications.</description><author>Wenhan Dong, Zhen Sun, Yuemeng Zhao, Zifan Peng, Jun Wu, Jingyi Zheng, Yule Liu, Xinlei He, Yu Wang, Ruiming Wang, Xinyi Huang, Lei Mo</author><pubDate>Wed, 20 Aug 2025 03:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14377v1</guid></item><item><title>One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks</title><link>http://arxiv.org/abs/2505.15009v2</link><description>We study the approximation capabilities and on-convergence behaviors ofone-layer transformers on the noiseless and noisy in-context reasoning ofnext-token prediction. Existing theoretical results focus on understanding thein-context reasoning behaviors for either the first gradient step or when thenumber of samples is infinite. Furthermore, no convergence rates norgeneralization abilities were known. Our work addresses these gaps by showingthat there exists a class of one-layer transformers that are provablyBayes-optimal with both linear and ReLU attention. When being trained withgradient descent, we show via a finite-sample analysis that the expected lossof these transformers converges at linear rate to the Bayes risk. Moreover, weprove that the trained models generalize to unseen samples as well as exhibitlearning behaviors that were empirically observed in previous works. Ourtheoretical findings are further supported by extensive empirical validations.</description><author>Quan Nguyen, Thanh Nguyen-Tang</author><pubDate>Wed, 20 Aug 2025 03:05:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.15009v2</guid></item><item><title>Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices</title><link>http://arxiv.org/abs/2503.10652v3</link><description>Stated preference (SP) surveys are a key method to research how individualsmake trade-offs in hypothetical, also futuristic, scenarios. In energy contextthis includes key decarbonisation enablement contexts, such as low-carbontechnologies, distributed renewable energy generation, and demand-side response[1,2]. However, they tend to be costly, time-consuming, and can be affected byrespondent fatigue and ethical constraints. Large language models (LLMs) havedemonstrated remarkable capabilities in generating human-like textualresponses, prompting growing interest in their application to survey research.This study investigates the use of LLMs to simulate consumer choices inenergy-related SP surveys and explores their integration into data analysisworkflows. A series of test scenarios were designed to systematically assessthe simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 andDeepSeek-R1) at both individual and aggregated levels, considering contextsfactors such as prompt design, in-context learning (ICL), chain-of-thought(CoT) reasoning, LLM types, integration with traditional choice models, andpotential biases. Cloud-based LLMs do not consistently outperform smaller localmodels. In this study, the reasoning model DeepSeek-R1 achieves the highestaverage accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factoridentification, and choice distribution alignment. Across models, systematicbiases are observed against the gas boiler and no-retrofit options, with apreference for more energy-efficient alternatives. The findings suggest thatprevious SP choices are the most effective input factor, while longer promptswith additional factors and varied formats can cause LLMs to lose focus,reducing accuracy.</description><author>Han Wang, Jacek Pawlak, Aruna Sivakumar</author><pubDate>Fri, 22 Aug 2025 17:01:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.10652v3</guid></item><item><title>CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention</title><link>http://arxiv.org/abs/2505.17097v2</link><description>Multimodal in-context learning (ICL) is emerging as a key capability thatenables large vision-language models (LVLMs) to adapt to novel tasks withoutparameter updates, expanding their utility across various real-worldapplications. However, ICL remains unstable, even with well-matched in-contextdemonstrations (ICDs), suggesting that LVLMs struggle to fully utilize theprovided context. While existing efforts focus on prompt engineering orpost-hoc logit calibration, we instead investigate the underlying attentiondynamics to overcome LVLMs' inherent limitations. We identify two criticaldeficits in their self-attention that impair effective ICL. To bridge the gap,we propose \textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-playand training-free method that dynamically modulates LVLM's attention logitsbased on the input in-context sequence. CAMA employs a two-stage attentionmodulation to address both identified deficits, enhancing the focus onsemantically significant tokens, particularly visual ones. Across four LVLMsand seven benchmarks, CAMA consistently outperforms vanilla models andbaselines, demonstrating great effectiveness and generalization. It can alsoactivate the desired effects of prompt engineering methods and remains robustunder diverse sequence configurations. Thus, CAMA paves the way for deeperexplorations of attention dynamics to advance multimodal reasoning.</description><author>Yanshu Li, Jianjiang Yang, Ziteng Yang, Bozheng Li, Hongyang He, Zhengtao Yao, Ligong Han, Yingjie Victor Chen, Songlin Fei, Dongfang Liu, Ruixiang Tang</author><pubDate>Fri, 22 Aug 2025 14:44:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.17097v2</guid></item><item><title>Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis</title><link>http://arxiv.org/abs/2406.12719v4</link><description>Large Language Models (LLMs), already shown to ace various unstructured textcomprehension tasks, have also remarkably been shown to tackle table(structured) comprehension tasks without specific training. Building on earlierstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),model scale, instruction tuning, and domain bias affect Tabular QA (TQA)robustness by testing LLMs, under diverse augmentations and perturbations, ondiverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$,and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newerLLMs deliver stronger, more robust TQA performance, data contamination andreliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through anin-depth attention analysis, we reveal a strong correlation betweenperturbation-induced shifts in attention dispersion and the drops inperformance, with sensitivity peaking in the model's middle layers. Wehighlight the need for improved interpretable methodologies to develop morereliable LLMs for table comprehension. Through an in-depth attention analysis,we reveal a strong correlation between perturbation-induced shifts in attentiondispersion and performance drops, with sensitivity peaking in the model'smiddle layers. Based on these findings, we argue for the development ofstructure-aware self-attention mechanisms and domain-adaptive processingtechniques to improve the transparency, generalization, and real-worldreliability of LLMs on tabular data.</description><author>Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao</author><pubDate>Tue, 26 Aug 2025 15:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12719v4</guid></item><item><title>Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models</title><link>http://arxiv.org/abs/2410.19195v2</link><description>In-context learning (ICL) performance is highly sensitive to prompt design,yet the impact of class label options (e.g. lexicon or order) in zero-shotclassification remains underexplored. This study proposes LOADS (Label setOptimization via Activation Distribution kurtosiS), a post-hoc method forselecting optimal label sets in zero-shot ICL with large language models(LLMs). LOADS is built upon the observations in our empirical analysis, thefirst to systematically examine how label option design (i.e., lexical choice,order, and elaboration) impacts classification performance. This analysis showsthat the lexical choice of the labels in the prompt (such as agree vs. supportin stance classification) plays an important role in both model performance andmodel's sensitivity to the label order. A further investigation demonstratesthat optimal label words tend to activate fewer outlier neurons in LLMs'feed-forward networks. LOADS then leverages kurtosis to measure the neuronactivation distribution for label selection, requiring only a single forwardpass without gradient propagation or labelled data. The LOADS-selected labelwords consistently demonstrate effectiveness for zero-shot ICL acrossclassification tasks, datasets, models and languages, achieving maximumperformance gain from 0.54 to 0.76 compared to the conventional approach ofusing original dataset label words.</description><author>Yue Li, Zhixue Zhao, Carolina Scarton</author><pubDate>Tue, 26 Aug 2025 15:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.19195v2</guid></item><item><title>It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs</title><link>http://arxiv.org/abs/2508.19089v1</link><description>Extremely low-resource languages, especially those written in rare scripts,as shown in Figure 1, remain largely unsupported by large language models(LLMs). This is due in part to compounding factors such as the lack of trainingdata. This paper delivers the first comprehensive analysis of whether LLMs canacquire such languages purely via in-context learning (ICL), with or withoutauxiliary alignment signals, and how these methods compare toparameter-efficient fine-tuning (PEFT). We systematically evaluate 20under-represented languages across three state-of-the-art multilingual LLMs.Our findings highlight the limitation of PEFT when both language and its scriptare extremely under-represented by the LLM. In contrast, zero-shot ICL withlanguage alignment is impressively effective on extremely low-resourcelanguages, while few-shot ICL or PEFT is more beneficial for languagesrelatively better represented by LLMs. For LLM practitioners working onextremely low-resource languages, we summarise guidelines grounded by ourresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuninga multilingual model on languages of unseen scripts.</description><author>Yue Li, Zhixue Zhao, Carolina Scarton</author><pubDate>Tue, 26 Aug 2025 14:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19089v1</guid></item><item><title>When recalling in-context, Transformers are not SSMs</title><link>http://arxiv.org/abs/2508.19029v1</link><description>Despite the advantageous subquadratic complexity of modern recurrent deeplearning models -- such as state-space models (SSMs) -- recent studies havehighlighted their potential shortcomings compared to transformers on reasoningand memorization tasks. In this paper, we dive deeper into one of suchbenchmarks: associative recall (AR), which has been shown to correlate wellwith language modeling performance, and inspect in detail the effects ofscaling and optimization issues in recently proposed token mixing strategies.We first demonstrate that, unlike standard transformers, the choice of learningrate plays a critical role in the performance of modern recurrent models: anissue that can severely affect reported performance in previous works andsuggests further research is needed to stabilize training. Next, we show thatrecurrent and attention-based models exhibit contrasting benefits when scalingin width as opposed to depth, with attention being notably unable to solve ARwhen limited to a single layer. We then further inspect 1-layer transformers,revealing that despite their poor performance, their training dynamicssurprisingly resemble the formation of induction heads, a phenomenon previouslyobserved only in their 2-layer counterparts. Finally, through architecturalablations, we study how components affects Transformer and Mamba's performanceand optimization stability.</description><author>Destiny Okpekpe, Antonio Orvieto</author><pubDate>Tue, 26 Aug 2025 13:45:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19029v1</guid></item><item><title>M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering</title><link>http://arxiv.org/abs/2504.04633v3</link><description>Multimodal in-context learning (ICL) equips Large Vision-language Models(LVLMs) with the ability to adapt to new tasks via multiple user-provideddemonstrations, without requiring any model parameter updates. However, itseffectiveness is constrained by the token-intensive nature of multimodal inputsand the complexity of cross-modal few-shot reasoning, which together hinderLVLMs from extracting useful patterns from demonstrations. To address thesechallenges, we propose \textbf{M$^2$IV}, a novel representation engineeringapproach that replaces explicit token-level demonstrations with a set oflearnable Multimodal In-context Vectors directly injected into the residualstreams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA)and multi-layer perceptrons (MLP) in the ICL process, we design a trainingstrategy that enables M$^2$IV to perform fine-grained semantic distillation androbust cross-modal representation learning. M$^2$IV not only improvesperformance across diverse tasks and LVLMs but also significantly reduces tokenoverhead, enabling graceful scaling to many-shot scenarios. To further enhanceusability, we introduce \textbf{VLibrary}, a repository that stores trainedM$^2$IVs for flexible retrieval and injection. With VLibrary, users can steerpre-trained LVLMs in a customized manner that meets diverse requirements.Extensive experiments demonstrate that M$^2$IV consistently outperforms vanillaICL and prior representation engineering baselines, achieving an averageaccuracy gain of 3.74\% with substantial improvements in overall efficiency.</description><author>Yanshu Li, Yi Cao, Hongyang He, Qisen Cheng, Xiang Fu, Xi Xiao, Tianyang Wang, Ruixiang Tang</author><pubDate>Tue, 26 Aug 2025 10:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.04633v3</guid></item><item><title>Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes</title><link>http://arxiv.org/abs/2412.01240v3</link><description>As large-scale foundation models trained on billions of image--mask pairscovering a vast diversity of scenes, objects, and contexts, SAM and itsupgraded version, SAM~2, have significantly influenced multiple fields withincomputer vision. Leveraging such unprecedented data diversity, they exhibitstrong open-world segmentation capabilities, with SAM~2 further enhancing thesecapabilities to support high-quality video segmentation. While SAMs (SAM andSAM~2) have demonstrated excellent performance in segmentingcontext-independent concepts like people, cars, and roads, they overlook morechallenging context-dependent (CD) concepts, such as visual saliency,camouflage, industrial defects, and medical lesions. CD concepts rely heavilyon global and local contextual information, making them susceptible to shiftsin different contexts, which requires strong discriminative capabilities fromthe model. The lack of comprehensive evaluation of SAMs limits understanding oftheir performance boundaries, which may hinder the design of future models. Inthis paper, we conduct a thorough evaluation of SAMs on 11 CD concepts across2D and 3D images and videos in various visual modalities within natural,medical, and industrial scenes. We develop a unified evaluation framework forSAM and SAM~2 that supports manual, automatic, and intermediate self-prompting,aided by our specific prompt generation and interaction strategies. We furtherexplore the potential of SAM~2 for in-context learning and introduce promptrobustness testing to simulate real-world imperfect prompts. Finally, weanalyze the benefits and limitations of SAMs in understanding CD concepts anddiscuss their future development in segmentation tasks.</description><author>Xiaoqi Zhao, Youwei Pang, Shijie Chang, Yuan Zhao, Lihe Zhang, Chenyang Yu, Hanqi Liu, Jiaming Zuo, Jinsong Ouyang, Weisi Lin, Georges El Fakhri, Huchuan Lu, Xiaofeng Liu</author><pubDate>Tue, 26 Aug 2025 08:58:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01240v3</guid></item><item><title>LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning</title><link>http://arxiv.org/abs/2406.05881v6</link><description>Large language models (LLMs) have shown remarkable abilities in logicalreasoning, in-context learning, and code generation. However, translatingnatural language instructions into effective robotic control policies remains asignificant challenge, especially for tasks requiring long-horizon planning andoperating under sparse reward conditions. Hierarchical Reinforcement Learning(HRL) provides a natural framework to address this challenge in robotics;however, it typically suffers from non-stationarity caused by the changingbehavior of the lower-level policy during training, destabilizing higher-levelpolicy learning. We introduce LGR2, a novel HRL framework that leverages LLMsto generate language-guided reward functions for the higher-level policy. Bydecoupling high-level reward generation from low-level policy changes, LGR2fundamentally mitigates the non-stationarity problem in off-policy HRL,enabling stable and efficient learning. To further enhance sample efficiency insparse environments, we integrate goal-conditioned hindsight experiencerelabeling. Extensive experiments across simulated and real-world roboticnavigation and manipulation tasks demonstrate LGR2 outperforms bothhierarchical and non-hierarchical baselines, achieving over 55% success rateson challenging tasks and robust transfer to real robots, without additionalfine-tuning.</description><author>Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri</author><pubDate>Wed, 27 Aug 2025 17:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05881v6</guid></item><item><title>Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation</title><link>http://arxiv.org/abs/2508.19999v1</link><description>This paper introduces an algorithm to select demonstration examples forin-context learning of a query set. Given a set of $n$ examples, how can wequickly select $k$ out of $n$ to best serve as the conditioning for downstreaminference? This problem has broad applications in prompt tuning andchain-of-thought reasoning. Since model weights remain fixed during in-contextlearning, previous work has sought to design methods based on the similarity oftoken embeddings. This work proposes a new approach based on gradients of theoutput taken in the input embedding space. Our approach estimates model outputsthrough a first-order approximation using the gradients. Then, we apply thisestimation to multiple randomly sampled subsets. Finally, we aggregate thesampled subset outcomes to form an influence score for each demonstration, andselect $k$ most relevant examples. This procedure only requires pre-computingmodel outputs and gradients once, resulting in a linear-time algorithm relativeto model and training set sizes. Extensive experiments across various modelsand datasets validate the efficiency of our approach. We show that the gradientestimation procedure yields approximations of full inference with less than$\mathbf{1}\%$ error across six datasets. This allows us to scale up subsetselection that would otherwise run full inference by up to$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, andoutperform existing selection methods based on input embeddings by$\mathbf{11}\%$ on average.</description><author>Ziniu Zhang, Zhenshuo Zhang, Dongyue Li, Lu Wang, Jennifer Dy, Hongyang R. Zhang</author><pubDate>Wed, 27 Aug 2025 15:59:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19999v1</guid></item><item><title>AI-Powered Detection of Inappropriate Language in Medical School Curricula</title><link>http://arxiv.org/abs/2508.19883v1</link><description>The use of inappropriate language -- such as outdated, exclusionary, ornon-patient-centered terms -- medical instructional materials can significantlyinfluence clinical training, patient interactions, and health outcomes. Despitetheir reputability, many materials developed over past decades contain examplesnow considered inappropriate by current medical standards. Given the volume ofcurricular content, manually identifying instances of inappropriate use oflanguage (IUL) and its subcategories for systematic review is prohibitivelycostly and impractical. To address this challenge, we conduct a first-in-classevaluation of small language models (SLMs) fine-tuned on labeled data andpre-trained LLMs with in-context learning on a dataset containing approximately500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IULclassifier, (2) subcategory-specific binary classifiers, (3) a multilabelclassifier, and (4) a two-stage hierarchical pipeline for general IUL detectionfollowed by multilabel classification. For LLMs, we consider variations ofprompts that include subcategory definitions and/or shots. We found that bothLLama-3 8B and 70B, even with carefully curated shots, are largely outperformedby SLMs. While the multilabel classifier performs best on annotated data,supplementing training with unflagged excerpts as negative examples boosts thespecific classifiers' AUC by up to 25%, making them most effective models formitigating harmful language in medical curricula.</description><author>Chiman Salavati, Shannon Song, Scott A. Hale, Roberto E. Montenegro, Shiri Dori-Hacohen, Fabricio Murai</author><pubDate>Wed, 27 Aug 2025 13:40:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19883v1</guid></item><item><title>X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models</title><link>http://arxiv.org/abs/2412.01824v2</link><description>In-context generation is a key component of large language models' (LLMs)open-task generalization capability. By leveraging a few examples as context,LLMs can perform both in-domain and out-of-domain tasks. Recent advancements inauto-regressive vision-language models (VLMs) built upon LLMs have showcasedimpressive performance in text-to-image generation. However, the potential ofin-context learning for general image generation tasks remains largelyunexplored. To address this, we introduce X-Prompt, a purely auto-regressivelarge-vision language model designed to deliver competitive performance acrossa wide range of both seen and unseen image generation tasks, all within aunified in-context learning framework. X-Prompt incorporates a specializeddesign that efficiently compresses valuable features from in-context examples,supporting longer in-context token sequences and improving its ability togeneralize to unseen tasks. A unified training task for both text and imageprediction enables X-Prompt to handle general image generation with enhancedtask awareness from in-context examples. Extensive experiments validate themodel's performance across diverse seen image generation tasks and its capacityto generalize to previously unseen tasks.</description><author>Zeyi Sun, Ziyang Chu, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuanjun Xiong, Dahua Lin, Jiaqi Wang</author><pubDate>Wed, 27 Aug 2025 12:26:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.01824v2</guid></item><item><title>A Survey on Training-free Alignment of Large Language Models</title><link>http://arxiv.org/abs/2508.09016v2</link><description>The alignment of large language models (LLMs) aims to ensure their outputsadhere to human values, ethical standards, and legal norms. Traditionalalignment methods often rely on resource-intensive fine-tuning (FT), which maysuffer from knowledge degradation and face challenges in scenarios where themodel accessibility or computational resources are constrained. In contrast,training-free (TF) alignment techniques--leveraging in-context learning,decoding-time adjustments, and post-generation corrections--offer a promisingalternative by enabling alignment without heavily retraining LLMs, making themadaptable to both open-source and closed-source environments. This paperpresents the first systematic review of TF alignment methods, categorizing themby stages of pre-decoding, in-decoding, and post-decoding. For each stage, weprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs(MLLMs), highlighting their mechanisms and limitations. Furthermore, weidentify key challenges and future directions, paving the way for moreinclusive and effective TF alignment techniques. By synthesizing and organizingthe rapidly growing body of research, this survey offers a guidance forpractitioners and advances the development of safer and more reliable LLMs.</description><author>Birong Pan, Yongqi Li, Weiyu Zhang, Wenpeng Lu, Mayi Xu, Shen Zhou, Yuanyuan Zhu, Ming Zhong, Tieyun Qian</author><pubDate>Wed, 27 Aug 2025 05:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09016v2</guid></item><item><title>Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</title><link>http://arxiv.org/abs/2508.19563v1</link><description>Large Language Models (LLMs) are being applied in a wide array of settings,well beyond the typical language-oriented use cases. In particular, LLMs areincreasingly used as a plug-and-play method for fitting data and generatingpredictions. Prior work has shown that LLMs, via in-context learning orsupervised fine-tuning, can perform competitively with many tabular supervisedlearning techniques in terms of predictive performance. However, we identify acritical vulnerability of using LLMs for data fitting -- making changes to datarepresentation that are completely irrelevant to the underlying learning taskcan drastically alter LLMs' predictions on the same data. For example, simplychanging variable names can sway the size of prediction error by as much as 82%in certain settings. Such prediction sensitivity with respect totask-irrelevant variations manifests under both in-context learning andsupervised fine-tuning, for both close-weight and open-weight general-purposeLLMs. Moreover, by examining the attention scores of an open-weight LLM, wediscover a non-uniform attention pattern: training examples and variablenames/values which happen to occupy certain positions in the prompt receivemore attention when output tokens are generated, even though differentpositions are expected to receive roughly the same attention. This partiallyexplains the sensitivity in the presence of task-irrelevant variations. We alsoconsider a state-of-the-art tabular foundation model (TabPFN) trainedspecifically for data fitting. Despite being explicitly designed to achieveprediction robustness, TabPFN is still not immune to task-irrelevantvariations. Overall, despite LLMs' impressive predictive capabilities,currently they lack even the basic level of robustness to be used as aprincipled data-fitting tool.</description><author>Hejia Liu, Mochen Yang, Gediminas Adomavicius</author><pubDate>Wed, 27 Aug 2025 04:46:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19563v1</guid></item><item><title>ICL CIPHERS: Quantifying "Learning" in In-Context Learning via Substitution Ciphers</title><link>http://arxiv.org/abs/2504.19395v2</link><description>Recent works have suggested that In-Context Learning (ICL) operates in dualmodes, i.e. task retrieval (remember learned patterns from pre-training) andtask learning (inference-time ''learning'' from demonstrations). However,disentangling these the two modes remains a challenging goal. We introduce ICLCIPHERS, a class of task reformulations based on substitution ciphers borrowedfrom classic cryptography. In this approach, a subset of tokens in thein-context inputs are substituted with other (irrelevant) tokens, renderingEnglish sentences less comprehensible to human eye. However, by design, thereis a latent, fixed pattern to this substitution, making it reversible. Thisbijective (reversible) cipher ensures that the task remains a well-defined taskin some abstract sense, despite the transformations. It is a curious questionif LLMs can solve tasks reformulated by ICL CIPHERS with a BIJECTIVE mapping,which requires ''deciphering'' the latent cipher. We show that LLMs are betterat solving tasks reformulated by ICL CIPHERS with BIJECTIVE mappings than theNON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify''learning'' in ICL. While this gap is small, it is consistent across the boardon four datasets and six models. Finally, we examine LLMs' internalrepresentations and identify evidence in their ability to decode the cipheredinputs.</description><author>Zhouxiang Fang, Aayush Mishra, Muhan Gao, Anqi Liu, Daniel Khashabi</author><pubDate>Wed, 27 Aug 2025 01:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19395v2</guid></item><item><title>InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity</title><link>http://arxiv.org/abs/2508.21003v1</link><description>In this paper, we introduce InSQuAD, designed to enhance the performance ofIn-Context Learning (ICL) models through Submodular Mutual Information} (SMI)enforcing Quality and Diversity among in-context exemplars. InSQuAD achievesthis through two principal strategies: First, we model the ICL task as atargeted selection problem and introduce a unified selection strategy based onSMIs which mines relevant yet diverse in-context examples encapsulating thenotions of quality and diversity. Secondly, we address a common pitfall inexisting retrieval models which model query relevance, often overlookingdiversity, critical for ICL. InSQuAD introduces a combinatorial trainingparadigm which learns the parameters of an SMI function to enforce both qualityand diversity in the retrieval model through a novel likelihood-based loss. Tofurther aid the learning process we augment an existing multi-hop questionanswering dataset with synthetically generated paraphrases. Adopting theretrieval model trained using this strategy alongside the novel targetedselection formulation for ICL on nine benchmark datasets shows significantimprovements validating the efficacy of our approach.</description><author>Souradeep Nanda, Anay Majee, Rishabh Iyer</author><pubDate>Thu, 28 Aug 2025 17:04:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21003v1</guid></item><item><title>Transformers Meet In-Context Learning: A Universal Approximation Theory</title><link>http://arxiv.org/abs/2506.05200v2</link><description>Large language models are capable of in-context learning, the ability toperform new tasks at test time using a handful of input-output examples,without parameter updates. We develop a universal approximation theory toelucidate how transformers enable in-context learning. For a general class offunctions (each representing a distinct task), we demonstrate how to constructa transformer that, without any further weight updates, can predict based on afew noisy in-context examples with vanishingly small risk. Unlike prior workthat frames transformers as approximators of optimization algorithms (e.g.,gradient descent) for statistical learning tasks, we integrate Barron'suniversal function approximation theory with the algorithm approximatorviewpoint. Our approach yields approximation guarantees that are notconstrained by the effectiveness of the optimization algorithms being mimicked,extending far beyond convex problems like linear regression. The key is to showthat (i) any target function can be nearly linearly represented, with small$\ell_1$-norm, over a set of universal features, and (ii) a transformer can beconstructed to find the linear representation -- akin to solving Lasso -- attest time.</description><author>Gen Li, Yuchen Jiao, Yu Huang, Yuting Wei, Yuxin Chen</author><pubDate>Thu, 28 Aug 2025 16:07:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05200v2</guid></item><item><title>STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment</title><link>http://arxiv.org/abs/2508.20944v1</link><description>In-Context Learning (ICL) has become a powerful paradigm that enables LLMs toperform a wide range of tasks without task-specific fine-tuning. However, theeffectiveness of ICL heavily depends on the quality of exemplar selection. Inparticular, for structured prediction tasks such as semantic parsing, existingICL selection strategies often overlook structural alignment, leading tosuboptimal performance and poor generalization. To address this issue, wepropose a novel two-stage exemplar selection strategy that achieves a strongbalance between efficiency, generalizability, and performance. First, wefine-tune a BERT-based retriever using structure-aware supervision, guiding itto select exemplars that are both semantically relevant and structurallyaligned. Then, we enhance the retriever with a plug-in module, which amplifiessyntactically meaningful information in the hidden representations. Thisplug-in is model-agnostic, requires minimal overhead, and can be seamlesslyintegrated into existing pipelines. Experiments on four benchmarks spanningthree semantic parsing tasks demonstrate that our method consistentlyoutperforms existing baselines with multiple recent LLMs as inference-timemodels.</description><author>Jiaqian Li, Qisheng Hu, Jing Li, Wenya Wang</author><pubDate>Thu, 28 Aug 2025 16:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20944v1</guid></item><item><title>Turning Tabular Foundation Models into Graph Foundation Models</title><link>http://arxiv.org/abs/2508.20906v1</link><description>While foundation models have revolutionized such fields as natural languageprocessing and computer vision, their application and potential within graphmachine learning remain largely unexplored. One of the key challenges indesigning graph foundation models (GFMs) is handling diverse node features thatcan vary across different graph datasets. Although many works on GFMs have beenfocused exclusively on text-attributed graphs, the problem of handlingarbitrary features of other types in GFMs has not been fully addressed.However, this problem is not unique to the graph domain, as it also arises inthe field of machine learning for tabular data. In this work, motivated by therecent success of tabular foundation models like TabPFNv2, we propose G2T-FM, asimple graph foundation model that employs TabPFNv2 as a backbone.Specifically, G2T-FM augments the original node features with neighborhoodfeature aggregation, adds structural embeddings, and then applies TabPFNv2 tothe constructed node representations. Even in a fully in-context regime, ourmodel achieves strong results, significantly outperforming publicly availableGFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting thepotential of the proposed approach. More broadly, our paper reveals apreviously overlooked direction of utilizing tabular foundation models forgraph machine learning tasks.</description><author>Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova</author><pubDate>Thu, 28 Aug 2025 15:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20906v1</guid></item></channel></rss>