<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model customization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 28 Sep 2025 13:00:05 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Reference Points in LLM Sentiment Analysis: The Role of Structured Context</title><link>http://arxiv.org/abs/2508.11454v1</link><description>Large language models (LLMs) are now widely used across many fields,including marketing research. Sentiment analysis, in particular, helps firmsunderstand consumer preferences. While most NLP studies classify sentiment fromreview text alone, marketing theories, such as prospect theory andexpectation--disconfirmation theory, point out that customer evaluations areshaped not only by the actual experience but also by additional referencepoints. This study therefore investigates how the content and format of suchsupplementary information affect sentiment analysis using LLMs. We comparenatural language (NL) and JSON-formatted prompts using a lightweight 3Bparameter model suitable for practical marketing applications. Experiments ontwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt withadditional information outperforms all baselines without fine-tuning: Macro-F1rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making itdeployable in resource-constrained edge devices. Furthermore, a follow-upanalysis confirms that performance gains stem from genuine contextual reasoningrather than label proxying. This work demonstrates that structured promptingcan enable smaller models to achieve competitive performance, offering apractical alternative to large-scale model deployment.</description><author>Junichiro Niimi</author><pubDate>Fri, 15 Aug 2025 13:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11454v1</guid></item><item><title>Personalized LLM for Generating Customized Responses to the Same Query from Different Users</title><link>http://arxiv.org/abs/2412.11736v2</link><description>Existing work on large language model (LLM) personalization assigneddifferent responding roles to LLMs, but overlooked the diversity of queriers.In this work, we propose a new form of querier-aware LLM personalization,generating different responses even for the same query from different queriers.We design a dual-tower model architecture with a cross-querier general encoderand a querier-specific encoder. We further apply contrastive learning withmulti-view augmentation, pulling close the dialogue representations of the samequerier, while pulling apart those of different queriers. To mitigate theimpact of query diversity on querier-contrastive learning, we cluster thedialogues based on query similarity and restrict the scope of contrastivelearning within each cluster. To address the lack of datasets designed forquerier-aware personalization, we also build a multi-querier dataset fromEnglish and Chinese scripts, as well as WeChat records, called MQDialog,containing 173 queriers and 12 responders. Extensive evaluations demonstratethat our design significantly improves the quality of personalized responsegeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scoresand winning rates ranging from 54% to 82% compared with various baselinemethods.</description><author>Hang Zeng, Chaoyue Niu, Fan Wu, Chengfei Lv, Guihai Chen</author><pubDate>Fri, 15 Aug 2025 08:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11736v2</guid></item><item><title>Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems</title><link>http://arxiv.org/abs/2508.14982v1</link><description>Conversational explainable artificial intelligence (ConvXAI) systems based onlarge language models (LLMs) have garnered considerable attention for theirability to enhance user comprehension through dialogue-based explanations.Current ConvXAI systems often are based on intent recognition to accuratelyidentify the user's desired intention and map it to an explainability method.While such methods offer great precision and reliability in discerning users'underlying intentions for English, a significant challenge in the scarcity oftraining data persists, which impedes multilingual generalization. Besides, thesupport for free-form custom inputs, which are user-defined data distinct frompre-configured dataset instances, remains largely limited. To bridge thesegaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQLdataset spanning five typologically diverse languages, including onelow-resource language. Subsequently, we propose a new parsing approach aimed atenhancing multilingual parsing performance, and evaluate three LLMs onMultiCoXQL using various parsing strategies. Furthermore, we present Compass, anew multilingual dataset designed for custom input extraction in ConvXAIsystems, encompassing 11 intents across the same five languages as MultiCoXQL.We conduct monolingual, cross-lingual, and multilingual evaluations on Compass,employing three LLMs of varying sizes alongside BERT-type models.</description><author>Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Fedor Splitt, Jiaao Li, Yoana Tsoneva, Sebastian MÃ¶ller, Vera Schmitt</author><pubDate>Wed, 20 Aug 2025 18:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14982v1</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v2</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</author><pubDate>Thu, 21 Aug 2025 18:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v2</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title><link>http://arxiv.org/abs/2504.19254v3</link><description>Hallucinations are a persistent problem with Large Language Models (LLMs). Asthese models become increasingly used in high-stakes domains, such ashealthcare and finance, the need for effective hallucination detection iscrucial. To this end, we outline a versatile framework for zero-resourcehallucination detection that practitioners can apply to real-world use cases.To achieve this, we adapt a variety of existing uncertainty quantification (UQ)techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,transforming them as necessary into standardized response-level confidencescores ranging from 0 to 1. To enhance flexibility, we propose a tunableensemble approach that incorporates any combination of the individualconfidence scores. This approach enables practitioners to optimize the ensemblefor a specific use case for improved performance. To streamline implementation,the full suite of scorers is offered in this paper's companion Python toolkit,UQLM. To evaluate the performance of the various scorers, we conduct anextensive set of experiments using several LLM question-answering benchmarks.We find that our tunable ensemble typically surpasses its individual componentsand outperforms existing hallucination detection methods. Our resultsdemonstrate the benefits of customized hallucination detection strategies forimproving the accuracy and reliability of LLMs.</description><author>Dylan Bouchard, Mohit Singh Chauhan</author><pubDate>Wed, 20 Aug 2025 14:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19254v3</guid></item><item><title>The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation</title><link>http://arxiv.org/abs/2508.14718v1</link><description>We established a rigorous benchmark for text-based recipe generation, afundamental task in natural language generation. We present a comprehensivecomparative study contrasting a fine-tuned GPT-2 large (774M) model against theGPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisinecorpus from RecipeDB. Our key contribution is a targeted tokenization strategythat augments the vocabulary with 23 common fraction tokens and customstructural markers. This approach addresses a critical limitation of generictokenizers by preserving essential recipe structures and precise numericalquantities, thereby enhancing domain specificity. Performance is evaluatedusing a comprehensive suite of seven automatic metrics spanning fluency(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), anddiversity. Our experiments show that the large transformer-based approachyields a &gt;20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over thebest recurrent baseline, while reducing perplexity by 69.8%. We conclude with adiscussion of remaining challenges, particularly regarding factual accuracy,and outline how this foundational study paves the way for integratingreal-world constraints and multi-modal inputs in advanced recipe generationresearch.</description><author>Shubham Pundhir, Ganesh Bagler</author><pubDate>Wed, 20 Aug 2025 13:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14718v1</guid></item><item><title>PathGPT: Reframing Path Recommendation as a Natural Language Generation Task with Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2504.05846v2</link><description>Path recommendation (PR) aims to generate travel paths that are customized toa user's specific preferences and constraints. Conventional approaches oftenemploy explicit optimization objectives or specialized machine learningarchitectures; however, these methods typically exhibit limited flexibility andgeneralizability, necessitating costly retraining to accommodate new scenarios.This paper introduces an alternative paradigm that conceptualizes PR as anatural language generation task. We present PathGPT, a retrieval-augmentedlarge language model (LLM) system that leverages historical trajectory data andnatural language user constraints to generate plausible paths. The proposedmethodology first converts raw trajectory data into a human-interpretabletextual format, which is then stored in a database. Subsequently, a hybridretrieval system extracts path-specific context from this database to inform apretrained LLM. The primary contribution of this work is a novel framework thatdemonstrates how integrating established information retrieval and generativemodel components can enable adaptive, zero-shot path generation across diversescenarios. Extensive experiments on large-scale trajectory datasets indicatethat PathGPT's performance is competitive with specialized, learning-basedmethods, underscoring its potential as a flexible and generalizable pathgeneration system that avoids the need for retraining inherent in previousdata-driven models.</description><author>Steeve Cuthbert Marcelyn, Yucen Gao, Yuzhe Zhang, Xiaofeng Gao</author><pubDate>Wed, 20 Aug 2025 06:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05846v2</guid></item><item><title>JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</title><link>http://arxiv.org/abs/2508.10287v2</link><description>Recent advances in Vision-Language Models (VLMs) and large language models(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AIagents like robots. However, existing visual reasoning benchmarks often sufferfrom several limitations: they lack a clear definition of reasoning complexity,offer have no control to generate questions over varying difficulty and taskcustomization, and fail to provide structured, step-by-step reasoningannotations (workflows). To bridge these gaps, we formalize reasoningcomplexity, introduce an adaptive query engine that generates customizablequestions of varying complexity with detailed intermediate annotations, andextend the JRDB dataset with human-object interaction and geometricrelationship annotations to create JRDB-Reasoning, a benchmark tailored forvisual reasoning in human-crowded environments. Our engine and benchmark enablefine-grained evaluation of visual reasoning frameworks and dynamic assessmentof visual-language models across reasoning levels.</description><author>Simindokht Jahangard, Mehrzad Mohammadi, Yi Shen, Zhixi Cai, Hamid Rezatofighi</author><pubDate>Wed, 20 Aug 2025 04:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10287v2</guid></item><item><title>A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models</title><link>http://arxiv.org/abs/2508.17994v1</link><description>Aspect-based sentiment analysis enhances sentiment detection by associatingit with specific aspects, offering deeper insights than traditional sentimentanalysis. This study introduces a manually annotated dataset of 10,814multilingual customer reviews covering brick-and-mortar retail stores, labeledwith eight aspect categories and their sentiment. Using this dataset, theperformance of GPT-4 and LLaMA-3 in aspect based sentiment analysis isevaluated to establish a baseline for the newly introduced data. The resultsshow both models achieving over 85% accuracy, while GPT-4 outperforms LLaMA-3overall with regard to all relevant metrics.</description><author>Oleg Silcenco, Marcos R. Machad, Wallace C. Ugulino, Daniel Braun</author><pubDate>Mon, 25 Aug 2025 13:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17994v1</guid></item><item><title>M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering</title><link>http://arxiv.org/abs/2504.04633v3</link><description>Multimodal in-context learning (ICL) equips Large Vision-language Models(LVLMs) with the ability to adapt to new tasks via multiple user-provideddemonstrations, without requiring any model parameter updates. However, itseffectiveness is constrained by the token-intensive nature of multimodal inputsand the complexity of cross-modal few-shot reasoning, which together hinderLVLMs from extracting useful patterns from demonstrations. To address thesechallenges, we propose \textbf{M$^2$IV}, a novel representation engineeringapproach that replaces explicit token-level demonstrations with a set oflearnable Multimodal In-context Vectors directly injected into the residualstreams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA)and multi-layer perceptrons (MLP) in the ICL process, we design a trainingstrategy that enables M$^2$IV to perform fine-grained semantic distillation androbust cross-modal representation learning. M$^2$IV not only improvesperformance across diverse tasks and LVLMs but also significantly reduces tokenoverhead, enabling graceful scaling to many-shot scenarios. To further enhanceusability, we introduce \textbf{VLibrary}, a repository that stores trainedM$^2$IVs for flexible retrieval and injection. With VLibrary, users can steerpre-trained LVLMs in a customized manner that meets diverse requirements.Extensive experiments demonstrate that M$^2$IV consistently outperforms vanillaICL and prior representation engineering baselines, achieving an averageaccuracy gain of 3.74\% with substantial improvements in overall efficiency.</description><author>Yanshu Li, Yi Cao, Hongyang He, Qisen Cheng, Xiang Fu, Xi Xiao, Tianyang Wang, Ruixiang Tang</author><pubDate>Tue, 26 Aug 2025 10:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.04633v3</guid></item><item><title>Controllable Conversational Theme Detection Track at DSTC 12</title><link>http://arxiv.org/abs/2508.18783v1</link><description>Conversational analytics has been on the forefront of transformation drivenby the advances in Speech and Natural Language Processing techniques. Rapidadoption of Large Language Models (LLMs) in the analytics field has taken theproblems that can be automated to a new level of complexity and scale. In thispaper, we introduce Theme Detection as a critical task in conversationalanalytics, aimed at automatically identifying and categorizing topics withinconversations. This process can significantly reduce the manual effort involvedin analyzing expansive dialogs, particularly in domains like customer supportor sales. Unlike traditional dialog intent detection, which often relies on afixed set of intents for downstream system logic, themes are intended as adirect, user-facing summary of the conversation's core inquiry. Thisdistinction allows for greater flexibility in theme surface forms anduser-specific customizations. We pose Controllable Conversational ThemeDetection problem as a public competition track at Dialog System TechnologyChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling ofdialog utterances, with the distinctive aspect being controllability of theresulting theme clusters' granularity achieved via the provided user preferencedata. We give an overview of the problem, the associated dataset and theevaluation metrics, both automatic and human. Finally, we discuss theparticipant teams' submissions and provide insights from those. The trackmaterials (data and code) are openly available in the GitHub repository.</description><author>Igor Shalyminov, Hang Su, Jake Vincent, Siffi Singh, Jason Cai, James Gung, Raphael Shu, Saab Mansour</author><pubDate>Tue, 26 Aug 2025 08:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18783v1</guid></item><item><title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title><link>http://arxiv.org/abs/2508.19650v2</link><description>Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding modelenhancement.https://github.com/Cola-any/Video-LevelGauge</description><author>Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang</author><pubDate>Thu, 28 Aug 2025 09:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19650v2</guid></item><item><title>Occlusion Robustness of CLIP for Military Vehicle Classification</title><link>http://arxiv.org/abs/2508.20760v1</link><description>Vision-language models (VLMs) like CLIP enable zero-shot classification byaligning images and text in a shared embedding space, offering advantages fordefense applications with scarce labeled data. However, CLIP's robustness inchallenging military environments, with partial occlusion and degradedsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIPvariants' robustness to occlusion using a custom dataset of 18 military vehicleclasses and evaluate using Normalized Area Under the Curve (NAUC) acrossocclusion percentages. Four key insights emerge: (1) Transformer-based CLIPmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusionsdegrade performance more than larger contiguous occlusions, (3) despiteimproved accuracy, performance of linear-probed models sharply drops at around35% occlusion, (4) by finetuning the model's backbone, this performance dropoccurs at more than 60% occlusion. These results underscore the importance ofocclusion-specific augmentations during training and the need for furtherexploration into patch-level sensitivity and architectural resilience forreal-world deployment of CLIP.</description><author>Jan Erik van Woerden, Gertjan Burghouts, Lotte Nijskens, Alma M. Liezenga, Sabina van Rooij, Frank Ruis, Hugo J. Kuijf</author><pubDate>Thu, 28 Aug 2025 13:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20760v1</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v3</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Junwei Liu, Jinjie Gu</author><pubDate>Mon, 01 Sep 2025 15:33:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v3</guid></item><item><title>AI Assistants to Enhance and Exploit the PETSc Knowledge Base</title><link>http://arxiv.org/abs/2506.20608v2</link><description>Generative AI, especially through large language models (LLMs), istransforming how technical knowledge can be accessed, reused, and extended.PETSc, a widely used numerical library for high-performance scientificcomputing, has accumulated a rich but fragmented knowledge base over its threedecades of development, spanning source code, documentation, mailing lists,GitLab issues, Discord conversations, technical papers, and more. Much of thisknowledge remains informal and inaccessible to users and new developers. Toactivate and utilize this knowledge base more effectively, the PETSc team hasbegun building an LLM-powered system that combines PETSc content with customLLM tools -- including retrieval-augmented generation (RAG), rerankingalgorithms, and chatbots -- to assist users, support developers, and proposeupdates to formal documentation. This paper presents initial experiencesdesigning and evaluating these tools, focusing on system architecture, usingRAG and reranking for PETSc-specific information, evaluation methodologies forvarious LLMs and embedding models, and user interface design. Leveraging theArgonne Leadership Computing Facility resources, we analyze how LLM responsescan enhance the development and use of numerical software, with an initialfocus on scalable Krylov solvers. Our goal is to establish an extensibleframework for knowledge-centered AI in scientific software, enabling scalablesupport, enriched documentation, and enhanced workflows for research anddevelopment. We conclude by outlining directions for expanding this system intoa robust, evolving platform that advances software ecosystems to acceleratescientific discovery.</description><author>Barry Smith, Junchao Zhang, Hong Zhang, Lois Curfman McInnes, Murat Keceli, Archit Vasan, Satish Balay, Toby Isaac, Le Chen, Venkatram Vishwanath</author><pubDate>Mon, 22 Sep 2025 14:54:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.20608v2</guid></item><item><title>CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation</title><link>http://arxiv.org/abs/2507.08325v2</link><description>In e-commerce private-domain channels such as instant messaging and e-mail,merchants engage customers directly as part of their Customer RelationshipManagement (CRM) programmes to drive retention and conversion. While a few topperformers excel at crafting outbound messages, most merchants struggle towrite persuasive copy because they lack both expertise and scalable tools. Weintroduce CRMAgent, a multi-agent system built on large language models (LLMs)that generates high-quality message templates and actionable writing guidancethrough three complementary modes. First, group-based learning enables theagent to learn from a merchant's own top-performing messages within the sameaudience segment and rewrite low-performing ones. Second,retrieval-and-adaptation fetches templates that share the same audience segmentand exhibit high similarity in voucher type and product category, learns theirsuccessful patterns, and adapts them to the current campaign. Third, arule-based fallback provides a lightweight zero-shot rewrite when no suitablereferences are available. Extensive experiments show that CRMAgent consistentlyoutperforms merchants' original templates, delivering significant gains in bothaudience-match and marketing-effectiveness metrics.</description><author>Yinzhu Quan, Xinrui Li, Ying Chen</author><pubDate>Sun, 31 Aug 2025 23:18:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.08325v2</guid></item><item><title>General Table Question Answering via Answer-Formula Joint Generation</title><link>http://arxiv.org/abs/2503.12345v3</link><description>Advanced table question answering (TableQA) methods prompt large languagemodels (LLMs) to generate answer text, SQL query, Python code, or customoperation, which impressively improve the complex reasoning problems in theTableQA task. However, these methods lack the versatility to cope with specificquestion types or table structures. In contrast, the Spreadsheet Formula, thewidely used and well-defined operation language for tabular data, has not beenthoroughly explored to solve TableQA. In this paper, we first attempt to usethe Formula as the executable representation for solving complex reasoning ontables with different structures. Specifically, we construct\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existingdatasets. In addition, we propose \texttt{TabAF}, a general table answeringframework to solve multiple types of tasks over multiple types of tablessimultaneously, which decodes answers and Formulas with a single LLM backbone.Extensive experiments demonstrate the versatility and generalization of\texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves newstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.</description><author>Zhongyuan Wang, Richong Zhang, Zhijie Nie, Hangyu Mao</author><pubDate>Sun, 31 Aug 2025 14:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.12345v3</guid></item><item><title>PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions</title><link>http://arxiv.org/abs/2509.07370v1</link><description>Recent advancements in Large Language Models (LLMs) demonstrate remarkablecapabilities across various fields. These developments have led to more directcommunication between humans and LLMs in various situations, such as socialcompanionship and psychological support. However, LLMs often exhibitlimitations in emotional perception and social competence during real-worldconversations. These limitations partly originate from their inability to adapttheir communication style and emotional expression to different social and taskcontexts. In this work, we introduce PersonaFuse, a novel LLM post-trainingframework that enables LLMs to adapt and express different personalities forvarying situations. Inspired by Trait Activation Theory and the Big Fivepersonality model, PersonaFuse employs a Mixture-of-Expert architecture thatcombines persona adapters with a dynamic routing network, enabling contextualtrait expression. Experimental results show that PersonaFuse substantiallyoutperforms baseline models across multiple dimensions of social-emotionalintelligence. Importantly, these gains are achieved without sacrificing generalreasoning ability or model safety, which remain common limitations of directprompting and supervised fine-tuning approaches. PersonaFuse also deliversconsistent improvements in downstream human-centered applications, such asmental health counseling and review-based customer service. Finally, humanpreference evaluations against leading LLMs, including GPT-4o and DeepSeek,demonstrate that PersonaFuse achieves competitive response quality despite itscomparatively smaller model size. These findings demonstrate thatPersonaFuse~offers a theoretically grounded and practical approach fordeveloping social-emotional enhanced LLMs, marking a significant advancementtoward more human-centric AI systems.</description><author>Yixuan Tang, Yi Yang, Ahmed Abbasi</author><pubDate>Tue, 09 Sep 2025 03:39:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07370v1</guid></item><item><title>CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation</title><link>http://arxiv.org/abs/2509.07325v1</link><description>The National Comprehensive Cancer Network (NCCN) provides evidence-basedguidelines for cancer treatment. Translating complex patient presentations intoguideline-compliant treatment recommendations is time-intensive, requiresspecialized expertise, and is prone to error. Advances in large language model(LLM) capabilities promise to reduce the time required to generate treatmentrecommendations and improve accuracy. We present an LLM agent-based approach toautomatically generate guideline-concordant treatment trajectories for patientswith non-small cell lung cancer (NSCLC). Our contributions are threefold.First, we construct a novel longitudinal dataset of 121 cases of NSCLC patientsthat includes clinical encounters, diagnostic results, and medical histories,each expertly annotated with the corresponding NCCN guideline trajectories byboard-certified oncologists. Second, we demonstrate that existing LLMs possessdomain-specific knowledge that enables high-quality proxy benchmark generationfor both model development and evaluation, achieving strong correlation(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.Third, we develop a hybrid approach combining expensive human annotations withmodel consistency information to create both the agent framework that predictsthe relevant guidelines for a patient, as well as a meta-classifier thatverifies prediction accuracy with calibrated confidence scores for treatmentrecommendations (AUROC=0.800), a critical capability for communicating theaccuracy of outputs, custom-tailoring tradeoffs in performance, and supportingregulatory compliance. This work establishes a framework for clinically viableLLM-based guideline adherence systems that balance accuracy, interpretability,and regulatory requirements while reducing annotation costs, providing ascalable pathway toward automated clinical decision support.</description><author>Alyssa Unell, Noel C. F. Codella, Sam Preston, Peniel Argaw, Wen-wai Yim, Zelalem Gero, Cliff Wong, Rajesh Jena, Eric Horvitz, Amanda K. Hall, Ruican Rachel Zhong, Jiachen Li, Shrey Jain, Mu Wei, Matthew Lungren, Hoifung Poon</author><pubDate>Tue, 09 Sep 2025 01:49:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07325v1</guid></item><item><title>A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor</title><link>http://arxiv.org/abs/2405.16887v2</link><description>As customer demand for multi-variety and small-batch production increases,dynamic disturbances place greater demands on manufacturing systems. To addresssuch challenges, researchers proposed the multi-agent manufacturing system.However, conventional agent negotiation typically relies on pre-defined andfixed heuristic rules, which are ill-suited to managing complex and fluctuatingdisturbances. In current implementations, mainstream approaches based onreinforcement learning require the development of simulators and trainingmodels specific to a given shopfloor, necessitating substantial computationalresources and lacking scalability. To overcome this limitation, the presentstudy proposes a Large Language Model-based (LLM-based) multi-agentmanufacturing system for intelligent shopfloor management. By defining thediverse modules of agents and their collaborative methods, this systemfacilitates the processing of all workpieces with minimal human intervention.The agents in this system consist of the Machine Server Module (MSM), BidInviter Module (BIM), Bidder Module (BM), Thinking Module (TM), and DecisionModule (DM). By harnessing the reasoning capabilities of LLMs, these modulesenable agents to dynamically analyze shopfloor information and selectappropriate processing machines. The LLM-based modules, predefined by systemprompts, provide dynamic functionality for the system without the need forpre-training. Extensive experiments were conducted in physical shopfloorsettings. The results demonstrate that the proposed system exhibits strongadaptability, and achieves superior performance (makespan) and stability (asmeasured by sample standard deviation) compared to other approaches withoutrequiring pre-training.</description><author>Zhen Zhao, Dunbing Tang, Changchun Liu, Liping Wang, Zequn Zhang, Haihua Zhu, Kai Chen, Qingwei Nie, Yuchen Ji</author><pubDate>Mon, 22 Sep 2025 13:20:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16887v2</guid></item><item><title>LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology</title><link>http://arxiv.org/abs/2509.13978v2</link><description>Modern scientific discovery increasingly relies on workflows that processdata across the Edge, Cloud, and High Performance Computing (HPC) continuum.Comprehensive and in-depth analyses of these data are critical for hypothesisvalidation, anomaly detection, reproducibility, and impactful findings.Although workflow provenance techniques support such analyses, at large scale,the provenance data become complex and difficult to analyze. Existing systemsdepend on custom scripts, structured queries, or static dashboards, limitingdata interaction. In this work, we introduce an evaluation methodology,reference architecture, and open-source implementation that leveragesinteractive Large Language Model (LLM) agents for runtime data analysis. Ourapproach uses a lightweight, metadata-driven design that translates naturallanguage into structured provenance queries. Evaluations across LLaMA, GPT,Gemini, and Claude, covering diverse query classes and a real-world chemistryworkflow, show that modular design, prompt tuning, and Retrieval-AugmentedGeneration (RAG) enable accurate and insightful LLM agent responses beyondrecorded provenance.</description><author>Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, Rafael Ferreira da Silva</author><pubDate>Tue, 23 Sep 2025 13:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13978v2</guid></item><item><title>LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines</title><link>http://arxiv.org/abs/2509.19580v2</link><description>Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our viewof the world. For example, Large Language Models (LLMs) based applications suchas ChatGPT have shown the capability of generating human-like conversation onextensive topics. Due to the impressive performance on a variety oflanguage-related tasks (e.g., open-domain question answering, translation, anddocument summarization), one can envision the far-reaching impacts that can bebrought by the LLMs with broader real-world applications (e.g., customerservice, education and accessibility, and scientific discovery). Inspired bytheir success, this paper will offer an overview of state-of-the-art LLMs andtheir integration into a wide range of academic disciplines, including: (1)arts, letters, and law (e.g., history, philosophy, political science, arts andarchitecture, law), (2) economics and business (e.g., finance, economics,accounting, marketing), and (3) science and engineering (e.g., mathematics,physics and mechanical engineering, chemistry and chemical engineering, lifesciences and bioengineering, earth sciences and civil engineering, computerscience and electrical engineering). Integrating humanity and technology, inthis paper, we will explore how LLMs are shaping research and practice in thesefields, while also discussing key limitations, open challenges, and futuredirections in the era of generative AI. The review of how LLMs are engagedacross disciplines-along with key observations and insights-can helpresearchers and practitioners interested in exploiting LLMs to advance theirworks in diverse real-world applications.</description><author>Yanfang Fanny Ye, Zheyuan Zhang, Tianyi Ma, Zehong Wang, Yiyang Li, Shifu Hou, Weixiang Sun, Kaiwen Shi, Yijun Ma, Wei Song, Ahmed Abbasi, Ying Cheng, Jane Cleland-Huang, Steven Corcelli, Patricia Culligan, Robert Goulding, Ming Hu, Ting Hua, John Lalor, Fang Liu, Tengfei Luo, Ed Maginn, Nuno Moniz, Jason Rohr, Brett Savoie, Daniel Slate, Tom Stapleford, Matthew Webber, Olaf Wiest, Johnny Zhang, Nitesh Chawla</author><pubDate>Thu, 25 Sep 2025 15:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.19580v2</guid></item></channel></rss>