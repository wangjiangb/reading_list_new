<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model customization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 01 Sep 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Reference Points in LLM Sentiment Analysis: The Role of Structured Context</title><link>http://arxiv.org/abs/2508.11454v1</link><description>Large language models (LLMs) are now widely used across many fields,including marketing research. Sentiment analysis, in particular, helps firmsunderstand consumer preferences. While most NLP studies classify sentiment fromreview text alone, marketing theories, such as prospect theory andexpectation--disconfirmation theory, point out that customer evaluations areshaped not only by the actual experience but also by additional referencepoints. This study therefore investigates how the content and format of suchsupplementary information affect sentiment analysis using LLMs. We comparenatural language (NL) and JSON-formatted prompts using a lightweight 3Bparameter model suitable for practical marketing applications. Experiments ontwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt withadditional information outperforms all baselines without fine-tuning: Macro-F1rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making itdeployable in resource-constrained edge devices. Furthermore, a follow-upanalysis confirms that performance gains stem from genuine contextual reasoningrather than label proxying. This work demonstrates that structured promptingcan enable smaller models to achieve competitive performance, offering apractical alternative to large-scale model deployment.</description><author>Junichiro Niimi</author><pubDate>Fri, 15 Aug 2025 13:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11454v1</guid></item><item><title>Personalized LLM for Generating Customized Responses to the Same Query from Different Users</title><link>http://arxiv.org/abs/2412.11736v2</link><description>Existing work on large language model (LLM) personalization assigneddifferent responding roles to LLMs, but overlooked the diversity of queriers.In this work, we propose a new form of querier-aware LLM personalization,generating different responses even for the same query from different queriers.We design a dual-tower model architecture with a cross-querier general encoderand a querier-specific encoder. We further apply contrastive learning withmulti-view augmentation, pulling close the dialogue representations of the samequerier, while pulling apart those of different queriers. To mitigate theimpact of query diversity on querier-contrastive learning, we cluster thedialogues based on query similarity and restrict the scope of contrastivelearning within each cluster. To address the lack of datasets designed forquerier-aware personalization, we also build a multi-querier dataset fromEnglish and Chinese scripts, as well as WeChat records, called MQDialog,containing 173 queriers and 12 responders. Extensive evaluations demonstratethat our design significantly improves the quality of personalized responsegeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scoresand winning rates ranging from 54% to 82% compared with various baselinemethods.</description><author>Hang Zeng, Chaoyue Niu, Fan Wu, Chengfei Lv, Guihai Chen</author><pubDate>Fri, 15 Aug 2025 08:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11736v2</guid></item><item><title>Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems</title><link>http://arxiv.org/abs/2508.14982v1</link><description>Conversational explainable artificial intelligence (ConvXAI) systems based onlarge language models (LLMs) have garnered considerable attention for theirability to enhance user comprehension through dialogue-based explanations.Current ConvXAI systems often are based on intent recognition to accuratelyidentify the user's desired intention and map it to an explainability method.While such methods offer great precision and reliability in discerning users'underlying intentions for English, a significant challenge in the scarcity oftraining data persists, which impedes multilingual generalization. Besides, thesupport for free-form custom inputs, which are user-defined data distinct frompre-configured dataset instances, remains largely limited. To bridge thesegaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQLdataset spanning five typologically diverse languages, including onelow-resource language. Subsequently, we propose a new parsing approach aimed atenhancing multilingual parsing performance, and evaluate three LLMs onMultiCoXQL using various parsing strategies. Furthermore, we present Compass, anew multilingual dataset designed for custom input extraction in ConvXAIsystems, encompassing 11 intents across the same five languages as MultiCoXQL.We conduct monolingual, cross-lingual, and multilingual evaluations on Compass,employing three LLMs of varying sizes alongside BERT-type models.</description><author>Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Fedor Splitt, Jiaao Li, Yoana Tsoneva, Sebastian MÃ¶ller, Vera Schmitt</author><pubDate>Wed, 20 Aug 2025 18:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14982v1</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v2</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</author><pubDate>Thu, 21 Aug 2025 18:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v2</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title><link>http://arxiv.org/abs/2504.19254v3</link><description>Hallucinations are a persistent problem with Large Language Models (LLMs). Asthese models become increasingly used in high-stakes domains, such ashealthcare and finance, the need for effective hallucination detection iscrucial. To this end, we outline a versatile framework for zero-resourcehallucination detection that practitioners can apply to real-world use cases.To achieve this, we adapt a variety of existing uncertainty quantification (UQ)techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,transforming them as necessary into standardized response-level confidencescores ranging from 0 to 1. To enhance flexibility, we propose a tunableensemble approach that incorporates any combination of the individualconfidence scores. This approach enables practitioners to optimize the ensemblefor a specific use case for improved performance. To streamline implementation,the full suite of scorers is offered in this paper's companion Python toolkit,UQLM. To evaluate the performance of the various scorers, we conduct anextensive set of experiments using several LLM question-answering benchmarks.We find that our tunable ensemble typically surpasses its individual componentsand outperforms existing hallucination detection methods. Our resultsdemonstrate the benefits of customized hallucination detection strategies forimproving the accuracy and reliability of LLMs.</description><author>Dylan Bouchard, Mohit Singh Chauhan</author><pubDate>Wed, 20 Aug 2025 14:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19254v3</guid></item><item><title>The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation</title><link>http://arxiv.org/abs/2508.14718v1</link><description>We established a rigorous benchmark for text-based recipe generation, afundamental task in natural language generation. We present a comprehensivecomparative study contrasting a fine-tuned GPT-2 large (774M) model against theGPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisinecorpus from RecipeDB. Our key contribution is a targeted tokenization strategythat augments the vocabulary with 23 common fraction tokens and customstructural markers. This approach addresses a critical limitation of generictokenizers by preserving essential recipe structures and precise numericalquantities, thereby enhancing domain specificity. Performance is evaluatedusing a comprehensive suite of seven automatic metrics spanning fluency(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), anddiversity. Our experiments show that the large transformer-based approachyields a &gt;20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over thebest recurrent baseline, while reducing perplexity by 69.8%. We conclude with adiscussion of remaining challenges, particularly regarding factual accuracy,and outline how this foundational study paves the way for integratingreal-world constraints and multi-modal inputs in advanced recipe generationresearch.</description><author>Shubham Pundhir, Ganesh Bagler</author><pubDate>Wed, 20 Aug 2025 13:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14718v1</guid></item><item><title>PathGPT: Reframing Path Recommendation as a Natural Language Generation Task with Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2504.05846v2</link><description>Path recommendation (PR) aims to generate travel paths that are customized toa user's specific preferences and constraints. Conventional approaches oftenemploy explicit optimization objectives or specialized machine learningarchitectures; however, these methods typically exhibit limited flexibility andgeneralizability, necessitating costly retraining to accommodate new scenarios.This paper introduces an alternative paradigm that conceptualizes PR as anatural language generation task. We present PathGPT, a retrieval-augmentedlarge language model (LLM) system that leverages historical trajectory data andnatural language user constraints to generate plausible paths. The proposedmethodology first converts raw trajectory data into a human-interpretabletextual format, which is then stored in a database. Subsequently, a hybridretrieval system extracts path-specific context from this database to inform apretrained LLM. The primary contribution of this work is a novel framework thatdemonstrates how integrating established information retrieval and generativemodel components can enable adaptive, zero-shot path generation across diversescenarios. Extensive experiments on large-scale trajectory datasets indicatethat PathGPT's performance is competitive with specialized, learning-basedmethods, underscoring its potential as a flexible and generalizable pathgeneration system that avoids the need for retraining inherent in previousdata-driven models.</description><author>Steeve Cuthbert Marcelyn, Yucen Gao, Yuzhe Zhang, Xiaofeng Gao</author><pubDate>Wed, 20 Aug 2025 06:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05846v2</guid></item><item><title>JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</title><link>http://arxiv.org/abs/2508.10287v2</link><description>Recent advances in Vision-Language Models (VLMs) and large language models(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AIagents like robots. However, existing visual reasoning benchmarks often sufferfrom several limitations: they lack a clear definition of reasoning complexity,offer have no control to generate questions over varying difficulty and taskcustomization, and fail to provide structured, step-by-step reasoningannotations (workflows). To bridge these gaps, we formalize reasoningcomplexity, introduce an adaptive query engine that generates customizablequestions of varying complexity with detailed intermediate annotations, andextend the JRDB dataset with human-object interaction and geometricrelationship annotations to create JRDB-Reasoning, a benchmark tailored forvisual reasoning in human-crowded environments. Our engine and benchmark enablefine-grained evaluation of visual reasoning frameworks and dynamic assessmentof visual-language models across reasoning levels.</description><author>Simindokht Jahangard, Mehrzad Mohammadi, Yi Shen, Zhixi Cai, Hamid Rezatofighi</author><pubDate>Wed, 20 Aug 2025 04:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10287v2</guid></item><item><title>A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models</title><link>http://arxiv.org/abs/2508.17994v1</link><description>Aspect-based sentiment analysis enhances sentiment detection by associatingit with specific aspects, offering deeper insights than traditional sentimentanalysis. This study introduces a manually annotated dataset of 10,814multilingual customer reviews covering brick-and-mortar retail stores, labeledwith eight aspect categories and their sentiment. Using this dataset, theperformance of GPT-4 and LLaMA-3 in aspect based sentiment analysis isevaluated to establish a baseline for the newly introduced data. The resultsshow both models achieving over 85% accuracy, while GPT-4 outperforms LLaMA-3overall with regard to all relevant metrics.</description><author>Oleg Silcenco, Marcos R. Machad, Wallace C. Ugulino, Daniel Braun</author><pubDate>Mon, 25 Aug 2025 13:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17994v1</guid></item><item><title>M$^2$IV: Towards Efficient and Fine-grained Multimodal In-Context Learning via Representation Engineering</title><link>http://arxiv.org/abs/2504.04633v3</link><description>Multimodal in-context learning (ICL) equips Large Vision-language Models(LVLMs) with the ability to adapt to new tasks via multiple user-provideddemonstrations, without requiring any model parameter updates. However, itseffectiveness is constrained by the token-intensive nature of multimodal inputsand the complexity of cross-modal few-shot reasoning, which together hinderLVLMs from extracting useful patterns from demonstrations. To address thesechallenges, we propose \textbf{M$^2$IV}, a novel representation engineeringapproach that replaces explicit token-level demonstrations with a set oflearnable Multimodal In-context Vectors directly injected into the residualstreams of LVLMs. By analyzing the distinct roles of multi-head attention (MHA)and multi-layer perceptrons (MLP) in the ICL process, we design a trainingstrategy that enables M$^2$IV to perform fine-grained semantic distillation androbust cross-modal representation learning. M$^2$IV not only improvesperformance across diverse tasks and LVLMs but also significantly reduces tokenoverhead, enabling graceful scaling to many-shot scenarios. To further enhanceusability, we introduce \textbf{VLibrary}, a repository that stores trainedM$^2$IVs for flexible retrieval and injection. With VLibrary, users can steerpre-trained LVLMs in a customized manner that meets diverse requirements.Extensive experiments demonstrate that M$^2$IV consistently outperforms vanillaICL and prior representation engineering baselines, achieving an averageaccuracy gain of 3.74\% with substantial improvements in overall efficiency.</description><author>Yanshu Li, Yi Cao, Hongyang He, Qisen Cheng, Xiang Fu, Xi Xiao, Tianyang Wang, Ruixiang Tang</author><pubDate>Tue, 26 Aug 2025 10:19:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.04633v3</guid></item><item><title>Controllable Conversational Theme Detection Track at DSTC 12</title><link>http://arxiv.org/abs/2508.18783v1</link><description>Conversational analytics has been on the forefront of transformation drivenby the advances in Speech and Natural Language Processing techniques. Rapidadoption of Large Language Models (LLMs) in the analytics field has taken theproblems that can be automated to a new level of complexity and scale. In thispaper, we introduce Theme Detection as a critical task in conversationalanalytics, aimed at automatically identifying and categorizing topics withinconversations. This process can significantly reduce the manual effort involvedin analyzing expansive dialogs, particularly in domains like customer supportor sales. Unlike traditional dialog intent detection, which often relies on afixed set of intents for downstream system logic, themes are intended as adirect, user-facing summary of the conversation's core inquiry. Thisdistinction allows for greater flexibility in theme surface forms anduser-specific customizations. We pose Controllable Conversational ThemeDetection problem as a public competition track at Dialog System TechnologyChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling ofdialog utterances, with the distinctive aspect being controllability of theresulting theme clusters' granularity achieved via the provided user preferencedata. We give an overview of the problem, the associated dataset and theevaluation metrics, both automatic and human. Finally, we discuss theparticipant teams' submissions and provide insights from those. The trackmaterials (data and code) are openly available in the GitHub repository.</description><author>Igor Shalyminov, Hang Su, Jake Vincent, Siffi Singh, Jason Cai, James Gung, Raphael Shu, Saab Mansour</author><pubDate>Tue, 26 Aug 2025 08:10:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18783v1</guid></item><item><title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title><link>http://arxiv.org/abs/2508.19650v2</link><description>Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding modelenhancement.https://github.com/Cola-any/Video-LevelGauge</description><author>Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang</author><pubDate>Thu, 28 Aug 2025 09:44:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19650v2</guid></item><item><title>Occlusion Robustness of CLIP for Military Vehicle Classification</title><link>http://arxiv.org/abs/2508.20760v1</link><description>Vision-language models (VLMs) like CLIP enable zero-shot classification byaligning images and text in a shared embedding space, offering advantages fordefense applications with scarce labeled data. However, CLIP's robustness inchallenging military environments, with partial occlusion and degradedsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIPvariants' robustness to occlusion using a custom dataset of 18 military vehicleclasses and evaluate using Normalized Area Under the Curve (NAUC) acrossocclusion percentages. Four key insights emerge: (1) Transformer-based CLIPmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusionsdegrade performance more than larger contiguous occlusions, (3) despiteimproved accuracy, performance of linear-probed models sharply drops at around35% occlusion, (4) by finetuning the model's backbone, this performance dropoccurs at more than 60% occlusion. These results underscore the importance ofocclusion-specific augmentations during training and the need for furtherexploration into patch-level sensitivity and architectural resilience forreal-world deployment of CLIP.</description><author>Jan Erik van Woerden, Gertjan Burghouts, Lotte Nijskens, Alma M. Liezenga, Sabina van Rooij, Frank Ruis, Hugo J. Kuijf</author><pubDate>Thu, 28 Aug 2025 13:16:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20760v1</guid></item></channel></rss>