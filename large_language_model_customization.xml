<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model customization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 27 Aug 2025 01:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Reference Points in LLM Sentiment Analysis: The Role of Structured Context</title><link>http://arxiv.org/abs/2508.11454v1</link><description>Large language models (LLMs) are now widely used across many fields,including marketing research. Sentiment analysis, in particular, helps firmsunderstand consumer preferences. While most NLP studies classify sentiment fromreview text alone, marketing theories, such as prospect theory andexpectation--disconfirmation theory, point out that customer evaluations areshaped not only by the actual experience but also by additional referencepoints. This study therefore investigates how the content and format of suchsupplementary information affect sentiment analysis using LLMs. We comparenatural language (NL) and JSON-formatted prompts using a lightweight 3Bparameter model suitable for practical marketing applications. Experiments ontwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt withadditional information outperforms all baselines without fine-tuning: Macro-F1rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making itdeployable in resource-constrained edge devices. Furthermore, a follow-upanalysis confirms that performance gains stem from genuine contextual reasoningrather than label proxying. This work demonstrates that structured promptingcan enable smaller models to achieve competitive performance, offering apractical alternative to large-scale model deployment.</description><author>Junichiro Niimi</author><pubDate>Fri, 15 Aug 2025 13:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11454v1</guid></item><item><title>Personalized LLM for Generating Customized Responses to the Same Query from Different Users</title><link>http://arxiv.org/abs/2412.11736v2</link><description>Existing work on large language model (LLM) personalization assigneddifferent responding roles to LLMs, but overlooked the diversity of queriers.In this work, we propose a new form of querier-aware LLM personalization,generating different responses even for the same query from different queriers.We design a dual-tower model architecture with a cross-querier general encoderand a querier-specific encoder. We further apply contrastive learning withmulti-view augmentation, pulling close the dialogue representations of the samequerier, while pulling apart those of different queriers. To mitigate theimpact of query diversity on querier-contrastive learning, we cluster thedialogues based on query similarity and restrict the scope of contrastivelearning within each cluster. To address the lack of datasets designed forquerier-aware personalization, we also build a multi-querier dataset fromEnglish and Chinese scripts, as well as WeChat records, called MQDialog,containing 173 queriers and 12 responders. Extensive evaluations demonstratethat our design significantly improves the quality of personalized responsegeneration, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scoresand winning rates ranging from 54% to 82% compared with various baselinemethods.</description><author>Hang Zeng, Chaoyue Niu, Fan Wu, Chengfei Lv, Guihai Chen</author><pubDate>Fri, 15 Aug 2025 08:12:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11736v2</guid></item><item><title>Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems</title><link>http://arxiv.org/abs/2508.14982v1</link><description>Conversational explainable artificial intelligence (ConvXAI) systems based onlarge language models (LLMs) have garnered considerable attention for theirability to enhance user comprehension through dialogue-based explanations.Current ConvXAI systems often are based on intent recognition to accuratelyidentify the user's desired intention and map it to an explainability method.While such methods offer great precision and reliability in discerning users'underlying intentions for English, a significant challenge in the scarcity oftraining data persists, which impedes multilingual generalization. Besides, thesupport for free-form custom inputs, which are user-defined data distinct frompre-configured dataset instances, remains largely limited. To bridge thesegaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQLdataset spanning five typologically diverse languages, including onelow-resource language. Subsequently, we propose a new parsing approach aimed atenhancing multilingual parsing performance, and evaluate three LLMs onMultiCoXQL using various parsing strategies. Furthermore, we present Compass, anew multilingual dataset designed for custom input extraction in ConvXAIsystems, encompassing 11 intents across the same five languages as MultiCoXQL.We conduct monolingual, cross-lingual, and multilingual evaluations on Compass,employing three LLMs of varying sizes alongside BERT-type models.</description><author>Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Fedor Splitt, Jiaao Li, Yoana Tsoneva, Sebastian MÃ¶ller, Vera Schmitt</author><pubDate>Wed, 20 Aug 2025 18:09:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14982v1</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v2</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</author><pubDate>Thu, 21 Aug 2025 18:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v2</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title><link>http://arxiv.org/abs/2504.19254v3</link><description>Hallucinations are a persistent problem with Large Language Models (LLMs). Asthese models become increasingly used in high-stakes domains, such ashealthcare and finance, the need for effective hallucination detection iscrucial. To this end, we outline a versatile framework for zero-resourcehallucination detection that practitioners can apply to real-world use cases.To achieve this, we adapt a variety of existing uncertainty quantification (UQ)techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,transforming them as necessary into standardized response-level confidencescores ranging from 0 to 1. To enhance flexibility, we propose a tunableensemble approach that incorporates any combination of the individualconfidence scores. This approach enables practitioners to optimize the ensemblefor a specific use case for improved performance. To streamline implementation,the full suite of scorers is offered in this paper's companion Python toolkit,UQLM. To evaluate the performance of the various scorers, we conduct anextensive set of experiments using several LLM question-answering benchmarks.We find that our tunable ensemble typically surpasses its individual componentsand outperforms existing hallucination detection methods. Our resultsdemonstrate the benefits of customized hallucination detection strategies forimproving the accuracy and reliability of LLMs.</description><author>Dylan Bouchard, Mohit Singh Chauhan</author><pubDate>Wed, 20 Aug 2025 14:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19254v3</guid></item><item><title>The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation</title><link>http://arxiv.org/abs/2508.14718v1</link><description>We established a rigorous benchmark for text-based recipe generation, afundamental task in natural language generation. We present a comprehensivecomparative study contrasting a fine-tuned GPT-2 large (774M) model against theGPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisinecorpus from RecipeDB. Our key contribution is a targeted tokenization strategythat augments the vocabulary with 23 common fraction tokens and customstructural markers. This approach addresses a critical limitation of generictokenizers by preserving essential recipe structures and precise numericalquantities, thereby enhancing domain specificity. Performance is evaluatedusing a comprehensive suite of seven automatic metrics spanning fluency(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), anddiversity. Our experiments show that the large transformer-based approachyields a &gt;20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over thebest recurrent baseline, while reducing perplexity by 69.8%. We conclude with adiscussion of remaining challenges, particularly regarding factual accuracy,and outline how this foundational study paves the way for integratingreal-world constraints and multi-modal inputs in advanced recipe generationresearch.</description><author>Shubham Pundhir, Ganesh Bagler</author><pubDate>Wed, 20 Aug 2025 13:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14718v1</guid></item><item><title>PathGPT: Reframing Path Recommendation as a Natural Language Generation Task with Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2504.05846v2</link><description>Path recommendation (PR) aims to generate travel paths that are customized toa user's specific preferences and constraints. Conventional approaches oftenemploy explicit optimization objectives or specialized machine learningarchitectures; however, these methods typically exhibit limited flexibility andgeneralizability, necessitating costly retraining to accommodate new scenarios.This paper introduces an alternative paradigm that conceptualizes PR as anatural language generation task. We present PathGPT, a retrieval-augmentedlarge language model (LLM) system that leverages historical trajectory data andnatural language user constraints to generate plausible paths. The proposedmethodology first converts raw trajectory data into a human-interpretabletextual format, which is then stored in a database. Subsequently, a hybridretrieval system extracts path-specific context from this database to inform apretrained LLM. The primary contribution of this work is a novel framework thatdemonstrates how integrating established information retrieval and generativemodel components can enable adaptive, zero-shot path generation across diversescenarios. Extensive experiments on large-scale trajectory datasets indicatethat PathGPT's performance is competitive with specialized, learning-basedmethods, underscoring its potential as a flexible and generalizable pathgeneration system that avoids the need for retraining inherent in previousdata-driven models.</description><author>Steeve Cuthbert Marcelyn, Yucen Gao, Yuzhe Zhang, Xiaofeng Gao</author><pubDate>Wed, 20 Aug 2025 06:37:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05846v2</guid></item><item><title>JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics</title><link>http://arxiv.org/abs/2508.10287v2</link><description>Recent advances in Vision-Language Models (VLMs) and large language models(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AIagents like robots. However, existing visual reasoning benchmarks often sufferfrom several limitations: they lack a clear definition of reasoning complexity,offer have no control to generate questions over varying difficulty and taskcustomization, and fail to provide structured, step-by-step reasoningannotations (workflows). To bridge these gaps, we formalize reasoningcomplexity, introduce an adaptive query engine that generates customizablequestions of varying complexity with detailed intermediate annotations, andextend the JRDB dataset with human-object interaction and geometricrelationship annotations to create JRDB-Reasoning, a benchmark tailored forvisual reasoning in human-crowded environments. Our engine and benchmark enablefine-grained evaluation of visual reasoning frameworks and dynamic assessmentof visual-language models across reasoning levels.</description><author>Simindokht Jahangard, Mehrzad Mohammadi, Yi Shen, Zhixi Cai, Hamid Rezatofighi</author><pubDate>Wed, 20 Aug 2025 04:35:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10287v2</guid></item><item><title>A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models</title><link>http://arxiv.org/abs/2508.17994v1</link><description>Aspect-based sentiment analysis enhances sentiment detection by associatingit with specific aspects, offering deeper insights than traditional sentimentanalysis. This study introduces a manually annotated dataset of 10,814multilingual customer reviews covering brick-and-mortar retail stores, labeledwith eight aspect categories and their sentiment. Using this dataset, theperformance of GPT-4 and LLaMA-3 in aspect based sentiment analysis isevaluated to establish a baseline for the newly introduced data. The resultsshow both models achieving over 85% accuracy, while GPT-4 outperforms LLaMA-3overall with regard to all relevant metrics.</description><author>Oleg Silcenco, Marcos R. Machad, Wallace C. Ugulino, Daniel Braun</author><pubDate>Mon, 25 Aug 2025 13:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17994v1</guid></item></channel></rss>