<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 15 Jul 2024 01:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</title><link>http://arxiv.org/abs/2407.07796v1</link><description>We introduce a novel and extensible benchmark for large language models(LLMs) through grid-based games such as Tic-Tac-Toe, Connect-Four, and Gomoku.The open-source game simulation code, available on GitHub, allows LLMs tocompete and generates detailed data files in JSON, CSV, TXT, and PNG formatsfor leaderboard rankings and further analysis. We present the results of gamesamong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet byAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo andGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions ofresults from other LLMs. In total, we simulated 2,310 matches (5 sessions foreach pair among 7 LLMs and a random player) across three types of games, usingthree distinct prompt types: list, illustration, and image. The resultsrevealed significant variations in LLM performance across different games andprompt types, with analysis covering win and disqualification rates, missedopportunity analysis, and invalid move analysis. The details of the leaderboardand result matrix data are available as open-access data on GitHub. This studyenhances our understanding of LLMs' capabilities in playing games they were notspecifically trained for, helping to assess their rule comprehension andstrategic thinking. On the path to Artificial General Intelligence (AGI), thisstudy lays the groundwork for future exploration into their utility in complexdecision-making scenarios, illuminating their strategic thinking abilities andoffering directions for further inquiry into the limits of LLMs withingame-based frameworks.</description><author>Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper</author><pubDate>Wed, 10 Jul 2024 16:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07796v1</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v3</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Wed, 10 Jul 2024 15:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v3</guid></item><item><title>Training A Small Emotional Vision Language Model for Visual Art Comprehension</title><link>http://arxiv.org/abs/2403.11150v2</link><description>This paper develops small vision language models to understand visual art,which, given an art work, aims to identify its emotion category and explainthis prediction with natural language. While small models are computationallyefficient, their capacity is much limited compared with large models. To breakthis trade-off, this paper builds a small emotional vision language model(SEVLM) by emotion modeling and input-output feature alignment. On the onehand, based on valence-arousal-dominance (VAD) knowledge annotated bypsychology experts, we introduce and fuse emotional features derived throughVAD dictionary and a VAD head to align VAD vectors of predicted emotionexplanation and the ground truth. This allows the vision language model tobetter understand and generate emotional texts, compared with using traditionaltext embeddings alone. On the other hand, we design a contrastive head to pullclose embeddings of the image, its emotion class, and explanation, which alignsmodel outputs and inputs. On two public affective explanation datasets, we showthat the proposed techniques consistently improve the visual art understandingperformance of baseline SEVLMs. Importantly, the proposed model can be trainedand evaluated on a single RTX 2080 Ti while exhibiting very strong performance:it not only outperforms the state-of-the-art small models but is alsocompetitive compared with LLaVA 7B after fine-tuning and GPT4(V). The code isavailable at https://github.com/BetterZH/SEVLM-code.</description><author>Jing Zhang, Liang Zheng, Meng Wang, Dan Guo</author><pubDate>Wed, 10 Jul 2024 13:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11150v2</guid></item><item><title>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</title><link>http://arxiv.org/abs/2407.07796v2</link><description>We introduce a novel and extensible benchmark for large language models(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.The open-source game simulation code, available on GitHub, allows LLMs tocompete and generates detailed data files in JSON, CSV, TXT, and PNG formatsfor leaderboard rankings and further analysis. We present the results of gamesamong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet byAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo andGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions ofresults from other LLMs. In total, we simulated 2,310 matches (5 sessions foreach pair among 7 LLMs and a random player) across three types of games, usingthree distinct prompt types: list, illustration, and image. The resultsrevealed significant variations in LLM performance across different games andprompt types, with analysis covering win and disqualification rates, missedopportunity analysis, and invalid move analysis. The details of the leaderboardand result matrix data are available as open-access data on GitHub. This studyenhances our understanding of LLMs' capabilities in playing games they were notspecifically trained for, helping to assess their rule comprehension andstrategic thinking. On the path to Artificial General Intelligence (AGI), thisstudy lays the groundwork for future exploration into their utility in complexdecision-making scenarios, illuminating their strategic thinking abilities andoffering directions for further inquiry into the limits of LLMs withingame-based frameworks.</description><author>Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper</author><pubDate>Thu, 11 Jul 2024 03:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07796v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v4</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Thu, 11 Jul 2024 14:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v4</guid></item><item><title>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</title><link>http://arxiv.org/abs/2407.06863v2</link><description>Text-to-Image (T2I) models are being increasingly adopted in diverse globalcommunities where they create visual representations of their unique cultures.Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realismof generated images, overlooking the critical dimension of cultural competence.In this work, we introduce a framework to evaluate cultural competence of T2Imodels along two crucial dimensions: cultural awareness and cultural diversity,and present a scalable approach using a combination of structured knowledgebases and large language models to build a large dataset of cultural artifactsto enable this evaluation. In particular, we apply this approach to build CUBE(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark toevaluate cultural competence of T2I models. CUBE covers cultural artifactsassociated with 8 countries across different geo-cultural regions and along 3concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set ofhigh-quality prompts that enable the evaluation of cultural awareness, and 2)CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding toevaluate cultural diversity. We also introduce cultural diversity as a novelT2I evaluation component, leveraging quality-weighted Vendi score. Ourevaluations reveal significant gaps in the cultural awareness of existingmodels across countries and provide valuable insights into the culturaldiversity of T2I outputs for under-specified prompts. Our methodology isextendable to other cultural regions and concepts, and can facilitate thedevelopment of T2I models that better cater to the global population.</description><author>Nithish Kannen, Arif Ahmad, Marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, Shachi Dave</author><pubDate>Thu, 11 Jul 2024 17:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06863v2</guid></item><item><title>Robotic Control via Embodied Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2407.08693v1</link><description>A key limitation of learned robot control policies is their inability togeneralize outside their training data. Recent works on vision-language-actionmodels (VLAs) have shown that the use of large, internet pre-trainedvision-language models as the backbone of learned robot policies cansubstantially improve their robustness and generalization ability. Yet, one ofthe most exciting capabilities of large vision-language models in other domainsis their ability to reason iteratively through complex problems. Can that samecapability be brought into robotics to allow policies to improve performance byreasoning about a given task before acting? Naive use of "chain-of-thought"(CoT) style prompting is significantly less effective with standard VLAsbecause of the relatively simple training examples that are available to them.Additionally, purely semantic reasoning about sub-tasks, as is common inregular CoT, is insufficient for robot policies that need to ground theirreasoning in sensory observations and the robot state. To this end, weintroduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which wetrain VLAs to perform multiple steps of reasoning about plans, sub-tasks,motions, and visually grounded features like object bounding boxes and endeffector positions, before predicting the robot action. We design a scalablepipeline for generating synthetic training data for ECoT on large robotdatasets. We demonstrate, that ECoT increases the absolute success rate ofOpenVLA, the current strongest open-source VLA policy, by 28% acrosschallenging generalization tasks, without any additional robot training data.Additionally, ECoT makes it easier for humans to interpret a policy's failuresand correct its behavior using natural language.</description><author>Zawalski Micha≈Ç, Chen William, Pertsch Karl, Mees Oier, Finn Chelsea, Levine Sergey</author><pubDate>Thu, 11 Jul 2024 17:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08693v1</guid></item></channel></rss>