<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Aug 2025 17:37:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Controlling Multimodal LLMs via Reward-guided Decoding</title><link>http://arxiv.org/abs/2508.11616v1</link><description>As Multimodal Large Language Models (MLLMs) gain widespread applicability, itis becoming increasingly desirable to adapt them for diverse user needs. Inthis paper, we study the adaptation of MLLMs through controlled decoding. Toachieve this, we introduce the first method for reward-guided decoding of MLLMsand demonstrate its application in improving their visual grounding. Our methodinvolves building reward models for visual grounding and using them to guidethe MLLM's decoding process. Concretely, we build two separate reward models toindependently control the degree of object precision and recall in the model'soutput. Our approach enables on-the-fly controllability of an MLLM's inferenceprocess in two ways: first, by giving control over the relative importance ofeach reward function during decoding, allowing a user to dynamically trade offobject precision for recall in image captioning tasks; second, by givingcontrol over the breadth of the search during decoding, allowing the user tocontrol the trade-off between the amount of test-time compute and the degree ofvisual grounding. We evaluate our method on standard object hallucinationbenchmarks, showing that it provides significant controllability over MLLMinference, while consistently outperforming existing hallucination mitigationmethods.</description><author>Oscar Ma√±as, Pierluca D'Oro, Koustuv Sinha, Adriana Romero-Soriano, Michal Drozdzal, Aishwarya Agrawal</author><pubDate>Fri, 15 Aug 2025 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11616v1</guid></item><item><title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title><link>http://arxiv.org/abs/2508.11538v1</link><description>Video reasoning segmentation (VRS) endeavors to delineate referred objects invideos guided by implicit instructions that encapsulate human intent andtemporal logic. Previous approaches leverage large vision language models(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.However, this paradigm suffers from limited interpretability during inferenceand suboptimal performance due to inadequate spatiotemporal reasoning. Drawinginspiration from seminal breakthroughs in reinforcement learning, we introduceVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning insegmentation. Veason-R1 is trained through Group Relative Policy Optimization(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, wecurate high-quality CoT training data to instill structured reasoningtrajectories, bridging video-level semantics and frame-level spatial grounding,yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPOfine-tuning encourages efficient exploration of the reasoning space byoptimizing reasoning chains. To this end, we incorporate a holistic rewardmechanism that synergistically enhances spatial alignment and temporalconsistency, bolstering keyframe localization and fine-grained grounding.Comprehensive empirical evaluations demonstrate that Veason-R1 achievesstate-of-the-art performance on multiple benchmarks, surpassing prior art bysignificant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),while exhibiting robustness to hallucinations (+8.8 R). Our code and modelweights will be available at Veason-R1.</description><author>Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</author><pubDate>Fri, 15 Aug 2025 15:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11538v1</guid></item><item><title>A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</title><link>http://arxiv.org/abs/2508.11529v1</link><description>Artificial intelligence is reshaping science and industry, yet many usersstill regard its models as opaque "black boxes". Conventional explainableartificial-intelligence methods clarify individual predictions but overlook theupstream decisions and downstream quality checks that determine whetherinsights can be trusted. In this work, we present Holistic ExplainableArtificial Intelligence (HXAI), a user-centric framework that embedsexplanation into every stage of the data-analysis workflow and tailors thoseexplanations to users. HXAI unifies six components (data, analysis set-up,learning process, model output, model quality, communication channel) into asingle taxonomy and aligns each component with the needs of domain experts,data analysts and data scientists. A 112-item question bank covers these needs;our survey of contemporary tools highlights critical coverage gaps. Grounded intheories of human explanation, principles from human-computer interaction andfindings from empirical user studies, HXAI identifies the characteristics thatmake explanations clear, actionable and cognitively manageable. A comprehensivetaxonomy operationalises these insights, reducing terminological ambiguity andenabling rigorous coverage analysis of existing toolchains. We furtherdemonstrate how AI agents that embed large-language models can orchestratediverse explanation techniques, translating technical artifacts intostakeholder-specific narratives that bridge the gap between AI developers anddomain experts. Departing from traditional surveys or perspective articles,this work melds concepts from multiple disciplines, lessons from real-worldprojects and a critical synthesis of the literature to advance a novel,end-to-end viewpoint on transparency, trustworthiness and responsible AIdeployment.</description><author>George Paterakis, Andrea Castellani, George Papoutsoglou, Tobias Rodemann, Ioannis Tsamardinos</author><pubDate>Fri, 15 Aug 2025 15:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11529v1</guid></item><item><title>UI-Venus Technical Report: Building High-performance UI Agents with RFT</title><link>http://arxiv.org/abs/2508.10833v2</link><description>We present UI-Venus, a native UI agent that takes only screenshots as inputbased on a multimodal large language model. UI-Venus achieves SOTA performanceon both UI grounding and navigation tasks using only several hundred thousandhigh-quality training samples through reinforcement finetune (RFT) based onQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,Screenspot-V2 / Pro, surpassing the previous SOTA baselines includingopen-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary andplaning ability, we also evaluate it on the AndroidWorld, an online UInavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%success rate, also beating existing models. To achieve this, we introducecarefully designed reward functions for both UI grounding and navigation tasksand corresponding efficient data cleaning strategies. To further boostnavigation performance, we propose Self-Evolving Trajectory History Alignment &amp;Sparse Action Enhancement that refine historical reasoning traces and balancesthe distribution of sparse but critical actions, leading to more coherentplanning and better generalization in complex UI tasks. Our contributionsinclude the publish of SOTA open-source UI agents, comprehensive data cleaningprotocols and a novel self-evolving framework for improving navigationperformance, which encourage further research and development in the community.Code is available at https://github.com/inclusionAI/UI-Venus.</description><author>Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</author><pubDate>Fri, 15 Aug 2025 14:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10833v2</guid></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2507.01006v5</link><description>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models(VLMs) designed to advance general-purpose multimodal understanding andreasoning. In this report, we share our key findings in the development of thereasoning-centric training framework. We first develop a capable visionfoundation model with significant potential through large-scale pre-training,which arguably sets the upper bound for the final performance. We then proposeReinforcement Learning with Curriculum Sampling (RLCS) to unlock the fullpotential of the model, leading to comprehensive capability enhancement acrossa diverse range of tasks, including STEM problem solving, video understanding,content recognition, coding, grounding, GUI-based agents, and long documentinterpretation. In a comprehensive evaluation across 42 public benchmarks,GLM-4.5V achieves state-of-the-art performance on nearly all tasks amongopen-source models of similar size, and demonstrates competitive or evensuperior results compared to closed-source models such as Gemini-2.5-Flash onchallenging tasks including Coding and GUI Agents. Meanwhile, the smallerGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results tothe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source bothGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information arereleased at https://github.com/zai-org/GLM-V.</description><author>GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan</author><pubDate>Fri, 15 Aug 2025 13:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01006v5</guid></item><item><title>MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</title><link>http://arxiv.org/abs/2508.11433v1</link><description>Multimodal Large Language Models (MLLMs) with unified architectures excelacross a wide range of vision-language tasks, yet aligning them withpersonalized image generation remains a significant challenge. Existing methodsfor MLLMs are frequently subject-specific, demanding a data-intensivefine-tuning process for every new subject, which limits their scalability. Inthis paper, we introduce MM-R1, a framework that integrates a cross-modalChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential ofunified MLLMs for personalized image generation. Specifically, we structurepersonalization as an integrated visual reasoning and generation process: (1)grounding subject concepts by interpreting and understanding user-providedimages and contextual cues, and (2) generating personalized images conditionedon both the extracted subject representations and user prompts. To furtherenhance the reasoning capability, we adopt Grouped Reward Proximal PolicyOptimization (GRPO) to explicitly align the generation. Experiments demonstratethat MM-R1 unleashes the personalization capability of unified MLLMs togenerate images with high subject fidelity and strong text alignment in azero-shot manner.</description><author>Qian Liang, Yujia Wu, Kuncheng Li, Jiwei Wei, Shiyuan He, Jinyu Guo, Ning Xie</author><pubDate>Fri, 15 Aug 2025 12:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11433v1</guid></item><item><title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title><link>http://arxiv.org/abs/2506.05982v4</link><description>As automated attack techniques rapidly advance, CAPTCHAs remain a criticaldefense mechanism against malicious bots. However, existing CAPTCHA schemesencompass a diverse range of modalities -- from static distorted text andobfuscated images to interactive clicks, sliding puzzles, and logic-basedquestions -- yet the community still lacks a unified, large-scale, multimodalbenchmark to rigorously evaluate their security robustness. To address thisgap, we introduce MCA-Bench, a comprehensive and reproducible benchmarkingsuite that integrates heterogeneous CAPTCHA types into a single evaluationprotocol. Leveraging a shared vision-language model backbone, we fine-tunespecialized cracking agents for each CAPTCHA category, enabling consistent,cross-modal assessments. Extensive experiments reveal that MCA-Bencheffectively maps the vulnerability spectrum of modern CAPTCHA designs undervaried attack settings, and crucially offers the first quantitative analysis ofhow challenge complexity, interaction depth, and model solvability interrelate.Based on these findings, we propose three actionable design principles andidentify key open challenges, laying the groundwork for systematic CAPTCHAhardening, fair benchmarking, and broader community collaboration. Datasets andcode are available online.</description><author>Zonglin Wu, Yule Xue, Yaoyao Feng, Xiaolong Wang, Yiren Song</author><pubDate>Fri, 15 Aug 2025 10:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05982v4</guid></item><item><title>SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models</title><link>http://arxiv.org/abs/2504.18684v2</link><description>Interpreting object-referential language and grounding objects in 3D withspatial relations and attributes is essential for robots operating alongsidehumans. However, this task is often challenging due to the diversity of scenes,large number of fine-grained objects, and complex free-form nature of languagereferences. Furthermore, in the 3D domain, obtaining large amounts of naturallanguage training data is difficult. Thus, it is important for methods to learnfrom little data and zero-shot generalize to new environments. To address thesechallenges, we propose SORT3D, an approach that utilizes rich object attributesfrom 2D data and merges a heuristics-based spatial reasoning toolbox with theability of large language models (LLMs) to perform sequential reasoning.Importantly, our method does not require text-to-3D data for training and canbe applied zero-shot to unseen environments. We show that SORT3D achievesstate-of-the-art zero-shot performance on complex view-dependent groundingtasks on two benchmarks. We also implement the pipeline to run real-time on twoautonomous vehicles and demonstrate that our approach can be used forobject-goal navigation on previously unseen real-world environments. All sourcecode for the system pipeline is publicly released athttps://github.com/nzantout/SORT3D.</description><author>Nader Zantout, Haochen Zhang, Pujith Kachana, Jinkai Qiu, Guofei Chen, Ji Zhang, Wenshan Wang</author><pubDate>Fri, 15 Aug 2025 00:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.18684v2</guid></item><item><title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title><link>http://arxiv.org/abs/2508.09922v2</link><description>Diffusion models have emerged as a leading framework for high-quality imagegeneration, offering stable training and strong performance across diversedomains. However, they remain computationally intensive, particularly duringthe iterative denoising process. Latent-space models like Stable Diffusionalleviate some of this cost by operating in compressed representations, thoughat the expense of fine-grained detail. More recent approaches such asRetrieval-Augmented Diffusion Models (RDM) address efficiency by conditioningdenoising on similar examples retrieved from large external memory banks. Whileeffective, these methods introduce drawbacks: they require costly storage andretrieval infrastructure, depend on static vision-language models like CLIP forsimilarity, and lack adaptability during training. We propose the PrototypeDiffusion Model (PDM), a method that integrates prototype learning directlyinto the diffusion process for efficient and adaptive visual conditioning -without external memory. Instead of retrieving reference samples, PDMconstructs a dynamic set of compact visual prototypes from clean image featuresusing contrastive learning. These prototypes guide the denoising steps byaligning noisy representations with semantically relevant visual patterns,enabling efficient generation with strong semantic grounding. Experiments showthat PDM maintains high generation quality while reducing computational andstorage overhead, offering a scalable alternative to retrieval-basedconditioning in diffusion models.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah</author><pubDate>Thu, 14 Aug 2025 21:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09922v2</guid></item><item><title>Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs</title><link>http://arxiv.org/abs/2508.11068v1</link><description>Abstract meaning representation (AMR) is a semantic formalism used torepresent the meaning of sentences as directed acyclic graphs. In this paper,we describe how real digital dictionaries can be embedded into AMR directedgraphs (digraphs), using state-of-the-art pre-trained large language models.Then, we reduce those graphs in a confluent manner, i.e. with transformationsthat preserve their circuit space. Finally, the properties of these reducesdigraphs are analyzed and discussed in relation to the symbol groundingproblem.</description><author>Nicolas Goulet, Alexandre Blondin Mass√©, Moussa Abdendi</author><pubDate>Thu, 14 Aug 2025 20:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11068v1</guid></item><item><title>Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?</title><link>http://arxiv.org/abs/2508.11011v1</link><description>Construction safety inspections typically involve a human inspectoridentifying safety concerns on-site. With the rise of powerful Vision LanguageModels (VLMs), researchers are exploring their use for tasks such as detectingsafety rule violations from on-site images. However, there is a lack of opendatasets to comprehensively evaluate and further fine-tune VLMs in constructionsafety inspection. Current applications of VLMs use small, supervised datasets,limiting their applicability in tasks they are not directly trained for. Inthis paper, we propose the ConstructionSite 10k, featuring 10,000 constructionsite images with annotations for three inter-connected tasks, including imagecaptioning, safety rule violation visual question answering (VQA), andconstruction element visual grounding. Our subsequent evaluation of currentstate-of-the-art large pre-trained VLMs shows notable generalization abilitiesin zero-shot and few-shot settings, while additional training is needed to makethem applicable to actual construction sites. This dataset allows researchersto train and evaluate their own VLMs with new architectures and techniques,providing a valuable benchmark for construction safety inspection.</description><author>Xuezheng Chen, Zhengbo Zou</author><pubDate>Thu, 14 Aug 2025 18:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11011v1</guid></item><item><title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title><link>http://arxiv.org/abs/2508.11009v1</link><description>The rapid proliferation of large language models (LLMs) in applicationstargeting children and adolescents necessitates a fundamental reassessment ofprevailing AI safety frameworks, which are largely tailored to adult users andneglect the distinct developmental vulnerabilities of minors. This paperhighlights key deficiencies in existing LLM safety benchmarks, including theirinadequate coverage of age-specific cognitive, emotional, and social risksspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence(13--18). To bridge these gaps, we introduce SproutBench, an innovativeevaluation suite comprising 1,283 developmentally grounded adversarial promptsdesigned to probe risks such as emotional dependency, privacy violations, andimitation of hazardous behaviors. Through rigorous empirical evaluation of 47diverse LLMs, we uncover substantial safety vulnerabilities, corroborated byrobust inter-dimensional correlations (e.g., between Safety and RiskPrevention) and a notable inverse relationship between Interactivity and AgeAppropriateness. These insights yield practical guidelines for advancingchild-centric AI design and deployment.</description><author>Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han</author><pubDate>Thu, 14 Aug 2025 18:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11009v1</guid></item><item><title>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</title><link>http://arxiv.org/abs/2508.13023v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhancedthe reasoning abilities of large language models (LLMs). Its success, however,largely depends on strong base models with rich world knowledge, yielding onlymodest improvements for small-size language models (SLMs). To address thislimitation, we investigate Guided GRPO, which injects ground-truth reasoningsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.Through a comprehensive study of various guidance configurations, we find thatnaively adding guidance delivers limited gains. These insights motivateG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strengthin response to the model's evolving training dynamics. Experiments onmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-Asubstantially outperforms vanilla GRPO. Our code and models are available athttps://github.com/T-Lab-CUHKSZ/G2RPO-A.</description><author>Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</author><pubDate>Mon, 18 Aug 2025 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13023v1</guid></item><item><title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</title><link>http://arxiv.org/abs/2505.14425v2</link><description>Instruction-tuned large language models (LLMs) have shown strong performanceon a variety of tasks; however, generalizing from synthetic to human-authoredinstructions in grounded environments remains a challenge for them. In thiswork, we study generalization challenges in spatial grounding tasks wheremodels interpret and translate instructions for building object arrangements ona $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluatetheir performance on a benchmark dataset containing both synthetic andhuman-written instructions. Our results reveal that while models generalizewell on simple tasks, their performance degrades significantly on more complextasks. We present a detailed error analysis of the gaps in instructiongeneralization.</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14425v2</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title><link>http://arxiv.org/abs/2507.22752v2</link><description>We introduce CUS-QA, a benchmark for open-ended regional question answeringthat encompasses both textual and visual modalities. We also provide strongbaselines using state-of-the-art large language models (LLMs). Our datasetconsists of manually curated questions and answers grounded in Wikipedia,created by native speakers from Czechia, Slovakia, and Ukraine, withaccompanying English translations. It includes both purely textual questionsand those requiring visual understanding. We evaluate state-of-the-art LLMsthrough prompting and complement this with human judgments of answercorrectness. Using these human evaluations, we analyze the reliability ofexisting automatic evaluation metrics. Our baseline results show that even thebest open-weight LLMs achieve only around 50% accuracy on textual questions andbelow 30% on visual questions. LLM-based evaluation metrics show strongcorrelation with human judgment, while traditional string-overlap metricsperform surprisingly well due to the prevalence of named entities in answers.</description><author>Jind≈ôich Libovick√Ω, Jind≈ôich Helcl, Andrei Manea, Gianluca Vico</author><pubDate>Thu, 21 Aug 2025 12:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22752v2</guid></item></channel></rss>