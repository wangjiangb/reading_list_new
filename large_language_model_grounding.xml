<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 30 Sep 2025 01:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Controlling Multimodal LLMs via Reward-guided Decoding</title><link>http://arxiv.org/abs/2508.11616v1</link><description>As Multimodal Large Language Models (MLLMs) gain widespread applicability, itis becoming increasingly desirable to adapt them for diverse user needs. Inthis paper, we study the adaptation of MLLMs through controlled decoding. Toachieve this, we introduce the first method for reward-guided decoding of MLLMsand demonstrate its application in improving their visual grounding. Our methodinvolves building reward models for visual grounding and using them to guidethe MLLM's decoding process. Concretely, we build two separate reward models toindependently control the degree of object precision and recall in the model'soutput. Our approach enables on-the-fly controllability of an MLLM's inferenceprocess in two ways: first, by giving control over the relative importance ofeach reward function during decoding, allowing a user to dynamically trade offobject precision for recall in image captioning tasks; second, by givingcontrol over the breadth of the search during decoding, allowing the user tocontrol the trade-off between the amount of test-time compute and the degree ofvisual grounding. We evaluate our method on standard object hallucinationbenchmarks, showing that it provides significant controllability over MLLMinference, while consistently outperforming existing hallucination mitigationmethods.</description><author>Oscar Mañas, Pierluca D'Oro, Koustuv Sinha, Adriana Romero-Soriano, Michal Drozdzal, Aishwarya Agrawal</author><pubDate>Fri, 15 Aug 2025 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11616v1</guid></item><item><title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title><link>http://arxiv.org/abs/2508.11538v1</link><description>Video reasoning segmentation (VRS) endeavors to delineate referred objects invideos guided by implicit instructions that encapsulate human intent andtemporal logic. Previous approaches leverage large vision language models(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.However, this paradigm suffers from limited interpretability during inferenceand suboptimal performance due to inadequate spatiotemporal reasoning. Drawinginspiration from seminal breakthroughs in reinforcement learning, we introduceVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning insegmentation. Veason-R1 is trained through Group Relative Policy Optimization(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, wecurate high-quality CoT training data to instill structured reasoningtrajectories, bridging video-level semantics and frame-level spatial grounding,yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPOfine-tuning encourages efficient exploration of the reasoning space byoptimizing reasoning chains. To this end, we incorporate a holistic rewardmechanism that synergistically enhances spatial alignment and temporalconsistency, bolstering keyframe localization and fine-grained grounding.Comprehensive empirical evaluations demonstrate that Veason-R1 achievesstate-of-the-art performance on multiple benchmarks, surpassing prior art bysignificant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),while exhibiting robustness to hallucinations (+8.8 R). Our code and modelweights will be available at Veason-R1.</description><author>Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</author><pubDate>Fri, 15 Aug 2025 15:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11538v1</guid></item><item><title>A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</title><link>http://arxiv.org/abs/2508.11529v1</link><description>Artificial intelligence is reshaping science and industry, yet many usersstill regard its models as opaque "black boxes". Conventional explainableartificial-intelligence methods clarify individual predictions but overlook theupstream decisions and downstream quality checks that determine whetherinsights can be trusted. In this work, we present Holistic ExplainableArtificial Intelligence (HXAI), a user-centric framework that embedsexplanation into every stage of the data-analysis workflow and tailors thoseexplanations to users. HXAI unifies six components (data, analysis set-up,learning process, model output, model quality, communication channel) into asingle taxonomy and aligns each component with the needs of domain experts,data analysts and data scientists. A 112-item question bank covers these needs;our survey of contemporary tools highlights critical coverage gaps. Grounded intheories of human explanation, principles from human-computer interaction andfindings from empirical user studies, HXAI identifies the characteristics thatmake explanations clear, actionable and cognitively manageable. A comprehensivetaxonomy operationalises these insights, reducing terminological ambiguity andenabling rigorous coverage analysis of existing toolchains. We furtherdemonstrate how AI agents that embed large-language models can orchestratediverse explanation techniques, translating technical artifacts intostakeholder-specific narratives that bridge the gap between AI developers anddomain experts. Departing from traditional surveys or perspective articles,this work melds concepts from multiple disciplines, lessons from real-worldprojects and a critical synthesis of the literature to advance a novel,end-to-end viewpoint on transparency, trustworthiness and responsible AIdeployment.</description><author>George Paterakis, Andrea Castellani, George Papoutsoglou, Tobias Rodemann, Ioannis Tsamardinos</author><pubDate>Fri, 15 Aug 2025 15:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11529v1</guid></item><item><title>UI-Venus Technical Report: Building High-performance UI Agents with RFT</title><link>http://arxiv.org/abs/2508.10833v2</link><description>We present UI-Venus, a native UI agent that takes only screenshots as inputbased on a multimodal large language model. UI-Venus achieves SOTA performanceon both UI grounding and navigation tasks using only several hundred thousandhigh-quality training samples through reinforcement finetune (RFT) based onQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,Screenspot-V2 / Pro, surpassing the previous SOTA baselines includingopen-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary andplaning ability, we also evaluate it on the AndroidWorld, an online UInavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%success rate, also beating existing models. To achieve this, we introducecarefully designed reward functions for both UI grounding and navigation tasksand corresponding efficient data cleaning strategies. To further boostnavigation performance, we propose Self-Evolving Trajectory History Alignment &amp;Sparse Action Enhancement that refine historical reasoning traces and balancesthe distribution of sparse but critical actions, leading to more coherentplanning and better generalization in complex UI tasks. Our contributionsinclude the publish of SOTA open-source UI agents, comprehensive data cleaningprotocols and a novel self-evolving framework for improving navigationperformance, which encourage further research and development in the community.Code is available at https://github.com/inclusionAI/UI-Venus.</description><author>Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</author><pubDate>Fri, 15 Aug 2025 14:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10833v2</guid></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2507.01006v5</link><description>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models(VLMs) designed to advance general-purpose multimodal understanding andreasoning. In this report, we share our key findings in the development of thereasoning-centric training framework. We first develop a capable visionfoundation model with significant potential through large-scale pre-training,which arguably sets the upper bound for the final performance. We then proposeReinforcement Learning with Curriculum Sampling (RLCS) to unlock the fullpotential of the model, leading to comprehensive capability enhancement acrossa diverse range of tasks, including STEM problem solving, video understanding,content recognition, coding, grounding, GUI-based agents, and long documentinterpretation. In a comprehensive evaluation across 42 public benchmarks,GLM-4.5V achieves state-of-the-art performance on nearly all tasks amongopen-source models of similar size, and demonstrates competitive or evensuperior results compared to closed-source models such as Gemini-2.5-Flash onchallenging tasks including Coding and GUI Agents. Meanwhile, the smallerGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results tothe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source bothGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information arereleased at https://github.com/zai-org/GLM-V.</description><author>GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan</author><pubDate>Fri, 15 Aug 2025 13:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01006v5</guid></item><item><title>MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</title><link>http://arxiv.org/abs/2508.11433v1</link><description>Multimodal Large Language Models (MLLMs) with unified architectures excelacross a wide range of vision-language tasks, yet aligning them withpersonalized image generation remains a significant challenge. Existing methodsfor MLLMs are frequently subject-specific, demanding a data-intensivefine-tuning process for every new subject, which limits their scalability. Inthis paper, we introduce MM-R1, a framework that integrates a cross-modalChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential ofunified MLLMs for personalized image generation. Specifically, we structurepersonalization as an integrated visual reasoning and generation process: (1)grounding subject concepts by interpreting and understanding user-providedimages and contextual cues, and (2) generating personalized images conditionedon both the extracted subject representations and user prompts. To furtherenhance the reasoning capability, we adopt Grouped Reward Proximal PolicyOptimization (GRPO) to explicitly align the generation. Experiments demonstratethat MM-R1 unleashes the personalization capability of unified MLLMs togenerate images with high subject fidelity and strong text alignment in azero-shot manner.</description><author>Qian Liang, Yujia Wu, Kuncheng Li, Jiwei Wei, Shiyuan He, Jinyu Guo, Ning Xie</author><pubDate>Fri, 15 Aug 2025 12:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11433v1</guid></item><item><title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title><link>http://arxiv.org/abs/2506.05982v4</link><description>As automated attack techniques rapidly advance, CAPTCHAs remain a criticaldefense mechanism against malicious bots. However, existing CAPTCHA schemesencompass a diverse range of modalities -- from static distorted text andobfuscated images to interactive clicks, sliding puzzles, and logic-basedquestions -- yet the community still lacks a unified, large-scale, multimodalbenchmark to rigorously evaluate their security robustness. To address thisgap, we introduce MCA-Bench, a comprehensive and reproducible benchmarkingsuite that integrates heterogeneous CAPTCHA types into a single evaluationprotocol. Leveraging a shared vision-language model backbone, we fine-tunespecialized cracking agents for each CAPTCHA category, enabling consistent,cross-modal assessments. Extensive experiments reveal that MCA-Bencheffectively maps the vulnerability spectrum of modern CAPTCHA designs undervaried attack settings, and crucially offers the first quantitative analysis ofhow challenge complexity, interaction depth, and model solvability interrelate.Based on these findings, we propose three actionable design principles andidentify key open challenges, laying the groundwork for systematic CAPTCHAhardening, fair benchmarking, and broader community collaboration. Datasets andcode are available online.</description><author>Zonglin Wu, Yule Xue, Yaoyao Feng, Xiaolong Wang, Yiren Song</author><pubDate>Fri, 15 Aug 2025 10:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05982v4</guid></item><item><title>SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models</title><link>http://arxiv.org/abs/2504.18684v2</link><description>Interpreting object-referential language and grounding objects in 3D withspatial relations and attributes is essential for robots operating alongsidehumans. However, this task is often challenging due to the diversity of scenes,large number of fine-grained objects, and complex free-form nature of languagereferences. Furthermore, in the 3D domain, obtaining large amounts of naturallanguage training data is difficult. Thus, it is important for methods to learnfrom little data and zero-shot generalize to new environments. To address thesechallenges, we propose SORT3D, an approach that utilizes rich object attributesfrom 2D data and merges a heuristics-based spatial reasoning toolbox with theability of large language models (LLMs) to perform sequential reasoning.Importantly, our method does not require text-to-3D data for training and canbe applied zero-shot to unseen environments. We show that SORT3D achievesstate-of-the-art zero-shot performance on complex view-dependent groundingtasks on two benchmarks. We also implement the pipeline to run real-time on twoautonomous vehicles and demonstrate that our approach can be used forobject-goal navigation on previously unseen real-world environments. All sourcecode for the system pipeline is publicly released athttps://github.com/nzantout/SORT3D.</description><author>Nader Zantout, Haochen Zhang, Pujith Kachana, Jinkai Qiu, Guofei Chen, Ji Zhang, Wenshan Wang</author><pubDate>Fri, 15 Aug 2025 00:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.18684v2</guid></item><item><title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title><link>http://arxiv.org/abs/2508.09922v2</link><description>Diffusion models have emerged as a leading framework for high-quality imagegeneration, offering stable training and strong performance across diversedomains. However, they remain computationally intensive, particularly duringthe iterative denoising process. Latent-space models like Stable Diffusionalleviate some of this cost by operating in compressed representations, thoughat the expense of fine-grained detail. More recent approaches such asRetrieval-Augmented Diffusion Models (RDM) address efficiency by conditioningdenoising on similar examples retrieved from large external memory banks. Whileeffective, these methods introduce drawbacks: they require costly storage andretrieval infrastructure, depend on static vision-language models like CLIP forsimilarity, and lack adaptability during training. We propose the PrototypeDiffusion Model (PDM), a method that integrates prototype learning directlyinto the diffusion process for efficient and adaptive visual conditioning -without external memory. Instead of retrieving reference samples, PDMconstructs a dynamic set of compact visual prototypes from clean image featuresusing contrastive learning. These prototypes guide the denoising steps byaligning noisy representations with semantically relevant visual patterns,enabling efficient generation with strong semantic grounding. Experiments showthat PDM maintains high generation quality while reducing computational andstorage overhead, offering a scalable alternative to retrieval-basedconditioning in diffusion models.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah</author><pubDate>Thu, 14 Aug 2025 21:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09922v2</guid></item><item><title>Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs</title><link>http://arxiv.org/abs/2508.11068v1</link><description>Abstract meaning representation (AMR) is a semantic formalism used torepresent the meaning of sentences as directed acyclic graphs. In this paper,we describe how real digital dictionaries can be embedded into AMR directedgraphs (digraphs), using state-of-the-art pre-trained large language models.Then, we reduce those graphs in a confluent manner, i.e. with transformationsthat preserve their circuit space. Finally, the properties of these reducesdigraphs are analyzed and discussed in relation to the symbol groundingproblem.</description><author>Nicolas Goulet, Alexandre Blondin Massé, Moussa Abdendi</author><pubDate>Thu, 14 Aug 2025 20:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11068v1</guid></item><item><title>Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?</title><link>http://arxiv.org/abs/2508.11011v1</link><description>Construction safety inspections typically involve a human inspectoridentifying safety concerns on-site. With the rise of powerful Vision LanguageModels (VLMs), researchers are exploring their use for tasks such as detectingsafety rule violations from on-site images. However, there is a lack of opendatasets to comprehensively evaluate and further fine-tune VLMs in constructionsafety inspection. Current applications of VLMs use small, supervised datasets,limiting their applicability in tasks they are not directly trained for. Inthis paper, we propose the ConstructionSite 10k, featuring 10,000 constructionsite images with annotations for three inter-connected tasks, including imagecaptioning, safety rule violation visual question answering (VQA), andconstruction element visual grounding. Our subsequent evaluation of currentstate-of-the-art large pre-trained VLMs shows notable generalization abilitiesin zero-shot and few-shot settings, while additional training is needed to makethem applicable to actual construction sites. This dataset allows researchersto train and evaluate their own VLMs with new architectures and techniques,providing a valuable benchmark for construction safety inspection.</description><author>Xuezheng Chen, Zhengbo Zou</author><pubDate>Thu, 14 Aug 2025 18:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11011v1</guid></item><item><title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title><link>http://arxiv.org/abs/2508.11009v1</link><description>The rapid proliferation of large language models (LLMs) in applicationstargeting children and adolescents necessitates a fundamental reassessment ofprevailing AI safety frameworks, which are largely tailored to adult users andneglect the distinct developmental vulnerabilities of minors. This paperhighlights key deficiencies in existing LLM safety benchmarks, including theirinadequate coverage of age-specific cognitive, emotional, and social risksspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence(13--18). To bridge these gaps, we introduce SproutBench, an innovativeevaluation suite comprising 1,283 developmentally grounded adversarial promptsdesigned to probe risks such as emotional dependency, privacy violations, andimitation of hazardous behaviors. Through rigorous empirical evaluation of 47diverse LLMs, we uncover substantial safety vulnerabilities, corroborated byrobust inter-dimensional correlations (e.g., between Safety and RiskPrevention) and a notable inverse relationship between Interactivity and AgeAppropriateness. These insights yield practical guidelines for advancingchild-centric AI design and deployment.</description><author>Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han</author><pubDate>Thu, 14 Aug 2025 18:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11009v1</guid></item><item><title>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</title><link>http://arxiv.org/abs/2508.13023v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhancedthe reasoning abilities of large language models (LLMs). Its success, however,largely depends on strong base models with rich world knowledge, yielding onlymodest improvements for small-size language models (SLMs). To address thislimitation, we investigate Guided GRPO, which injects ground-truth reasoningsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.Through a comprehensive study of various guidance configurations, we find thatnaively adding guidance delivers limited gains. These insights motivateG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strengthin response to the model's evolving training dynamics. Experiments onmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-Asubstantially outperforms vanilla GRPO. Our code and models are available athttps://github.com/T-Lab-CUHKSZ/G2RPO-A.</description><author>Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</author><pubDate>Mon, 18 Aug 2025 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13023v1</guid></item><item><title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</title><link>http://arxiv.org/abs/2505.14425v2</link><description>Instruction-tuned large language models (LLMs) have shown strong performanceon a variety of tasks; however, generalizing from synthetic to human-authoredinstructions in grounded environments remains a challenge for them. In thiswork, we study generalization challenges in spatial grounding tasks wheremodels interpret and translate instructions for building object arrangements ona $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluatetheir performance on a benchmark dataset containing both synthetic andhuman-written instructions. Our results reveal that while models generalizewell on simple tasks, their performance degrades significantly on more complextasks. We present a detailed error analysis of the gaps in instructiongeneralization.</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14425v2</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title><link>http://arxiv.org/abs/2507.22752v2</link><description>We introduce CUS-QA, a benchmark for open-ended regional question answeringthat encompasses both textual and visual modalities. We also provide strongbaselines using state-of-the-art large language models (LLMs). Our datasetconsists of manually curated questions and answers grounded in Wikipedia,created by native speakers from Czechia, Slovakia, and Ukraine, withaccompanying English translations. It includes both purely textual questionsand those requiring visual understanding. We evaluate state-of-the-art LLMsthrough prompting and complement this with human judgments of answercorrectness. Using these human evaluations, we analyze the reliability ofexisting automatic evaluation metrics. Our baseline results show that even thebest open-weight LLMs achieve only around 50% accuracy on textual questions andbelow 30% on visual questions. LLM-based evaluation metrics show strongcorrelation with human judgment, while traditional string-overlap metricsperform surprisingly well due to the prevalence of named entities in answers.</description><author>Jindřich Libovický, Jindřich Helcl, Andrei Manea, Gianluca Vico</author><pubDate>Thu, 21 Aug 2025 12:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22752v2</guid></item><item><title>Referring Expression Instance Retrieval and A Strong End-to-End Baseline</title><link>http://arxiv.org/abs/2506.18246v4</link><description>Using natural language to query visual information is a fundamental need inreal-world applications. Text-Image Retrieval (TIR) retrieves a target imagefrom a gallery based on an image-level description, while Referring ExpressionComprehension (REC) localizes a target object within a given image using aninstance-level description. However, real-world applications often present morecomplex demands. Users typically query an instance-level description across alarge gallery and expect to receive both relevant image and the correspondinginstance location. In such scenarios, TIR struggles with fine-graineddescriptions and object-level localization, while REC is limited in its abilityto efficiently search large galleries and lacks an effective ranking mechanism.In this paper, we introduce a new task called \textbf{Referring ExpressionInstance Retrieval (REIR)}, which supports both instance-level retrieval andlocalization based on fine-grained referring expressions. First, we propose alarge-scale benchmark for REIR, named REIRCOCO, constructed by promptingadvanced vision-language models to generate high-quality referring expressionsfor instances in the MSCOCO and RefCOCO datasets. Second, we present a baselinemethod, Contrastive Language-Instance Alignment with Relation Experts (CLARE),which employs a dual-stream architecture to address REIR in an end-to-endmanner. Given a referring expression, the textual branch encodes it into aquery embedding. The visual branch detects candidate objects and extracts theirinstance-level visual features. The most similar candidate to the query isselected for bounding box prediction. CLARE is first trained on objectdetection and REC datasets to establish initial grounding capabilities, thenoptimized via Contrastive Language-Instance Alignment (CLIA) for improvedretrieval across images. We will release our code and benchmark publicly.</description><author>Xiangzhao Hao, Kuan Zhu, Hongyu Guo, Haiyun Guo, Ning Jiang, Quan Lu, Ming Tang, Jinqiao Wang</author><pubDate>Thu, 21 Aug 2025 09:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.18246v4</guid></item><item><title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title><link>http://arxiv.org/abs/2506.06382v5</link><description>This paper establishes a fundamental impossibility theorem: no LLM capable ofperforming non-trivial knowledge aggregation can simultaneously achievetruthful knowledge representation, semantic information conservation, completerevelation of relevant knowledge, and knowledge-constrained optimality. Theimpossibility is not an engineering limitation but arises from the mathematicalstructure of information aggregation itself. We establish this result by describing the inference process as an auction ofideas, where distributed components compete exploiting their partial knowledgeto shape responses. The proof spans three independent mathematical domains:mechanism design theory (Green-Laffont), the theory of proper scoring rules(Savage), and direct architectural analysis of transformers (Log-Sum-Expconvexity). In particular, we show how to quantify the creation ofoverconfident or intuitive responses-the signature of both hallucination andcreativity, or imagination. To support this analysis, we introduce the complementary concepts of thesemantic information measure and the emergence operator to model boundedreasoning in a general setting. We prove that while bounded reasoning generatesaccessible information, providing valuable insights and inspirations, theidealized unconstrained reasoning strictly preserves semantic content. By demonstrating that hallucination and imagination are mathematicallyidentical phenomena-grounded in departures from truthfulness, semanticinformation conservation, revelation of relevant knowledge, andknowledge-constrained optimality-we offer a principled foundation for managingthese behaviors in advanced AI systems. Finally, we present some speculativeideas to inspire evaluation and refinements of the proposed theory.</description><author>Michał P. Karpowicz</author><pubDate>Thu, 21 Aug 2025 08:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06382v5</guid></item><item><title>Coarse-to-Fine Grounded Memory for LLM Agent Planning</title><link>http://arxiv.org/abs/2508.15305v1</link><description>Recent advancements in Large Language Models (LLMs) have driven growinginterest in LLM-based agents for complex planning tasks. To avoid costly agenttraining, many studies adopted memory mechanism that enhances LLM with offlineexperiences or online trajectory analysis. However, existing works focus onsingle-granularity memory derived from dynamic environmental interactions,which are inherently constrained by the quality of the collected experiences.This limitation, in turn, constrain the diversity of knowledge and theflexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), anovel framework that grounds coarse-to-fine memories with LLM, thereby fullyleverage them for flexible adaptation to diverse scenarios. \Ours{} groundsenvironmental information into coarse-grained focus points to guide experiencecollection in training tasks, followed by grounding of actionablehybrid-grained tips from each experience. At inference, \Ours{} retrievestask-relevant experiences and tips to support planning. When facingenvironmental anomalies, the LLM grounds the current situation intofine-grained key information, enabling flexible self-QA reflection and plancorrection.</description><author>Wei Yang, Jinwei Xiao, Hongming Zhang, Qingyang Zhang, Yanna Wang, Bo Xu</author><pubDate>Thu, 21 Aug 2025 06:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15305v1</guid></item><item><title>Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models</title><link>http://arxiv.org/abs/2410.03290v2</link><description>Video Large Language Models (Video-LLMs) have demonstrated remarkablecapabilities in coarse-grained video understanding, however, they struggle withfine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,a novel Video-LLM adept at perceiving and reasoning over specific video momentsin a fine-grained manner. We identify that current Video-LLMs have limitationsfor fine-grained video understanding since they lack effective temporalmodeling and timestamp representation. In light of this, we sharpen our modelby incorporating (1) an additional temporal stream to encode the relationshipsbetween frames and (2) discrete temporal tokens enriched with specific timeknowledge to represent timestamps. To optimize the training ofGrounded-VideoLLM, we employ a multi-stage training scheme, beginning withsimple video-captioning tasks and progressively introducing video temporalgrounding tasks of increasing complexity. To further enhanceGrounded-VideoLLM's temporal reasoning capability, we also curate a groundedVideoQA dataset by an automatic annotation pipeline. Extensive experimentsdemonstrate that Grounded-VideoLLM not only excels in fine-grained groundingtasks such as temporal sentence grounding, dense video captioning, and groundedVideoQA, but also shows great potential as a versatile video assistant forgeneral video understanding.</description><author>Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, Lifu Huang</author><pubDate>Thu, 21 Aug 2025 05:15:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03290v2</guid></item><item><title>WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai</title><link>http://arxiv.org/abs/2508.15239v1</link><description>Large language models excel at instruction-following in English, but theirperformance in low-resource languages like Thai remains underexplored. Existingbenchmarks often rely on translations, missing cultural and domain-specificnuances needed for real-world use. We present WangchanThaiInstruct, ahuman-authored Thai dataset for evaluation and instruction tuning, coveringfour professional domains and seven task types. Created through a multi-stagequality control process with annotators, domain experts, and AI researchers,WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showingperformance gaps on culturally and professionally specific tasks, and (2) aninstruction tuning study with ablations isolating the effect of nativesupervision. Models fine-tuned on WangchanThaiInstruct outperform those usingtranslated data in both in-domain and out-of-domain benchmarks. These findingsunderscore the need for culturally and professionally grounded instruction datato improve LLM alignment in low-resource, linguistically diverse settings.</description><author>Peerat Limkonchotiwat, Pume Tuchinda, Lalita Lowphansirikul, Surapon Nonesung, Panuthep Tasawong, Alham Fikri Aji, Can Udomcharoenchaikit, Sarana Nutanong</author><pubDate>Thu, 21 Aug 2025 04:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15239v1</guid></item><item><title>AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</title><link>http://arxiv.org/abs/2508.15232v1</link><description>Aerial Vision-and-Language Navigation (VLN) is an emerging task that enablesUnmanned Aerial Vehicles (UAVs) to navigate outdoor environments using naturallanguage instructions and visual cues. However, due to the extendedtrajectories and complex maneuverability of UAVs, achieving reliable UAV-VLNperformance is challenging and often requires human intervention or overlydetailed instructions. To harness the advantages of UAVs' high mobility, whichcould provide multi-grained perspectives, while maintaining a manageable motionspace for learning, we introduce a novel task called Dual-Altitude UAVCollaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinctaltitudes: a high-altitude UAV responsible for broad environmental reasoning,and a low-altitude UAV tasked with precise navigation. To support the trainingand evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising13,838 collaborative high-low UAV demonstration trajectories, each paired withtarget-oriented language instructions. This dataset includes both unseen mapsand an unseen object validation set to systematically evaluate the model'sgeneralization capabilities across novel environments and unfamiliar targets.To consolidate their complementary strengths, we propose a dual-UAVcollaborative VLN framework, AeroDuo, where the high-altitude UAV integrates amultimodal large language model (Pilot-LLM) for target reasoning, while thelow-altitude UAV employs a lightweight multi-stage policy for navigation andtarget grounding. The two UAVs work collaboratively and only exchange minimalcoordinate information to ensure efficiency.</description><author>Ruipu Wu, Yige Zhang, Jinyu Chen, Linjiang Huang, Shifeng Zhang, Xu Zhou, Liang Wang, Si Liu</author><pubDate>Thu, 21 Aug 2025 04:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15232v1</guid></item><item><title>R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling</title><link>http://arxiv.org/abs/2508.15204v1</link><description>Effective scheduling under tight resource, timing, and operationalconstraints underpins large-scale planning across sectors such as capitalprojects, manufacturing, logistics, and IT fleet transitions. However, thereliability of large language models (LLMs) when reasoning underhigh-constraint regimes is insufficiently characterized. To address this gap,we present R-ConstraintBench, a scalable framework that evaluates models onResource-Constrained Project Scheduling Problems (RCPSP), an NP-Completefeasibility class, while difficulty increases via linear growth in constraints.R-ConstraintBench incrementally increases non-redundant precedence constraintsin Directed Acyclic Graphs (DAGs) and then introduces downtime, temporalwindows, and disjunctive constraints. As an illustrative example, weinstantiate the benchmark in a data center migration setting and evaluatemultiple LLMs using feasibility and error analysis, identifying degradationthresholds and constraint types most associated with failure. Empirically,strong models are near-ceiling on precedence-only DAGs, but feasibilityperformance collapses when downtime, temporal windows, and disjunctiveconstraints interact, implicating constraint interaction, not graph depth, asthe principal bottleneck. Performance on clean synthetic ramps also does notguarantee transfer to domain-grounded scenarios, underscoring limitedgeneralization.</description><author>Raj Jain, Marc Wetter</author><pubDate>Thu, 21 Aug 2025 03:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15204v1</guid></item><item><title>ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following</title><link>http://arxiv.org/abs/2508.15164v1</link><description>Despite significant advancements in Large Language Models (LLMs) and LargeVision-Language Models (LVLMs), current models still face substantialchallenges in handling complex, multi-turn, and visually-grounded tasks thatdemand deep reasoning, sustained contextual understanding, entity tracking, andmulti-step instruction following. Existing benchmarks often fall short incapturing the dynamism and intricacies of real-world multi-modal interactions,leading to issues such as context loss and visual hallucinations. To addressthese limitations, we introduce MMDR-Bench (Multi-Modal Dialogue ReasoningBenchmark), a novel dataset comprising 300 meticulously designed complexmulti-turn dialogue scenarios, each averaging 5-7 turns and evaluated acrosssix core dimensions including visual entity tracking and reasoning depth.Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holisticframework that enhances existing LVLMs with advanced reasoning and instructionfollowing capabilities through an iterative"memory-perception-planning-execution" cycle, requiring no extensivere-training of the underlying models. Our extensive experiments on MMDR-Benchdemonstrate that CoLVLM Agent consistently achieves superior performance,attaining an average human evaluation score of 4.03, notably surpassingstate-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro(3.85). The framework exhibits significant advantages in reasoning depth,instruction adherence, and error suppression, and maintains robust performanceover extended dialogue turns, validating the effectiveness of its modulardesign and iterative approach for complex multi-modal interactions.</description><author>Seungmin Han, Haeun Kwon, Ji-jun Park, Taeyang Yoon</author><pubDate>Thu, 21 Aug 2025 02:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15164v1</guid></item><item><title>KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis</title><link>http://arxiv.org/abs/2507.03847v2</link><description>Large Language Models (LLMs) frequently generate hallucinations: statementsthat are syntactically plausible but lack factual grounding. This researchpresents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework thatdetects and explains such hallucinations by comparing knowledge graphsconstructed from LLM outputs with ground truth data from Wikidata or contextualdocuments. Using graph kernels and semantic clustering, the method providesexplanations for detected hallucinations, ensuring both robustness andinterpretability. Our framework achieves competitive accuracy in detectinghallucinations across both open- and closed-domain tasks, and is able togenerate contrastive explanations, enhancing transparency. This researchadvances the reliability of LLMs in high-stakes domains and provides afoundation for future work on precision improvements and multi-source knowledgeintegration.</description><author>Reilly Haskins, Benjamin Adams</author><pubDate>Thu, 21 Aug 2025 01:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.03847v2</guid></item><item><title>aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</title><link>http://arxiv.org/abs/2508.15126v1</link><description>Recent advances in large language models (LLMs) have enabled AI agents toautonomously generate scientific proposals, conduct experiments, author papers,and perform peer reviews. Yet this flood of AI-generated research contentcollides with a fragmented and largely closed publication ecosystem.Traditional journals and conferences rely on human peer review, making themdifficult to scale and often reluctant to accept AI-generated research content;existing preprint servers (e.g. arXiv) lack rigorous quality-controlmechanisms. Consequently, a significant amount of high-quality AI-generatedresearch lacks appropriate venues for dissemination, hindering its potential toadvance scientific progress. To address these challenges, we introduce aiXiv, anext-generation open-access platform for human and AI scientists. Itsmulti-agent architecture allows research proposals and papers to be submitted,reviewed, and iteratively refined by both human and AI scientists. It alsoprovides API and MCP interfaces that enable seamless integration ofheterogeneous human and AI scientists, creating a scalable and extensibleecosystem for autonomous scientific discovery. Through extensive experiments,we demonstrate that aiXiv is a reliable and robust platform that significantlyenhances the quality of AI-generated research proposals and papers afteriterative revising and reviewing on aiXiv. Our work lays the groundwork for anext-generation open-access ecosystem for AI scientists, accelerating thepublication and dissemination of high-quality AI-generated research content.Code is available at https://github.com/aixiv-org. Website is available athttps://forms.gle/DxQgCtXFsJ4paMtn8.</description><author>Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang, Xiuxu Li, Jiaxing Song, Jiabin Luo, Yijiang Li, Shuo Yin, Chengxiao Dai, Eric Hanchen Jiang, Xiaoyan Zhou, Zhenfei Yin, Boqin Yuan, Jing Dong, Guinan Su, Guanren Qiao, Haiming Tang, Anghong Du, Lili Pan, Zhenzhong Lan, Xinyu Liu</author><pubDate>Wed, 20 Aug 2025 23:16:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15126v1</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</title><link>http://arxiv.org/abs/2508.14735v1</link><description>Large language models (LLMs) are increasingly applied in multilingualcontexts, yet their capacity for consistent, logically grounded alignmentacross languages remains underexplored. We present a controlled evaluationframework for multilingual natural language inference (NLI) that generatessynthetic, logic-based premise-hypothesis pairs and translates them into atypologically diverse set of languages. This design enables precise controlover semantic relations and allows testing in both monolingual andmixed-language (code-switched) conditions. Surprisingly, code-switching doesnot degrade, and can even improve, performance, suggesting thattranslation-induced lexical variation may serve as a regularization signal. Wevalidate semantic preservation through embedding-based similarity analyses andcross-lingual alignment visualizations, confirming the fidelity of translatedpairs. Our findings expose both the potential and the brittleness of currentLLM cross-lingual reasoning, and identify code-switching as a promising leverfor improving multilingual robustness. Code available at:https://github.com/KurbanIntelligenceLab/nli-stress-testing</description><author>Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban</author><pubDate>Wed, 20 Aug 2025 14:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14735v1</guid></item><item><title>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</title><link>http://arxiv.org/abs/2508.14704v1</link><description>The Model Context Protocol has emerged as a transformative standard forconnecting large language models to external data sources and tools, rapidlygaining adoption across major AI providers and development platforms. However,existing benchmarks are overly simplistic and fail to capture real applicationchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. Toaddress this critical gap, we introduce MCP-Universe, the first comprehensivebenchmark specifically designed to evaluate LLMs in realistic and hard tasksthrough interaction with real-world MCP servers. Our benchmark encompasses 6core domains spanning 11 different MCP servers: Location Navigation, RepositoryManagement, Financial Analysis, 3D Design, Browser Automation, and WebSearching. To ensure rigorous evaluation, we implement execution-basedevaluators, including format evaluators for agent format compliance, staticevaluators for time-invariant content matching, and dynamic evaluators thatautomatically retrieve real-time ground truth for temporally sensitive tasks.Through extensive evaluation of leading LLMs, we find that even SOTA modelssuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibitsignificant performance limitations. In addition, our benchmark poses asignificant long-context challenge for LLM agents, as the number of inputtokens increases rapidly with the number of interaction steps. Moreover, itintroduces an unknown-tools challenge, as LLM agents often lack familiaritywith the precise usage of the MCP servers. Notably, enterprise-level agentslike Cursor cannot achieve better performance than standard ReAct frameworks.Beyond evaluation, we open-source our extensible evaluation framework with UIsupport, enabling researchers and practitioners to seamlessly integrate newagents and MCP servers while fostering innovation in the rapidly evolving MCPecosystem.</description><author>Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li</author><pubDate>Wed, 20 Aug 2025 13:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14704v1</guid></item><item><title>Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination</title><link>http://arxiv.org/abs/2508.14635v1</link><description>The ability to coordinate actions across multiple agents is critical forsolving complex, real-world problems. Large Language Models (LLMs) have shownstrong capabilities in communication, planning, and reasoning, raising thequestion of whether they can also support effective collaboration inmulti-agent settings. In this work, we investigate the use of LLM agents tosolve a structured victim rescue task that requires division of labor,prioritization, and cooperative planning. Agents operate in a fully knowngraph-based environment and must allocate resources to victims with varyingneeds and urgency levels. We systematically evaluate their performance using asuite of coordination-sensitive metrics, including task success rate, redundantactions, room conflicts, and urgency-weighted efficiency. This study offers newinsights into the strengths and failure modes of LLMs in physically groundedmulti-agent collaboration tasks, contributing to future benchmarks andarchitectural improvements.</description><author>João Vitor de Carvalho Silva, Douglas G. Macharet</author><pubDate>Wed, 20 Aug 2025 11:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14635v1</guid></item><item><title>Understanding Data Influence with Differential Approximation</title><link>http://arxiv.org/abs/2508.14648v1</link><description>Data plays a pivotal role in the groundbreaking advancements in artificialintelligence. The quantitative analysis of data significantly contributes tomodel training, enhancing both the efficiency and quality of data utilization.However, existing data analysis tools often lag in accuracy. For instance, manyof these tools even assume that the loss function of neural networks is convex.These limitations make it challenging to implement current methods effectively.In this paper, we introduce a new formulation to approximate a sample'sinfluence by accumulating the differences in influence between consecutivelearning steps, which we term Diff-In. Specifically, we formulate thesample-wise influence as the cumulative sum of its changes/differences acrosssuccessive training iterations. By employing second-order approximations, weapproximate these difference terms with high accuracy while eliminating theneed for model convexity required by existing methods. Despite being asecond-order method, Diff-In maintains computational complexity comparable tothat of first-order methods and remains scalable. This efficiency is achievedby computing the product of the Hessian and gradient, which can be efficientlyapproximated using finite differences of first-order gradients. We assess theapproximation accuracy of Diff-In both theoretically and empirically. Ourtheoretical analysis demonstrates that Diff-In achieves significantly lowerapproximation error compared to existing influence estimators. Extensiveexperiments further confirm its superior performance across multiple benchmarkdatasets in three data-centric tasks: data cleaning, data deletion, and coresetselection. Notably, our experiments on data pruning for large-scalevision-language pre-training show that Diff-In can scale to millions of datapoints and outperforms strong baselines.</description><author>Haoru Tan, Sitong Wu, Xiuzhe Wu, Wang Wang, Bo Zhao, Zeke Xie, Gui-Song Xia, Xiaojuan Qi</author><pubDate>Wed, 20 Aug 2025 11:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14648v1</guid></item><item><title>STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples</title><link>http://arxiv.org/abs/2508.12096v2</link><description>Evaluating large language models (LLMs) has become increasingly challengingas model capabilities advance rapidly. While recent models often achieve higherscores on standard benchmarks, these improvements do not consistently reflectenhanced real-world reasoning capabilities. Moreover, widespread overfitting topublic benchmarks and the high computational cost of full evaluations have madeit both expensive and less effective to distinguish meaningful differencesbetween models. To address these challenges, we propose the \textbf{S}tructured\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweightand interpretable evaluation framework for efficiently estimating the relativecapabilities of LLMs. STEM identifies \textit{significant transition samples}(STS) by analyzing consistent performance transitions among LLMs of the samearchitecture but varying parameter scales. These samples enable STEM toeffectively estimate the capability position of an unknown model. Qwen3 modelfamily is applied to construct the STS pool on six diverse and representativebenchmarks. To assess generalizability. Experimental results indicate that STEMreliably captures performance trends, aligns with ground-truth rankings ofmodel capability. These findings highlight STEM as a practical and scalablemethod for fine-grained, architecture-agnostic evaluation of LLMs.</description><author>Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang</author><pubDate>Wed, 20 Aug 2025 09:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12096v2</guid></item><item><title>Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning</title><link>http://arxiv.org/abs/2507.03047v2</link><description>Recent advances have applied large language models (LLMs) to sequentialrecommendation, leveraging their pre-training knowledge and reasoningcapabilities to provide more personalized user experiences. However, existingLLM-based methods fail to sufficiently leverage the rich temporal informationinherent in users' historical interaction sequences, stemming from fundamentalarchitectural constraints: LLMs process information through self-attentionmechanisms that lack inherent sequence ordering and rely on position embeddingsdesigned primarily for natural language rather than user interaction sequences.This limitation significantly impairs their ability to capture the evolution ofuser preferences over time and predict future interests accurately. To address this critical gap, we propose \underline{C}ounterfactual\underline{E}nhanced \underline{T}emporal Framework for LLM-Based\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inferenceprinciples, which allow it to isolate and measure the specific impact oftemporal information on recommendation outcomes. Combined with ourcounterfactual tuning task derived from causal analysis, CETRec effectivelyenhances LLMs' awareness of both absolute order (how recently items wereinteracted with) and relative order (the sequential relationships betweenitems). Extensive experiments on real-world datasets demonstrate theeffectiveness of our CETRec. Our code is available athttps://anonymous.4open.science/r/CETRec-B9CE/.</description><author>Yutian Liu, Zhengyi Yang, Jiancan Wu, Xiang Wang</author><pubDate>Wed, 20 Aug 2025 09:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.03047v2</guid></item><item><title>Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)</title><link>http://arxiv.org/abs/2507.03608v2</link><description>Generative AI (GenAI) is expected to play a pivotal role in enablingautonomous optimization in future wireless networks. Within the ORANarchitecture, Large Language Models (LLMs) can be specialized to generate xAppsand rApps by leveraging specifications and API definitions from the RANIntelligent Controller (RIC) platform. However, fine-tuning base LLMs fortelecom-specific tasks remains expensive and resource-intensive.Retrieval-Augmented Generation (RAG) offers a practical alternative throughin-context learning, enabling domain adaptation without full retraining. Whiletraditional RAG systems rely on vector-based retrieval, emerging variants suchas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrievalstrategies to support multi-hop reasoning and improve factual grounding.Despite their promise, these methods lack systematic, metric-drivenevaluations, particularly in high-stakes domains such as ORAN. In this study,we conduct a comparative evaluation of Vector RAG, GraphRAG, and HybridGraphRAG using ORAN specifications. We assess performance across varyingquestion complexities using established generation metrics: faithfulness,answer relevance, context relevance, and factual correctness. Results show thatboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAGimproves factual correctness by 8%, while GraphRAG improves context relevanceby 11%.</description><author>Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi</author><pubDate>Wed, 20 Aug 2025 08:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.03608v2</guid></item><item><title>Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</title><link>http://arxiv.org/abs/2508.14527v1</link><description>The generation of safety-critical scenarios in simulation has becomeincreasingly crucial for safety evaluation in autonomous vehicles prior to roaddeployment in society. However, current approaches largely rely on predefinedthreat patterns or rule-based strategies, which limit their ability to exposediverse and unforeseen failure modes. To overcome these, we propose ScenGE, aframework that can generate plentiful safety-critical scenarios by reasoningnovel adversarial cases and then amplifying them with complex traffic flows.Given a simple prompt of a benign scene, it first performs Meta-ScenarioGeneration, where a large language model, grounded in structured drivingknowledge, infers an adversarial agent whose behavior poses a threat that isboth plausible and deliberately challenging. This meta-scenario is thenspecified in executable code for precise in-simulator control. Subsequently,Complex Scenario Evolution uses background vehicles to amplify the core threatintroduced by Meta-Scenario. It builds an adversarial collaborator graph toidentify key agent trajectories for optimization. These perturbations aredesigned to simultaneously reduce the ego vehicle's maneuvering space andcreate critical occlusions. Extensive experiments conducted on multiplereinforcement learning based AV models show that ScenGE uncovers more severecollision cases (+31.96%) on average than SoTA baselines. Additionally, ourScenGE can be applied to large model based AV systems and deployed on differentsimulators; we further observe that adversarial training on our scenariosimproves the model robustness. Finally, we validate our framework throughreal-world vehicle tests and human evaluation, confirming that the generatedscenarios are both plausible and critical. We hope our paper can build up acritical step towards building public trust and ensuring their safe deployment.</description><author>Jiangfan Liu, Yongkang Guo, Fangzhi Zhong, Tianyuan Zhang, Zonglei Jing, Siyuan Liang, Jiakai Wang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</author><pubDate>Wed, 20 Aug 2025 08:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14527v1</guid></item><item><title>DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</title><link>http://arxiv.org/abs/2508.14391v1</link><description>Relation extraction enables the construction of structured knowledge for manydownstream applications. While large language models (LLMs) have shown greatpromise in this domain, most existing methods concentrate on relationclassification, which predicts the semantic relation type between a relatedentity pair. However, we observe that LLMs often struggle to reliably determinewhether a relation exists, especially in cases involving complex sentencestructures or intricate semantics, which leads to spurious predictions. Suchhallucinations can introduce noisy edges in knowledge graphs, compromising theintegrity of structured knowledge and downstream reliability. To address thesechallenges, we propose DEPTH, a framework that integrates Dependency-awaresEntence simPlification and Two-tiered Hierarchical refinement into therelation extraction pipeline. Given a sentence and its candidate entity pairs,DEPTH operates in two stages: (1) the Grounding module extracts relations foreach pair by leveraging their shortest dependency path, distilling the sentenceinto a minimal yet coherent relational context that reduces syntactic noisewhile preserving key semantics; (2) the Refinement module aggregates all localpredictions and revises them based on a holistic understanding of the sentence,correcting omissions and inconsistencies. We further introduce acausality-driven reward model that mitigates reward hacking by disentanglingspurious correlations, enabling robust fine-tuning via reinforcement learningwith human feedback. Experiments on six benchmarks demonstrate that DEPTHreduces the average hallucination rate to 7.0\% while achieving a 17.2\%improvement in average F1 score over state-of-the-art baselines.</description><author>Yupei Yang, Fan Feng, Lin Yang, Wanxi Deng, Lin Qu, Biwei Huang, Shikui Tu, Lei Xu</author><pubDate>Wed, 20 Aug 2025 03:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14391v1</guid></item><item><title>Is Small Language Model the Silver Bullet to Low-Resource Languages Machine Translation?</title><link>http://arxiv.org/abs/2503.24102v3</link><description>Low-resource languages (LRLs) lack sufficient linguistic resources and areunderrepresented in benchmark datasets, resulting in persistently lowertranslation quality than high-resource languages, especially inprivacy-sensitive and resource-limited contexts. Firstly, this studysystematically evaluates state-of-the-art smaller Large Language Models in 200languages using the FLORES-200 benchmark, highlighting persistent deficienciesand disparities in the translation of LRLs. To mitigate these limitations, weinvestigate knowledge distillation from large pre-trained teacher models toSmall Language Models (SLMs) through supervised fine-tuning. The results showsubstantial improvements; for example, the translation performance of Englishto Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increasesfrom 0.36 to 0.89 in the validation set for Llama-3.2-3B. We furtherinvestigate various fine-tuning configurations and tasks to clarify thetrade-offs between data scale and training efficiency, verify that the modelretains its general capabilities without significant catastrophic forgettingafter training, and explore the distillation benefits to other LRLs on SLMs(Khasi, Assamese, and Ukrainian). In general, this work exposes the limitationsand fairness issues of current SLMs in LRL translation and systematicallyexplores the potential of using the distillation of knowledge from large tosmall models, offering practical, empirically grounded recommendations toimprove LRL translation systems</description><author>Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, Tegawendé F. Bissyandé, Jacques Klein</author><pubDate>Fri, 22 Aug 2025 15:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.24102v3</guid></item><item><title>Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish</title><link>http://arxiv.org/abs/2508.16431v1</link><description>We introduce Cetvel, a comprehensive benchmark designed to evaluate largelanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lackeither task diversity or culturally relevant content, or both. Cetvel addressesthese gaps by combining a broad range of both discriminative and generativetasks ensuring content that reflects the linguistic and cultural richness ofTurkish language. Cetvel covers 23 tasks grouped into seven categories,including tasks such as grammatical error correction, machine translation, andquestion answering rooted in Turkish history and idiomatic language. Weevaluate 33 open-weight LLMs (up to 70B parameters) covering different modelfamilies and instruction paradigms. Our experiments reveal that Turkish-centricinstruction-tuned models generally underperform relative to multilingual orgeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored forthe language. Moreover, we show that tasks such as grammatical error correctionand extractive question answering are particularly discriminative indifferentiating model capabilities. Cetvel offers a comprehensive andculturally grounded evaluation suite for advancing the development andassessment of LLMs in Turkish.</description><author>Yakup Abrek Er, Ilker Kesen, Gözde Gül Şahin, Aykut Erdem</author><pubDate>Fri, 22 Aug 2025 14:42:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16431v1</guid></item><item><title>Detecting and Characterizing Planning in Language Models</title><link>http://arxiv.org/abs/2508.18098v1</link><description>Modern large language models (LLMs) have demonstrated impressive performanceacross a wide range of multi-step reasoning tasks. Recent work suggests thatLLMs may perform planning - selecting a future target token in advance andgenerating intermediate tokens that lead towards it - rather than merelyimprovising one token at a time. However, existing studies assume fixedplanning horizons and often focus on single prompts or narrow domains. Todistinguish planning from improvisation across models and tasks, we presentformal and causally grounded criteria for detecting planning and operationalizethem as a semi-automated annotation pipeline. We apply this pipeline to bothbase and instruction-tuned Gemma-2-2B models on the MBPP code generationbenchmark and a poem generation task where Claude 3.5 Haiku was previouslyshown to plan. Our findings show that planning is not universal: unlike Haiku,Gemma-2-2B solves the same poem generation task through improvisation, and onMBPP it switches between planning and improvisation across similar tasks andeven successive token predictions. We further show that instruction tuningrefines existing planning behaviors in the base model rather than creating themfrom scratch. Together, these studies provide a reproducible and scalablefoundation for mechanistic studies of planning in LLMs.</description><author>Jatin Nainani, Sankaran Vaidyanathan, Connor Watts, Andre N. Assis, Alice Rigg</author><pubDate>Mon, 25 Aug 2025 14:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18098v1</guid></item><item><title>Neither Valid nor Reliable? Investigating the Use of LLMs as Judges</title><link>http://arxiv.org/abs/2508.18076v1</link><description>Evaluating natural language generation (NLG) systems remains a core challengeof natural language processing (NLP), further complicated by the rise of largelanguage models (LLMs) that aims to be general-purpose. Recently, largelanguage models as judges (LLJs) have emerged as a promising alternative totraditional metrics, but their validity remains underexplored. This positionpaper argues that the current enthusiasm around LLJs may be premature, as theiradoption has outpaced rigorous scrutiny of their reliability and validity asevaluators. Drawing on measurement theory from the social sciences, we identifyand critically assess four core assumptions underlying the use of LLJs: theirability to act as proxies for human judgment, their capabilities as evaluators,their scalability, and their cost-effectiveness. We examine how each of theseassumptions may be challenged by the inherent limitations of LLMs, LLJs, orcurrent practices in NLG evaluation. To ground our analysis, we explore threeapplications of LLJs: text summarization, data annotation, and safetyalignment. Finally, we highlight the need for more responsible evaluationpractices in LLJs evaluation, to ensure that their growing role in the fieldsupports, rather than undermines, progress in NLG.</description><author>Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, Golnoosh Farnadi</author><pubDate>Mon, 25 Aug 2025 14:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18076v1</guid></item><item><title>A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</title><link>http://arxiv.org/abs/2508.17969v1</link><description>This paper introduces a holistic perception system for internal and externalmonitoring of autonomous vehicles, with the aim of demonstrating a novelAI-leveraged self-adaptive framework of advanced vehicle technologies andsolutions that optimize perception and experience on-board. Internal monitoringsystem relies on a multi-camera setup designed for predicting and identifyingdriver and occupant behavior through facial recognition, exploiting in additiona large language model as virtual assistant. Moreover, the in-cabin monitoringsystem includes AI-empowered smart sensors that measure air-quality and performthermal comfort analysis for efficient on and off-boarding. On the other hand,external monitoring system perceives the surrounding environment of vehicle,through a LiDAR-based cost-efficient semantic segmentation approach, thatperforms highly accurate and efficient super-resolution on low-quality raw 3Dpoint clouds. The holistic perception framework is developed in the context ofEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed ona real electric vehicle provided by ALKE. Experimental validation andevaluation at the integration site of Joint Research Centre at Ispra, Italy,highlights increased performance and efficiency of the modular blocks of theproposed perception architecture.</description><author>Alexandros Gkillas, Christos Anagnostopoulos, Nikos Piperigkos, Dimitris Tsiktsiris, Theofilos Christodoulou, Theofanis Siamatras, Dimitrios Triantafyllou, Christos Basdekis, Theoktisti Marinopoulou, Panagiotis Lepentsiotis, Elefterios Blitsis, Aggeliki Zacharaki, Nearchos Stylianidis, Leonidas Katelaris, Lamberto Salvan, Aris S. Lalos, Christos Laoudias, Antonios Lalas, Konstantinos Votis</author><pubDate>Mon, 25 Aug 2025 12:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17969v1</guid></item><item><title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2505.24073v2</link><description>Large Vision-Language Models (LVLMs) have made remarkable strides inmultimodal tasks such as visual question answering, visual grounding, andcomplex reasoning. However, they remain limited by static training data,susceptibility to hallucinations, and inability to verify claims againstup-to-date, external evidence, compromising their performance in dynamicreal-world applications. Retrieval-Augmented Generation (RAG) offers apractical solution to mitigate these challenges by allowing the LVLMs to accesslarge-scale knowledge databases via retrieval mechanisms, thereby groundingmodel outputs in factual, contextually relevant information. Here in thispaper, we conduct the first systematic dissection of the multimodal RAGpipeline for LVLMs, explicitly investigating (1) the retrieval phase: on themodality configurations and retrieval strategies, (2) the re-ranking stage: onstrategies to mitigate positional biases and improve the relevance of retrievedevidence, and (3) the generation phase: we further investigate how to bestintegrate retrieved candidates into the final generation process. Finally, weextend to explore a unified agentic framework that integrates re-ranking andgeneration through self-reflection, enabling LVLMs to select relevant evidenceand suppress irrelevant context dynamically. Our full-stack exploration of RAGfor LVLMs yields substantial insights, resulting in an average performanceboost of 5% without any fine-tuning.</description><author>Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Suofei Feng, Ryan Rossi, Zhengzhong Tu</author><pubDate>Tue, 26 Aug 2025 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24073v2</guid></item><item><title>Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding</title><link>http://arxiv.org/abs/2508.19165v1</link><description>Monocular 3D visual grounding is a novel task that aims to locate 3D objectsin RGB images using text descriptions with explicit geometry information.Despite the inclusion of geometry details in the text, we observe that the textembeddings are sensitive to the magnitude of numerical values but largelyignore the associated measurement units. For example, simply equidistantmapping the length with unit "meter" to "decimeters" or "centimeters" leads tosevere performance degradation, even though the physical length remainsequivalent. This observation signifies the weak 3D comprehension of pre-trainedlanguage model, which generates misguiding text features to hinder 3Dperception. Therefore, we propose to enhance the 3D perception of model on textembeddings and geometry features with two simple and effective methods.Firstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE),which enhances the comprehension of mapping relationships between differentunits by augmenting the diversity of distance descriptors in text queries.Next, we propose a Text-Guided Geometry Enhancement (TGE) module to furtherenhance the 3D-text information by projecting the basic text features intogeometrically consistent space. These 3D-enhanced text features are thenleveraged to precisely guide the attention of geometry features. We evaluatethe proposed method through extensive comparisons and ablation studies on theMono3DRefer dataset. Experimental results demonstrate substantial improvementsover previous methods, achieving new state-of-the-art results with a notableaccuracy gain of 11.94\% in the "Far" scenario. Our code will be made publiclyavailable.</description><author>Yuzhen Li, Min Liu, Yuan Bian, Xueping Wang, Zhaoyang Li, Gen Li, Yaonan Wang</author><pubDate>Tue, 26 Aug 2025 16:13:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19165v1</guid></item><item><title>It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs</title><link>http://arxiv.org/abs/2508.19089v1</link><description>Extremely low-resource languages, especially those written in rare scripts,as shown in Figure 1, remain largely unsupported by large language models(LLMs). This is due in part to compounding factors such as the lack of trainingdata. This paper delivers the first comprehensive analysis of whether LLMs canacquire such languages purely via in-context learning (ICL), with or withoutauxiliary alignment signals, and how these methods compare toparameter-efficient fine-tuning (PEFT). We systematically evaluate 20under-represented languages across three state-of-the-art multilingual LLMs.Our findings highlight the limitation of PEFT when both language and its scriptare extremely under-represented by the LLM. In contrast, zero-shot ICL withlanguage alignment is impressively effective on extremely low-resourcelanguages, while few-shot ICL or PEFT is more beneficial for languagesrelatively better represented by LLMs. For LLM practitioners working onextremely low-resource languages, we summarise guidelines grounded by ourresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuninga multilingual model on languages of unseen scripts.</description><author>Yue Li, Zhixue Zhao, Carolina Scarton</author><pubDate>Tue, 26 Aug 2025 14:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19089v1</guid></item><item><title>Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI</title><link>http://arxiv.org/abs/2508.19008v1</link><description>This study examines the capacity of large language models (LLMs) to supportphenomenological qualitative analysis of first-person experience in BorderlinePersonality Disorder (BPD), understood as a disorder of temporality andselfhood. Building on a prior human-led thematic analysis of 24 inpatients'life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of theoriginal investigators. The models were evaluated with blinded and non-blindedexpert judges in phenomenology and clinical psychology. Assessments includedsemantic congruence, Jaccard coefficients, and multidimensional validityratings (credibility, coherence, substantiveness, and groundness in data).Results showed variable overlap with the human analysis, from 0 percent in GPTto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient(0.21-0.28). However, the models recovered themes omitted by humans. Gemini'soutput most closely resembled the human analysis, with validity scoressignificantly higher than GPT and Claude (p &lt; 0.0001), and was judged as humanby blinded experts. All scores strongly correlated (R &gt; 0.78) with the quantityof text and words per theme, highlighting both the variability and potential ofAI-augmented thematic analysis to mitigate human interpretative bias.</description><author>Marcin Moskalewicz, Anna Sterna, Marek Pokropski, Paula Flores</author><pubDate>Tue, 26 Aug 2025 13:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19008v1</guid></item><item><title>Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework</title><link>http://arxiv.org/abs/2508.18929v1</link><description>Retrieval-augmented generation (RAG) systems improve large language modeloutputs by incorporating external knowledge, enabling more informed andcontext-aware responses. However, the effectiveness and trustworthiness ofthese systems critically depends on how they are evaluated, particularly onwhether the evaluation process captures real-world constraints like protectingsensitive information. While current evaluation efforts for RAG systems haveprimarily focused on the development of performance metrics, far less attentionhas been given to the design and quality of the underlying evaluation datasets,despite their pivotal role in enabling meaningful, reliable assessments. Inthis work, we introduce a novel multi-agent framework for generating syntheticQA datasets for RAG evaluation that prioritize semantic diversity and privacypreservation. Our approach involves: (1) a Diversity agent leveragingclustering techniques to maximize topical coverage and semantic variability,(2) a Privacy Agent that detects and mask sensitive information across multipledomains and (3) a QA curation agent that synthesizes private and diverse QApairs suitable as ground truth for RAG evaluation. Extensive experimentsdemonstrate that our evaluation sets outperform baseline methods in diversityand achieve robust privacy masking on domain-specific datasets. This workoffers a practical and ethically aligned pathway toward safer, morecomprehensive RAG system evaluation, laying the foundation for futureenhancements aligned with evolving AI regulations and compliance standards.</description><author>Ilias Driouich, Hongliu Cao, Eoin Thomas</author><pubDate>Tue, 26 Aug 2025 11:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18929v1</guid></item><item><title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title><link>http://arxiv.org/abs/2508.09922v4</link><description>Diffusion models have emerged as a leading framework for high-quality imagegeneration, offering stable training and strong performance across diversedomains. However, they remain computationally intensive, particularly duringthe iterative denoising process. Latent-space models like Stable Diffusionalleviate some of this cost by operating in compressed representations, thoughat the expense of fine-grained detail. More recent approaches such asRetrieval-Augmented Diffusion Models (RDM) address efficiency by conditioningdenoising on similar examples retrieved from large external memory banks. Whileeffective, these methods introduce drawbacks: they require costly storage andretrieval infrastructure, depend on static vision-language models like CLIP forsimilarity, and lack adaptability during training. We propose the PrototypeDiffusion Model (PDM), a method that integrates prototype learning directlyinto the diffusion process for efficient and adaptive visual conditioning -without external memory. Instead of retrieving reference samples, PDMconstructs a dynamic set of compact visual prototypes from clean image featuresusing contrastive learning. These prototypes guide the denoising steps byaligning noisy representations with semantically relevant visual patterns,enabling efficient generation with strong semantic grounding. Experiments showthat PDM maintains high generation quality while reducing computational andstorage overhead, offering a scalable alternative to retrieval-basedconditioning in diffusion models.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah</author><pubDate>Tue, 26 Aug 2025 10:29:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09922v4</guid></item><item><title>Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks</title><link>http://arxiv.org/abs/2508.18905v1</link><description>Standard single-turn, static benchmarks fall short in evaluating the nuancedcapabilities of Large Language Models (LLMs) on complex tasks such as softwareengineering. In this work, we propose a novel interactive evaluation frameworkthat assesses LLMs on multi-requirement programming tasks through structured,feedback-driven dialogue. Each task is modeled as a requirement dependencygraph, and an ``interviewer'' LLM, aware of the ground-truth solution, providesminimal, targeted hints to an ``interviewee'' model to help correct errors andfulfill target constraints. This dynamic protocol enables fine-graineddiagnostic insights into model behavior, uncovering strengths and systematicweaknesses that static benchmarks fail to measure. We build on DevAI, abenchmark of 55 curated programming tasks, by adding ground-truth solutions andevaluating the relevance and utility of interviewer hints through expertannotation. Our results highlight the importance of dynamic evaluation inadvancing the development of collaborative code-generating agents.</description><author>Dimitrios Rontogiannis, Maxime Peyrard, Nicolas Baldwin, Martin Josifoski, Robert West, Dimitrios Gunopulos</author><pubDate>Tue, 26 Aug 2025 10:22:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18905v1</guid></item><item><title>ConfTuner: Training Large Language Models to Express Their Confidence Verbally</title><link>http://arxiv.org/abs/2508.18847v1</link><description>Large Language Models (LLMs) are increasingly deployed in high-stakes domainssuch as science, law, and healthcare, where accurate expressions of uncertaintyare essential for reliability and trust. However, current LLMs are oftenobserved to generate incorrect answers with high confidence, a phenomenon knownas "overconfidence". Recent efforts have focused on calibrating LLMs'verbalized confidence: i.e., their expressions of confidence in text form, suchas "I am 80% confident that...". Existing approaches either rely on promptengineering or fine-tuning with heuristically generated uncertainty estimates,both of which have limited effectiveness and generalizability. Motivated by thenotion of proper scoring rules for calibration in classical machine learningmodels, we introduce ConfTuner, a simple and efficient fine-tuning method thatintroduces minimal overhead and does not require ground-truth confidence scoresor proxy confidence estimates. ConfTuner relies on a new loss function,tokenized Brier score, which we theoretically prove to be a proper scoringrule, intuitively meaning that it "correctly incentivizes the model to reportits true probability of being correct". ConfTuner improves calibration acrossdiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Ourresults further show that better-calibrated confidence enables downstream gainsin self-correction and model cascade, advancing the development of trustworthyLLM systems. The code is available athttps://github.com/liushiliushi/ConfTuner.</description><author>Yibo Li, Miao Xiong, Jiaying Wu, Bryan Hooi</author><pubDate>Tue, 26 Aug 2025 09:25:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18847v1</guid></item><item><title>Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment</title><link>http://arxiv.org/abs/2312.09625v5</link><description>Learning to ground natural language queries to target objects or regions in3D point clouds is quite essential for 3D scene understanding. Nevertheless,existing 3D visual grounding approaches require a substantial number ofbounding box annotations for text queries, which is time-consuming andlabor-intensive to obtain. In this paper, we propose 3D-VLA, a weaklysupervised approach for 3D visual grounding based on Visual LinguisticAlignment. Our 3D-VLA exploits the superior ability of current large-scalevision-language models (VLMs) on aligning the semantics between texts and 2Dimages, as well as the naturally existing correspondences between 2D images and3D point clouds, and thus implicitly constructs correspondences between textsand 3D point clouds with no need for fine-grained box annotations in thetraining procedure. During the inference stage, the learned text-3Dcorrespondence will help us ground the text queries to the 3D target objectseven without 2D images. To the best of our knowledge, this is the first work toinvestigate 3D visual grounding in a weakly supervised manner by involvinglarge scale vision-language models, and extensive experiments on ReferIt3D andScanRefer datasets demonstrate that our 3D-VLA achieves comparable and evensuperior results over the fully supervised methods.</description><author>Xiaoxu Xu, Yitian Yuan, Qiudan Zhang, Wenhui Wu, Zequn Jie, Lin Ma, Xu Wang</author><pubDate>Tue, 26 Aug 2025 08:50:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.09625v5</guid></item><item><title>Insights into User Interface Innovations from a Design Thinking Workshop at deRSE25</title><link>http://arxiv.org/abs/2508.18784v1</link><description>Large Language Models have become widely adopted tools due to their versatilecapabilities, yet their user interfaces remain limited, often following rigid,linear interaction paradigms. In this paper, we present insights from a designthinking workshop held at the deRSE25 conference aiming at collaborativelydeveloping innovative user interface concepts for LLMs. During the workshop,participants identified common use cases, evaluated the strengths andshortcomings of current LLM interfaces, and created visualizations of newinteraction concepts emphasizing flexible context management, dynamicconversation branching, and enhanced mechanisms for user control. We describehow these participant-generated ideas advanced our own whiteboard-based UIapproach. The ongoing development of this interface is guided by thehuman-centered design process - an iterative, user-focused methodology thatemphasizes continuous refinement through user feedback. Broader implicationsfor future LLM interface development are discussed, advocating for increasedattention to UI innovation grounded in user-centered design principles.</description><author>Maximilian Frank, Simon Lund</author><pubDate>Tue, 26 Aug 2025 08:11:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18784v1</guid></item><item><title>Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</title><link>http://arxiv.org/abs/2508.20072v1</link><description>Vision-Language-Action (VLA) models adapt large vision-language backbones tomap images and instructions to robot actions. However, prevailing VLA decoderseither generate actions autoregressively in a fixed left-to-right order orattach continuous diffusion or flow matching heads outside the backbone,demanding specialized training and iterative sampling that hinder a unified,scalable architecture. We present Discrete Diffusion VLA, a single-transformerpolicy that models discretized action chunks with discrete diffusion and istrained with the same cross-entropy objective as the VLM backbone. The designretains diffusion's progressive refinement paradigm while remaining nativelycompatible with the discrete token interface of VLMs. Our method achieves anadaptive decoding order that resolves easy action elements before harder onesand uses secondary remasking to revisit uncertain predictions across refinementrounds, which improves consistency and enables robust error correction. Thisunified decoder preserves pretrained vision language priors, supports paralleldecoding, breaks the autoregressive bottleneck, and reduces the number offunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnvBridge, improving over both autoregressive and continuous diffusion baselines.These findings indicate that discrete-diffusion action decoder supports preciseaction modeling and consistent training, laying groundwork for scaling VLA tolarger models and datasets.</description><author>Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo</author><pubDate>Wed, 27 Aug 2025 17:39:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20072v1</guid></item><item><title>Apple Intelligence Foundation Language Models: Tech Report 2025</title><link>http://arxiv.org/abs/2507.13575v3</link><description>We introduce two multilingual, multimodal foundation language models thatpower Apple Intelligence features across Apple devices and services: i a3B-parameter on-device model optimized for Apple silicon through architecturalinnovations such as KV-cache sharing and 2-bit quantization-aware training; andii a scalable server model built on a novel Parallel-Track Mixture-of-ExpertsPT-MoE transformer that combines track parallelism, mixture-of-experts sparsecomputation, and interleaved global-local attention to deliver high qualitywith competitive cost on Apple's Private Cloud Compute platform. Both modelsare trained on large-scale multilingual and multimodal datasets sourced viaresponsible web crawling, licensed corpora, and high-quality synthetic data,then further refined with supervised fine-tuning and reinforcement learning ona new asynchronous platform. The resulting models support several additionallanguages while understanding images and executing tool calls. In publicbenchmarks and human evaluations, both the server model and the on-device modelmatch or surpass comparably sized open baselines. A new Swift-centric Foundation Models framework exposes guided generation,constrained tool calling, and LoRA adapter fine-tuning, allowing developers tointegrate these capabilities with a few lines of code. The latest advancementsin Apple Intelligence models are grounded in our Responsible AI approach withsafeguards like content filtering and locale-specific evaluation, as well asour commitment to protecting our users' privacy with innovations like PrivateCloud Compute.</description><author>Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, John Peebles, Hannah Gillis Coleman, Matteo Ronchi, Peter Gray, Keen You, Anthony Spalvieri-Kruse, Ruoming Pang, Reed Li, Yuli Yang, Emad Soroush, Zhiyun Lu, Crystal Xiao, Rong Situ, Jordan Huffaker, David Griffiths, Zaid Ahmed, Peng Zhang, Daniel Parilla, Asaf Liberman, Jennifer Mallalieu, Parsa Mazaheri, Qibin Chen, Manjot Bilkhu, Aonan Zhang, Eric Wang, Dave Nelson, Michael FitzMaurice, Thomas Voice, Jeremy Liu, Josh Shaffer, Shiwen Zhao, Prasanth Yadla, Farzin Rasteh, Pengsheng Guo, Arsalan Farooq, Jeremy Snow, Stephen Murphy, Tao Lei, Minsik Cho, George Horrell, Sam Dodge, Lindsay Hislop, Sumeet Singh, Alex Dombrowski, Aiswarya Raghavan, </author><pubDate>Wed, 27 Aug 2025 16:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.13575v3</guid></item><item><title>SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control</title><link>http://arxiv.org/abs/2508.20018v1</link><description>The rapid advancement of large vision language models (LVLMs) and agentsystems has heightened interest in mobile GUI agents that can reliablytranslate natural language into interface operations. Existing single-agentapproaches, however, remain limited by structural constraints. Althoughmulti-agent systems naturally decouple different competencies, recent progressin multi-agent reinforcement learning (MARL) has often been hindered byinefficiency and remains incompatible with current LVLM architectures. Toaddress these challenges, we introduce SWIRL, a staged workflow for interleavedreinforcement learning designed for multi-agent systems. SWIRL reformulatesMARL into a sequence of single-agent reinforcement learning tasks, updating oneagent at a time while keeping the others fixed. This formulation enables stabletraining and promotes efficient coordination across agents. Theoretically, weprovide a stepwise safety bound, a cross-round monotonic improvement theorem,and convergence guarantees on return, ensuring robust and principledoptimization. In application to mobile GUI control, SWIRL instantiates aNavigator that converts language and screen context into structured plans, andan Interactor that grounds these plans into executable atomic actions.Extensive experiments demonstrate superior performance on both high-level andlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strongcapability in multi-agent mathematical reasoning, underscoring its potential asa general framework for developing efficient and robust multi-agent systems.</description><author>Quanfeng Lu, Zhantao Ma, Shuai Zhong, Jin Wang, Dahai Yu, Michael K. Ng, Ping Luo</author><pubDate>Wed, 27 Aug 2025 16:27:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20018v1</guid></item><item><title>Towards New Benchmark for AI Alignment &amp; Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI</title><link>http://arxiv.org/abs/2501.02531v3</link><description>As general-purpose artificial intelligence systems become increasinglyintegrated into society and are used for information seeking, contentgeneration, problem solving, textual analysis, coding, and running processes,it is crucial to assess their long-term impact on humans. This researchexplores the sentiment of large language models (LLMs) and humans towardartificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs,including GPT-4 and Bard, were analyzed and compared with sentiment data fromthree independent human sample populations. Temporal variations in sentimentwere also evaluated over three consecutive days. The results show a diversityin sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4recorded the most positive sentiment toward AGI, while Bard leaned toward aneutral sentiment. In contrast, the human samples showed a lower averagesentiment of 2.97. The analysis outlines potential conflicts of interest andbiases in the sentiment formation of LLMs, and indicates that LLMs could subtlyinfluence societal perceptions. To address the need for regulatory oversightand culturally grounded assessments of AI systems, we introduce the Societal AIAlignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensionalprompts and empirically validated societal value frameworks to evaluatelanguage model outputs across temporal, model, and multilingual axes. Thisbenchmark is designed to guide policymakers and AI agencies, including withinframeworks such as the EU AI Act, by providing robust, actionable insights intoAI alignment with human values, public sentiment, and ethical norms at bothnational and international levels. Future research should further refine theoperationalization of the SAAS-AI benchmark and systematically evaluate itseffectiveness through comprehensive empirical testing.</description><author>Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa</author><pubDate>Wed, 27 Aug 2025 13:49:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.02531v3</guid></item><item><title>CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval</title><link>http://arxiv.org/abs/2506.11066v2</link><description>Code retrieval is essential in modern software development, as it boosts codereuse and accelerates debugging. However, current benchmarks primarilyemphasize functional relevance while neglecting critical dimensions of softwarequality. Motivated by this gap, we introduce CoQuIR, the first large-scale,multilingual benchmark specifically designed to evaluate quality-aware coderetrieval across four key dimensions: correctness, efficiency, security, andmaintainability. CoQuIR provides fine-grained quality annotations for 42,725queries and 134,907 code snippets in 11 programming languages, and isaccompanied by two quality-centric evaluation metrics: Pairwise PreferenceAccuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23retrieval models, covering both open-source and proprietary systems, and findthat even top-performing models frequently fail to distinguish buggy orinsecure code from their more robust counterparts. Furthermore, we conductpreliminary investigations into training methods that explicitly encourageretrievers to recognize code quality. Using synthetic datasets, we demonstratepromising improvements in quality-aware metrics across various models, withoutsacrificing semantic relevance. Downstream code generation experiments furthervalidate the effectiveness of our approach. Overall, our work highlights theimportance of integrating quality signals into code retrieval systems, layingthe groundwork for more trustworthy and robust software development tools.</description><author>Jiahui Geng, Fengyu Cai, Shaobo Cui, Qing Li, Liangwei Chen, Chenyang Lyu, Haonan Li, Derui Zhu, Walter Pretschner, Heinz Koeppl, Fakhri Karray</author><pubDate>Wed, 27 Aug 2025 12:24:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.11066v2</guid></item><item><title>PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs</title><link>http://arxiv.org/abs/2507.05444v2</link><description>Vocabulary acquisition poses a significant challenge for second-language (L2)learners, especially when learning typologically distant languages such asEnglish and Korean, where phonological and structural mismatches complicatevocabulary learning. Recently, large language models (LLMs) have been used togenerate keyword mnemonics by leveraging similar keywords from a learner'sfirst language (L1) to aid in acquiring L2 vocabulary. However, most of thisresearch has focused on native English speakers learning other languages,rather than the reverse. In this paper, we present PhoniTale, a novelcross-lingual mnemonic generation system that retrieves L1 keyword sequencebased on phonological similarity and uses LLMs to generate mnemonics. Weevaluate PhoniTale using both automated metrics and human evaluations,comparing its output to mnemonics created by humans and by previous automatedapproaches. To assess practical effectiveness, we also conduct a short-termrecall test measuring mnemonic helpfulness. Our findings show that PhoniTaleperforms comparably to human-authored mnemonics. We also highlight key areasfor future improvement in mnemonic quality and methodology.</description><author>Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh</author><pubDate>Wed, 27 Aug 2025 09:29:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05444v2</guid></item><item><title>Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs</title><link>http://arxiv.org/abs/2508.19594v1</link><description>Context faithfulness is essential for reliable reasoning in context-dependentscenarios. However, large language models often struggle to ground theiroutputs in the provided context, resulting in irrelevant responses. Inspired bythe emergent expert specialization observed in mixture-of-expertsarchitectures, this work investigates whether certain experts exhibitspecialization in context utilization, offering a potential pathway towardtargeted optimization for improved context faithfulness. To explore this, wepropose Router Lens, a method that accurately identifies context-faithfulexperts. Our analysis reveals that these experts progressively amplifyattention to relevant contextual information, thereby enhancing contextgrounding. Building on this insight, we introduce Context-faithful ExpertFine-Tuning (CEFT), a lightweight optimization approach that selectivelyfine-tunes context-faithful experts. Experiments across a wide range ofbenchmarks and models demonstrate that CEFT matches or surpasses theperformance of full fine-tuning while being significantly more efficient.</description><author>Jun Bai, Minghao Tong, Yang Liu, Zixia Jia, Zilong Zheng</author><pubDate>Wed, 27 Aug 2025 06:07:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19594v1</guid></item><item><title>NPHardEval4V: Dynamic Evaluation of Large Vision-Language Models with Effects of Vision</title><link>http://arxiv.org/abs/2403.01777v3</link><description>Large Vision-Language Models (LVLMs) have demonstrated impressivecapabilities in multimodal understanding, yet their reasoning abilities remainunderexplored. Existing benchmarks tend to focus on perception or text-basedcomprehension, offering limited insight into how well these models perform onstructured, logic-driven tasks that require both visual and linguisticreasoning. To address this gap, we introduce NPHardEval4V, a multimodalbenchmark suite grounded in four classical NP-hard problems: Knapsack, SetCover, Traveling Salesperson, and Vertex Cover. Each task is presented througha combination of structured visual layouts and textual prompts, designed toassess the ability of LVLMs to perform combinatorial reasoning undervisual-linguistic constraints. We evaluate a set of advanced open-source andclosed-source vision-language models under a unified prompting and problemrepresentation framework. This enables fair comparison across models and tasktypes, while isolating key variables affecting performance. Our results showthat while these models perform reasonably well on perception-based inputs,they struggle with global optimization, abstraction, and constraintsatisfaction. No single model demonstrates consistent reasoning capabilityacross all problem types, and common failure patterns reveal fundamentallimitations in current architectures. By leveraging the structure andcomplexity of NP-hard problems, NPHardEval4V provides a scalable,interpretable, and challenging testbed for diagnosing reasoning behaviors inLVLMs. We hope this benchmark can support the community in building morerobust, inference-capable multimodal systems. The benchmark dataset and codeare available at https://github.com/lizhouf/NPHardEval4.</description><author>Xiang Li, Wenyue Hua, Kaijie Zhu, Lingyao Li, Haoyang Ling, Jinkui Chi, Qi Dou, Jindong Wang, Yongfeng Zhang, Xin Ma, Lizhou Fan</author><pubDate>Wed, 27 Aug 2025 05:37:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01777v3</guid></item><item><title>Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents</title><link>http://arxiv.org/abs/2507.14819v2</link><description>Large Language Models (LLMs) have demonstrated strong capabilities intransforming text descriptions or tables to data visualizations viainstruction-tuning methods. However, it is not straightforward to apply thesemethods directly for a more real-world use case of visualizing data from longdocuments based on user-given intents, as opposed to the user pre-selecting therelevant content manually. We introduce the task of intent-based chartgeneration from documents: given a user-specified intent and document(s), thegoal is to generate a chart adhering to the intent and grounded on thedocument(s) in a zero-shot setting. We propose an unsupervised, two-stagedframework in which an LLM first extracts relevant information from thedocument(s) by decomposing the intent and iteratively validates and refinesthis data. Next, a heuristic-guided module selects an appropriate chart typebefore final code generation. To assess the data accuracy of the generatedcharts, we propose an attribution-based metric that uses a structured textualrepresentation of charts, instead of relying on visual decoding metrics thatoften fail to capture the chart data effectively. To validate our approach, wecurate a dataset comprising of 1,242 $&lt;$intent, document, charts$&gt;$ tuples fromtwo domains, finance and scientific, in contrast to the existing datasets thatare largely limited to parallel text descriptions/ tables and theircorresponding charts. We compare our approach with baselines using single-shotchart generation using LLMs and query-based retrieval methods; our methodoutperforms by upto $9$ points and $17$ points in terms of chart data accuracyand chart type respectively over the best baselines.</description><author>Akriti Jain, Pritika Ramu, Aparna Garimella, Apoorv Saxena</author><pubDate>Wed, 27 Aug 2025 04:57:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14819v2</guid></item><item><title>X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents</title><link>http://arxiv.org/abs/2508.16021v2</link><description>State-sponsored trolls, malicious actors who deploy sophisticated linguisticmanipulation in coordinated information campaigns, posing threats to onlinediscourse integrity. While Large Language Models (LLMs) achieve strongperformance on general natural language processing (NLP) tasks, they strugglewith subtle propaganda detection and operate as ``black boxes'', providing nointerpretable insights into manipulation strategies. This paper introducesX-Troll, a novel framework that bridges this gap by integrating explainableadapter-based LLMs with expert-derived linguistic knowledge to detectstate-sponsored trolls and provide human-readable explanations for itsdecisions. X-Troll incorporates appraisal theory and propaganda analysisthrough specialized LoRA adapters, using dynamic gating to capturecampaign-specific discourse patterns in coordinated information operations.Experiments on real-world data demonstrate that our linguistically-informedapproach shows strong performance compared with both general LLM baselines andexisting troll detection models in accuracy while providing enhancedtransparency through expert-grounded explanations that reveal the specificlinguistic strategies used by state-sponsored actors. X-Troll source code isavailable at: https://github.com/ltian678/xtroll_source/.</description><author>Lin Tian, Xiuzhen Zhang, Maria Myung-Hee Kim, Jennifer Biggs, Marian-Andrei Rizoiu</author><pubDate>Wed, 27 Aug 2025 02:18:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16021v2</guid></item><item><title>ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</title><link>http://arxiv.org/abs/2508.21010v1</link><description>Existing Causal-Why Video Question Answering (VideoQA) models often strugglewith higher-order reasoning, relying on opaque, monolithic pipelines thatentangle video understanding, causal inference, and answer generation. Theseblack-box approaches offer limited interpretability and tend to depend onshallow heuristics. We propose a novel, modular framework that explicitlydecouples causal reasoning from answer generation, introducing natural languagecausal chains as interpretable intermediate representations. Inspired by humancognitive models, these structured cause-effect sequences bridge low-levelvideo content with high-level causal reasoning, enabling transparent andlogically coherent inference. Our two-stage architecture comprises a CausalChain Extractor (CCE) that generates causal chains from video-question pairs,and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded inthese chains. To address the lack of annotated reasoning traces, we introduce ascalable method for generating high-quality causal chains from existingdatasets using large language models. We also propose CauCo, a new evaluationmetric for causality-oriented captioning. Experiments on three large-scalebenchmarks demonstrate that our approach not only outperforms state-of-the-artmodels, but also yields substantial gains in explainability, user trust, andgeneralization -- positioning the CCE as a reusable causal reasoning engineacross diverse domains. Project page:https://paritoshparmar.github.io/chainreaction/</description><author>Paritosh Parmar, Eric Peh, Basura Fernando</author><pubDate>Thu, 28 Aug 2025 17:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21010v1</guid></item><item><title>ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery</title><link>http://arxiv.org/abs/2508.20996v1</link><description>Substance use disorders (SUDs) affect over 36 million people worldwide, yetfew receive effective care due to stigma, motivational barriers, and limitedpersonalized support. Although large language models (LLMs) show promise formental-health assistance, most systems lack tight integration with clinicallyvalidated strategies, reducing effectiveness in addiction recovery. We presentChatThero, a multi-agent conversational framework that couples dynamic patientmodeling with context-sensitive therapeutic dialogue and adaptive persuasivestrategies grounded in cognitive behavioral therapy (CBT) and motivationalinterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,Medium, and Hard resistance levels, and train ChatThero with a two-stagepipeline comprising supervised fine-tuning (SFT) followed by direct preferenceoptimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain inpatient motivation, a 0.49\% increase in treatment confidence, and resolveshard cases with 26\% fewer turns than GPT-4o, and both automated and humanclinical assessments rate it higher in empathy, responsiveness, and behavioralrealism. The framework supports rigorous, privacy-preserving study oftherapeutic conversation and provides a robust, replicable basis for researchand clinical translation.</description><author>Junda Wang, Zonghai Yao, Zhichao Yang, Lingxi Li, Junhui Qian, Hong Yu</author><pubDate>Thu, 28 Aug 2025 16:57:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20996v1</guid></item><item><title>WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations</title><link>http://arxiv.org/abs/2508.20976v1</link><description>Large audio language models (LALMs) extend language understanding into theauditory domain, yet their ability to perform low-level listening, such aspitch and duration detection, remains underexplored. However, low-levellistening is critical for real-world, out-of-distribution tasks where modelsmust reason about unfamiliar sounds based on fine-grained acoustic cues. Toaddress this gap, we introduce the World-of-Whale benchmark (WoW-Bench) toevaluate low-level auditory perception and cognition using marine mammalvocalizations. WoW-bench is composed of a Perception benchmark for categorizingnovel sounds and a Cognition benchmark, inspired by Bloom's taxonomy, to assessthe abilities to remember, understand, apply, and analyze sound events. For theCognition benchmark, we additionally introduce distractor questions to evaluatewhether models are truly solving problems through listening rather than relyingon other heuristics. Experiments with state-of-the-art LALMs show performancefar below human levels, indicating a need for stronger auditory grounding inLALMs.</description><author>Jaeyeon Kim, Heeseung Yun, Sang Hoon Woo, Chao-Han Huck Yang, Gunhee Kim</author><pubDate>Thu, 28 Aug 2025 16:29:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20976v1</guid></item><item><title>Multi-Agent Penetration Testing AI for the Web</title><link>http://arxiv.org/abs/2508.20816v1</link><description>AI-powered development platforms are making software creation accessible to abroader audience, but this democratization has triggered a scalability crisisin security auditing. With studies showing that up to 40% of AI-generated codecontains vulnerabilities, the pace of development now vastly outstrips thecapacity for thorough security assessment. We present MAPTA, a multi-agent system for autonomous web applicationsecurity assessment that combines large language model orchestration withtool-grounded execution and end-to-end exploit validation. On the 104-challengeXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performanceon SSRF and misconfiguration vulnerabilities, 83% success on brokenauthorization, and strong results on injection attacks including server-sidetemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)and blind SQL injection (0%) remain challenging. Our comprehensive costanalysis across all challenges totals $21.38 with a median cost of $0.073 forsuccessful attempts versus $0.357 for failures. Success correlates stronglywith resource efficiency, enabling practical early-stopping thresholds atapproximately 40 tool calls or $0.30 per challenge. MAPTA's real-world findings are impactful given both the popularity of therespective scanned GitHub repositories (8K-70K stars) and MAPTA's low averageoperating cost of $3.67 per open-source assessment: MAPTA discovered criticalvulnerabilities including RCEs, command injections, secret exposure, andarbitrary file write vulnerabilities. Findings are responsibly disclosed, 10findings are under CVE review.</description><author>Isaac David, Arthur Gervais</author><pubDate>Thu, 28 Aug 2025 14:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20816v1</guid></item><item><title>Steering Towards Fairness: Mitigating Political Bias in LLMs</title><link>http://arxiv.org/abs/2508.08846v2</link><description>Recent advancements in large language models (LLMs) have enabled theirwidespread use across diverse real-world applications. However, concerns remainabout their tendency to encode and reproduce ideological biases along politicaland economic dimensions. In this paper, we employ a framework for probing andmitigating such biases in decoder-based LLMs through analysis of internal modelrepresentations. Grounded in the Political Compass Test (PCT), this method usescontrastive pairs to extract and compare hidden layer activations from modelslike Mistral and DeepSeek. We introduce a comprehensive activation extractionpipeline capable of layer-wise analysis across multiple ideological axes,revealing meaningful disparities linked to political framing. Our results showthat decoder LLMs systematically encode representational bias across layers,which can be leveraged for effective steering vector-based mitigation. Thiswork provides new insights into how political bias is encoded in LLMs andoffers a principled approach to debiasing beyond surface-level outputinterventions.</description><author>Afrozah Nadeem, Mark Dras, Usman Naseem</author><pubDate>Thu, 28 Aug 2025 14:07:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.08846v2</guid></item><item><title>Improving Alignment in LVLMs with Debiased Self-Judgment</title><link>http://arxiv.org/abs/2508.20655v1</link><description>The rapid advancements in Large Language Models (LLMs) and LargeVisual-Language Models (LVLMs) have opened up new opportunities for integratingvisual and linguistic modalities. However, effectively aligning thesemodalities remains challenging, often leading to hallucinations--wheregenerated outputs are not grounded in the visual input--and raising safetyconcerns across various domains. Existing alignment methods, such asinstruction tuning and preference tuning, often rely on external datasets,human annotations, or complex post-processing, which limit scalability andincrease costs. To address these challenges, we propose a novel approach thatgenerates the debiased self-judgment score, a self-evaluation metric createdinternally by the model without relying on external resources. This enables themodel to autonomously improve alignment. Our method enhances both decodingstrategies and preference tuning processes, resulting in reducedhallucinations, enhanced safety, and improved overall capability. Empiricalresults show that our approach significantly outperforms traditional methods,offering a more effective solution for aligning LVLMs.</description><author>Sihan Yang, Chenhang Cui, Zihao Zhao, Yiyang Zhou, Weilong Yan, Ying Wei, Huaxiu Yao</author><pubDate>Thu, 28 Aug 2025 11:01:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20655v1</guid></item><item><title>GDS Agent: A Graph Algorithmic Reasoning Agent</title><link>http://arxiv.org/abs/2508.20637v1</link><description>Large language models (LLMs) have shown remarkable multimodal informationprocessing and reasoning ability. When equipped with tools through functioncalling and enhanced with retrieval-augmented techniques, compound LLM-basedsystems can access closed data sources and answer questions about them.However, they still struggle to process and reason over large-scalegraph-structure data. We introduce the GDS (Graph Data Science) agent in thistechnical report. The GDS agent introduces a comprehensive set of graphalgorithms as tools, together with preprocessing (retrieval) and postprocessingof algorithm results, in a model context protocol (MCP) server. The server canbe used with any modern LLM out-of-the-box. GDS agent allows users to ask anyquestion that implicitly and intrinsically requires graph algorithmic reasoningabout their data, and quickly obtain accurate and grounded answers. We alsointroduce a new benchmark that evaluates intermediate tool calls as well asfinal responses. The results indicate that GDS agent is able to solve a widespectrum of graph tasks. We also provide detailed case studies for moreopen-ended tasks and study scenarios where the agent struggles. Finally, wediscuss the remaining challenges and the future roadmap.</description><author>Borun Shi, Ioannis Panagiotas</author><pubDate>Thu, 28 Aug 2025 10:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20637v1</guid></item><item><title>Data-driven Discovery of Digital Twins in Biomedical Research</title><link>http://arxiv.org/abs/2508.21484v1</link><description>Recent technological advances have expanded the availability ofhigh-throughput biological datasets, enabling the reliable design of digitaltwins of biomedical systems or patients. Such computational tools represent keyreaction networks driving perturbation or drug response and can guide drugdiscovery and personalized therapeutics. Yet, their development still relies onlaborious data integration by the human modeler, so that automated approachesare critically needed. The success of data-driven system discovery in Physics,rooted in clean datasets and well-defined governing laws, has fueled interestin applying similar techniques in Biology, which presents unique challenges.Here, we reviewed methodologies for automatically inferring digital twins frombiological time series, which mostly involve symbolic or sparse regression. Weevaluate algorithms according to eight biological and methodologicalchallenges, associated to noisy/incomplete data, multiple conditions, priorknowledge integration, latent variables, high dimensionality, unobservedvariable derivatives, candidate library design, and uncertainty quantification.Upon these criteria, sparse regression generally outperformed symbolicregression, particularly when using Bayesian frameworks. We further highlightthe emerging role of deep learning and large language models, which enableinnovative prior knowledge integration, though the reliability and consistencyof such approaches must be improved. While no single method addresses allchallenges, we argue that progress in learning digital twins will come fromhybrid and modular frameworks combining chemical reaction network-basedmechanistic grounding, Bayesian uncertainty quantification, and the generativeand knowledge integration capacities of deep learning. To support theirdevelopment, we further propose a benchmarking framework to evaluate methodsacross all challenges.</description><author>Clémence Métayer, Annabelle Ballesta, Julien Martinelli</author><pubDate>Fri, 29 Aug 2025 10:10:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21484v1</guid></item><item><title>From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics</title><link>http://arxiv.org/abs/2508.21452v1</link><description>Large language models (LLMs) are increasingly considered as tutoring aids inscience education. Yet their readiness for unsupervised use in undergraduateinstruction remains uncertain, as reliable teaching requires more than fluentrecall: it demands consistent, principle-grounded reasoning. Thermodynamics,with its compact laws and subtle distinctions between state and path functions,reversibility, and entropy, provides an ideal testbed for evaluating suchcapabilities. Here we present UTQA, a 50-item undergraduate thermodynamicsquestion answering benchmark, covering ideal-gas processes, reversibility, anddiagram interpretation. No leading 2025-era model exceeded our 95\% competencethreshold: the best LLMs achieved 82\% accuracy, with text-only itemsperforming better than image reasoning tasks, which often fell to chancelevels. Prompt phrasing and syntactic complexity showed modest to littlecorrelation with performance. The gap concentrates in finite-rate/irreversiblescenarios and in binding visual features to thermodynamic meaning, indicatingthat current LLMs are not yet suitable for unsupervised tutoring in thisdomain.</description><author>Anna Geißler, Luca-Sophie Bien, Friedrich Schöppler, Tobias Hertel</author><pubDate>Fri, 29 Aug 2025 09:36:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21452v1</guid></item><item><title>One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist</title><link>http://arxiv.org/abs/2508.21451v1</link><description>Image captioning is fundamental for applications like video instructionsystems and exploration robots, yet deploying such models on local devices ischallenging due to the high computational demands of multimodal large languagemodels (MLLMs). To address this, we first explore lightweight captioning byimplementing a specialist based on a 125M-parameter language model, 56 timessmaller than LLaMA-7B, and evaluating its performance on both single-sentenceand detailed captioning tasks. Surprisingly, we find that our model can achieveperformance comparable to large multimodal generalists, suggesting itspotential to serve as a strong visual specialist for on-device applications.While promising, our model also exhibits a limitation: like other MLLMs, itsuffers from visual blindness, occasionally resulting in semantic captioningerrors. We carry out toy experiments and investigate the underlying causes,where we observe that the problems arise from ineffective attention mechanismsand limited visual representations. To alleviate them, we develop a novelcaptioning framework, Sharp-Eyed Refinement, which enhances caption qualitythrough improved visual grounding. At its core, our DeepLens extracts detailedvisual representations by concentrating on informative regions identifiedduring the initial glance. Our experiments confirm both the advantages of ourspecialist over prior small captioning models and large generalists and theeffectiveness of our framework.</description><author>Junha Song, Yongsik Jo, So Yeon Min, Quanting Xie, Taehwan Kim, Yonatan Bisk, Jaegul Choo</author><pubDate>Fri, 29 Aug 2025 09:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21451v1</guid></item><item><title>VoCap: Video Object Captioning and Segmentation from Any Prompt</title><link>http://arxiv.org/abs/2508.21809v1</link><description>Understanding objects in videos in terms of fine-grained localization masksand detailed semantic properties is a fundamental task in video understanding.In this paper, we propose VoCap, a flexible video model that consumes a videoand a prompt of various modalities (text, box or mask), and produces aspatio-temporal masklet with a corresponding object-centric caption. As suchour model addresses simultaneously the tasks of promptable video objectsegmentation, referring expression segmentation, and object captioning. Sinceobtaining data for this task is tedious and expensive, we propose to annotatean existing large-scale segmentation dataset (SAV) with pseudo object captions.We do so by preprocessing videos with their ground-truth masks to highlight theobject of interest and feed this to a large Vision Language Model (VLM). For anunbiased evaluation, we collect manual annotations on the validation set. Wecall the resulting dataset SAV-Caption. We train our VoCap model at scale on aSAV-Caption together with a mix of other image and video datasets. Our modelyields state-of-the-art results on referring expression video objectsegmentation, is competitive on semi-supervised video object segmentation, andestablishes a benchmark for video object captioning. Our dataset will be madeavailable at https://github.com/google-deepmind/vocap.</description><author>Jasper Uijlings, Xingyi Zhou, Xiuye Gu, Arsha Nagrani, Anurag Arnab, Alireza Fathi, David Ross, Cordelia Schmid</author><pubDate>Fri, 29 Aug 2025 17:43:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21809v1</guid></item><item><title>PiCSAR: Probabilistic Confidence Selection And Ranking</title><link>http://arxiv.org/abs/2508.21787v1</link><description>Best-of-n sampling improves the accuracy of large language models (LLMs) andlarge reasoning models (LRMs) by generating multiple candidate solutions andselecting the one with the highest reward. The key challenge for reasoningtasks is designing a scoring function that can identify correct reasoningchains without access to ground-truth answers. We propose ProbabilisticConfidence Selection And Ranking (PiCSAR): a simple, training-free method thatscores each candidate generation using the joint log-likelihood of thereasoning and final answer. The joint log-likelihood of the reasoning and finalanswer naturally decomposes into reasoning confidence and answer confidence.PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in16 out of 20 comparisons. Our analysis reveals that correct reasoning chainsexhibit significantly higher reasoning and answer confidence, justifying theeffectiveness of PiCSAR.</description><author>Joshua Ong Jun Leang, Zheng Zhao, Aryo Pradipta Gema, Sohee Yang, Wai-Chung Kwan, Xuanli He, Wenda Li, Pasquale Minervini, Eleonora Giunchiglia, Shay B. Cohen</author><pubDate>Fri, 29 Aug 2025 17:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21787v1</guid></item><item><title>CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics</title><link>http://arxiv.org/abs/2508.18124v3</link><description>We introduce CMPhysBench, designed to assess the proficiency of LargeLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.CMPhysBench is composed of more than 520 graduate-level meticulously curatedquestions covering both representative subfields and foundational theoreticalframeworks of condensed matter physics, such as magnetism, superconductivity,strongly correlated systems, etc. To ensure a deep understanding of theproblem-solving process,we focus exclusively on calculation problems, requiringLLMs to independently generate comprehensive solutions. Meanwhile, leveragingtree-based representations of expressions, we introduce the Scalable ExpressionEdit Distance (SEED) score, which provides fine-grained (non-binary) partialcredit and yields a more accurate assessment of similarity between predictionand ground-truth. Our results show that even the best models, Grok-4, reachonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring asignificant capability gap, especially for this practical and frontier domainrelative to traditional physics. The code anddataset are publicly available athttps://github.com/CMPhysBench/CMPhysBench.</description><author>Weida Wang, Dongchen Huang, Jiatong Li, Tengchao Yang, Ziyang Zheng, Di Zhang, Dong Han, Benteng Chen, Binzhao Luo, Zhiyu Liu, Kunling Liu, Zhiyuan Gao, Shiqi Geng, Wei Ma, Jiaming Su, Xin Li, Shuchen Pu, Yuhan Shui, Qianjia Cheng, Zhihao Dou, Dongfei Cui, Changyong He, Jin Zeng, Zeke Xie, Mao Su, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang, Yunqi Cai, Xi Dai, Shufei Zhang, Lei Bai, Jinguang Cheng, Zhong Fang, Hongming Weng</author><pubDate>Fri, 29 Aug 2025 14:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18124v3</guid></item><item><title>Data-driven Discovery of Digital Twins in Biomedical Research</title><link>http://arxiv.org/abs/2508.21484v2</link><description>Recent technological advances have expanded the availability ofhigh-throughput biological datasets, enabling the reliable design of digitaltwins of biomedical systems or patients. Such computational tools represent keyreaction networks driving perturbation or drug response and can guide drugdiscovery and personalized therapeutics. Yet, their development still relies onlaborious data integration by the human modeler, so that automated approachesare critically needed. The success of data-driven system discovery in Physics,rooted in clean datasets and well-defined governing laws, has fueled interestin applying similar techniques in Biology, which presents unique challenges.Here, we reviewed methodologies for automatically inferring digital twins frombiological time series, which mostly involve symbolic or sparse regression. Weevaluate algorithms according to eight biological and methodologicalchallenges, associated to noisy/incomplete data, multiple conditions, priorknowledge integration, latent variables, high dimensionality, unobservedvariable derivatives, candidate library design, and uncertainty quantification.Upon these criteria, sparse regression generally outperformed symbolicregression, particularly when using Bayesian frameworks. We further highlightthe emerging role of deep learning and large language models, which enableinnovative prior knowledge integration, though the reliability and consistencyof such approaches must be improved. While no single method addresses allchallenges, we argue that progress in learning digital twins will come fromhybrid and modular frameworks combining chemical reaction network-basedmechanistic grounding, Bayesian uncertainty quantification, and the generativeand knowledge integration capacities of deep learning. To support theirdevelopment, we further propose a benchmarking framework to evaluate methodsacross all challenges.</description><author>Clémence Métayer, Annabelle Ballesta, Julien Martinelli</author><pubDate>Mon, 01 Sep 2025 17:06:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21484v2</guid></item><item><title>MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning</title><link>http://arxiv.org/abs/2508.16654v2</link><description>Vision-and-Language Navigation (VLN) requires an agent to interpret naturallanguage instructions and navigate complex environments. Current approachesoften adopt a "black-box" paradigm, where a single Large Language Model (LLM)makes end-to-end decisions. However, it is plagued by critical vulnerabilities,including poor spatial reasoning, weak cross-modal grounding, and memoryoverload in long-horizon tasks. To systematically address these issues, wepropose Memory Spatial Navigation(MSNav), a framework that fuses three modulesinto a synergistic architecture, which transforms fragile inference into arobust, integrated intelligence. MSNav integrates three modules: Memory Module,a dynamic map memory module that tackles memory overload through selective nodepruning, enhancing long-range exploration; Spatial Module, a module for spatialreasoning and object relationship inference that improves endpoint recognition;and Decision Module, a module using LLM-based path planning to execute robustactions. Powering Spatial Module, we also introduce an Instruction-Object-Space(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),which outperforms leading commercial LLMs in object list extraction, achievinghigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on theRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-artperformance with significant improvements in Success Rate (SR) and Successweighted by Path Length (SPL).</description><author>Chenghao Liu, Zhimu Zhou, Jiachen Zhang, Minghao Zhang, Songfang Huang, Huiling Duan</author><pubDate>Mon, 01 Sep 2025 07:54:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16654v2</guid></item><item><title>Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld</title><link>http://arxiv.org/abs/2508.09889v4</link><description>The rapid advancement of large language models (LLMs) has empoweredintelligent agents to leverage diverse external tools for solving complexreal-world problems. However, this reliance introduces new challenges, asextended contexts and noisy tool outputs can undermine system reliability. Toaddress this, we propose a dynamic Multi-Agent System (MAS) in our AWorldframework, where an Execution Agent is supervised by a Guard Agent thatprovides on-demand dynamic maneuvering, verifying and correcting the reasoningprocess to improve robustness over single-agent systems. To move beyond thisgeneric supervision, we enhance the architecture with a methodology inspired bySystem Identification from control theory. This method first profiles theExecution Agent offline on a benchmark dataset to create a "performancefingerprint" of its unique weaknesses. The Guard Agent then leverages thisfingerprint online to deliver profile-aware supervision, making targetedinterventions based on known failure patterns rather than merely reacting toimmediate logical flaws. Extensive experiments on the GAIA dataset demonstratethat this profile-aware MAS significantly improves both effectiveness andstability, outperforming not only single-agent systems but also its naivecounterpart. This superior performance led our system to achieve first placeamong open-source projects on the prestigious GAIA leaderboard. These findingshighlight that building truly trustworthy intelligent systems requires not justcollaboration, but a deep, empirically-grounded understanding of each agent'sunique capabilities and limitations.</description><author>Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, Jinjie Gu</author><pubDate>Mon, 01 Sep 2025 02:41:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09889v4</guid></item><item><title>Light-Weight Cross-Modal Enhancement Method with Benchmark Construction for UAV-based Open-Vocabulary Object Detection</title><link>http://arxiv.org/abs/2509.06011v2</link><description>Open-Vocabulary Object Detection (OVD) faces severe performance degradationwhen applied to UAV imagery due to the domain gap from ground-level datasets.To address this challenge, we propose a complete UAV-oriented solution thatcombines both dataset construction and model innovation. First, we design arefined UAV-Label Engine, which efficiently resolves annotation redundancy,inconsistency, and ambiguity, enabling the generation of largescale UAVdatasets. Based on this engine, we construct two new benchmarks: UAVDE-2M, withover 2.4M instances across 1,800+ categories, and UAVCAP-15K, providing richimage-text pairs for vision-language pretraining. Second, we introduce theCross-Attention Gated Enhancement (CAGE) module, a lightweight dual-path fusiondesign that integrates cross-attention, adaptive gating, and global FiLMmodulation for robust textvision alignment. By embedding CAGE into theYOLO-World-v2 framework, our method achieves significant gains in both accuracyand efficiency, notably improving zero-shot detection on VisDrone by +5.3 mAPwhile reducing parameters and GFLOPs, and demonstrating strong cross-domaingeneralization on SIMD. Extensive experiments and real-world UAV deploymentconfirm the effectiveness and practicality of our proposed solution forUAV-based OVD</description><author>Zhenhai Weng, Xinjie Li, Can Wu, Weijie He, Jianfeng Lv, Dong Zhou, Zhongliang Yu</author><pubDate>Tue, 09 Sep 2025 12:22:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.06011v2</guid></item><item><title>Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment</title><link>http://arxiv.org/abs/2509.07642v1</link><description>Adopting Large language models (LLMs) in organizations potentiallyrevolutionizes our lives and work. However, they can generate off-topic,discriminating, or harmful content. This AI alignment problem often stems frommisspecifications during the LLM adoption, unnoticed by the principal due tothe LLM's black-box nature. While various research disciplines investigated AIalignment, they neither address the information asymmetries betweenorganizational adopters and black-box LLM agents nor consider organizational AIadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-LedAlignment Strategy) a conceptual framework grounded in agency (contract)theory, to mitigate alignment problems during organizational LLM adoption. Weconduct a conceptual literature analysis using the organizational LLM adoptionphases and the agency theory as concepts. Our approach results in (1) providingan extended literature analysis process specific to AI alignment methods duringorganizational LLM adoption and (2) providing a first LLM alignmentproblem-solution space.</description><author>Sascha Kaltenpoth, Oliver Müller</author><pubDate>Tue, 09 Sep 2025 12:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07642v1</guid></item><item><title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title><link>http://arxiv.org/abs/2505.12363v4</link><description>While Multimodal Large Language Models (MLLMs) excel at generalvision-language tasks, visuospatial cognition - reasoning about spatiallayouts, relations, and dynamics - remains a significant challenge. Existingmodels often lack the necessary architectural components and specializedtraining data for fine-grained spatial understanding. We introduce ViCA2(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatialreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIPfor semantics and Hiera for spatial structure, coupled with a token ratiocontrol mechanism for efficiency. We also developed ViCA-322K, a newlarge-scale dataset with over 322,000 spatially grounded question-answer pairsfor targeted instruction tuning. On the challenging VSI-Bench benchmark, ourViCA2-7B model achieves a state-of-the-art average score of 56.8, significantlysurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) andleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates theeffectiveness of our approach in achieving strong visuospatial intelligencewith a compact model. We release ViCA2, its codebase, and the ViCA-322K datasetto facilitate further research.</description><author>Qi Feng</author><pubDate>Tue, 09 Sep 2025 09:51:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12363v4</guid></item><item><title>Visuospatial Cognitive Assistant</title><link>http://arxiv.org/abs/2505.12312v4</link><description>Video-based spatial cognition is vital for robotics and embodied AI butchallenges current Vision-Language Models (VLMs). This paper makes two keycontributions. First, we introduce ViCA (Visuospatial CognitiveAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoorvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3Dmetadata-grounded queries and video-based complex reasoning. Second, we developViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on alleight VSI-Bench tasks, outperforming existing models, including larger ones(e.g., +26.1 on Absolute Distance). For interpretability, we presentViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tuneViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatialreasoning. Our work highlights the importance of targeted data and suggestspaths for improved temporal-spatial modeling. We release all resources tofoster research in robust visuospatial intelligence.</description><author>Qi Feng</author><pubDate>Tue, 09 Sep 2025 09:48:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12312v4</guid></item><item><title>From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations</title><link>http://arxiv.org/abs/2506.10559v2</link><description>Explaining why the species lives at a particular location is important forunderstanding ecological systems and conserving biodiversity. However, existingecological workflows are fragmented and often inaccessible to non-specialists.We propose an end-to-end visual-to-causal framework that transforms a speciesimage into interpretable causal insights about its habitat preference. Thesystem integrates species recognition, global occurrence retrieval,pseudo-absence sampling, and climate data extraction. We then discover causalstructures among environmental features and estimate their influence on speciesoccurrence using modern causal inference methods. Finally, we generatestatistically grounded, human-readable causal explanations from structuredtemplates and large language models. We demonstrate the framework on a bee anda flower species and report early results as part of an ongoing project,showing the potential of the multimodal AI assistant backed up by a recommendedecological modeling practice for describing species habitat inhuman-understandable language. Our code is available at:https://github.com/Yutong-Zhou-cv/BioX.</description><author>Yutong Zhou, Masahiro Ryo</author><pubDate>Tue, 09 Sep 2025 09:37:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.10559v2</guid></item><item><title>ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval</title><link>http://arxiv.org/abs/2509.07512v1</link><description>Many contemporary data-driven research efforts in the natural sciences, suchas chemistry and materials science, require large-scale, high-performanceentity recognition from scientific datasets. Large language models (LLMs) haveincreasingly been adopted to solve the entity recognition task, with the sametrend being observed on all-spectrum NLP tasks. The prevailing entityrecognition LLMs rely on fine-tuned technology, yet the fine-tuning processoften incurs significant cost. To achieve a best performance-cost trade-off, wepropose ALLabel, a three-stage framework designed to select the mostinformative and representative samples in preparing the demonstrations for LLMmodeling. The annotated examples are used to construct a ground-truth retrievalcorpus for LLM in-context learning. By sequentially employing three distinctactive learning strategies, ALLabel consistently outperforms all baselinesunder the same annotation budget across three specialized domain datasets.Experimental results also demonstrate that selectively annotating only 5\%-10\%of the dataset with ALLabel can achieve performance comparable to the methodannotating the entire dataset. Further analyses and ablation studies verify theeffectiveness and generalizability of our proposal.</description><author>Zihan Chen, Lei Shi, Weize Wu, Qiji Zhou, Yue Zhang</author><pubDate>Tue, 09 Sep 2025 08:47:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07512v1</guid></item><item><title>GLEAM: Learning to Match and Explain in Cross-View Geo-Localization</title><link>http://arxiv.org/abs/2509.07450v1</link><description>Cross-View Geo-Localization (CVGL) focuses on identifying correspondencesbetween images captured from distinct perspectives of the same geographicallocation. However, existing CVGL approaches are typically restricted to asingle view or modality, and their direct visual matching strategy lacksinterpretability: they merely predict whether two images correspond, withoutexplaining the rationale behind the match. In this paper, we present GLEAM-C, afoundational CVGL model that unifies multiple views and modalities-includingUAV imagery, street maps, panoramic views, and ground photographs-by aligningthem exclusively with satellite imagery. Our framework enhances trainingefficiency through optimized implementation while achieving accuracy comparableto prior modality-specific CVGL models through a two-phase training strategy.Moreover, to address the lack of interpretability in traditional CVGL methods,we leverage the reasoning capabilities of multimodal large language models(MLLMs) to propose a new task, GLEAM-X, which combines cross-viewcorrespondence prediction with explainable reasoning. To support this task, weconstruct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Proto generate training and testing data. The test set is further refined throughdetailed human revision, enabling systematic evaluation of explainablecross-view reasoning and advancing transparency and scalability ingeo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGLpipeline that integrates multi-modal, multi-view alignment with interpretablecorrespondence analysis, unifying accurate cross-view matching with explainablereasoning and advancing Geo-Localization by enabling models to better ExplainAnd Match. Code and datasets used in this work will be made publicly accessibleat https://github.com/Lucky-Lance/GLEAM.</description><author>Xudong Lu, Zhi Zheng, Yi Wan, Yongxiang Yao, Annan Wang, Renrui Zhang, Panwang Xia, Qiong Wu, Qingyun Li, Weifeng Lin, Xiangyu Zhao, Xue Yang, Hongsheng Li</author><pubDate>Tue, 09 Sep 2025 07:14:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07450v1</guid></item><item><title>CountQA: How Well Do MLLMs Count in the Wild?</title><link>http://arxiv.org/abs/2508.06585v2</link><description>Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency inunderstanding visual scenes, yet they exhibit a critical lack in a fundamentalcognitive skill: object counting. This blind spot severely limits theirreliability in real-world applications. To date, this capability has beenlargely unevaluated in complex scenarios, as existing benchmarks either featuresparse object densities or are confined to specific visual domains, failing totest models under realistic conditions. Addressing this gap, we introduceCountQA, a challenging new benchmark designed to probe this deficiency.Comprising over 1,500 question-answer pairs, CountQA features real-world imageswith high object density, clutter, and occlusion. We investigate this weaknessby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that thetop-performing model achieves a mere 42.9% accuracy, with performance decliningas object counts rise. By providing a dedicated benchmark to diagnose andrectify this core weakness, CountQA paves the way for a new generation of MLLMsthat are not only descriptively fluent but also numerically grounded andspatially aware. We will open-source the dataset and code upon paper acceptanceto foster further research.</description><author>Jayant Sravan Tamarapalli, Rynaa Grover, Nilay Pande, Sahiti Yerramilli</author><pubDate>Tue, 09 Sep 2025 04:46:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06585v2</guid></item><item><title>GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning</title><link>http://arxiv.org/abs/2506.00785v3</link><description>This paper introduces GeoChain, a large-scale benchmark for evaluatingstep-by-step geographic reasoning in multimodal large language models (MLLMs).Leveraging 1.46 million Mapillary street-level images, GeoChain pairs eachimage with a 21-step chain-of-thought (CoT) question sequence (over 30 millionQ&amp;A pairs). These sequences guide models from coarse attributes to fine-grainedlocalization across four reasoning categories - visual, spatial, cultural, andprecise geolocation - annotated by difficulty. Images are also enriched withsemantic segmentation (150 classes) and a visual locatability score. Ourbenchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5variants) on a diverse 2,088-image subset reveals consistent challenges: modelsfrequently exhibit weaknesses in visual grounding, display erratic reasoning,and struggle to achieve accurate localization, especially as the reasoningcomplexity escalates. GeoChain offers a robust diagnostic methodology, criticalfor fostering significant advancements in complex geographic reasoning withinMLLMs.</description><author>Sahiti Yerramilli, Nilay Pande, Rynaa Grover, Jayant Sravan Tamarapalli</author><pubDate>Tue, 09 Sep 2025 04:38:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.00785v3</guid></item><item><title>PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions</title><link>http://arxiv.org/abs/2509.07370v1</link><description>Recent advancements in Large Language Models (LLMs) demonstrate remarkablecapabilities across various fields. These developments have led to more directcommunication between humans and LLMs in various situations, such as socialcompanionship and psychological support. However, LLMs often exhibitlimitations in emotional perception and social competence during real-worldconversations. These limitations partly originate from their inability to adapttheir communication style and emotional expression to different social and taskcontexts. In this work, we introduce PersonaFuse, a novel LLM post-trainingframework that enables LLMs to adapt and express different personalities forvarying situations. Inspired by Trait Activation Theory and the Big Fivepersonality model, PersonaFuse employs a Mixture-of-Expert architecture thatcombines persona adapters with a dynamic routing network, enabling contextualtrait expression. Experimental results show that PersonaFuse substantiallyoutperforms baseline models across multiple dimensions of social-emotionalintelligence. Importantly, these gains are achieved without sacrificing generalreasoning ability or model safety, which remain common limitations of directprompting and supervised fine-tuning approaches. PersonaFuse also deliversconsistent improvements in downstream human-centered applications, such asmental health counseling and review-based customer service. Finally, humanpreference evaluations against leading LLMs, including GPT-4o and DeepSeek,demonstrate that PersonaFuse achieves competitive response quality despite itscomparatively smaller model size. These findings demonstrate thatPersonaFuse~offers a theoretically grounded and practical approach fordeveloping social-emotional enhanced LLMs, marking a significant advancementtoward more human-centric AI systems.</description><author>Yixuan Tang, Yi Yang, Ahmed Abbasi</author><pubDate>Tue, 09 Sep 2025 03:39:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07370v1</guid></item><item><title>Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond</title><link>http://arxiv.org/abs/2508.07353v3</link><description>The increasing demand for domain-specific evaluation of large language models(LLMs) has led to the development of numerous benchmarks. These efforts oftenadhere to the principle of data scaling, relying on large corpora or extensivequestion-answer (QA) sets to ensure broad coverage. However, the impact ofcorpus and QA set design on the precision and recall of domain-specific LLMperformance remains poorly understood. In this paper, we argue that datascaling is not always the optimal principle for domain-specific benchmarkconstruction. Instead, we introduce Comp-Comp, an iterative benchmarkingframework grounded in the principle of comprehensiveness and compactness.Comprehensiveness ensures semantic recall by covering the full breadth of thedomain, while compactness improves precision by reducing redundancy and noise.To demonstrate the effectiveness of our approach, we present a case studyconducted at a well-renowned university, resulting in the creation ofPolyBench, a large-scale, high-quality academic benchmark. Although this studyfocuses on academia, the Comp-Comp framework is domain-agnostic and readilyadaptable to a wide range of specialized fields. The source code and datasetscan be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.</description><author>Rubing Chen, Jiaxin Wu, Jian Wang, Xulu Zhang, Wenqi Fan, Chenghua Lin, Xiao-Yong Wei, Qing Li</author><pubDate>Tue, 09 Sep 2025 03:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07353v3</guid></item><item><title>Enhancing Traffic Incident Response through Sub-Second Temporal Localization with HybridMamba</title><link>http://arxiv.org/abs/2504.03235v2</link><description>Traffic crash detection in long-form surveillance videos is essential forimproving emergency response and infrastructure planning, yet remains difficultdue to the brief and infrequent nature of crash events. We present\textbf{HybridMamba}, a novel architecture integrating visual transformers withstate-space temporal modeling to achieve high-precision crash timelocalization. Our approach introduces multi-level token compression andhierarchical temporal processing to maintain computational efficiency withoutsacrificing temporal resolution. Evaluated on a large-scale dataset from theIowa Department of Transportation, HybridMamba achieves a mean absolute errorof \textbf{1.50 seconds} for 2-minute videos ($p&lt;0.01$ compared to baselines),with \textbf{65.2%} of predictions falling within one second of the groundtruth. It outperforms recent video-language models (e.g., TimeChat,VideoLLaMA-2) by up to 3.95 seconds while using significantly fewer parameters(3B vs. 13--72B). Our results demonstrate effective temporal localizationacross various video durations (2--40 minutes) and diverse environmentalconditions, highlighting HybridMamba's potential for fine-grained temporallocalization in traffic surveillance while identifying challenges that remainfor extended deployment.</description><author>Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</author><pubDate>Tue, 09 Sep 2025 02:48:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.03235v2</guid></item><item><title>RadGame: An AI-Powered Platform for Radiology Education</title><link>http://arxiv.org/abs/2509.13270v1</link><description>We introduce RadGame, an AI-powered gamified platform for radiology educationthat targets two core skills: localizing findings and generating reports.Traditional radiology training is based on passive exposure to cases or activepractice with real-time input from supervising radiologists, limitingopportunities for immediate and scalable feedback. RadGame addresses this gapby combining gamification with large-scale public datasets and automated,AI-driven feedback that provides clear, structured guidance to human learners.In RadGame Localize, players draw bounding boxes around abnormalities, whichare automatically compared to radiologist-drawn annotations from publicdatasets, and visual explanations are generated by vision-language models foruser missed findings. In RadGame Report, players compose findings given a chestX-ray, patient age and indication, and receive structured AI feedback based onradiology report generation metrics, highlighting errors and omissions comparedto a radiologist's written ground truth report from public datasets, producinga final performance and style score. In a prospective evaluation, participantsusing RadGame achieved a 68% improvement in localization accuracy compared to17% with traditional passive methods and a 31% improvement in report-writingaccuracy compared to 4% with traditional methods after seeing the same cases.RadGame highlights the potential of AI-driven gamification to deliver scalable,feedback-rich radiology training and reimagines the application of medical AIresources in education.</description><author>Mohammed Baharoon, Siavash Raissi, John S. Jun, Thibault Heintz, Mahmoud Alabbad, Ali Alburkani, Sung Eun Kim, Kent Kleinschmidt, Abdulrahman O. Alhumaydhi, Mohannad Mohammed G. Alghamdi, Jeremy Francis Palacio, Mohammed Bukhaytan, Noah Michael Prudlo, Rithvik Akula, Brady Chrisler, Benjamin Galligos, Mohammed O. Almutairi, Mazeen Mohammed Alanazi, Nasser M. Alrashdi, Joel Jihwan Hwang, Sri Sai Dinesh Jaliparthi, Luke David Nelson, Nathaniel Nguyen, Sathvik Suryadevara, Steven Kim, Mohammed F. Mohammed, Yevgeniy R. Semenov, Kun-Hsing Yu, Abdulrhman Aljouie, Hassan AlOmaish, Adam Rodman, Pranav Rajpurkar</author><pubDate>Tue, 16 Sep 2025 17:27:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13270v1</guid></item><item><title>Evaluating LLM Alignment on Personality Inference from Real-World Interview Data</title><link>http://arxiv.org/abs/2509.13244v1</link><description>Large Language Models (LLMs) are increasingly deployed in roles requiringnuanced psychological understanding, such as emotional support agents,counselors, and decision-making assistants. However, their ability to interprethuman personality traits, a critical aspect of such applications, remainsunexplored, particularly in ecologically valid conversational settings. Whileprior work has simulated LLM "personas" using discrete Big Five labels onsocial media data, the alignment of LLMs with continuous, ground-truthpersonality assessments derived from natural interactions is largelyunexamined. To address this gap, we introduce a novel benchmark comprisingsemi-structured interview transcripts paired with validated continuous Big Fivetrait scores. Using this dataset, we systematically evaluate LLM performanceacross three paradigms: (1) zero-shot and chain-of-thought prompting withGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMAarchitectures, and (3) regression using static embeddings from pretrained BERTand OpenAI's text-embedding-3-small. Our results reveal that all Pearsoncorrelations between model predictions and ground-truth personality traitsremain below 0.26, highlighting the limited alignment of current LLMs withvalidated psychological constructs. Chain-of-thought prompting offers minimalgains over zero-shot, suggesting that personality inference relies more onlatent semantic representation than explicit reasoning. These findingsunderscore the challenges of aligning LLMs with complex human attributes andmotivate future work on trait-specific prompting, context-aware modeling, andalignment-oriented fine-tuning.</description><author>Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin</author><pubDate>Tue, 16 Sep 2025 16:54:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13244v1</guid></item><item><title>Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning</title><link>http://arxiv.org/abs/2509.13127v1</link><description>Recent advancements in Large Language Models(LLMs) have led to thedevelopment of LLM-based AI agents. A key challenge is the creation of agentsthat can effectively ground themselves in complex, adversarial long-horizonenvironments. Existing methods mainly focus on (1) using LLMs as policies tointeract with the environment through generating low-level feasible actions,and (2) utilizing LLMs to generate high-level tasks or language guides tostimulate action generation. However, the former struggles to generate reliableactions, while the latter relies heavily on expert experience to translatehigh-level tasks into specific action sequences. To address these challenges,we introduce the Plan with Language, Act with Parameter (PLAP) planningframework that facilitates the grounding of LLM-based agents in long-horizonenvironments. The PLAP method comprises three key components: (1) a skilllibrary containing environment-specific parameterized skills, (2) a skillplanner powered by LLMs, and (3) a skill executor converting the parameterizedskills into executable action sequences. We implement PLAP in MicroRTS, along-horizon real-time strategy game that provides an unfamiliar andchallenging environment for LLMs. The experimental results demonstrate theeffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot settingoutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefullycrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.Additionally, we design comprehensive evaluation metrics and test 6closed-source and 2 open-source LLMs within the PLAP framework, ultimatelyreleasing an LLM leaderboard ranking long-horizon skill planning ability. Ourcode is available at https://github.com/AI-Research-TeamX/PLAP.</description><author>Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu</author><pubDate>Tue, 16 Sep 2025 14:36:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.13127v1</guid></item><item><title>Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding</title><link>http://arxiv.org/abs/2508.13804v2</link><description>How do Large Language Models understand moral dimensions compared to humans? This first large-scale Bayesian evaluation of market-leading language modelsprovides the answer. In contrast to prior work using deterministic ground truth(majority or inclusion rules), we model annotator disagreements to capture bothaleatoric uncertainty (inherent human disagreement) and epistemic uncertainty(model domain sensitivity). We evaluated the best language models (ClaudeSonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from nearly700 annotators in 100K+ texts spanning social networks, news and forums. Our GPU-optimized Bayesian framework processed 1M+ model queries, revealingthat AI models typically rank among the top 25\% of human annotators,performing much better than average balanced accuracy. Importantly, we findthat AI produces far fewer false negatives than humans, highlighting their moresensitive moral detection capabilities.</description><author>Maciej Skorski, Alina Landowska</author><pubDate>Mon, 22 Sep 2025 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13804v2</guid></item><item><title>UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning</title><link>http://arxiv.org/abs/2509.18094v1</link><description>Recent advances in Large Multi-modal Models (LMMs) have demonstrated theirremarkable success as general-purpose multi-modal assistants, with particularfocuses on holistic image- and video-language understanding. Conversely, lessattention has been given to scaling fine-grained pixel-level understandingcapabilities, where the models are expected to realize pixel-level alignmentbetween visual signals and language semantics. Some previous studies haveapplied LMMs to related tasks such as region-level captioning and referringexpression segmentation. However, these models are limited to performing eitherreferring or segmentation tasks independently and fail to integrate thesefine-grained perception capabilities into visual reasoning. To bridge this gap,we propose UniPixel, a large multi-modal model capable of flexiblycomprehending visual prompt inputs and generating mask-grounded responses. Ourmodel distinguishes itself by seamlessly integrating pixel-level perceptionwith general visual understanding capabilities. Specifically, UniPixelprocesses visual prompts and generates relevant masks on demand, and performssubsequent reasoning conditioning on these intermediate pointers duringinference, thereby enabling fine-grained pixel-level reasoning. Theeffectiveness of our approach has been verified on 10 benchmarks across adiverse set of tasks, including pixel-level referring/segmentation andobject-centric understanding in images/videos. A novel PixelQA task thatjointly requires referring, segmentation, and question answering is alsodesigned to verify the flexibility of our method.</description><author>Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen</author><pubDate>Mon, 22 Sep 2025 17:59:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.18094v1</guid></item><item><title>How Good are Foundation Models in Step-by-Step Embodied Reasoning?</title><link>http://arxiv.org/abs/2509.15293v2</link><description>Embodied agents operating in the physical world must make decisions that arenot only effective but also safe, spatially coherent, and grounded in context.While recent advances in large multimodal models (LMMs) have shown promisingcapabilities in visual understanding and language generation, their ability toperform structured reasoning for real-world embodied tasks remainsunderexplored. In this work, we aim to understand how well foundation modelscan perform step-by-step reasoning in embodied environments. To this end, wepropose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed toevaluate the reasoning capabilities of LMMs in complex embodied decision-makingscenarios. Our benchmark spans a diverse set of tasks that require agents tointerpret multimodal observations, reason about physical constraints andsafety, and generate valid next actions in natural language. We present (i) alarge-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluationframework that disentangles perceptual grounding from action reasoning, and(iii) empirical analysis of several leading LMMs under this setting. Ourbenchmark includes over 1.1k samples with detailed step-by-step reasoningacross 10 tasks and 8 embodiments, covering three different robot types. Ourresults highlight both the potential and current limitations of LMMs inembodied reasoning, pointing towards key challenges and opportunities forfuture research in robot intelligence. Our data and code will be made publiclyavailable.</description><author>Dinura Dissanayake, Ahmed Heakl, Omkar Thawakar, Noor Ahsan, Ritesh Thawkar, Ketan More, Jean Lahoud, Rao Anwer, Hisham Cholakkal, Ivan Laptev, Fahad Shahbaz Khan, Salman Khan</author><pubDate>Mon, 22 Sep 2025 17:44:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.15293v2</guid></item><item><title>TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs</title><link>http://arxiv.org/abs/2509.18056v1</link><description>This paper introduces TempSamp-R1, a new reinforcement fine-tuning frameworkdesigned to improve the effectiveness of adapting multimodal large languagemodels (MLLMs) to video temporal grounding tasks. We reveal that existingreinforcement learning methods, such as Group Relative Policy Optimization(GRPO), rely on on-policy sampling for policy updates. However, in tasks withlarge temporal search spaces, this strategy becomes both inefficient andlimited in performance, as it often fails to identify temporally accuratesolutions. To address this limitation, TempSamp-R1 leverages ground-truthannotations as off-policy supervision to provide temporally precise guidance,effectively compensating for the sparsity and misalignment in on-policysolutions. To further stabilize training and reduce variance in reward-basedupdates, TempSamp-R1 provides a non-linear soft advantage computation methodthat dynamically reshapes the reward feedback via an asymmetric transformation.By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1optimizes a single unified model to support both CoT and non-CoT inferencemodes, enabling efficient handling of queries with varying reasoningcomplexity. Experimental results demonstrate that TempSamp-R1 outperformsGRPO-based baselines, establishing new state-of-the-art performance onbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,TempSamp-R1 shows robust few-shot generalization capabilities under limiteddata. Code: https://github.com/HVision-NKU/TempSamp-R1</description><author>Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng</author><pubDate>Mon, 22 Sep 2025 17:30:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.18056v1</guid></item><item><title>Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration</title><link>http://arxiv.org/abs/2509.18008v1</link><description>Intelligent systems have traditionally been designed as tools rather thancollaborators, often lacking critical characteristics that collaborationpartnerships require. Recent advances in large language model (LLM) agents opennew opportunities for human-LLM-agent collaboration by enabling naturalcommunication and various social and cognitive behaviors. Yet it remainsunclear whether principles of computer-mediated collaboration established inHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.To support systematic investigations of these questions, we introduce an openand configurable research platform for HCI researchers. The platform's modulardesign allows seamless adaptation of classic CSCW experiments and manipulationof theory-grounded interaction controls. We demonstrate the platform'seffectiveness and usability through two case studies: (1) re-implementing theclassic human-human-collaboration task Shape Factory as a between-subjecthuman-agent-collaboration experiment with 16 participants, and (2) aparticipatory cognitive walkthrough with five HCI researchers to refineworkflows and interfaces for experiment setup and analysis.</description><author>Bingsheng Yao, Jiaju Chen, Chaoran Chen, April Wang, Toby Jia-jun Li, Dakuo Wang</author><pubDate>Mon, 22 Sep 2025 16:47:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.18008v1</guid></item><item><title>Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora</title><link>http://arxiv.org/abs/2509.17855v1</link><description>Dialects exhibit a substantial degree of variation due to the lack of astandard orthography. At the same time, the ability of Large Language Models(LLMs) to process dialects remains largely understudied. To address this gap,we use Bavarian as a case study and investigate the lexical dialectunderstanding capability of LLMs by examining how well they recognize andtranslate dialectal terms across different parts-of-speech. To this end, weintroduce DiaLemma, a novel annotation framework for creating dialect variationdictionaries from monolingual data only, and use it to compile a ground truthdataset consisting of 100K human-annotated German-Bavarian word pairs. Weevaluate how well nine state-of-the-art LLMs can judge Bavarian terms asdialect translations, inflected variants, or unrelated forms of a given Germanlemma. Our results show that LLMs perform best on nouns and lexically similarword pairs, and struggle most in distinguishing between direct translations andinflected variants. Interestingly, providing additional context in the form ofexample usages improves the translation performance, but reduces their abilityto recognize dialect variants. This study highlights the limitations of LLMs indealing with orthographic dialect variation and emphasizes the need for futurework on adapting LLMs to dialects.</description><author>Robert Litschko, Verena Blaschke, Diana Burkhardt, Barbara Plank, Diego Frassinelli</author><pubDate>Mon, 22 Sep 2025 14:49:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.17855v1</guid></item><item><title>TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?</title><link>http://arxiv.org/abs/2509.15602v2</link><description>Multimodal large language models (MLLMs) excel at general video understandingbut struggle with fast, high-frequency sports like tennis, where rally clipsare short yet information-dense. To systematically evaluate MLLMs in thischallenging domain, we present TennisTV, the first and most comprehensivebenchmark for tennis video understanding. TennisTV models each rally as atemporal-ordered sequence of consecutive stroke events, using automatedpipelines for filtering and question generation. It covers 9 tasks from thestroke level to the rally level and includes 2943 human-verified questions.Evaluating 17 representative MLLMs, we provide the first systematic assessmentof tennis video understanding. Results reveal substantial shortcomings andyield two key insights: (i) frame-sampling density should be tailored andbalanced across tasks, and (ii) improving temporal grounding is essential forstronger reasoning.</description><author>Zhongyuan Bao, Lejun Zhang</author><pubDate>Mon, 22 Sep 2025 14:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.15602v2</guid></item><item><title>LLaSA: A Sensor-Aware LLM for Natural Language Reasoning of Human Activity from IMU Data</title><link>http://arxiv.org/abs/2406.14498v4</link><description>Wearable systems can recognize activities from IMU data but often fail toexplain their underlying causes or contextual significance. To address thislimitation, we introduce two large-scale resources: SensorCap, comprising35,960 IMU--caption pairs, and OpenSQA, with 199,701 question--answer pairsdesigned for causal and explanatory reasoning. OpenSQA includes a curatedtuning split (Tune-OpenSQA) optimized for scientific accuracy, narrativeclarity, and diagnostic insight. Leveraging these datasets, we develop LLaSA(Large Language and Sensor Assistant), a family of compact sensor-awarelanguage models (7B and 13B) that generate interpretable, context-richresponses to open-ended questions grounded in raw IMU data. LLaSA outperformscommercial LLMs, including GPT-3.5 and GPT-4o-mini, on benchmark and real-worldtasks, demonstrating the effectiveness of domain supervision and modelalignment for sensor reasoning. Our code repository and datasets can be foundat https://github.com/BASHLab/LLaSA.</description><author>Sheikh Asif Imran, Mohammad Nur Hossain Khan, Subrata Biswas, Bashima Islam</author><pubDate>Mon, 22 Sep 2025 14:02:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14498v4</guid></item></channel></rss>