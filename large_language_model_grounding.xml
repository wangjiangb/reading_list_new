<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 26 Aug 2025 13:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Controlling Multimodal LLMs via Reward-guided Decoding</title><link>http://arxiv.org/abs/2508.11616v1</link><description>As Multimodal Large Language Models (MLLMs) gain widespread applicability, itis becoming increasingly desirable to adapt them for diverse user needs. Inthis paper, we study the adaptation of MLLMs through controlled decoding. Toachieve this, we introduce the first method for reward-guided decoding of MLLMsand demonstrate its application in improving their visual grounding. Our methodinvolves building reward models for visual grounding and using them to guidethe MLLM's decoding process. Concretely, we build two separate reward models toindependently control the degree of object precision and recall in the model'soutput. Our approach enables on-the-fly controllability of an MLLM's inferenceprocess in two ways: first, by giving control over the relative importance ofeach reward function during decoding, allowing a user to dynamically trade offobject precision for recall in image captioning tasks; second, by givingcontrol over the breadth of the search during decoding, allowing the user tocontrol the trade-off between the amount of test-time compute and the degree ofvisual grounding. We evaluate our method on standard object hallucinationbenchmarks, showing that it provides significant controllability over MLLMinference, while consistently outperforming existing hallucination mitigationmethods.</description><author>Oscar Mañas, Pierluca D'Oro, Koustuv Sinha, Adriana Romero-Soriano, Michal Drozdzal, Aishwarya Agrawal</author><pubDate>Fri, 15 Aug 2025 17:29:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11616v1</guid></item><item><title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title><link>http://arxiv.org/abs/2508.11538v1</link><description>Video reasoning segmentation (VRS) endeavors to delineate referred objects invideos guided by implicit instructions that encapsulate human intent andtemporal logic. Previous approaches leverage large vision language models(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.However, this paradigm suffers from limited interpretability during inferenceand suboptimal performance due to inadequate spatiotemporal reasoning. Drawinginspiration from seminal breakthroughs in reinforcement learning, we introduceVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning insegmentation. Veason-R1 is trained through Group Relative Policy Optimization(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, wecurate high-quality CoT training data to instill structured reasoningtrajectories, bridging video-level semantics and frame-level spatial grounding,yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPOfine-tuning encourages efficient exploration of the reasoning space byoptimizing reasoning chains. To this end, we incorporate a holistic rewardmechanism that synergistically enhances spatial alignment and temporalconsistency, bolstering keyframe localization and fine-grained grounding.Comprehensive empirical evaluations demonstrate that Veason-R1 achievesstate-of-the-art performance on multiple benchmarks, surpassing prior art bysignificant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),while exhibiting robustness to hallucinations (+8.8 R). Our code and modelweights will be available at Veason-R1.</description><author>Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</author><pubDate>Fri, 15 Aug 2025 15:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11538v1</guid></item><item><title>A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow</title><link>http://arxiv.org/abs/2508.11529v1</link><description>Artificial intelligence is reshaping science and industry, yet many usersstill regard its models as opaque "black boxes". Conventional explainableartificial-intelligence methods clarify individual predictions but overlook theupstream decisions and downstream quality checks that determine whetherinsights can be trusted. In this work, we present Holistic ExplainableArtificial Intelligence (HXAI), a user-centric framework that embedsexplanation into every stage of the data-analysis workflow and tailors thoseexplanations to users. HXAI unifies six components (data, analysis set-up,learning process, model output, model quality, communication channel) into asingle taxonomy and aligns each component with the needs of domain experts,data analysts and data scientists. A 112-item question bank covers these needs;our survey of contemporary tools highlights critical coverage gaps. Grounded intheories of human explanation, principles from human-computer interaction andfindings from empirical user studies, HXAI identifies the characteristics thatmake explanations clear, actionable and cognitively manageable. A comprehensivetaxonomy operationalises these insights, reducing terminological ambiguity andenabling rigorous coverage analysis of existing toolchains. We furtherdemonstrate how AI agents that embed large-language models can orchestratediverse explanation techniques, translating technical artifacts intostakeholder-specific narratives that bridge the gap between AI developers anddomain experts. Departing from traditional surveys or perspective articles,this work melds concepts from multiple disciplines, lessons from real-worldprojects and a critical synthesis of the literature to advance a novel,end-to-end viewpoint on transparency, trustworthiness and responsible AIdeployment.</description><author>George Paterakis, Andrea Castellani, George Papoutsoglou, Tobias Rodemann, Ioannis Tsamardinos</author><pubDate>Fri, 15 Aug 2025 15:15:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11529v1</guid></item><item><title>UI-Venus Technical Report: Building High-performance UI Agents with RFT</title><link>http://arxiv.org/abs/2508.10833v2</link><description>We present UI-Venus, a native UI agent that takes only screenshots as inputbased on a multimodal large language model. UI-Venus achieves SOTA performanceon both UI grounding and navigation tasks using only several hundred thousandhigh-quality training samples through reinforcement finetune (RFT) based onQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,Screenspot-V2 / Pro, surpassing the previous SOTA baselines includingopen-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary andplaning ability, we also evaluate it on the AndroidWorld, an online UInavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%success rate, also beating existing models. To achieve this, we introducecarefully designed reward functions for both UI grounding and navigation tasksand corresponding efficient data cleaning strategies. To further boostnavigation performance, we propose Self-Evolving Trajectory History Alignment &amp;Sparse Action Enhancement that refine historical reasoning traces and balancesthe distribution of sparse but critical actions, leading to more coherentplanning and better generalization in complex UI tasks. Our contributionsinclude the publish of SOTA open-source UI agents, comprehensive data cleaningprotocols and a novel self-evolving framework for improving navigationperformance, which encourage further research and development in the community.Code is available at https://github.com/inclusionAI/UI-Venus.</description><author>Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</author><pubDate>Fri, 15 Aug 2025 14:49:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10833v2</guid></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2507.01006v5</link><description>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models(VLMs) designed to advance general-purpose multimodal understanding andreasoning. In this report, we share our key findings in the development of thereasoning-centric training framework. We first develop a capable visionfoundation model with significant potential through large-scale pre-training,which arguably sets the upper bound for the final performance. We then proposeReinforcement Learning with Curriculum Sampling (RLCS) to unlock the fullpotential of the model, leading to comprehensive capability enhancement acrossa diverse range of tasks, including STEM problem solving, video understanding,content recognition, coding, grounding, GUI-based agents, and long documentinterpretation. In a comprehensive evaluation across 42 public benchmarks,GLM-4.5V achieves state-of-the-art performance on nearly all tasks amongopen-source models of similar size, and demonstrates competitive or evensuperior results compared to closed-source models such as Gemini-2.5-Flash onchallenging tasks including Coding and GUI Agents. Meanwhile, the smallerGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results tothe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source bothGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information arereleased at https://github.com/zai-org/GLM-V.</description><author>GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan</author><pubDate>Fri, 15 Aug 2025 13:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01006v5</guid></item><item><title>MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</title><link>http://arxiv.org/abs/2508.11433v1</link><description>Multimodal Large Language Models (MLLMs) with unified architectures excelacross a wide range of vision-language tasks, yet aligning them withpersonalized image generation remains a significant challenge. Existing methodsfor MLLMs are frequently subject-specific, demanding a data-intensivefine-tuning process for every new subject, which limits their scalability. Inthis paper, we introduce MM-R1, a framework that integrates a cross-modalChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential ofunified MLLMs for personalized image generation. Specifically, we structurepersonalization as an integrated visual reasoning and generation process: (1)grounding subject concepts by interpreting and understanding user-providedimages and contextual cues, and (2) generating personalized images conditionedon both the extracted subject representations and user prompts. To furtherenhance the reasoning capability, we adopt Grouped Reward Proximal PolicyOptimization (GRPO) to explicitly align the generation. Experiments demonstratethat MM-R1 unleashes the personalization capability of unified MLLMs togenerate images with high subject fidelity and strong text alignment in azero-shot manner.</description><author>Qian Liang, Yujia Wu, Kuncheng Li, Jiwei Wei, Shiyuan He, Jinyu Guo, Ning Xie</author><pubDate>Fri, 15 Aug 2025 12:20:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11433v1</guid></item><item><title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title><link>http://arxiv.org/abs/2506.05982v4</link><description>As automated attack techniques rapidly advance, CAPTCHAs remain a criticaldefense mechanism against malicious bots. However, existing CAPTCHA schemesencompass a diverse range of modalities -- from static distorted text andobfuscated images to interactive clicks, sliding puzzles, and logic-basedquestions -- yet the community still lacks a unified, large-scale, multimodalbenchmark to rigorously evaluate their security robustness. To address thisgap, we introduce MCA-Bench, a comprehensive and reproducible benchmarkingsuite that integrates heterogeneous CAPTCHA types into a single evaluationprotocol. Leveraging a shared vision-language model backbone, we fine-tunespecialized cracking agents for each CAPTCHA category, enabling consistent,cross-modal assessments. Extensive experiments reveal that MCA-Bencheffectively maps the vulnerability spectrum of modern CAPTCHA designs undervaried attack settings, and crucially offers the first quantitative analysis ofhow challenge complexity, interaction depth, and model solvability interrelate.Based on these findings, we propose three actionable design principles andidentify key open challenges, laying the groundwork for systematic CAPTCHAhardening, fair benchmarking, and broader community collaboration. Datasets andcode are available online.</description><author>Zonglin Wu, Yule Xue, Yaoyao Feng, Xiaolong Wang, Yiren Song</author><pubDate>Fri, 15 Aug 2025 10:39:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.05982v4</guid></item><item><title>SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models</title><link>http://arxiv.org/abs/2504.18684v2</link><description>Interpreting object-referential language and grounding objects in 3D withspatial relations and attributes is essential for robots operating alongsidehumans. However, this task is often challenging due to the diversity of scenes,large number of fine-grained objects, and complex free-form nature of languagereferences. Furthermore, in the 3D domain, obtaining large amounts of naturallanguage training data is difficult. Thus, it is important for methods to learnfrom little data and zero-shot generalize to new environments. To address thesechallenges, we propose SORT3D, an approach that utilizes rich object attributesfrom 2D data and merges a heuristics-based spatial reasoning toolbox with theability of large language models (LLMs) to perform sequential reasoning.Importantly, our method does not require text-to-3D data for training and canbe applied zero-shot to unseen environments. We show that SORT3D achievesstate-of-the-art zero-shot performance on complex view-dependent groundingtasks on two benchmarks. We also implement the pipeline to run real-time on twoautonomous vehicles and demonstrate that our approach can be used forobject-goal navigation on previously unseen real-world environments. All sourcecode for the system pipeline is publicly released athttps://github.com/nzantout/SORT3D.</description><author>Nader Zantout, Haochen Zhang, Pujith Kachana, Jinkai Qiu, Guofei Chen, Ji Zhang, Wenshan Wang</author><pubDate>Fri, 15 Aug 2025 00:22:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.18684v2</guid></item><item><title>Prototype-Guided Diffusion: Visual Conditioning without External Memory</title><link>http://arxiv.org/abs/2508.09922v2</link><description>Diffusion models have emerged as a leading framework for high-quality imagegeneration, offering stable training and strong performance across diversedomains. However, they remain computationally intensive, particularly duringthe iterative denoising process. Latent-space models like Stable Diffusionalleviate some of this cost by operating in compressed representations, thoughat the expense of fine-grained detail. More recent approaches such asRetrieval-Augmented Diffusion Models (RDM) address efficiency by conditioningdenoising on similar examples retrieved from large external memory banks. Whileeffective, these methods introduce drawbacks: they require costly storage andretrieval infrastructure, depend on static vision-language models like CLIP forsimilarity, and lack adaptability during training. We propose the PrototypeDiffusion Model (PDM), a method that integrates prototype learning directlyinto the diffusion process for efficient and adaptive visual conditioning -without external memory. Instead of retrieving reference samples, PDMconstructs a dynamic set of compact visual prototypes from clean image featuresusing contrastive learning. These prototypes guide the denoising steps byaligning noisy representations with semantically relevant visual patterns,enabling efficient generation with strong semantic grounding. Experiments showthat PDM maintains high generation quality while reducing computational andstorage overhead, offering a scalable alternative to retrieval-basedconditioning in diffusion models.</description><author>Bilal Faye, Hanane Azzag, Mustapha Lebbah</author><pubDate>Thu, 14 Aug 2025 21:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09922v2</guid></item><item><title>Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs</title><link>http://arxiv.org/abs/2508.11068v1</link><description>Abstract meaning representation (AMR) is a semantic formalism used torepresent the meaning of sentences as directed acyclic graphs. In this paper,we describe how real digital dictionaries can be embedded into AMR directedgraphs (digraphs), using state-of-the-art pre-trained large language models.Then, we reduce those graphs in a confluent manner, i.e. with transformationsthat preserve their circuit space. Finally, the properties of these reducesdigraphs are analyzed and discussed in relation to the symbol groundingproblem.</description><author>Nicolas Goulet, Alexandre Blondin Massé, Moussa Abdendi</author><pubDate>Thu, 14 Aug 2025 20:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11068v1</guid></item><item><title>Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?</title><link>http://arxiv.org/abs/2508.11011v1</link><description>Construction safety inspections typically involve a human inspectoridentifying safety concerns on-site. With the rise of powerful Vision LanguageModels (VLMs), researchers are exploring their use for tasks such as detectingsafety rule violations from on-site images. However, there is a lack of opendatasets to comprehensively evaluate and further fine-tune VLMs in constructionsafety inspection. Current applications of VLMs use small, supervised datasets,limiting their applicability in tasks they are not directly trained for. Inthis paper, we propose the ConstructionSite 10k, featuring 10,000 constructionsite images with annotations for three inter-connected tasks, including imagecaptioning, safety rule violation visual question answering (VQA), andconstruction element visual grounding. Our subsequent evaluation of currentstate-of-the-art large pre-trained VLMs shows notable generalization abilitiesin zero-shot and few-shot settings, while additional training is needed to makethem applicable to actual construction sites. This dataset allows researchersto train and evaluate their own VLMs with new architectures and techniques,providing a valuable benchmark for construction safety inspection.</description><author>Xuezheng Chen, Zhengbo Zou</author><pubDate>Thu, 14 Aug 2025 18:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11011v1</guid></item><item><title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title><link>http://arxiv.org/abs/2508.11009v1</link><description>The rapid proliferation of large language models (LLMs) in applicationstargeting children and adolescents necessitates a fundamental reassessment ofprevailing AI safety frameworks, which are largely tailored to adult users andneglect the distinct developmental vulnerabilities of minors. This paperhighlights key deficiencies in existing LLM safety benchmarks, including theirinadequate coverage of age-specific cognitive, emotional, and social risksspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence(13--18). To bridge these gaps, we introduce SproutBench, an innovativeevaluation suite comprising 1,283 developmentally grounded adversarial promptsdesigned to probe risks such as emotional dependency, privacy violations, andimitation of hazardous behaviors. Through rigorous empirical evaluation of 47diverse LLMs, we uncover substantial safety vulnerabilities, corroborated byrobust inter-dimensional correlations (e.g., between Safety and RiskPrevention) and a notable inverse relationship between Interactivity and AgeAppropriateness. These insights yield practical guidelines for advancingchild-centric AI design and deployment.</description><author>Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han</author><pubDate>Thu, 14 Aug 2025 18:21:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11009v1</guid></item><item><title>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</title><link>http://arxiv.org/abs/2508.13023v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhancedthe reasoning abilities of large language models (LLMs). Its success, however,largely depends on strong base models with rich world knowledge, yielding onlymodest improvements for small-size language models (SLMs). To address thislimitation, we investigate Guided GRPO, which injects ground-truth reasoningsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.Through a comprehensive study of various guidance configurations, we find thatnaively adding guidance delivers limited gains. These insights motivateG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strengthin response to the model's evolving training dynamics. Experiments onmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-Asubstantially outperforms vanilla GRPO. Our code and models are available athttps://github.com/T-Lab-CUHKSZ/G2RPO-A.</description><author>Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</author><pubDate>Mon, 18 Aug 2025 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13023v1</guid></item><item><title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</title><link>http://arxiv.org/abs/2505.14425v2</link><description>Instruction-tuned large language models (LLMs) have shown strong performanceon a variety of tasks; however, generalizing from synthetic to human-authoredinstructions in grounded environments remains a challenge for them. In thiswork, we study generalization challenges in spatial grounding tasks wheremodels interpret and translate instructions for building object arrangements ona $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluatetheir performance on a benchmark dataset containing both synthetic andhuman-written instructions. Our results reveal that while models generalizewell on simple tasks, their performance degrades significantly on more complextasks. We present a detailed error analysis of the gaps in instructiongeneralization.</description><author>Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen</author><pubDate>Mon, 18 Aug 2025 15:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.14425v2</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title><link>http://arxiv.org/abs/2507.22752v2</link><description>We introduce CUS-QA, a benchmark for open-ended regional question answeringthat encompasses both textual and visual modalities. We also provide strongbaselines using state-of-the-art large language models (LLMs). Our datasetconsists of manually curated questions and answers grounded in Wikipedia,created by native speakers from Czechia, Slovakia, and Ukraine, withaccompanying English translations. It includes both purely textual questionsand those requiring visual understanding. We evaluate state-of-the-art LLMsthrough prompting and complement this with human judgments of answercorrectness. Using these human evaluations, we analyze the reliability ofexisting automatic evaluation metrics. Our baseline results show that even thebest open-weight LLMs achieve only around 50% accuracy on textual questions andbelow 30% on visual questions. LLM-based evaluation metrics show strongcorrelation with human judgment, while traditional string-overlap metricsperform surprisingly well due to the prevalence of named entities in answers.</description><author>Jindřich Libovický, Jindřich Helcl, Andrei Manea, Gianluca Vico</author><pubDate>Thu, 21 Aug 2025 12:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22752v2</guid></item><item><title>Referring Expression Instance Retrieval and A Strong End-to-End Baseline</title><link>http://arxiv.org/abs/2506.18246v4</link><description>Using natural language to query visual information is a fundamental need inreal-world applications. Text-Image Retrieval (TIR) retrieves a target imagefrom a gallery based on an image-level description, while Referring ExpressionComprehension (REC) localizes a target object within a given image using aninstance-level description. However, real-world applications often present morecomplex demands. Users typically query an instance-level description across alarge gallery and expect to receive both relevant image and the correspondinginstance location. In such scenarios, TIR struggles with fine-graineddescriptions and object-level localization, while REC is limited in its abilityto efficiently search large galleries and lacks an effective ranking mechanism.In this paper, we introduce a new task called \textbf{Referring ExpressionInstance Retrieval (REIR)}, which supports both instance-level retrieval andlocalization based on fine-grained referring expressions. First, we propose alarge-scale benchmark for REIR, named REIRCOCO, constructed by promptingadvanced vision-language models to generate high-quality referring expressionsfor instances in the MSCOCO and RefCOCO datasets. Second, we present a baselinemethod, Contrastive Language-Instance Alignment with Relation Experts (CLARE),which employs a dual-stream architecture to address REIR in an end-to-endmanner. Given a referring expression, the textual branch encodes it into aquery embedding. The visual branch detects candidate objects and extracts theirinstance-level visual features. The most similar candidate to the query isselected for bounding box prediction. CLARE is first trained on objectdetection and REC datasets to establish initial grounding capabilities, thenoptimized via Contrastive Language-Instance Alignment (CLIA) for improvedretrieval across images. We will release our code and benchmark publicly.</description><author>Xiangzhao Hao, Kuan Zhu, Hongyu Guo, Haiyun Guo, Ning Jiang, Quan Lu, Ming Tang, Jinqiao Wang</author><pubDate>Thu, 21 Aug 2025 09:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.18246v4</guid></item><item><title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title><link>http://arxiv.org/abs/2506.06382v5</link><description>This paper establishes a fundamental impossibility theorem: no LLM capable ofperforming non-trivial knowledge aggregation can simultaneously achievetruthful knowledge representation, semantic information conservation, completerevelation of relevant knowledge, and knowledge-constrained optimality. Theimpossibility is not an engineering limitation but arises from the mathematicalstructure of information aggregation itself. We establish this result by describing the inference process as an auction ofideas, where distributed components compete exploiting their partial knowledgeto shape responses. The proof spans three independent mathematical domains:mechanism design theory (Green-Laffont), the theory of proper scoring rules(Savage), and direct architectural analysis of transformers (Log-Sum-Expconvexity). In particular, we show how to quantify the creation ofoverconfident or intuitive responses-the signature of both hallucination andcreativity, or imagination. To support this analysis, we introduce the complementary concepts of thesemantic information measure and the emergence operator to model boundedreasoning in a general setting. We prove that while bounded reasoning generatesaccessible information, providing valuable insights and inspirations, theidealized unconstrained reasoning strictly preserves semantic content. By demonstrating that hallucination and imagination are mathematicallyidentical phenomena-grounded in departures from truthfulness, semanticinformation conservation, revelation of relevant knowledge, andknowledge-constrained optimality-we offer a principled foundation for managingthese behaviors in advanced AI systems. Finally, we present some speculativeideas to inspire evaluation and refinements of the proposed theory.</description><author>Michał P. Karpowicz</author><pubDate>Thu, 21 Aug 2025 08:58:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.06382v5</guid></item><item><title>Coarse-to-Fine Grounded Memory for LLM Agent Planning</title><link>http://arxiv.org/abs/2508.15305v1</link><description>Recent advancements in Large Language Models (LLMs) have driven growinginterest in LLM-based agents for complex planning tasks. To avoid costly agenttraining, many studies adopted memory mechanism that enhances LLM with offlineexperiences or online trajectory analysis. However, existing works focus onsingle-granularity memory derived from dynamic environmental interactions,which are inherently constrained by the quality of the collected experiences.This limitation, in turn, constrain the diversity of knowledge and theflexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), anovel framework that grounds coarse-to-fine memories with LLM, thereby fullyleverage them for flexible adaptation to diverse scenarios. \Ours{} groundsenvironmental information into coarse-grained focus points to guide experiencecollection in training tasks, followed by grounding of actionablehybrid-grained tips from each experience. At inference, \Ours{} retrievestask-relevant experiences and tips to support planning. When facingenvironmental anomalies, the LLM grounds the current situation intofine-grained key information, enabling flexible self-QA reflection and plancorrection.</description><author>Wei Yang, Jinwei Xiao, Hongming Zhang, Qingyang Zhang, Yanna Wang, Bo Xu</author><pubDate>Thu, 21 Aug 2025 06:50:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15305v1</guid></item><item><title>Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models</title><link>http://arxiv.org/abs/2410.03290v2</link><description>Video Large Language Models (Video-LLMs) have demonstrated remarkablecapabilities in coarse-grained video understanding, however, they struggle withfine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,a novel Video-LLM adept at perceiving and reasoning over specific video momentsin a fine-grained manner. We identify that current Video-LLMs have limitationsfor fine-grained video understanding since they lack effective temporalmodeling and timestamp representation. In light of this, we sharpen our modelby incorporating (1) an additional temporal stream to encode the relationshipsbetween frames and (2) discrete temporal tokens enriched with specific timeknowledge to represent timestamps. To optimize the training ofGrounded-VideoLLM, we employ a multi-stage training scheme, beginning withsimple video-captioning tasks and progressively introducing video temporalgrounding tasks of increasing complexity. To further enhanceGrounded-VideoLLM's temporal reasoning capability, we also curate a groundedVideoQA dataset by an automatic annotation pipeline. Extensive experimentsdemonstrate that Grounded-VideoLLM not only excels in fine-grained groundingtasks such as temporal sentence grounding, dense video captioning, and groundedVideoQA, but also shows great potential as a versatile video assistant forgeneral video understanding.</description><author>Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, Lifu Huang</author><pubDate>Thu, 21 Aug 2025 05:15:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.03290v2</guid></item><item><title>WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai</title><link>http://arxiv.org/abs/2508.15239v1</link><description>Large language models excel at instruction-following in English, but theirperformance in low-resource languages like Thai remains underexplored. Existingbenchmarks often rely on translations, missing cultural and domain-specificnuances needed for real-world use. We present WangchanThaiInstruct, ahuman-authored Thai dataset for evaluation and instruction tuning, coveringfour professional domains and seven task types. Created through a multi-stagequality control process with annotators, domain experts, and AI researchers,WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showingperformance gaps on culturally and professionally specific tasks, and (2) aninstruction tuning study with ablations isolating the effect of nativesupervision. Models fine-tuned on WangchanThaiInstruct outperform those usingtranslated data in both in-domain and out-of-domain benchmarks. These findingsunderscore the need for culturally and professionally grounded instruction datato improve LLM alignment in low-resource, linguistically diverse settings.</description><author>Peerat Limkonchotiwat, Pume Tuchinda, Lalita Lowphansirikul, Surapon Nonesung, Panuthep Tasawong, Alham Fikri Aji, Can Udomcharoenchaikit, Sarana Nutanong</author><pubDate>Thu, 21 Aug 2025 04:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15239v1</guid></item><item><title>AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation</title><link>http://arxiv.org/abs/2508.15232v1</link><description>Aerial Vision-and-Language Navigation (VLN) is an emerging task that enablesUnmanned Aerial Vehicles (UAVs) to navigate outdoor environments using naturallanguage instructions and visual cues. However, due to the extendedtrajectories and complex maneuverability of UAVs, achieving reliable UAV-VLNperformance is challenging and often requires human intervention or overlydetailed instructions. To harness the advantages of UAVs' high mobility, whichcould provide multi-grained perspectives, while maintaining a manageable motionspace for learning, we introduce a novel task called Dual-Altitude UAVCollaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinctaltitudes: a high-altitude UAV responsible for broad environmental reasoning,and a low-altitude UAV tasked with precise navigation. To support the trainingand evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising13,838 collaborative high-low UAV demonstration trajectories, each paired withtarget-oriented language instructions. This dataset includes both unseen mapsand an unseen object validation set to systematically evaluate the model'sgeneralization capabilities across novel environments and unfamiliar targets.To consolidate their complementary strengths, we propose a dual-UAVcollaborative VLN framework, AeroDuo, where the high-altitude UAV integrates amultimodal large language model (Pilot-LLM) for target reasoning, while thelow-altitude UAV employs a lightweight multi-stage policy for navigation andtarget grounding. The two UAVs work collaboratively and only exchange minimalcoordinate information to ensure efficiency.</description><author>Ruipu Wu, Yige Zhang, Jinyu Chen, Linjiang Huang, Shifeng Zhang, Xu Zhou, Liang Wang, Si Liu</author><pubDate>Thu, 21 Aug 2025 04:43:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15232v1</guid></item><item><title>R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling</title><link>http://arxiv.org/abs/2508.15204v1</link><description>Effective scheduling under tight resource, timing, and operationalconstraints underpins large-scale planning across sectors such as capitalprojects, manufacturing, logistics, and IT fleet transitions. However, thereliability of large language models (LLMs) when reasoning underhigh-constraint regimes is insufficiently characterized. To address this gap,we present R-ConstraintBench, a scalable framework that evaluates models onResource-Constrained Project Scheduling Problems (RCPSP), an NP-Completefeasibility class, while difficulty increases via linear growth in constraints.R-ConstraintBench incrementally increases non-redundant precedence constraintsin Directed Acyclic Graphs (DAGs) and then introduces downtime, temporalwindows, and disjunctive constraints. As an illustrative example, weinstantiate the benchmark in a data center migration setting and evaluatemultiple LLMs using feasibility and error analysis, identifying degradationthresholds and constraint types most associated with failure. Empirically,strong models are near-ceiling on precedence-only DAGs, but feasibilityperformance collapses when downtime, temporal windows, and disjunctiveconstraints interact, implicating constraint interaction, not graph depth, asthe principal bottleneck. Performance on clean synthetic ramps also does notguarantee transfer to domain-grounded scenarios, underscoring limitedgeneralization.</description><author>Raj Jain, Marc Wetter</author><pubDate>Thu, 21 Aug 2025 03:35:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15204v1</guid></item><item><title>ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following</title><link>http://arxiv.org/abs/2508.15164v1</link><description>Despite significant advancements in Large Language Models (LLMs) and LargeVision-Language Models (LVLMs), current models still face substantialchallenges in handling complex, multi-turn, and visually-grounded tasks thatdemand deep reasoning, sustained contextual understanding, entity tracking, andmulti-step instruction following. Existing benchmarks often fall short incapturing the dynamism and intricacies of real-world multi-modal interactions,leading to issues such as context loss and visual hallucinations. To addressthese limitations, we introduce MMDR-Bench (Multi-Modal Dialogue ReasoningBenchmark), a novel dataset comprising 300 meticulously designed complexmulti-turn dialogue scenarios, each averaging 5-7 turns and evaluated acrosssix core dimensions including visual entity tracking and reasoning depth.Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holisticframework that enhances existing LVLMs with advanced reasoning and instructionfollowing capabilities through an iterative"memory-perception-planning-execution" cycle, requiring no extensivere-training of the underlying models. Our extensive experiments on MMDR-Benchdemonstrate that CoLVLM Agent consistently achieves superior performance,attaining an average human evaluation score of 4.03, notably surpassingstate-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro(3.85). The framework exhibits significant advantages in reasoning depth,instruction adherence, and error suppression, and maintains robust performanceover extended dialogue turns, validating the effectiveness of its modulardesign and iterative approach for complex multi-modal interactions.</description><author>Seungmin Han, Haeun Kwon, Ji-jun Park, Taeyang Yoon</author><pubDate>Thu, 21 Aug 2025 02:09:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15164v1</guid></item><item><title>KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis</title><link>http://arxiv.org/abs/2507.03847v2</link><description>Large Language Models (LLMs) frequently generate hallucinations: statementsthat are syntactically plausible but lack factual grounding. This researchpresents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework thatdetects and explains such hallucinations by comparing knowledge graphsconstructed from LLM outputs with ground truth data from Wikidata or contextualdocuments. Using graph kernels and semantic clustering, the method providesexplanations for detected hallucinations, ensuring both robustness andinterpretability. Our framework achieves competitive accuracy in detectinghallucinations across both open- and closed-domain tasks, and is able togenerate contrastive explanations, enhancing transparency. This researchadvances the reliability of LLMs in high-stakes domains and provides afoundation for future work on precision improvements and multi-source knowledgeintegration.</description><author>Reilly Haskins, Benjamin Adams</author><pubDate>Thu, 21 Aug 2025 01:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.03847v2</guid></item><item><title>aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</title><link>http://arxiv.org/abs/2508.15126v1</link><description>Recent advances in large language models (LLMs) have enabled AI agents toautonomously generate scientific proposals, conduct experiments, author papers,and perform peer reviews. Yet this flood of AI-generated research contentcollides with a fragmented and largely closed publication ecosystem.Traditional journals and conferences rely on human peer review, making themdifficult to scale and often reluctant to accept AI-generated research content;existing preprint servers (e.g. arXiv) lack rigorous quality-controlmechanisms. Consequently, a significant amount of high-quality AI-generatedresearch lacks appropriate venues for dissemination, hindering its potential toadvance scientific progress. To address these challenges, we introduce aiXiv, anext-generation open-access platform for human and AI scientists. Itsmulti-agent architecture allows research proposals and papers to be submitted,reviewed, and iteratively refined by both human and AI scientists. It alsoprovides API and MCP interfaces that enable seamless integration ofheterogeneous human and AI scientists, creating a scalable and extensibleecosystem for autonomous scientific discovery. Through extensive experiments,we demonstrate that aiXiv is a reliable and robust platform that significantlyenhances the quality of AI-generated research proposals and papers afteriterative revising and reviewing on aiXiv. Our work lays the groundwork for anext-generation open-access ecosystem for AI scientists, accelerating thepublication and dissemination of high-quality AI-generated research content.Code is available at https://github.com/aixiv-org. Website is available athttps://forms.gle/DxQgCtXFsJ4paMtn8.</description><author>Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang, Xiuxu Li, Jiaxing Song, Jiabin Luo, Yijiang Li, Shuo Yin, Chengxiao Dai, Eric Hanchen Jiang, Xiaoyan Zhou, Zhenfei Yin, Boqin Yuan, Jing Dong, Guinan Su, Guanren Qiao, Haiming Tang, Anghong Du, Lili Pan, Zhenzhong Lan, Xinyu Liu</author><pubDate>Wed, 20 Aug 2025 23:16:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15126v1</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</title><link>http://arxiv.org/abs/2508.14735v1</link><description>Large language models (LLMs) are increasingly applied in multilingualcontexts, yet their capacity for consistent, logically grounded alignmentacross languages remains underexplored. We present a controlled evaluationframework for multilingual natural language inference (NLI) that generatessynthetic, logic-based premise-hypothesis pairs and translates them into atypologically diverse set of languages. This design enables precise controlover semantic relations and allows testing in both monolingual andmixed-language (code-switched) conditions. Surprisingly, code-switching doesnot degrade, and can even improve, performance, suggesting thattranslation-induced lexical variation may serve as a regularization signal. Wevalidate semantic preservation through embedding-based similarity analyses andcross-lingual alignment visualizations, confirming the fidelity of translatedpairs. Our findings expose both the potential and the brittleness of currentLLM cross-lingual reasoning, and identify code-switching as a promising leverfor improving multilingual robustness. Code available at:https://github.com/KurbanIntelligenceLab/nli-stress-testing</description><author>Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban</author><pubDate>Wed, 20 Aug 2025 14:30:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14735v1</guid></item><item><title>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</title><link>http://arxiv.org/abs/2508.14704v1</link><description>The Model Context Protocol has emerged as a transformative standard forconnecting large language models to external data sources and tools, rapidlygaining adoption across major AI providers and development platforms. However,existing benchmarks are overly simplistic and fail to capture real applicationchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. Toaddress this critical gap, we introduce MCP-Universe, the first comprehensivebenchmark specifically designed to evaluate LLMs in realistic and hard tasksthrough interaction with real-world MCP servers. Our benchmark encompasses 6core domains spanning 11 different MCP servers: Location Navigation, RepositoryManagement, Financial Analysis, 3D Design, Browser Automation, and WebSearching. To ensure rigorous evaluation, we implement execution-basedevaluators, including format evaluators for agent format compliance, staticevaluators for time-invariant content matching, and dynamic evaluators thatautomatically retrieve real-time ground truth for temporally sensitive tasks.Through extensive evaluation of leading LLMs, we find that even SOTA modelssuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibitsignificant performance limitations. In addition, our benchmark poses asignificant long-context challenge for LLM agents, as the number of inputtokens increases rapidly with the number of interaction steps. Moreover, itintroduces an unknown-tools challenge, as LLM agents often lack familiaritywith the precise usage of the MCP servers. Notably, enterprise-level agentslike Cursor cannot achieve better performance than standard ReAct frameworks.Beyond evaluation, we open-source our extensible evaluation framework with UIsupport, enabling researchers and practitioners to seamlessly integrate newagents and MCP servers while fostering innovation in the rapidly evolving MCPecosystem.</description><author>Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li</author><pubDate>Wed, 20 Aug 2025 13:28:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14704v1</guid></item><item><title>Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination</title><link>http://arxiv.org/abs/2508.14635v1</link><description>The ability to coordinate actions across multiple agents is critical forsolving complex, real-world problems. Large Language Models (LLMs) have shownstrong capabilities in communication, planning, and reasoning, raising thequestion of whether they can also support effective collaboration inmulti-agent settings. In this work, we investigate the use of LLM agents tosolve a structured victim rescue task that requires division of labor,prioritization, and cooperative planning. Agents operate in a fully knowngraph-based environment and must allocate resources to victims with varyingneeds and urgency levels. We systematically evaluate their performance using asuite of coordination-sensitive metrics, including task success rate, redundantactions, room conflicts, and urgency-weighted efficiency. This study offers newinsights into the strengths and failure modes of LLMs in physically groundedmulti-agent collaboration tasks, contributing to future benchmarks andarchitectural improvements.</description><author>João Vitor de Carvalho Silva, Douglas G. Macharet</author><pubDate>Wed, 20 Aug 2025 11:44:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14635v1</guid></item><item><title>Understanding Data Influence with Differential Approximation</title><link>http://arxiv.org/abs/2508.14648v1</link><description>Data plays a pivotal role in the groundbreaking advancements in artificialintelligence. The quantitative analysis of data significantly contributes tomodel training, enhancing both the efficiency and quality of data utilization.However, existing data analysis tools often lag in accuracy. For instance, manyof these tools even assume that the loss function of neural networks is convex.These limitations make it challenging to implement current methods effectively.In this paper, we introduce a new formulation to approximate a sample'sinfluence by accumulating the differences in influence between consecutivelearning steps, which we term Diff-In. Specifically, we formulate thesample-wise influence as the cumulative sum of its changes/differences acrosssuccessive training iterations. By employing second-order approximations, weapproximate these difference terms with high accuracy while eliminating theneed for model convexity required by existing methods. Despite being asecond-order method, Diff-In maintains computational complexity comparable tothat of first-order methods and remains scalable. This efficiency is achievedby computing the product of the Hessian and gradient, which can be efficientlyapproximated using finite differences of first-order gradients. We assess theapproximation accuracy of Diff-In both theoretically and empirically. Ourtheoretical analysis demonstrates that Diff-In achieves significantly lowerapproximation error compared to existing influence estimators. Extensiveexperiments further confirm its superior performance across multiple benchmarkdatasets in three data-centric tasks: data cleaning, data deletion, and coresetselection. Notably, our experiments on data pruning for large-scalevision-language pre-training show that Diff-In can scale to millions of datapoints and outperforms strong baselines.</description><author>Haoru Tan, Sitong Wu, Xiuzhe Wu, Wang Wang, Bo Zhao, Zeke Xie, Gui-Song Xia, Xiaojuan Qi</author><pubDate>Wed, 20 Aug 2025 11:59:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14648v1</guid></item><item><title>STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples</title><link>http://arxiv.org/abs/2508.12096v2</link><description>Evaluating large language models (LLMs) has become increasingly challengingas model capabilities advance rapidly. While recent models often achieve higherscores on standard benchmarks, these improvements do not consistently reflectenhanced real-world reasoning capabilities. Moreover, widespread overfitting topublic benchmarks and the high computational cost of full evaluations have madeit both expensive and less effective to distinguish meaningful differencesbetween models. To address these challenges, we propose the \textbf{S}tructured\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweightand interpretable evaluation framework for efficiently estimating the relativecapabilities of LLMs. STEM identifies \textit{significant transition samples}(STS) by analyzing consistent performance transitions among LLMs of the samearchitecture but varying parameter scales. These samples enable STEM toeffectively estimate the capability position of an unknown model. Qwen3 modelfamily is applied to construct the STS pool on six diverse and representativebenchmarks. To assess generalizability. Experimental results indicate that STEMreliably captures performance trends, aligns with ground-truth rankings ofmodel capability. These findings highlight STEM as a practical and scalablemethod for fine-grained, architecture-agnostic evaluation of LLMs.</description><author>Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang</author><pubDate>Wed, 20 Aug 2025 09:52:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.12096v2</guid></item><item><title>Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning</title><link>http://arxiv.org/abs/2507.03047v2</link><description>Recent advances have applied large language models (LLMs) to sequentialrecommendation, leveraging their pre-training knowledge and reasoningcapabilities to provide more personalized user experiences. However, existingLLM-based methods fail to sufficiently leverage the rich temporal informationinherent in users' historical interaction sequences, stemming from fundamentalarchitectural constraints: LLMs process information through self-attentionmechanisms that lack inherent sequence ordering and rely on position embeddingsdesigned primarily for natural language rather than user interaction sequences.This limitation significantly impairs their ability to capture the evolution ofuser preferences over time and predict future interests accurately. To address this critical gap, we propose \underline{C}ounterfactual\underline{E}nhanced \underline{T}emporal Framework for LLM-Based\underline{Rec}ommendation (CETRec). CETRec is grounded in causal inferenceprinciples, which allow it to isolate and measure the specific impact oftemporal information on recommendation outcomes. Combined with ourcounterfactual tuning task derived from causal analysis, CETRec effectivelyenhances LLMs' awareness of both absolute order (how recently items wereinteracted with) and relative order (the sequential relationships betweenitems). Extensive experiments on real-world datasets demonstrate theeffectiveness of our CETRec. Our code is available athttps://anonymous.4open.science/r/CETRec-B9CE/.</description><author>Yutian Liu, Zhengyi Yang, Jiancan Wu, Xiang Wang</author><pubDate>Wed, 20 Aug 2025 09:09:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.03047v2</guid></item><item><title>Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)</title><link>http://arxiv.org/abs/2507.03608v2</link><description>Generative AI (GenAI) is expected to play a pivotal role in enablingautonomous optimization in future wireless networks. Within the ORANarchitecture, Large Language Models (LLMs) can be specialized to generate xAppsand rApps by leveraging specifications and API definitions from the RANIntelligent Controller (RIC) platform. However, fine-tuning base LLMs fortelecom-specific tasks remains expensive and resource-intensive.Retrieval-Augmented Generation (RAG) offers a practical alternative throughin-context learning, enabling domain adaptation without full retraining. Whiletraditional RAG systems rely on vector-based retrieval, emerging variants suchas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrievalstrategies to support multi-hop reasoning and improve factual grounding.Despite their promise, these methods lack systematic, metric-drivenevaluations, particularly in high-stakes domains such as ORAN. In this study,we conduct a comparative evaluation of Vector RAG, GraphRAG, and HybridGraphRAG using ORAN specifications. We assess performance across varyingquestion complexities using established generation metrics: faithfulness,answer relevance, context relevance, and factual correctness. Results show thatboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAGimproves factual correctness by 8%, while GraphRAG improves context relevanceby 11%.</description><author>Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi</author><pubDate>Wed, 20 Aug 2025 08:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.03608v2</guid></item><item><title>Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles</title><link>http://arxiv.org/abs/2508.14527v1</link><description>The generation of safety-critical scenarios in simulation has becomeincreasingly crucial for safety evaluation in autonomous vehicles prior to roaddeployment in society. However, current approaches largely rely on predefinedthreat patterns or rule-based strategies, which limit their ability to exposediverse and unforeseen failure modes. To overcome these, we propose ScenGE, aframework that can generate plentiful safety-critical scenarios by reasoningnovel adversarial cases and then amplifying them with complex traffic flows.Given a simple prompt of a benign scene, it first performs Meta-ScenarioGeneration, where a large language model, grounded in structured drivingknowledge, infers an adversarial agent whose behavior poses a threat that isboth plausible and deliberately challenging. This meta-scenario is thenspecified in executable code for precise in-simulator control. Subsequently,Complex Scenario Evolution uses background vehicles to amplify the core threatintroduced by Meta-Scenario. It builds an adversarial collaborator graph toidentify key agent trajectories for optimization. These perturbations aredesigned to simultaneously reduce the ego vehicle's maneuvering space andcreate critical occlusions. Extensive experiments conducted on multiplereinforcement learning based AV models show that ScenGE uncovers more severecollision cases (+31.96%) on average than SoTA baselines. Additionally, ourScenGE can be applied to large model based AV systems and deployed on differentsimulators; we further observe that adversarial training on our scenariosimproves the model robustness. Finally, we validate our framework throughreal-world vehicle tests and human evaluation, confirming that the generatedscenarios are both plausible and critical. We hope our paper can build up acritical step towards building public trust and ensuring their safe deployment.</description><author>Jiangfan Liu, Yongkang Guo, Fangzhi Zhong, Tianyuan Zhang, Zonglei Jing, Siyuan Liang, Jiakai Wang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</author><pubDate>Wed, 20 Aug 2025 08:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14527v1</guid></item><item><title>DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</title><link>http://arxiv.org/abs/2508.14391v1</link><description>Relation extraction enables the construction of structured knowledge for manydownstream applications. While large language models (LLMs) have shown greatpromise in this domain, most existing methods concentrate on relationclassification, which predicts the semantic relation type between a relatedentity pair. However, we observe that LLMs often struggle to reliably determinewhether a relation exists, especially in cases involving complex sentencestructures or intricate semantics, which leads to spurious predictions. Suchhallucinations can introduce noisy edges in knowledge graphs, compromising theintegrity of structured knowledge and downstream reliability. To address thesechallenges, we propose DEPTH, a framework that integrates Dependency-awaresEntence simPlification and Two-tiered Hierarchical refinement into therelation extraction pipeline. Given a sentence and its candidate entity pairs,DEPTH operates in two stages: (1) the Grounding module extracts relations foreach pair by leveraging their shortest dependency path, distilling the sentenceinto a minimal yet coherent relational context that reduces syntactic noisewhile preserving key semantics; (2) the Refinement module aggregates all localpredictions and revises them based on a holistic understanding of the sentence,correcting omissions and inconsistencies. We further introduce acausality-driven reward model that mitigates reward hacking by disentanglingspurious correlations, enabling robust fine-tuning via reinforcement learningwith human feedback. Experiments on six benchmarks demonstrate that DEPTHreduces the average hallucination rate to 7.0\% while achieving a 17.2\%improvement in average F1 score over state-of-the-art baselines.</description><author>Yupei Yang, Fan Feng, Lin Yang, Wanxi Deng, Lin Qu, Biwei Huang, Shikui Tu, Lei Xu</author><pubDate>Wed, 20 Aug 2025 03:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14391v1</guid></item><item><title>Is Small Language Model the Silver Bullet to Low-Resource Languages Machine Translation?</title><link>http://arxiv.org/abs/2503.24102v3</link><description>Low-resource languages (LRLs) lack sufficient linguistic resources and areunderrepresented in benchmark datasets, resulting in persistently lowertranslation quality than high-resource languages, especially inprivacy-sensitive and resource-limited contexts. Firstly, this studysystematically evaluates state-of-the-art smaller Large Language Models in 200languages using the FLORES-200 benchmark, highlighting persistent deficienciesand disparities in the translation of LRLs. To mitigate these limitations, weinvestigate knowledge distillation from large pre-trained teacher models toSmall Language Models (SLMs) through supervised fine-tuning. The results showsubstantial improvements; for example, the translation performance of Englishto Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increasesfrom 0.36 to 0.89 in the validation set for Llama-3.2-3B. We furtherinvestigate various fine-tuning configurations and tasks to clarify thetrade-offs between data scale and training efficiency, verify that the modelretains its general capabilities without significant catastrophic forgettingafter training, and explore the distillation benefits to other LRLs on SLMs(Khasi, Assamese, and Ukrainian). In general, this work exposes the limitationsand fairness issues of current SLMs in LRL translation and systematicallyexplores the potential of using the distillation of knowledge from large tosmall models, offering practical, empirically grounded recommendations toimprove LRL translation systems</description><author>Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, Tegawendé F. Bissyandé, Jacques Klein</author><pubDate>Fri, 22 Aug 2025 15:58:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.24102v3</guid></item><item><title>Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish</title><link>http://arxiv.org/abs/2508.16431v1</link><description>We introduce Cetvel, a comprehensive benchmark designed to evaluate largelanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lackeither task diversity or culturally relevant content, or both. Cetvel addressesthese gaps by combining a broad range of both discriminative and generativetasks ensuring content that reflects the linguistic and cultural richness ofTurkish language. Cetvel covers 23 tasks grouped into seven categories,including tasks such as grammatical error correction, machine translation, andquestion answering rooted in Turkish history and idiomatic language. Weevaluate 33 open-weight LLMs (up to 70B parameters) covering different modelfamilies and instruction paradigms. Our experiments reveal that Turkish-centricinstruction-tuned models generally underperform relative to multilingual orgeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored forthe language. Moreover, we show that tasks such as grammatical error correctionand extractive question answering are particularly discriminative indifferentiating model capabilities. Cetvel offers a comprehensive andculturally grounded evaluation suite for advancing the development andassessment of LLMs in Turkish.</description><author>Yakup Abrek Er, Ilker Kesen, Gözde Gül Şahin, Aykut Erdem</author><pubDate>Fri, 22 Aug 2025 14:42:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16431v1</guid></item><item><title>Detecting and Characterizing Planning in Language Models</title><link>http://arxiv.org/abs/2508.18098v1</link><description>Modern large language models (LLMs) have demonstrated impressive performanceacross a wide range of multi-step reasoning tasks. Recent work suggests thatLLMs may perform planning - selecting a future target token in advance andgenerating intermediate tokens that lead towards it - rather than merelyimprovising one token at a time. However, existing studies assume fixedplanning horizons and often focus on single prompts or narrow domains. Todistinguish planning from improvisation across models and tasks, we presentformal and causally grounded criteria for detecting planning and operationalizethem as a semi-automated annotation pipeline. We apply this pipeline to bothbase and instruction-tuned Gemma-2-2B models on the MBPP code generationbenchmark and a poem generation task where Claude 3.5 Haiku was previouslyshown to plan. Our findings show that planning is not universal: unlike Haiku,Gemma-2-2B solves the same poem generation task through improvisation, and onMBPP it switches between planning and improvisation across similar tasks andeven successive token predictions. We further show that instruction tuningrefines existing planning behaviors in the base model rather than creating themfrom scratch. Together, these studies provide a reproducible and scalablefoundation for mechanistic studies of planning in LLMs.</description><author>Jatin Nainani, Sankaran Vaidyanathan, Connor Watts, Andre N. Assis, Alice Rigg</author><pubDate>Mon, 25 Aug 2025 14:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18098v1</guid></item><item><title>Neither Valid nor Reliable? Investigating the Use of LLMs as Judges</title><link>http://arxiv.org/abs/2508.18076v1</link><description>Evaluating natural language generation (NLG) systems remains a core challengeof natural language processing (NLP), further complicated by the rise of largelanguage models (LLMs) that aims to be general-purpose. Recently, largelanguage models as judges (LLJs) have emerged as a promising alternative totraditional metrics, but their validity remains underexplored. This positionpaper argues that the current enthusiasm around LLJs may be premature, as theiradoption has outpaced rigorous scrutiny of their reliability and validity asevaluators. Drawing on measurement theory from the social sciences, we identifyand critically assess four core assumptions underlying the use of LLJs: theirability to act as proxies for human judgment, their capabilities as evaluators,their scalability, and their cost-effectiveness. We examine how each of theseassumptions may be challenged by the inherent limitations of LLMs, LLJs, orcurrent practices in NLG evaluation. To ground our analysis, we explore threeapplications of LLJs: text summarization, data annotation, and safetyalignment. Finally, we highlight the need for more responsible evaluationpractices in LLJs evaluation, to ensure that their growing role in the fieldsupports, rather than undermines, progress in NLG.</description><author>Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, Golnoosh Farnadi</author><pubDate>Mon, 25 Aug 2025 14:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18076v1</guid></item><item><title>A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</title><link>http://arxiv.org/abs/2508.17969v1</link><description>This paper introduces a holistic perception system for internal and externalmonitoring of autonomous vehicles, with the aim of demonstrating a novelAI-leveraged self-adaptive framework of advanced vehicle technologies andsolutions that optimize perception and experience on-board. Internal monitoringsystem relies on a multi-camera setup designed for predicting and identifyingdriver and occupant behavior through facial recognition, exploiting in additiona large language model as virtual assistant. Moreover, the in-cabin monitoringsystem includes AI-empowered smart sensors that measure air-quality and performthermal comfort analysis for efficient on and off-boarding. On the other hand,external monitoring system perceives the surrounding environment of vehicle,through a LiDAR-based cost-efficient semantic segmentation approach, thatperforms highly accurate and efficient super-resolution on low-quality raw 3Dpoint clouds. The holistic perception framework is developed in the context ofEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed ona real electric vehicle provided by ALKE. Experimental validation andevaluation at the integration site of Joint Research Centre at Ispra, Italy,highlights increased performance and efficiency of the modular blocks of theproposed perception architecture.</description><author>Alexandros Gkillas, Christos Anagnostopoulos, Nikos Piperigkos, Dimitris Tsiktsiris, Theofilos Christodoulou, Theofanis Siamatras, Dimitrios Triantafyllou, Christos Basdekis, Theoktisti Marinopoulou, Panagiotis Lepentsiotis, Elefterios Blitsis, Aggeliki Zacharaki, Nearchos Stylianidis, Leonidas Katelaris, Lamberto Salvan, Aris S. Lalos, Christos Laoudias, Antonios Lalas, Konstantinos Votis</author><pubDate>Mon, 25 Aug 2025 12:32:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17969v1</guid></item></channel></rss>