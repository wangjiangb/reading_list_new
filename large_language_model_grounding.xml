<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivlarge language model grounding</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 12 Aug 2024 13:00:39 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</title><link>http://arxiv.org/abs/2407.07796v1</link><description>We introduce a novel and extensible benchmark for large language models(LLMs) through grid-based games such as Tic-Tac-Toe, Connect-Four, and Gomoku.The open-source game simulation code, available on GitHub, allows LLMs tocompete and generates detailed data files in JSON, CSV, TXT, and PNG formatsfor leaderboard rankings and further analysis. We present the results of gamesamong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet byAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo andGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions ofresults from other LLMs. In total, we simulated 2,310 matches (5 sessions foreach pair among 7 LLMs and a random player) across three types of games, usingthree distinct prompt types: list, illustration, and image. The resultsrevealed significant variations in LLM performance across different games andprompt types, with analysis covering win and disqualification rates, missedopportunity analysis, and invalid move analysis. The details of the leaderboardand result matrix data are available as open-access data on GitHub. This studyenhances our understanding of LLMs' capabilities in playing games they were notspecifically trained for, helping to assess their rule comprehension andstrategic thinking. On the path to Artificial General Intelligence (AGI), thisstudy lays the groundwork for future exploration into their utility in complexdecision-making scenarios, illuminating their strategic thinking abilities andoffering directions for further inquiry into the limits of LLMs withingame-based frameworks.</description><author>Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper</author><pubDate>Wed, 10 Jul 2024 16:14:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07796v1</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v3</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Wed, 10 Jul 2024 15:03:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v3</guid></item><item><title>Training A Small Emotional Vision Language Model for Visual Art Comprehension</title><link>http://arxiv.org/abs/2403.11150v2</link><description>This paper develops small vision language models to understand visual art,which, given an art work, aims to identify its emotion category and explainthis prediction with natural language. While small models are computationallyefficient, their capacity is much limited compared with large models. To breakthis trade-off, this paper builds a small emotional vision language model(SEVLM) by emotion modeling and input-output feature alignment. On the onehand, based on valence-arousal-dominance (VAD) knowledge annotated bypsychology experts, we introduce and fuse emotional features derived throughVAD dictionary and a VAD head to align VAD vectors of predicted emotionexplanation and the ground truth. This allows the vision language model tobetter understand and generate emotional texts, compared with using traditionaltext embeddings alone. On the other hand, we design a contrastive head to pullclose embeddings of the image, its emotion class, and explanation, which alignsmodel outputs and inputs. On two public affective explanation datasets, we showthat the proposed techniques consistently improve the visual art understandingperformance of baseline SEVLMs. Importantly, the proposed model can be trainedand evaluated on a single RTX 2080 Ti while exhibiting very strong performance:it not only outperforms the state-of-the-art small models but is alsocompetitive compared with LLaVA 7B after fine-tuning and GPT4(V). The code isavailable at https://github.com/BetterZH/SEVLM-code.</description><author>Jing Zhang, Liang Zheng, Meng Wang, Dan Guo</author><pubDate>Wed, 10 Jul 2024 13:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.11150v2</guid></item><item><title>Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard</title><link>http://arxiv.org/abs/2407.07796v2</link><description>We introduce a novel and extensible benchmark for large language models(LLMs) through grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku.The open-source game simulation code, available on GitHub, allows LLMs tocompete and generates detailed data files in JSON, CSV, TXT, and PNG formatsfor leaderboard rankings and further analysis. We present the results of gamesamong leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet byAnthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo andGPT-4o by OpenAI, and Llama3-70B by Meta. We also encourage submissions ofresults from other LLMs. In total, we simulated 2,310 matches (5 sessions foreach pair among 7 LLMs and a random player) across three types of games, usingthree distinct prompt types: list, illustration, and image. The resultsrevealed significant variations in LLM performance across different games andprompt types, with analysis covering win and disqualification rates, missedopportunity analysis, and invalid move analysis. The details of the leaderboardand result matrix data are available as open-access data on GitHub. This studyenhances our understanding of LLMs' capabilities in playing games they were notspecifically trained for, helping to assess their rule comprehension andstrategic thinking. On the path to Artificial General Intelligence (AGI), thisstudy lays the groundwork for future exploration into their utility in complexdecision-making scenarios, illuminating their strategic thinking abilities andoffering directions for further inquiry into the limits of LLMs withingame-based frameworks.</description><author>Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper</author><pubDate>Thu, 11 Jul 2024 03:46:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07796v2</guid></item><item><title>Using Natural Language Explanations to Rescale Human Judgments</title><link>http://arxiv.org/abs/2305.14770v4</link><description>The rise of large language models (LLMs) has brought a critical need forhigh-quality human-labeled data, particularly for processes like human feedbackand evaluation. A common practice is to label data via consensus annotationover human judgments. However, annotators' judgments for subjective tasks candiffer in many ways: they may reflect different qualitative judgments about anexample, and they may be mapped to a labeling scheme in different ways. We showthat these nuances can be captured by natural language explanations, andpropose a method to rescale ordinal annotations and explanations using LLMs.Specifically, we feed annotators' Likert ratings and corresponding explanationsinto an LLM and prompt it to produce a numeric score anchored in a scoringrubric. These scores should reflect the annotators' underlying assessments ofthe example. The rubric can be designed or modified after annotation, andinclude distinctions that may not have been known when the original errortaxonomy was devised. We explore our technique in the context of rating systemoutputs for a document-grounded question answering task, where LLMs achievenear-human performance. Our method rescales the raw judgments without impactingagreement and brings the scores closer to human judgments grounded in the samescoring rubric.</description><author>Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett</author><pubDate>Thu, 11 Jul 2024 14:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14770v4</guid></item><item><title>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</title><link>http://arxiv.org/abs/2407.06863v2</link><description>Text-to-Image (T2I) models are being increasingly adopted in diverse globalcommunities where they create visual representations of their unique cultures.Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realismof generated images, overlooking the critical dimension of cultural competence.In this work, we introduce a framework to evaluate cultural competence of T2Imodels along two crucial dimensions: cultural awareness and cultural diversity,and present a scalable approach using a combination of structured knowledgebases and large language models to build a large dataset of cultural artifactsto enable this evaluation. In particular, we apply this approach to build CUBE(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark toevaluate cultural competence of T2I models. CUBE covers cultural artifactsassociated with 8 countries across different geo-cultural regions and along 3concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set ofhigh-quality prompts that enable the evaluation of cultural awareness, and 2)CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding toevaluate cultural diversity. We also introduce cultural diversity as a novelT2I evaluation component, leveraging quality-weighted Vendi score. Ourevaluations reveal significant gaps in the cultural awareness of existingmodels across countries and provide valuable insights into the culturaldiversity of T2I outputs for under-specified prompts. Our methodology isextendable to other cultural regions and concepts, and can facilitate thedevelopment of T2I models that better cater to the global population.</description><author>Nithish Kannen, Arif Ahmad, Marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, Shachi Dave</author><pubDate>Thu, 11 Jul 2024 17:57:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06863v2</guid></item><item><title>Robotic Control via Embodied Chain-of-Thought Reasoning</title><link>http://arxiv.org/abs/2407.08693v1</link><description>A key limitation of learned robot control policies is their inability togeneralize outside their training data. Recent works on vision-language-actionmodels (VLAs) have shown that the use of large, internet pre-trainedvision-language models as the backbone of learned robot policies cansubstantially improve their robustness and generalization ability. Yet, one ofthe most exciting capabilities of large vision-language models in other domainsis their ability to reason iteratively through complex problems. Can that samecapability be brought into robotics to allow policies to improve performance byreasoning about a given task before acting? Naive use of "chain-of-thought"(CoT) style prompting is significantly less effective with standard VLAsbecause of the relatively simple training examples that are available to them.Additionally, purely semantic reasoning about sub-tasks, as is common inregular CoT, is insufficient for robot policies that need to ground theirreasoning in sensory observations and the robot state. To this end, weintroduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which wetrain VLAs to perform multiple steps of reasoning about plans, sub-tasks,motions, and visually grounded features like object bounding boxes and endeffector positions, before predicting the robot action. We design a scalablepipeline for generating synthetic training data for ECoT on large robotdatasets. We demonstrate, that ECoT increases the absolute success rate ofOpenVLA, the current strongest open-source VLA policy, by 28% acrosschallenging generalization tasks, without any additional robot training data.Additionally, ECoT makes it easier for humans to interpret a policy's failuresand correct its behavior using natural language.</description><author>Zawalski Michał, Chen William, Pertsch Karl, Mees Oier, Finn Chelsea, Levine Sergey</author><pubDate>Thu, 11 Jul 2024 17:31:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08693v1</guid></item><item><title>ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</title><link>http://arxiv.org/abs/2402.17766v3</link><description>This paper presents ShapeLLM, the first 3D Multimodal Large Language Model(LLM) designed for embodied interaction, exploring a universal 3D objectunderstanding with 3D point clouds and languages. ShapeLLM is built upon animproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-viewimage distillation for enhanced geometry understanding. By utilizing ReCon++ asthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructedinstruction-following data and tested on our newly human-curated benchmark, 3DMM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3Dgeometry understanding and language-unified 3D interaction tasks, such asembodied visual grounding. Project page: https://qizekun.github.io/shapellm/</description><author>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma</author><pubDate>Fri, 12 Jul 2024 15:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17766v3</guid></item><item><title>Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors</title><link>http://arxiv.org/abs/2407.09136v1</link><description>Large language models (LLMs) present an opportunity to scale high-qualitypersonalized education to all. A promising approach towards this means is tobuild dialog tutoring models that scaffold students' problem-solving. However,even though existing LLMs perform well in solving reasoning questions, theystruggle to precisely detect student's errors and tailor their feedback tothese errors. Inspired by real-world teaching practice where teachers identifystudent errors and customize their response based on them, we focus onverifying student solutions and show how grounding to such verificationimproves the overall quality of tutor response generation. We collect a datasetof 1K stepwise math reasoning chains with the first error step annotated byteachers. We show empirically that finding the mistake in a student solution ischallenging for current models. We propose and evaluate several verifiers fordetecting these errors. Using both automatic and human evaluation we show thatthe student solution verifiers steer the generation model towards highlytargeted responses to student errors which are more often correct with lesshallucinations compared to existing baselines.</description><author>Nico Daheim, Jakub Macina, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan</author><pubDate>Fri, 12 Jul 2024 10:11:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09136v1</guid></item><item><title>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</title><link>http://arxiv.org/abs/2403.14608v6</link><description>Large models represent a groundbreaking advancement in multiple applicationfields, enabling remarkable achievements across various tasks. However, theirunprecedented scale comes with significant computational costs. These models,often consisting of billions of parameters, require vast amounts ofcomputational resources for execution. Especially, the expansive scale andcomputational demands pose considerable challenges when customizing them forparticular downstream tasks, particularly over the hardware platformsconstrained by computational capabilities. Parameter Efficient Fine-Tuning(PEFT) provides a practical solution by efficiently adjusting the large modelsover the various downstream tasks. In particular, PEFT refers to the process ofadjusting the parameters of a pre-trained large models to adapt it to aspecific task or domain while minimizing the number of additional parametersintroduced or computational resources required. This approach is particularlyimportant when dealing with large-scale language models with high parametercounts, as fine-tuning these models from scratch can be computationallyexpensive and resource-intensive, posing considerable challenges in thesupporting system platform design. In this survey, we present comprehensivestudies of various PEFT algorithms, examining their performance andcomputational overhead. Moreover, we provide an overview of applicationsdeveloped using different PEFT algorithms and discuss common techniquesemployed to mitigate computation costs for PEFT. In addition to providing anextensive survey from an algorithmic standpoint, we also examine variousreal-world system designs to investigate the implementation costs associatedwith different PEFT approaches. This survey serves as an indispensable resourcefor researchers aiming to understand both the PEFT algorithm and its systemimplementation, offering detailed ......</description><author>Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang</author><pubDate>Fri, 12 Jul 2024 09:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14608v6</guid></item><item><title>Towards More Trustworthy and Interpretable LLMs for Code through Syntax-Grounded Explanations</title><link>http://arxiv.org/abs/2407.08983v1</link><description>Trustworthiness and interpretability are inextricably linked concepts forLLMs. The more interpretable an LLM is, the more trustworthy it becomes.However, current techniques for interpreting LLMs when applied to code-relatedtasks largely focus on accuracy measurements, measures of how models react tochange, or individual task performance instead of the fine-grained explanationsneeded at prediction time for greater interpretability, and hence trust. Toimprove upon this status quo, this paper introduces ASTrust, aninterpretability method for LLMs of code that generates explanations groundedin the relationship between model confidence and syntactic structures ofprogramming languages. ASTrust explains generated code in the context of syntaxcategories based on Abstract Syntax Trees and aids practitioners inunderstanding model predictions at both local (individual code snippets) andglobal (larger datasets of code) levels. By distributing and assigning modelconfidence scores to well-known syntactic structures that exist within ASTs,our approach moves beyond prior techniques that perform token-level confidencemapping by offering a view of model confidence that directly aligns withprogramming language concepts with which developers are familiar. To putASTrust into practice, we developed an automated visualization that illustratesthe aggregated model confidence scores superimposed on sequence, heat-map, andgraph-based visuals of syntactic structures from ASTs. We examine both thepractical benefit that ASTrust can provide through a data science study on 12popular LLMs on a curated set of GitHub repos and the usefulness of ASTrustthrough a human study.</description><author>David N. Palacio, Daniel Rodriguez-Cardenas, Alejandro Velasco, Dipin Khati, Kevin Moran, Denys Poshyvanyk</author><pubDate>Fri, 12 Jul 2024 04:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08983v1</guid></item><item><title>Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing</title><link>http://arxiv.org/abs/2406.14230v2</link><description>Warning: this paper contains model outputs exhibiting unethical information.Large Language Models (LLMs) have achieved significant breakthroughs, but theirgenerated unethical content poses potential risks. Measuring value alignment ofLLMs becomes crucial for their regulation and responsible deployment. Numerousdatasets have been constructed to assess social bias, toxicity, and ethics inLLMs, but they suffer from evaluation chronoeffect, that is, as models rapidlyevolve, existing data becomes leaked or undemanding, overestimatingever-developing LLMs. To tackle this problem, we propose GETA, a novelgenerative evolving testing approach that dynamically probes the underlyingmoral baselines of LLMs. Distinct from previous adaptive testing methods thatrely on static datasets with limited difficulty, GETA incorporates aniteratively-updated item generator which infers each LLM's moral boundaries andgenerates difficulty-tailored testing items, accurately reflecting the truealignment extent. This process theoretically learns a joint distribution ofitem and model response, with item difficulty and value conformity as latentvariables, where the generator co-evolves with the LLM, addressingchronoeffect. We evaluate various popular LLMs with diverse capabilities anddemonstrate that GETA can create difficulty-matching testing items and moreaccurately assess LLMs' values, better consistent with their performance onunseen OOD and i.i.d. items, laying the groundwork for future evaluationparadigms.</description><author>Han Jiang, Xiaoyuan Yi, Zhihua Wei, Shu Wang, Xing Xie</author><pubDate>Fri, 12 Jul 2024 03:47:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14230v2</guid></item><item><title>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation</title><link>http://arxiv.org/abs/2407.08940v1</link><description>The rapid growth of biomedical knowledge has outpaced our ability toefficiently extract insights and generate novel hypotheses. Large languagemodels (LLMs) have emerged as a promising tool to revolutionize knowledgeinteraction and potentially accelerate biomedical discovery. In this paper, wepresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.We construct a dataset of background-hypothesis pairs from biomedicalliterature, carefully partitioned into training, seen, and unseen test setsbased on publication date to mitigate data contamination. Using this dataset,we assess the hypothesis generation capabilities of top-tier instructed modelsin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration ofuncertainty, a crucial aspect of scientific discovery, we incorporate tool useand multi-agent interactions in our evaluation framework. Furthermore, wepropose four novel metrics grounded in extensive literature review to evaluatethe quality of generated hypotheses, considering both LLM-based and humanassessments. Our experiments yield two key findings: 1) LLMs can generate noveland validated hypotheses, even when tested on literature unseen duringtraining, and 2) Increasing uncertainty through multi-agent interactions andtool use can facilitate diverse candidate generation and improve zero-shothypothesis generation performance. However, we also observe that theintegration of additional knowledge through few-shot learning and tool use maynot always lead to performance gains, highlighting the need for carefulconsideration of the type and scope of external knowledge incorporated. Thesefindings underscore the potential of LLMs as powerful aids in biomedicalhypothesis generation and provide valuable insights to guide further researchin this area.</description><author>Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, Bowen Zhou</author><pubDate>Fri, 12 Jul 2024 02:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08940v1</guid></item><item><title>CXR-Agent: Vision-language models for chest X-ray interpretation with uncertainty aware radiology reporting</title><link>http://arxiv.org/abs/2407.08811v1</link><description>Recently large vision-language models have shown potential when interpretingcomplex images and generating natural language descriptions using advancedreasoning. Medicine's inherently multimodal nature incorporating scans andtext-based medical histories to write reports makes it conducive to benefitfrom these leaps in AI capabilities. We evaluate the publicly available, stateof the art, foundational vision-language models for chest X-ray interpretationacross several datasets and benchmarks. We use linear probes to evaluate theperformance of various components including CheXagent's vision transformer andQ-former, which outperform the industry-standard Torch X-ray Vision modelsacross many different datasets showing robust generalisation capabilities.Importantly, we find that vision-language models often hallucinate withconfident language, which slows down clinical interpretation. Based on thesefindings, we develop an agent-based vision-language approach for reportgeneration using CheXagent's linear probes and BioViL-T's phrase groundingtools to generate uncertainty-aware radiology reports with pathologieslocalised and described based on their likelihood. We thoroughly evaluate ourvision-language agents using NLP metrics, chest X-ray benchmarks and clinicalevaluations by developing an evaluation platform to perform a user study withrespiratory specialists. Our results show considerable improvements inaccuracy, interpretability and safety of the AI-generated reports. We stressthe importance of analysing results for normal and abnormal scans separately.Finally, we emphasise the need for larger paired (scan and report) datasetsalongside data augmentation to tackle overfitting seen in these largevision-language models.</description><author>Naman Sharma</author><pubDate>Thu, 11 Jul 2024 18:39:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08811v1</guid></item><item><title>SHINE: Saliency-aware HIerarchical NEgative Ranking for Compositional Temporal Grounding</title><link>http://arxiv.org/abs/2407.05118v2</link><description>Temporal grounding, also known as video moment retrieval, aims at locatingvideo segments corresponding to a given query sentence. The compositionalnature of natural language enables the localization beyond predefined events,posing a certain challenge to the compositional generalizability of existingmethods. Recent studies establish the correspondence between videos and queriesthrough a decompose-reconstruct manner to achieve compositional generalization.However, they only consider dominant primitives and build negative queriesthrough random sampling and recombination, resulting in semanticallyimplausible negatives that hinder the models from learning rationalcompositions. In addition, recent DETR-based methods still underperform incompositional temporal grounding, showing irrational saliency responses whengiven negative queries that have subtle differences from positive queries. Toaddress these limitations, we first propose a large language model-drivenmethod for negative query construction, utilizing GPT-3.5-Turbo to generatesemantically plausible hard negative queries. Subsequently, we introduce acoarse-to-fine saliency ranking strategy, which encourages the model to learnthe multi-granularity semantic relationships between videos and hierarchicalnegative queries to boost compositional generalization. Extensive experimentson two challenging benchmarks validate the effectiveness and generalizabilityof our proposed method. Our code is available athttps://github.com/zxccade/SHINE.</description><author>Zixu Cheng, Yujiang Pu, Shaogang Gong, Parisa Kordjamshidi, Yu Kong</author><pubDate>Mon, 15 Jul 2024 16:53:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05118v2</guid></item><item><title>Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models</title><link>http://arxiv.org/abs/2407.10873v1</link><description>Automated heuristic design (AHD) has gained considerable attention for itspotential to automate the development of effective heuristics. The recentadvent of large language models (LLMs) has paved a new avenue for AHD, withinitial efforts focusing on framing AHD as an evolutionary program search (EPS)problem. However, inconsistent benchmark settings, inadequate baselines, and alack of detailed component analysis have left the necessity of integrating LLMswith search strategies and the true progress achieved by existing LLM-based EPSmethods to be inadequately justified. This work seeks to fulfill these researchqueries by conducting a large-scale benchmark comprising four LLM-based EPSmethods and four AHD problems across nine LLMs and five independent runs. Ourextensive experiments yield meaningful insights, providing empirical groundingfor the importance of evolutionary search in LLM-based AHD approaches, whilealso contributing to the advancement of future EPS algorithmic development. Tofoster accessibility and reproducibility, we have fully open-sourced ourbenchmark and corresponding results.</description><author>Rui Zhang, Fei Liu, Xi Lin, Zhenkun Wang, Zhichao Lu, Qingfu Zhang</author><pubDate>Mon, 15 Jul 2024 16:21:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10873v1</guid></item><item><title>3D Feature Distillation with Object-Centric Priors</title><link>http://arxiv.org/abs/2406.18742v3</link><description>Grounding natural language to the physical world is a ubiquitous topic with awide range of applications in computer vision and robotics. Recently, 2Dvision-language models such as CLIP have been widely popularized, due to theirimpressive capabilities for open-vocabulary grounding in 2D images. Recentworks aim to elevate 2D CLIP features to 3D via feature distillation, buteither learn neural fields that are scene-specific and hence lackgeneralization, or focus on indoor room scan data that require access tomultiple camera views, which is not practical in robot manipulation scenarios.Additionally, related methods typically fuse features at pixel-level and assumethat all camera views are equally informative. In this work, we show that thisapproach leads to sub-optimal 3D features, both in terms of grounding accuracy,as well as segmentation crispness. To alleviate this, we propose a multi-viewfeature fusion strategy that employs object-centric priors to eliminateuninformative views based on semantic information, and fuse features atobject-level via instance segmentation masks. To distill our object-centric 3Dfeatures, we generate a large-scale synthetic multi-view dataset of clutteredtabletop scenes, spawning 15k scenes from over 3300 unique object instances,which we make publicly available. We show that our method reconstructs 3D CLIPfeatures with improved grounding capacity and spatial consistency, while doingso from single-view RGB-D, thus departing from the assumption of multiplecamera views at test time. Finally, we show that our approach can generalize tonovel tabletop domains and be re-purposed for 3D instance segmentation withoutfine-tuning, and demonstrate its utility for language-guided robotic graspingin clutter</description><author>Georgios Tziafas, Yucheng Xu, Zhibin Li, Hamidreza Kasaei</author><pubDate>Mon, 15 Jul 2024 14:23:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18742v3</guid></item><item><title>Towards Open-World Grasping with Large Vision-Language Models</title><link>http://arxiv.org/abs/2406.18722v3</link><description>The ability to grasp objects in-the-wild from open-ended languageinstructions constitutes a fundamental challenge in robotics. An open-worldgrasping system should be able to combine high-level contextual with low-levelphysical-geometric reasoning in order to be applicable in arbitrary scenarios.Recent works exploit the web-scale knowledge inherent in large language models(LLMs) to plan and reason in robotic context, but rely on external vision andaction models to ground such knowledge into the environment and parameterizeactuation. This setup suffers from two major bottlenecks: a) the LLM'sreasoning capacity is constrained by the quality of visual grounding, and b)LLMs do not contain low-level spatial understanding of the world, which isessential for grasping in contact-rich scenarios. In this work we demonstratethat modern vision-language models (VLMs) are capable of tackling suchlimitations, as they are implicitly grounded and can jointly reason aboutsemantics and geometry. We propose OWG, an open-world grasping pipeline thatcombines VLMs with segmentation and grasp synthesis models to unlock groundedworld understanding in three stages: open-ended referring segmentation,grounded grasp planning and grasp ranking via contact reasoning, all of whichcan be applied zero-shot via suitable visual prompting mechanisms. We conductextensive evaluation in cluttered indoor scene datasets to showcase OWG'srobustness in grounding from open-ended language, as well as open-world roboticgrasping experiments in both simulation and hardware that demonstrate superiorperformance compared to previous supervised and zero-shot LLM-based methods.</description><author>Georgios Tziafas, Hamidreza Kasaei</author><pubDate>Mon, 15 Jul 2024 14:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.18722v3</guid></item><item><title>Large Language Models and Games: A Survey and Roadmap</title><link>http://arxiv.org/abs/2402.18659v2</link><description>Recent years have seen an explosive increase in research on large languagemodels (LLMs), and accompanying public engagement on the topic. While startingas a niche area within natural language processing, LLMs have shown remarkablepotential across a broad range of applications and domains, including games.This paper surveys the current state of the art across the various applicationsof LLMs in and for games, and identifies the different roles LLMs can takewithin a game. Importantly, we discuss underexplored areas and promisingdirections for future uses of LLMs in games and we reconcile the potential andlimitations of LLMs within the games domain. As the first comprehensive surveyand roadmap at the intersection of LLMs and games, we are hopeful that thispaper will serve as the basis for groundbreaking research and innovation inthis exciting new field.</description><author>Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis</author><pubDate>Mon, 15 Jul 2024 13:10:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18659v2</guid></item><item><title>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation</title><link>http://arxiv.org/abs/2407.08940v2</link><description>The rapid growth of biomedical knowledge has outpaced our ability toefficiently extract insights and generate novel hypotheses. Large languagemodels (LLMs) have emerged as a promising tool to revolutionize knowledgeinteraction and potentially accelerate biomedical discovery. In this paper, wepresent a comprehensive evaluation of LLMs as biomedical hypothesis generators.We construct a dataset of background-hypothesis pairs from biomedicalliterature, carefully partitioned into training, seen, and unseen test setsbased on publication date to mitigate data contamination. Using this dataset,we assess the hypothesis generation capabilities of top-tier instructed modelsin zero-shot, few-shot, and fine-tuning settings. To enhance the exploration ofuncertainty, a crucial aspect of scientific discovery, we incorporate tool useand multi-agent interactions in our evaluation framework. Furthermore, wepropose four novel metrics grounded in extensive literature review to evaluatethe quality of generated hypotheses, considering both LLM-based and humanassessments. Our experiments yield two key findings: 1) LLMs can generate noveland validated hypotheses, even when tested on literature unseen duringtraining, and 2) Increasing uncertainty through multi-agent interactions andtool use can facilitate diverse candidate generation and improve zero-shothypothesis generation performance. However, we also observe that theintegration of additional knowledge through few-shot learning and tool use maynot always lead to performance gains, highlighting the need for carefulconsideration of the type and scope of external knowledge incorporated. Thesefindings underscore the potential of LLMs as powerful aids in biomedicalhypothesis generation and provide valuable insights to guide further researchin this area.</description><author>Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, Bowen Zhou</author><pubDate>Mon, 15 Jul 2024 06:27:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08940v2</guid></item><item><title>By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting</title><link>http://arxiv.org/abs/2407.10385v1</link><description>Large language models (LLMs) have demonstrated exceptional abilities acrossvarious domains. However, utilizing LLMs for ubiquitous sensing applicationsremains challenging as existing text-prompt methods show significantperformance degradation when handling long sensor data sequences. We propose avisual prompting approach for sensor data using multimodal LLMs (MLLMs). Wedesign a visual prompt that directs MLLMs to utilize visualized sensor dataalongside the target sensory task descriptions. Additionally, we introduce avisualization generator that automates the creation of optimal visualizationstailored to a given sensory task, eliminating the need for prior task-specificknowledge. We evaluated our approach on nine sensory tasks involving foursensing modalities, achieving an average of 10% higher accuracy than text-basedprompts and reducing token costs by 15.8x. Our findings highlight theeffectiveness and cost-efficiency of visual prompts with MLLMs for varioussensory tasks.</description><author>Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee</author><pubDate>Mon, 15 Jul 2024 01:33:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10385v1</guid></item><item><title>LMExplainer: Grounding Knowledge and Explaining Language Models</title><link>http://arxiv.org/abs/2303.16537v3</link><description>Language models (LMs) like GPT-4 are important in AI applications, but theiropaque decision-making process reduces user trust, especially insafety-critical areas. We introduce LMExplainer, a novel knowledge-groundedexplainer that clarifies the reasoning process of LMs through intuitive,human-understandable explanations. By leveraging a graph attention network(GAT) with a large-scale knowledge graph (KG), LMExplainer not only preciselynarrows the reasoning space to focus on the most relevant knowledge but alsogrounds its reasoning in structured, verifiable knowledge to reducehallucinations and enhance interpretability. LMExplainer effectively generateshuman-understandable explanations to enhance transparency and streamline thedecision-making process. Additionally, by incorporating debugging into theexplanation, it offers expertise suggestions that improve LMs from adevelopmental perspective. Thus, LMExplainer stands as an enhancement in makingLMs more accessible and understandable to users. We evaluate LMExplainer onbenchmark datasets such as CommonsenseQA and OpenBookQA, demonstrating that itoutperforms most existing methods. By comparing the explanations generated byLMExplainer with those of other models, we show that our approach offers morecomprehensive and clearer explanations of the reasoning process. LMExplainerprovides a deeper understanding of the inner workings of LMs, advancing towardsmore reliable, transparent, and equitable AI.</description><author>Zichen Chen, Jianda Chen, Yuanyuan Chen, Han Yu, Ambuj K Singh, Misha Sra</author><pubDate>Tue, 16 Jul 2024 17:53:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16537v3</guid></item><item><title>To Believe or Not to Believe Your LLM</title><link>http://arxiv.org/abs/2406.02543v2</link><description>We explore uncertainty quantification in large language models (LLMs), withthe goal to identify when uncertainty in responses given a query is large. Wesimultaneously consider both epistemic and aleatoric uncertainties, where theformer comes from the lack of knowledge about the ground truth (such as aboutfacts or the language), and the latter comes from irreducible randomness (suchas multiple possible answers). In particular, we derive aninformation-theoretic metric that allows to reliably detect when only epistemicuncertainty is large, in which case the output of the model is unreliable. Thiscondition can be computed based solely on the output of the model obtainedsimply by some special iterative prompting based on the previous responses.Such quantification, for instance, allows to detect hallucinations (cases whenepistemic uncertainty is high) in both single- and multi-answer responses. Thisis in contrast to many standard uncertainty quantification strategies (such asthresholding the log-likelihood of a response) where hallucinations in themulti-answer case cannot be detected. We conduct a series of experiments whichdemonstrate the advantage of our formulation. Further, our investigations shedsome light on how the probabilities assigned to a given output by an LLM can beamplified by iterative prompting, which might be of independent interest.</description><author>Yasin Abbasi Yadkori, Ilja Kuzborskij, András György, Csaba Szepesvári</author><pubDate>Wed, 17 Jul 2024 15:55:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.02543v2</guid></item><item><title>Instruction-Driven Game Engines on Large Language Models</title><link>http://arxiv.org/abs/2404.00276v3</link><description>The Instruction-Driven Game Engine (IDGE) project aims to democratize gamedevelopment by enabling a large language model (LLM) to follow free-form gamerules and autonomously generate game-play processes. The IDGE allows users tocreate games by issuing simple natural language instructions, whichsignificantly lowers the barrier for game development. We approach the learningprocess for IDGEs as a Next State Prediction task, wherein the modelautoregressively predicts in-game states given player actions. It is achallenging task because the computation of in-game states must be precise;otherwise, slight errors could disrupt the game-play. To address this, we trainthe IDGE in a curriculum manner that progressively increases the model'sexposure to complex scenarios. Our initial progress lies in developing an IDGEfor Poker, a universally cherished card game. The engine we've designed notonly supports a wide range of poker variants but also allows for highcustomization of rules through natural language inputs. Furthermore, it alsofavors rapid prototyping of new games from minimal samples, proposing aninnovative paradigm in game development that relies on minimal prompt and dataengineering. This work lays the groundwork for future advancements ininstruction-driven game creation, potentially transforming how games aredesigned and played.</description><author>Hongqiu Wu, Xingyuan Liu, Hai Zhao, Min Zhang</author><pubDate>Wed, 17 Jul 2024 15:27:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.00276v3</guid></item><item><title>Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models</title><link>http://arxiv.org/abs/2407.12532v1</link><description>Effective collaboration in multi-agent systems requires communicating goalsand intentions between agents. Current agent frameworks often suffer fromdependencies on single-agent execution and lack robust inter-modulecommunication, frequently leading to suboptimal multi-agent reinforcementlearning (MARL) policies and inadequate task coordination. To address thesechallenges, we present a framework for training large language models (LLMs) ascollaborative agents to enable coordinated behaviors in cooperative MARL. Eachagent maintains a private intention consisting of its current goal andassociated sub-tasks. Agents broadcast their intentions periodically, allowingother agents to infer coordination tasks. A propagation network transformsbroadcast intentions into teammate-specific communication messages, sharingrelevant goals with designated teammates. The architecture of our framework isstructured into planning, grounding, and execution modules. During execution,multiple agents interact in a downstream environment and communicate intentionsto enable coordinated behaviors. The grounding module dynamically adaptscomprehension strategies based on emerging coordination patterns, whilefeedback from execution agents influnces the planning module, enabling thedynamic re-planning of sub-tasks. Results in collaborative environmentsimulation demonstrate intention propagation reduces miscoordination errors byaligning sub-task dependencies between agents. Agents learn when to communicateintentions and which teammates require task details, resulting in emergentcoordinated behaviors. This demonstrates the efficacy of intention sharing forcooperative multi-agent RL based on LLMs.</description><author>Xihe Qiu, Haoyu Wang, Xiaoyu Tan, Chao Qu, Yujie Xiong, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi</author><pubDate>Wed, 17 Jul 2024 13:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12532v1</guid></item><item><title>Hierarchical Indexing for Retrieval-Augmented Opinion Summarization</title><link>http://arxiv.org/abs/2403.00435v2</link><description>We propose a method for unsupervised abstractive opinion summarization, thatcombines the attributability and scalability of extractive approaches with thecoherence and fluency of Large Language Models (LLMs). Our method, HIRO, learnsan index structure that maps sentences to a path through a semanticallyorganized discrete hierarchy. At inference time, we populate the index and useit to identify and retrieve clusters of sentences containing popular opinionsfrom input reviews. Then, we use a pretrained LLM to generate a readablesummary that is grounded in these extracted evidential clusters. The modularityof our approach allows us to evaluate its efficacy at each stage. We show thatHIRO learns an encoding space that is more semantically structured than priorwork, and generates summaries that are more representative of the opinions inthe input reviews. Human evaluation confirms that HIRO generates significantlymore coherent, detailed and accurate summaries.</description><author>Tom Hosking, Hao Tang, Mirella Lapata</author><pubDate>Wed, 17 Jul 2024 09:34:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.00435v2</guid></item><item><title>StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions</title><link>http://arxiv.org/abs/2407.12423v1</link><description>The integration of Large Language Models (LLMs), especially ChatGPT, intoeducation is poised to revolutionize students' learning experiences byintroducing innovative conversational learning methodologies. To empowerstudents to fully leverage the capabilities of ChatGPT in educationalscenarios, understanding students' interaction patterns with ChatGPT is crucialfor instructors. However, this endeavor is challenging due to the absence ofdatasets focused on student-ChatGPT conversations and the complexities inidentifying and analyzing the evolutional interaction patterns withinconversations. To address these challenges, we collected conversational datafrom 48 students interacting with ChatGPT in a master's level datavisualization course over one semester. We then developed a coding scheme,grounded in the literature on cognitive levels and thematic analysis, tocategorize students' interaction patterns with ChatGPT. Furthermore, we presenta visual analytics system, StuGPTViz, that tracks and compares temporalpatterns in student prompts and the quality of ChatGPT's responses at multiplescales, revealing significant pedagogical insights for instructors. Wevalidated the system's effectiveness through expert interviews with six datavisualization instructors and three case studies. The results confirmedStuGPTViz's capacity to enhance educators' insights into the pedagogical valueof ChatGPT. We also discussed the potential research opportunities of applyingvisual analytics in education and developing AI-driven personalized learningsolutions.</description><author>Zixin Chen, Jiachen Wang, Meng Xia, Kento Shigyo, Dingdong Liu, Rong Zhang, Huamin Qu</author><pubDate>Wed, 17 Jul 2024 09:20:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12423v1</guid></item><item><title>PersLLM: A Personified Training Approach for Large Language Models</title><link>http://arxiv.org/abs/2407.12393v2</link><description>Large language models exhibit aspects of human-level intelligence thatcatalyze their application as human-like agents in domains such as socialsimulations, human-machine interactions, and collaborative multi-agent systems.However, the absence of distinct personalities, such as displaying ingratiatingbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMsutility in practical applications. Addressing this, the development ofpersonality traits in LLMs emerges as a crucial area of research to unlocktheir latent potential. Existing methods to personify LLMs generally involvestrategies like employing stylized training data for instruction tuning orusing prompt engineering to simulate different personalities. These methodsonly capture superficial linguistic styles instead of the core of personalitiesand are therefore not stable. In this study, we propose PersLLM, integratingpsychology-grounded principles of personality: social practice, consistency,and dynamic development, into a comprehensive training methodology. Weincorporate personality traits directly into the model parameters, enhancingthe model's resistance to induction, promoting consistency, and supporting thedynamic evolution of personality. Single-agent evaluation validates ourmethod's superiority, as it produces responses more aligned with referencepersonalities compared to other approaches. Case studies for multi-agentcommunication highlight its benefits in enhancing opinion consistency withinindividual agents and fostering collaborative creativity among multiple agentsin dialogue contexts, potentially benefiting human simulation and multi-agentcooperation. Additionally, human-agent interaction evaluations indicate thatour personified models significantly enhance interactive experiences,underscoring the practical implications of our research.</description><author>Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhiyuan Liu, Maosong Sun</author><pubDate>Thu, 18 Jul 2024 04:18:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12393v2</guid></item><item><title>Towards Multimodal In-Context Learning for Vision &amp; Language Models</title><link>http://arxiv.org/abs/2403.12736v2</link><description>State-of-the-art Vision-Language Models (VLMs) ground the vision and thelanguage modality primarily via projecting the vision tokens from the encoderto language-like tokens, which are directly fed to the Large Language Model(LLM) decoder. While these models have shown unprecedented performance in manydownstream zero-shot tasks (eg image captioning, question answers, etc), stilllittle emphasis has been put on transferring one of the core LLM capability ofIn-Context Learning (ICL). ICL is the ability of a model to reason about adownstream task with a few examples demonstrations embedded in the prompt. Inthis work, through extensive evaluations, we find that the state-of-the-artVLMs somewhat lack the ability to follow ICL instructions. In particular, wediscover that even models that underwent large-scale mixed modalitypre-training and were implicitly guided to make use of interleaved image andtext information (intended to consume helpful context from multiple images)under-perform when prompted with few-shot demonstrations (in an ICL way),likely due to their lack of direct ICL instruction tuning. To enhance the ICLabilities of the present VLM, we propose a simple yet surprisingly effectivemulti-turn curriculum-based learning methodology with effective data mixes,leading up to a significant 21.03% (and 11.3% on average) ICL performance boostover the strongest VLM baselines and a variety of ICL benchmarks. Furthermore,we also contribute new benchmarks for ICL evaluation in VLMs and discuss theiradvantages over the prior art.</description><author>Sivan Doveh, Shaked Perek, M. Jehanzeb Mirza, Wei Lin, Amit Alfassy, Assaf Arbelle, Shimon Ullman, Leonid Karlinsky</author><pubDate>Wed, 17 Jul 2024 08:13:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.12736v2</guid></item><item><title>ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities</title><link>http://arxiv.org/abs/2407.01525v3</link><description>Although great progress has been made in 3D visual grounding, current modelsstill rely on explicit textual descriptions for grounding and lack the abilityto reason human intentions from implicit instructions. We propose a new taskcalled 3D reasoning grounding and introduce a new benchmark ScanReason whichprovides over 10K question-answer-location pairs from five reasoning types thatrequire the synerization of reasoning and grounding. We further design ourapproach, ReGround3D, composed of the visual-centric reasoning module empoweredby Multi-modal Large Language Model (MLLM) and the 3D grounding module toobtain accurate object locations by looking back to the enhanced geometry andfine-grained details from the 3D scenes. A chain-of-grounding mechanism isproposed to further boost the performance with interleaved reasoning andgrounding steps during inference. Extensive experiments on the proposedbenchmark validate the effectiveness of our proposed approach.</description><author>Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, Xihui Liu</author><pubDate>Wed, 17 Jul 2024 07:07:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01525v3</guid></item><item><title>INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages</title><link>http://arxiv.org/abs/2407.13522v1</link><description>Large Language Models (LLMs) have demonstrated remarkable zero-shot andfew-shot capabilities in unseen tasks, including context-grounded questionanswering (QA) in English. However, the evaluation of LLMs' capabilities innon-English languages for context-based QA is limited by the scarcity ofbenchmarks in non-English languages. To address this gap, we introduceIndic-QA, the largest publicly available context-grounded question-answeringdataset for 11 major Indian languages from two language families. The datasetcomprises both extractive and abstractive question-answering tasks and includesexisting datasets as well as English QA datasets translated into Indianlanguages. Additionally, we generate a synthetic dataset using the Gemini modelto create question-answer pairs given a passage, which is then manuallyverified for quality assurance. We evaluate various multilingual Large LanguageModels and their instruction-fine-tuned variants on the benchmark and observethat their performance is subpar, particularly for low-resource languages. Wehope that the release of this dataset will stimulate further research on thequestion-answering abilities of LLMs for low-resource languages.</description><author>Abhishek Kumar Singh, Rudra Murthy, Vishwajeet kumar, Jaydeep Sen, Ganesh Ramakrishnan</author><pubDate>Thu, 18 Jul 2024 13:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13522v1</guid></item><item><title>Robots Can Multitask Too: Integrating a Memory Architecture and LLMs for Enhanced Cross-Task Robot Action Generation</title><link>http://arxiv.org/abs/2407.13505v1</link><description>Large Language Models (LLMs) have been recently used in robot applicationsfor grounding LLM common-sense reasoning with the robot's perception andphysical abilities. In humanoid robots, memory also plays a critical role infostering real-world embodiment and facilitating long-term interactivecapabilities, especially in multi-task setups where the robot must rememberprevious task states, environment states, and executed actions. In this paper,we address incorporating memory processes with LLMs for generating cross-taskrobot actions, while the robot effectively switches between tasks. Our proposeddual-layered architecture features two LLMs, utilizing their complementaryskills of reasoning and following instructions, combined with a memory modelinspired by human cognition. Our results show a significant improvement inperformance over a baseline of five robotic tasks, demonstrating the potentialof integrating memory with LLMs for combining the robot's action and perceptionfor adaptive task execution.</description><author>Hassan Ali, Philipp Allgeuer, Carlo Mazzola, Giulia Belgiovine, Burak Can Kaplan, Stefan Wermter</author><pubDate>Thu, 18 Jul 2024 13:38:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13505v1</guid></item><item><title>Improving Retrieval in Sponsored Search by Leveraging Query Context Signals</title><link>http://arxiv.org/abs/2407.14346v1</link><description>Accurately retrieving relevant bid keywords for user queries is critical inSponsored Search but remains challenging, particularly for short, ambiguousqueries. Existing dense and generative retrieval models often fail to capturenuanced user intent in these cases. To address this, we propose an approach toenhance query understanding by augmenting queries with rich contextual signalsderived from web search results and large language models, stored in an onlinecache. Specifically, we use web search titles and snippets to ground queries inreal-world information and utilize GPT-4 to generate query rewrites andexplanations that clarify user intent. These signals are efficiently integratedthrough a Fusion-in-Decoder based Unity architecture, enabling both dense andgenerative retrieval with serving costs on par with traditional context-freemodels. To address scenarios where context is unavailable in the cache, weintroduce context glancing, a curriculum learning strategy that improves modelrobustness and performance even without contextual signals during inference.Extensive offline experiments demonstrate that our context-aware approachsubstantially outperforms context-free models. Furthermore, online A/B testingon a prominent search engine across 160+ countries shows significantimprovements in user engagement and revenue.</description><author>Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh</author><pubDate>Fri, 19 Jul 2024 14:28:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14346v1</guid></item><item><title>Multimodal Misinformation Detection using Large Vision-Language Models</title><link>http://arxiv.org/abs/2407.14321v1</link><description>The increasing proliferation of misinformation and its alarming impact havemotivated both industry and academia to develop approaches for misinformationdetection and fact checking. Recent advances on large language models (LLMs)have shown remarkable performance in various tasks, but whether and how LLMscould help with misinformation detection remains relatively underexplored. Mostof existing state-of-the-art approaches either do not consider evidence andsolely focus on claim related features or assume the evidence to be provided.Few approaches consider evidence retrieval as part of the misinformationdetection but rely on fine-tuning models. In this paper, we investigate thepotential of LLMs for misinformation detection in a zero-shot setting. Weincorporate an evidence retrieval component into the process as it is crucialto gather pertinent information from various sources to detect the veracity ofclaims. To this end, we propose a novel re-ranking approach for multimodalevidence retrieval using both LLMs and large vision-language models (LVLM). Theretrieved evidence samples (images and texts) serve as the input for anLVLM-based approach for multimodal fact verification (LVLM4FV). To enable afair evaluation, we address the issue of incomplete ground truth for evidencesamples in an existing evidence retrieval dataset by annotating a more completeset of evidence samples for both image and text retrieval. Our experimentalresults on two datasets demonstrate the superiority of the proposed approach inboth evidence retrieval and fact verification tasks and also bettergeneralization capability across dataset compared to the supervised baseline.</description><author>Sahar Tahmasebi, Eric Müller-Budack, Ralph Ewerth</author><pubDate>Fri, 19 Jul 2024 13:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14321v1</guid></item><item><title>AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description</title><link>http://arxiv.org/abs/2407.15850v1</link><description>Our objective is to generate Audio Descriptions (ADs) for both movies and TVseries in a training-free manner. We use the power of off-the-shelfVisual-Language Models (VLMs) and Large Language Models (LLMs), and developvisual and text prompting strategies for this task. Our contributions arethree-fold: (i) We demonstrate that a VLM can successfully name and refer tocharacters if directly prompted with character information through visualindications without requiring any fine-tuning; (ii) A two-stage process isdeveloped to generate ADs, with the first stage asking the VLM tocomprehensively describe the video, followed by a second stage utilising a LLMto summarise dense textual information into one succinct AD sentence; (iii) Anew dataset for TV audio description is formulated. Our approach, namedAutoAD-Zero, demonstrates outstanding performance (even competitive with somemodels fine-tuned on ground truth ADs) in AD generation for both movies and TVseries, achieving state-of-the-art CRITIC scores.</description><author>Junyu Xie, Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman</author><pubDate>Mon, 22 Jul 2024 17:59:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15850v1</guid></item><item><title>ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models</title><link>http://arxiv.org/abs/2403.20262v2</link><description>Research on Large Language Models (LLMs) has recently witnessed an increasinginterest in extending models' context size to better capture dependencieswithin long documents. While benchmarks have been proposed to assess long-rangeabilities, existing efforts primarily considered generic tasks that are notnecessarily aligned with real-world applications. In contrast, our workproposes a new benchmark for long-context LLMs focused on a practical meetingassistant scenario. In this scenario, the long contexts consist of transcriptsobtained by automatic speech recognition, presenting unique challenges for LLMsdue to the inherent noisiness and oral nature of such data. Our benchmark,named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271manually crafted questions and their ground-truth answers. Our experiments withrecent long-context LLMs on ELITR-Bench highlight a gap between open-source andproprietary models, especially when questions are asked sequentially within aconversation. We also provide a thorough analysis of our GPT-4-based evaluationmethod, encompassing insights from a crowdsourcing study. Our findings suggestthat while GPT-4's evaluation scores are correlated with human judges', itsability to differentiate among more than three score levels may be limited.</description><author>Thibaut Thonet, Jos Rozen, Laurent Besacier</author><pubDate>Mon, 22 Jul 2024 17:24:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.20262v2</guid></item><item><title>Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models</title><link>http://arxiv.org/abs/2407.16565v1</link><description>Recent surge in the accessibility of large language models (LLMs) to thegeneral population can lead to untrackable use of such models formedical-related recommendations. Language generation via LLMs models has twokey problems: firstly, they are prone to hallucination and therefore, for anymedical purpose they require scientific and factual grounding; secondly, LLMspose tremendous challenge to computational resources due to their giganticmodel size. In this work, we introduce pRAGe, a pipeline for RetrievalAugmented Generation and evaluation of medical paraphrases generation usingSmall Language Models (SLM). We study the effectiveness of SLMs and the impactof external knowledge base for medical paraphrase generation in French.</description><author>Ioana Buhnila, Aman Sinha, Mathieu Constant</author><pubDate>Tue, 23 Jul 2024 15:17:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16565v1</guid></item><item><title>Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering</title><link>http://arxiv.org/abs/2401.10711v4</link><description>Video Question Answering (VideoQA) aims to answer natural language questionsbased on the information observed in videos. Despite the recent success ofLarge Multimodal Models (LMMs) in image-language understanding and reasoning,they deal with VideoQA insufficiently, by simply taking uniformly sampledframes as visual inputs, which ignores question-relevant visual clues.Moreover, there are no human annotations for question-critical timestamps inexisting VideoQA datasets. In light of this, we propose a novel weaklysupervised framework to enforce the LMMs to reason out the answers withquestion-critical moments as visual inputs. Specifically, we first fuse thequestion and answer pairs as event descriptions to find multiple keyframes astarget moments and pseudo-labels, with the visual-language alignment capabilityof the CLIP models. With these pseudo-labeled keyframes as additionally weaksupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)module. GCG learns multiple Gaussian functions to characterize the temporalstructure of the video, and sample question-critical frames as positive momentsto be the visual inputs of LMMs. Extensive experiments on several benchmarksverify the effectiveness of our framework, and we achieve substantialimprovements compared to previous state-of-the-art methods.</description><author>Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge</author><pubDate>Tue, 23 Jul 2024 10:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.10711v4</guid></item><item><title>DOPRA: Decoding Over-accumulation Penalization and Re-allocation in Specific Weighting Layer</title><link>http://arxiv.org/abs/2407.15130v2</link><description>In this work, we introduce DOPRA, a novel approach designed to mitigatehallucinations in multi-modal large language models (MLLMs). Unlike existingsolutions that typically involve costly supplementary training data or theintegration of external knowledge sources, DOPRA innovatively addresseshallucinations by decoding specific weighted layer penalties andredistribution, offering an economical and effective solution withoutadditional resources. DOPRA is grounded in unique insights into the intrinsicmechanisms controlling hallucinations within MLLMs, especially the models'tendency to over-rely on a subset of summary tokens in the self-attentionmatrix, neglecting critical image-related information. This phenomenon isparticularly pronounced in certain strata. To counteract this over-reliance,DOPRA employs a strategy of weighted overlay penalties and redistribution inspecific layers, such as the 12th layer, during the decoding process.Furthermore, DOPRA includes a retrospective allocation process that re-examinesthe sequence of generated tokens, allowing the algorithm to reallocate tokenselection to better align with the actual image content, thereby reducing theincidence of hallucinatory descriptions in auto-generated captions. Overall,DOPRA represents a significant step forward in improving the output quality ofMLLMs by systematically reducing hallucinations through targeted adjustmentsduring the decoding process.</description><author>Jinfeng Wei, Xiaofeng Zhang</author><pubDate>Tue, 23 Jul 2024 09:30:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.15130v2</guid></item><item><title>PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets</title><link>http://arxiv.org/abs/2407.16329v1</link><description>Acute stroke demands prompt diagnosis and treatment to achieve optimalpatient outcomes. However, the intricate and irregular nature of clinical dataassociated with acute stroke, particularly blood pressure (BP) measurements,presents substantial obstacles to effective visual analytics anddecision-making. Through a year-long collaboration with experiencedneurologists, we developed PhenoFlow, a visual analytics system that leveragesthe collaboration between human and Large Language Models (LLMs) to analyze theextensive and complex data of acute ischemic stroke patients. PhenoFlowpioneers an innovative workflow, where the LLM serves as a data wrangler whileneurologists explore and supervise the output using visualizations and naturallanguage interactions. This approach enables neurologists to focus more ondecision-making with reduced cognitive load. To protect sensitive patientinformation, PhenoFlow only utilizes metadata to make inferences and synthesizeexecutable codes, without accessing raw patient data. This ensures that theresults are both reproducible and interpretable while maintaining patientprivacy. The system incorporates a slice-and-wrap design that employs temporalfolding to create an overlaid circular visualization. Combined with a linearbar graph, this design aids in exploring meaningful patterns within irregularlymeasured BP data. Through case studies, PhenoFlow has demonstrated itscapability to support iterative analysis of extensive clinical datasets,reducing cognitive load and enabling neurologists to make well-informeddecisions. Grounded in long-term collaboration with domain experts, ourresearch demonstrates the potential of utilizing LLMs to tackle currentchallenges in data-driven clinical decision-making for acute ischemic strokepatients.</description><author>Jaeyoung Kim, Sihyeon Lee, Hyeon Jeon, Keon-Joo Lee, Hee-Joon Bae, Bohyoung Kim, Jinwook Seo</author><pubDate>Tue, 23 Jul 2024 09:25:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16329v1</guid></item><item><title>Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion</title><link>http://arxiv.org/abs/2407.16127v1</link><description>Traditional knowledge graph (KG) completion models learn embeddings topredict missing facts. Recent works attempt to complete KGs in atext-generation manner with large language models (LLMs). However, they need toground the output of LLMs to KG entities, which inevitably brings errors. Inthis paper, we present a finetuning framework, DIFT, aiming to unleash the KGcompletion ability of LLMs and avoid grounding errors. Given an incompletefact, DIFT employs a lightweight model to obtain candidate entities andfinetunes an LLM with discrimination instructions to select the correct onefrom the given candidates. To improve performance while reducing instructiondata, DIFT uses a truncated sampling method to select useful facts forfinetuning and injects KG embeddings into the LLM. Extensive experiments onbenchmark datasets demonstrate the effectiveness of our proposed framework.</description><author>Yang Liu, Xiaobin Tian, Zequn Sun, Wei Hu</author><pubDate>Tue, 23 Jul 2024 02:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16127v1</guid></item><item><title>Tree-Planner: Efficient Close-loop Task Planning with Large Language Models</title><link>http://arxiv.org/abs/2310.08582v2</link><description>This paper studies close-loop task planning, which refers to the process ofgenerating a sequence of skills (a plan) to accomplish a specific goal whileadapting the plan based on real-time observations. Recently, prompting LargeLanguage Models (LLMs) to generate actions iteratively has become a prevalentparadigm due to its superior performance and user-friendliness. However, thisparadigm is plagued by two inefficiencies: high token consumption and redundanterror correction, both of which hinder its scalability for large-scale testingand applications. To address these issues, we propose Tree-Planner, whichreframes task planning with LLMs into three distinct phases: plan sampling,action tree construction, and grounded deciding. Tree-Planner starts by usingan LLM to sample a set of potential plans before execution, followed by theaggregation of them to form an action tree. Finally, the LLM performs atop-down decision-making process on the tree, taking into account real-timeenvironmental information. Experiments show that Tree-Planner achievesstate-of-the-art performance while maintaining high efficiency. By decomposingLLM queries into a single plan-sampling call and multiple grounded-decidingcalls, a considerable part of the prompt are less likely to be repeatedlyconsumed. As a result, token consumption is reduced by 92.2% compared to thepreviously best-performing model. Additionally, by enabling backtracking on theaction tree as needed, the correction process becomes more flexible, leading toa 40.5% decrease in error corrections.</description><author>Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo</author><pubDate>Wed, 24 Jul 2024 12:25:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.08582v2</guid></item><item><title>The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2407.18044v1</link><description>Digital health chatbots powered by Large Language Models (LLMs) have thepotential to significantly improve personal health management for chronicconditions by providing accessible and on-demand health coaching andquestion-answering. However, these chatbots risk providing unverified andinaccurate information because LLMs generate responses based on patternslearned from diverse internet data. Retrieval Augmented Generation (RAG) canhelp mitigate hallucinations and inaccuracies in LLM responses by grounding iton reliable content. However, efficiently and accurately retrieving mostrelevant set of content for real-time user questions remains a challenge. Inthis work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), anovel approach that pre-computes a database of potential queries from a contentbase using LLMs. For an incoming patient question, QB-RAG efficiently matchesit against this pre-generated query database using vector search, improvingalignment between user questions and the content. We establish a theoreticalfoundation for QB-RAG and provide a comparative analysis of existing retrievalenhancement techniques for RAG systems. Finally, our empirical evaluationdemonstrates that QB-RAG significantly improves the accuracy of healthcarequestion answering, paving the way for robust and trustworthy LLM applicationsin digital health.</description><author>Eric Yang, Jonathan Amar, Jong Ha Lee, Bhawesh Kumar, Yugang Jia</author><pubDate>Thu, 25 Jul 2024 13:47:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18044v1</guid></item><item><title>PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric Videos</title><link>http://arxiv.org/abs/2407.09503v2</link><description>Intelligent assistance involves not only understanding but also action.Existing ego-centric video datasets contain rich annotations of the videos, butnot of actions that an intelligent assistant could perform in the moment. Toaddress this gap, we release PARSE-Ego4D, a new set of personal actionrecommendation annotations for the Ego4D dataset. We take a multi-stageapproach to generating and evaluating these annotations. First, we used aprompt-engineered large language model (LLM) to generate context-aware actionsuggestions and identified over 18,000 action suggestions. While thesesynthetic action suggestions are valuable, the inherent limitations of LLMsnecessitate human evaluation. To ensure high-quality and user-centeredrecommendations, we conducted a large-scale human annotation study thatprovides grounding in human preferences for all of PARSE-Ego4D. We analyze theinter-rater agreement and evaluate subjective preferences of participants.Based on our synthetic dataset and complete human annotations, we proposeseveral new tasks for action suggestions based on ego-centric videos. Weencourage novel solutions that improve latency and energy requirements. Theannotations in PARSE-Ego4D will support researchers and developers who areworking on building action recommendation systems for augmented and virtualreality systems.</description><author>Steven Abreu, Tiffany D. Do, Karan Ahuja, Eric J. Gonzalez, Lee Payne, Daniel McDuff, Mar Gonzalez-Franco</author><pubDate>Thu, 25 Jul 2024 13:29:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09503v2</guid></item><item><title>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</title><link>http://arxiv.org/abs/2407.06863v3</link><description>Text-to-Image (T2I) models are being increasingly adopted in diverse globalcommunities where they create visual representations of their unique cultures.Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realismof generated images, overlooking the critical dimension of cultural competence.In this work, we introduce a framework to evaluate cultural competence of T2Imodels along two crucial dimensions: cultural awareness and cultural diversity,and present a scalable approach using a combination of structured knowledgebases and large language models to build a large dataset of cultural artifactsto enable this evaluation. In particular, we apply this approach to build CUBE(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark toevaluate cultural competence of T2I models. CUBE covers cultural artifactsassociated with 8 countries across different geo-cultural regions and along 3concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set ofhigh-quality prompts that enable the evaluation of cultural awareness, and 2)CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding toevaluate cultural diversity. We also introduce cultural diversity as a novelT2I evaluation component, leveraging quality-weighted Vendi score. Ourevaluations reveal significant gaps in the cultural awareness of existingmodels across countries and provide valuable insights into the culturaldiversity of T2I outputs for under-specified prompts. Our methodology isextendable to other cultural regions and concepts, and can facilitate thedevelopment of T2I models that better cater to the global population.</description><author>Nithish Kannen, Arif Ahmad, Marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, Shachi Dave</author><pubDate>Wed, 24 Jul 2024 18:09:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06863v3</guid></item><item><title>Neurosymbolic AI for Enhancing Instructability in Generative AI</title><link>http://arxiv.org/abs/2407.18722v1</link><description>Generative AI, especially via Large Language Models (LLMs), has transformedcontent creation across text, images, and music, showcasing capabilities infollowing instructions through prompting, largely facilitated by instructiontuning. Instruction tuning is a supervised fine-tuning method where LLMs aretrained on datasets formatted with specific tasks and correspondinginstructions. This method systematically enhances the model's ability tocomprehend and execute the provided directives. Despite these advancements,LLMs still face challenges in consistently interpreting complex, multi-stepinstructions and generalizing them to novel tasks, which are essential forbroader applicability in real-world scenarios. This article explores whyneurosymbolic AI offers a better path to enhance the instructability of LLMs.We explore the use a symbolic task planner to decompose high-level instructionsinto structured tasks, a neural semantic parser to ground these tasks intoexecutable actions, and a neuro-symbolic executor to implement these actionswhile dynamically maintaining an explicit representation of state. We also seekto show that neurosymbolic approach enhances the reliability andcontext-awareness of task execution, enabling LLMs to dynamically interpret andrespond to a wider range of instructional contexts with greater precision andflexibility.</description><author>Amit Sheth, Vishal Pallagani, Kaushik Roy</author><pubDate>Fri, 26 Jul 2024 13:15:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18722v1</guid></item><item><title>Grounding Language Models for Visual Entity Recognition</title><link>http://arxiv.org/abs/2402.18695v2</link><description>We introduce AutoVER, an Autoregressive model for Visual Entity Recognition.Our model extends an autoregressive Multi-modal Large Language Model byemploying retrieval augmented constrained generation. It mitigates lowperformance on out-of-domain entities while excelling in queries that requirevisually-situated reasoning. Our method learns to distinguish similar entitieswithin a vast label space by contrastively training on hard negative pairs inparallel with a sequence-to-sequence objective without an external retriever.During inference, a list of retrieved candidate answers explicitly guideslanguage generation by removing invalid decoding paths. The proposed methodachieves significant improvements across different dataset splits in therecently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split risesfrom 32.7% to 61.5%. It also demonstrates superior performance on the unseenand query splits by a substantial double-digit margin.</description><author>Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, Vicente Ordonez</author><pubDate>Fri, 26 Jul 2024 06:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.18695v2</guid></item><item><title>FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization</title><link>http://arxiv.org/abs/2404.13671v2</link><description>Zero-shot anomaly detection (ZSAD) methods entail detecting anomaliesdirectly without access to any known normal or abnormal samples within thetarget item categories. Existing approaches typically rely on the robustgeneralization capabilities of multimodal pretrained models, computingsimilarities between manually crafted textual features representing "normal" or"abnormal" semantics and image features to detect anomalies and localizeanomalous patches. However, the generic descriptions of "abnormal" often failto precisely match diverse types of anomalies across different objectcategories. Additionally, computing feature similarities for single patchesstruggles to pinpoint specific locations of anomalies with various sizes andscales. To address these issues, we propose a novel ZSAD method called FiLo,comprising two components: adaptively learned Fine-Grained Description (FG-Des)and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introducesfine-grained anomaly descriptions for each category using Large Language Models(LLMs) and employs adaptively learned textual templates to enhance the accuracyand interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO forpreliminary localization, position-enhanced text prompts, and Multi-scaleMulti-shape Cross-modal Interaction (MMCI) module, facilitates more accuratelocalization of anomalies of different sizes and shapes. Experimental resultson datasets like MVTec and VisA demonstrate that FiLo significantly improvesthe performance of ZSAD in both detection and localization, achievingstate-of-the-art performance with an image-level AUC of 83.9% and a pixel-levelAUC of 95.9% on the VisA dataset. Code is available athttps://github.com/CASIA-IVA-Lab/FiLo.</description><author>Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Hao Li, Ming Tang, Jinqiao Wang</author><pubDate>Fri, 26 Jul 2024 02:42:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13671v2</guid></item><item><title>PersLLM: A Personified Training Approach for Large Language Models</title><link>http://arxiv.org/abs/2407.12393v3</link><description>Large language models exhibit aspects of human-level intelligence thatcatalyze their application as human-like agents in domains such as socialsimulations, human-machine interactions, and collaborative multi-agent systems.However, the absence of distinct personalities, such as displaying ingratiatingbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMsutility in practical applications. Addressing this, the development ofpersonality traits in LLMs emerges as a crucial area of research to unlocktheir latent potential. Existing methods to personify LLMs generally involvestrategies like employing stylized training data for instruction tuning orusing prompt engineering to simulate different personalities. These methodsonly capture superficial linguistic styles instead of the core of personalitiesand are therefore not stable. In this study, we propose PersLLM, integratingpsychology-grounded principles of personality: social practice, consistency,and dynamic development, into a comprehensive training methodology. Weincorporate personality traits directly into the model parameters, enhancingthe model's resistance to induction, promoting consistency, and supporting thedynamic evolution of personality. Single-agent evaluation validates ourmethod's superiority, as it produces responses more aligned with referencepersonalities compared to other approaches. Case studies for multi-agentcommunication highlight its benefits in enhancing opinion consistency withinindividual agents and fostering collaborative creativity among multiple agentsin dialogue contexts, potentially benefiting human simulation and multi-agentcooperation. Additionally, human-agent interaction evaluations indicate thatour personified models significantly enhance interactive experiences,underscoring the practical implications of our research.</description><author>Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhenghao Liu, Zhiyuan Liu, Maosong Sun</author><pubDate>Fri, 26 Jul 2024 02:34:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12393v3</guid></item><item><title>Debating with More Persuasive LLMs Leads to More Truthful Answers</title><link>http://arxiv.org/abs/2402.06782v4</link><description>Common methods for aligning large language models (LLMs) with desiredbehaviour heavily rely on human-labelled data. However, as models growincreasingly sophisticated, they will surpass human expertise, and the role ofhuman evaluation will evolve into non-experts overseeing experts. Inanticipation of this, we ask: can weaker models assess the correctness ofstronger models? We investigate this question in an analogous setting, wherestronger models (experts) possess the necessary information to answer questionsand weaker models (non-experts) lack this information. The method we evaluateis debate, where two LLM experts each argue for a different answer, and anon-expert selects the answer. We find that debate consistently helps bothnon-expert models and humans answer questions, achieving 76% and 88% accuracyrespectively (naive baselines obtain 48% and 60%). Furthermore, optimisingexpert debaters for persuasiveness in an unsupervised manner improvesnon-expert ability to identify the truth in debates. Our results provideencouraging empirical evidence for the viability of aligning models with debatein the absence of ground truth.</description><author>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, Ethan Perez</author><pubDate>Thu, 25 Jul 2024 23:32:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06782v4</guid></item><item><title>UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</title><link>http://arxiv.org/abs/2407.18391v1</link><description>Smaller-scale Vision-Langauge Models (VLMs) often claim to perform on parwith larger models in general-domain visual grounding and question-answeringbenchmarks while offering advantages in computational efficiency and storage.However, their ability to handle rare objects, which fall into the long tail ofdata distributions, is less understood. To rigorously evaluate this aspect, weintroduce the "Uncontextualized Uncommon Objects" (UOUO) benchmark. Thisbenchmark focuses on systematically testing VLMs with both large and smallparameter counts on rare and specialized objects. Our comprehensive analysisreveals that while smaller VLMs maintain competitive performance on commondatasets, they significantly underperform on tasks involving uncommon objects.We also propose an advanced, scalable pipeline for data collection andcleaning, ensuring the UOUO benchmark provides high-quality, challenginginstances. These findings highlight the need to consider long-taildistributions when assessing the true capabilities of VLMs.</description><author>Xinyu Pi, Mingyuan Wu, Jize Jiang, Haozhen Zheng, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu</author><pubDate>Thu, 25 Jul 2024 20:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18391v1</guid></item><item><title>Generative Retrieval with Preference Optimization for E-commerce Search</title><link>http://arxiv.org/abs/2407.19829v1</link><description>Generative retrieval introduces a groundbreaking paradigm to documentretrieval by directly generating the identifier of a pertinent document inresponse to a specific query. This paradigm has demonstrated considerablebenefits and potential, particularly in representation and generalizationcapabilities, within the context of large language models. However, it facessignificant challenges in E-commerce search scenarios, including the complexityof generating detailed item titles from brief queries, the presence of noise initem titles with weak language order, issues with long-tail queries, and theinterpretability of results. To address these challenges, we have developed aninnovative framework for E-commerce search, called generative retrieval withpreference optimization. This framework is designed to effectively learn andalign an autoregressive model with target data, subsequently generating thefinal item through constraint-based beam search. By employing multi-spanidentifiers to represent raw item titles and transforming the task ofgenerating titles from queries into the task of generating multi-spanidentifiers from queries, we aim to simplify the generation process. Theframework further aligns with human preferences using click data and employs aconstrained search method to identify key spans for retrieving the final item,thereby enhancing result interpretability. Our extensive experiments show thatthis framework achieves competitive performance on a real-world dataset, andonline A/B tests demonstrate the superiority and effectiveness in improvingconversion gains.</description><author>Mingming Li, Huimu Wang, Zuxu Chen, Guangtao Nie, Yiming Qiu, Binbin Wang, Guoyu Tang, Lin Liu, Jingwei Zhuo</author><pubDate>Mon, 29 Jul 2024 09:31:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19829v1</guid></item><item><title>Do Text-to-Vis Benchmarks Test Real Use of Visualisations?</title><link>http://arxiv.org/abs/2407.19726v1</link><description>Large language models are able to generate code for visualisations inresponse to user requests. This is a useful application, and an appealing onefor NLP research because plots of data provide grounding for language. However,there are relatively few benchmarks, and it is unknown whether those that existare representative of what people do in practice. This paper aims to answerthat question through an empirical study comparing benchmark datasets and codefrom public repositories. Our findings reveal a substantial gap in datasets,with evaluations not testing the same distribution of chart types, attributes,and the number of actions. The only representative dataset requiresmodification to become an end-to-end and practical benchmark. This shows thatnew, more benchmarks are needed to support the development of systems thattruly address users' visualisation needs. These observations will guide futuredata creation, highlighting which features hold genuine significance for users.</description><author>Hy Nguyen, Xuefei He, Andrew Reeson, Cecile Paris, Josiah Poon, Jonathan K. Kummerfeld</author><pubDate>Mon, 29 Jul 2024 06:13:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19726v1</guid></item><item><title>LLM4SGG: Large Language Models for Weakly Supervised Scene Graph Generation</title><link>http://arxiv.org/abs/2310.10404v8</link><description>Weakly-Supervised Scene Graph Generation (WSSGG) research has recentlyemerged as an alternative to the fully-supervised approach that heavily relieson costly annotations. In this regard, studies on WSSGG have utilized imagecaptions to obtain unlocalized triplets while primarily focusing on groundingthe unlocalized triplets over image regions. However, they have overlooked thetwo issues involved in the triplet formation process from the captions: 1)Semantic over-simplification issue arises when extracting triplets fromcaptions, where fine-grained predicates in captions are undesirably convertedinto coarse-grained predicates, resulting in a long-tailed predicatedistribution, and 2) Low-density scene graph issue arises when aligning thetriplets in the caption with entity/predicate classes of interest, where manytriplets are discarded and not used in training, leading to insufficientsupervision. To tackle the two issues, we propose a new approach, i.e., LargeLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the twoissues by leveraging the LLM's in-depth understanding of language and reasoningability during the extraction of triplets from captions and alignment ofentity/predicate classes with target data. To further engage the LLM in theseprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shotlearning strategy. To validate the effectiveness of LLM4SGG, we conductextensive experiments on Visual Genome and GQA datasets, showing significantimprovements in both Recall@K and mean Recall@K compared to thestate-of-the-art WSSGG methods. A further appeal is that LLM4SGG isdata-efficient, enabling effective model training with a small amount oftraining images.</description><author>Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park</author><pubDate>Mon, 29 Jul 2024 04:47:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10404v8</guid></item><item><title>Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models</title><link>http://arxiv.org/abs/2407.19474v1</link><description>Imagine observing someone scratching their arm; to understand why, additionalcontext would be necessary. However, spotting a mosquito nearby wouldimmediately offer a likely explanation for the person's discomfort, therebyalleviating the need for further information. This example illustrates howsubtle visual cues can challenge our cognitive skills and demonstrates thecomplexity of interpreting visual scenarios. To study these skills, we presentVisual Riddles, a benchmark aimed to test vision and language models on visualriddles requiring commonsense and world knowledge. The benchmark comprises 400visual riddles, each featuring a unique image created by a variety oftext-to-image models, question, ground-truth answer, textual hint, andattribution. Human evaluation reveals that existing models lag significantlybehind human performance, which is at 82\% accuracy, with Gemini-Pro-1.5leading with 40\% accuracy. Our benchmark comes with automatic evaluation tasksto make assessment scalable. These findings underscore the potential of VisualRiddles as a valuable resource for enhancing vision and language models'capabilities in interpreting complex visual scenarios.</description><author>Nitzan Bitton-Guetta, Aviv Slobodkin, Aviya Maimon, Eliya Habba, Royi Rassin, Yonatan Bitton, Idan Szpektor, Amir Globerson, Yuval Elovici</author><pubDate>Sun, 28 Jul 2024 11:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19474v1</guid></item><item><title>Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models</title><link>http://arxiv.org/abs/2407.00569v2</link><description>Though advanced in understanding visual information with human languages,Large Vision-Language Models (LVLMs) still suffer from multimodalhallucinations. A natural concern is that during multimodal interaction, thegenerated hallucinations could influence the LVLMs' subsequent generation.Thus, we raise a question: When presented with a query relevant to thepreviously generated hallucination, will LVLMs be misled and respondincorrectly, even though the ground visual information exists? To answer this,we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors whenencountering generated hallucinations, where LVLMs are required to answerspecific visual questions within a curated hallucinatory conversation.Crucially, our experiment shows that the performance of open-source LVLMs dropsby at least $31\%$, indicating that LVLMs are prone to accept the generatedhallucinations and make false claims that they would not have supported withoutdistractions. We term this phenomenon Multimodal Hallucination Snowballing. Tomitigate this, we further propose a training-free method called Residual VisualDecoding, where we revise the output distribution of LVLMs with the one derivedfrom the residual visual input, providing models with direct access to thevisual information. Experiments show that our method can mitigate more than$24\%$ of the snowballed multimodal hallucination while maintainingcapabilities.</description><author>Weihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma, Yuan Xu, Bing Qin</author><pubDate>Sun, 28 Jul 2024 08:08:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00569v2</guid></item><item><title>C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models</title><link>http://arxiv.org/abs/2402.03181v5</link><description>Despite the impressive capabilities of large language models (LLMs) acrossdiverse applications, they still suffer from trustworthiness issues, such ashallucinations and misalignments. Retrieval-augmented language models (RAG)have been proposed to enhance the credibility of generations by groundingexternal knowledge, but the theoretical understandings of their generationrisks remains unexplored. In this paper, we answer: 1) whether RAG can indeedlead to low generation risks, 2) how to provide provable guarantees on thegeneration risks of RAG and vanilla LLMs, and 3) what sufficient conditionsenable RAG models to reduce generation risks. We propose C-RAG, the firstframework to certify generation risks for RAG models. Specifically, we provideconformal risk analysis for RAG models and certify an upper confidence bound ofgeneration risks, which we refer to as conformal generation risk. We alsoprovide theoretical guarantees on conformal generation risks for generalbounded risk functions under test distribution shifts. We prove that RAGachieves a lower conformal generation risk than that of a single LLM when thequality of the retrieval model and transformer is non-trivial. Our intensiveempirical results demonstrate the soundness and tightness of our conformalgeneration risk guarantees across four widely-used NLP datasets on fourstate-of-the-art retrieval models.</description><author>Mintong Kang, Nezihe Merve Gürel, Ning Yu, Dawn Song, Bo Li</author><pubDate>Tue, 30 Jul 2024 02:47:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03181v5</guid></item><item><title>Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models</title><link>http://arxiv.org/abs/2407.00569v3</link><description>Though advanced in understanding visual information with human languages,Large Vision-Language Models (LVLMs) still suffer from multimodalhallucinations. A natural concern is that during multimodal interaction, thegenerated hallucinations could influence the LVLMs' subsequent generation.Thus, we raise a question: When presented with a query relevant to thepreviously generated hallucination, will LVLMs be misled and respondincorrectly, even though the ground visual information exists? To answer this,we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors whenencountering generated hallucinations, where LVLMs are required to answerspecific visual questions within a curated hallucinatory conversation.Crucially, our experiment shows that the performance of open-source LVLMs dropsby at least $31\%$, indicating that LVLMs are prone to accept the generatedhallucinations and make false claims that they would not have supported withoutdistractions. We term this phenomenon Multimodal Hallucination Snowballing. Tomitigate this, we further propose a training-free method called Residual VisualDecoding, where we revise the output distribution of LVLMs with the one derivedfrom the residual visual input, providing models with direct access to thevisual information. Experiments show that our method can mitigate more than$24\%$ of the snowballed multimodal hallucination while maintainingcapabilities.</description><author>Weihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma, Yuan Xu, Bing Qin</author><pubDate>Wed, 31 Jul 2024 13:08:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00569v3</guid></item><item><title>AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models</title><link>http://arxiv.org/abs/2401.09002v4</link><description>Ensuring the security of large language models (LLMs) against attacks hasbecome increasingly urgent, with jailbreak attacks representing one of the mostsophisticated threats. To deal with such risks, we introduce an innovativeframework that can help evaluate the effectiveness of jailbreak attacks onLLMs. Unlike traditional binary evaluations focusing solely on the robustnessof LLMs, our method assesses the effectiveness of the attacking promptsthemselves. We present two distinct evaluation frameworks: a coarse-grainedevaluation and a fine-grained evaluation. Each framework uses a scoring rangefrom 0 to 1, offering unique perspectives and allowing for the assessment ofattack effectiveness in different scenarios. Additionally, we develop acomprehensive ground truth dataset specifically tailored for jailbreak prompts.This dataset serves as a crucial benchmark for our current study and provides afoundational resource for future research. By comparing with traditionalevaluation methods, our study shows that the current results align withbaseline metrics while offering a more nuanced and fine-grained assessment. Italso helps identify potentially harmful attack prompts that might appearharmless in traditional evaluations. Overall, our work establishes a solidfoundation for assessing a broader range of attack prompts in the area ofprompt injection.</description><author>Dong shu, Mingyu Jin, Chong Zhang, Liangyao Li, Zihao Zhou, Yongfeng Zhang</author><pubDate>Wed, 31 Jul 2024 06:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09002v4</guid></item><item><title>Building AI Agents for Autonomous Clouds: Challenges and Design Principles</title><link>http://arxiv.org/abs/2407.12165v2</link><description>The rapid growth in the use of Large Language Models (LLMs) and AI Agents aspart of software development and deployment is revolutionizing the informationtechnology landscape. While code generation receives significant attention, ahigher-impact application lies in using AI agents for operational resilience ofcloud services, which currently require significant human effort and domainknowledge. There is a growing interest in AI for IT Operations (AIOps) whichaims to automate complex operational tasks, like fault localization and rootcause analysis, thereby reducing human intervention and customer impact.However, achieving the vision of autonomous and self-healing clouds throughAIOps is hampered by the lack of standardized frameworks for building,evaluating, and improving AIOps agents. This vision paper lays the groundworkfor such a framework by first framing the requirements and then discussingdesign decisions that satisfy them. We also propose AIOpsLab, a prototypeimplementation leveraging agent-cloud-interface that orchestrates anapplication, injects real-time faults using chaos engineering, and interfaceswith an agent to localize and resolve the faults. We report promising resultsand lay the groundwork to build a modular and robust framework for building,evaluating, and improving agents for autonomous clouds.</description><author>Manish Shetty, Yinfang Chen, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Xuchao Zhang, Jonathan Mace, Dax Vandevoorde, Pedro Las-Casas, Shachee Mishra Gupta, Suman Nath, Chetan Bansal, Saravan Rajmohan</author><pubDate>Wed, 31 Jul 2024 06:01:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12165v2</guid></item><item><title>M^2ConceptBase: A Fine-Grained Aligned Concept-Centric Multimodal Knowledge Base</title><link>http://arxiv.org/abs/2312.10417v2</link><description>Multimodal knowledge bases (MMKBs) provide cross-modal aligned knowledgecrucial for multimodal tasks. However, the images in existing MMKBs aregenerally collected for entities in encyclopedia knowledge graphs. Therefore,detailed groundings of visual semantics with linguistic concepts are lacking,which are essential for the visual concept cognition ability of multimodalmodels. Addressing this gap, we introduce M^2ConceptBase, the firstconcept-centric MMKB. M^2ConceptBase models concepts as nodes with associatedimages and detailed textual descriptions. We propose a context-aware multimodalsymbol grounding approach to align concept-image and concept-description pairsusing context information from image-text datasets. Comprising 951K images and152K concepts, M^2ConceptBase links each concept to an average of 6.27 imagesand a single description, ensuring comprehensive visual and textual semantics.Human studies confirm more than 95% alignment accuracy, underscoring itsquality. Additionally, our experiments demonstrate that M^2ConceptBasesignificantly enhances VQA model performance on the OK-VQA task. M^2ConceptBasealso substantially improves the fine-grained concept understanding capabilitiesof multimodal large language models through retrieval augmentation in twoconcept-related tasks, highlighting its value.</description><author>Zhiwei Zha, Jiaan Wang, Zhixu Li, Xiangru Zhu, Wei Song, Yanghua Xiao</author><pubDate>Thu, 01 Aug 2024 08:03:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10417v2</guid></item><item><title>OmniParser for Pure Vision Based GUI Agent</title><link>http://arxiv.org/abs/2408.00203v1</link><description>The recent success of large vision language models shows great potential indriving the agent system operating on user interfaces. However, we argue thatthe power multimodal models like GPT-4V as a general agent on multipleoperating systems across different applications is largely underestimated dueto the lack of a robust screen parsing technique capable of: 1) reliablyidentifying interactable icons within the user interface, and 2) understandingthe semantics of various elements in a screenshot and accurately associate theintended action with the corresponding region on the screen. To fill thesegaps, we introduce \textsc{OmniParser}, a comprehensive method for parsing userinterface screenshots into structured elements, which significantly enhancesthe ability of GPT-4V to generate actions that can be accurately grounded inthe corresponding regions of the interface. We first curated an interactableicon detection dataset using popular webpages and an icon description dataset.These datasets were utilized to fine-tune specialized models: a detection modelto parse interactable regions on the screen and a caption model to extract thefunctional semantics of the detected elements. \textsc{OmniParser}significantly improves GPT-4V's performance on ScreenSpot benchmark. And onMind2Web and AITW benchmark, \textsc{OmniParser} with screenshot only inputoutperforms the GPT-4V baselines requiring additional information outside ofscreenshot.</description><author>Yadong Lu, Jianwei Yang, Yelong Shen, Ahmed Awadallah</author><pubDate>Thu, 01 Aug 2024 00:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.00203v1</guid></item><item><title>Bridging Information Gaps in Dialogues With Grounded Exchanges Using Knowledge Graphs</title><link>http://arxiv.org/abs/2408.01088v1</link><description>Knowledge models are fundamental to dialogue systems for enablingconversational interactions, which require handling domain-specific knowledge.Ensuring effective communication in information-providing conversations entailsaligning user understanding with the knowledge available to the system.However, dialogue systems often face challenges arising from semanticinconsistencies in how information is expressed in natural language compared tohow it is represented within the system's internal knowledge. To address thisproblem, we study the potential of large language models for conversationalgrounding, a mechanism to bridge information gaps by establishing sharedknowledge between dialogue participants. Our approach involves annotating humanconversations across five knowledge domains to create a new dialogue corpuscalled BridgeKG. Through a series of experiments on this dataset, weempirically evaluate the capabilities of large language models in classifyinggrounding acts and identifying grounded information items within a knowledgegraph structure. Our findings offer insights into how these models usein-context learning for conversational grounding tasks and common predictionerrors, which we illustrate with examples from challenging dialogues. Wediscuss how the models handle knowledge graphs as a semantic layer betweenunstructured dialogue utterances and structured information items.</description><author>Phillip Schneider, Nektarios Machner, Kristiina Jokinen, Florian Matthes</author><pubDate>Fri, 02 Aug 2024 08:07:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01088v1</guid></item><item><title>UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model</title><link>http://arxiv.org/abs/2408.02503v1</link><description>Significant advancements has recently been achieved in the field ofmulti-modal large language models (MLLMs), demonstrating their remarkablecapabilities in understanding and reasoning across diverse tasks. However,these models are often trained for specific tasks and rely on task-specificinput-output formats, limiting their applicability to a broader range of tasks.This raises a fundamental question: Can we develop a unified approach torepresent and handle different multi-modal tasks to maximize thegeneralizability of MLLMs? In this paper, we propose UnifiedMLLM, acomprehensive model designed to represent various tasks using a unifiedrepresentation. Our model exhibits strong capabilities in comprehending theimplicit intent of user instructions and preforming reasoning. In addition togenerating textual responses, our model also outputs task tokens and groundingtokens, serving as indicators of task types and task granularity. These outputsare subsequently routed through the task router and directed to specific expertmodels for task completion. To train our model, we construct a task-specificdataset and an 100k multi-task dataset encompassing complex scenarios.Employing a three-stage training strategy, we equip our model with robustreasoning and task processing capabilities while preserving its generalizationcapacity and knowledge reservoir. Extensive experiments showcase the impressiveperformance of our unified representation approach across various tasks,surpassing existing methodologies. Furthermore, our approach exhibitsexceptional scalability and generality. Our code, model, and dataset will beavailable at \url{https://github.com/lzw-lzw/UnifiedMLLM}.</description><author>Zhaowei Li, Wei Wang, YiQing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, Tao Wang</author><pubDate>Mon, 05 Aug 2024 14:27:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02503v1</guid></item><item><title>Infusing Environmental Captions for Long-Form Video Language Grounding</title><link>http://arxiv.org/abs/2408.02336v1</link><description>In this work, we tackle the problem of long-form video-language grounding(VLG). Given a long-form video and a natural language query, a model shouldtemporally localize the precise moment that answers the query. Humans caneasily solve VLG tasks, even with arbitrarily long videos, by discardingirrelevant moments using extensive and robust knowledge gained from experience.Unlike humans, existing VLG methods are prone to fall into superficial cueslearned from small-scale datasets, even when they are within irrelevant frames.To overcome this challenge, we propose EI-VLG, a VLG method that leveragesricher textual information provided by a Multi-modal Large Language Model(MLLM) as a proxy for human experiences, helping to effectively excludeirrelevant frames. We validate the effectiveness of the proposed method viaextensive experiments on a challenging EgoNLQ benchmark.</description><author>Hyogun Lee, Soyeon Hong, Mujeen Sung, Jinwoo Choi</author><pubDate>Mon, 05 Aug 2024 09:19:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02336v1</guid></item><item><title>ViG-Bias: Visually Grounded Bias Discovery and Mitigation</title><link>http://arxiv.org/abs/2407.01996v3</link><description>The proliferation of machine learning models in critical decision makingprocesses has underscored the need for bias discovery and mitigationstrategies. Identifying the reasons behind a biased system is notstraightforward, since in many occasions they are associated with hiddenspurious correlations which are not easy to spot. Standard approaches rely onbias audits performed by analyzing model performance in pre-defined subgroupsof data samples, usually characterized by common attributes like gender orethnicity when it comes to people, or other specific attributes definingsemantically coherent groups of images. However, it is not always possible toknow a-priori the specific attributes defining the failure modes of visualrecognition systems. Recent approaches propose to discover these groups byleveraging large vision language models, which enable the extraction ofcross-modal embeddings and the generation of textual descriptions tocharacterize the subgroups where a certain model is underperforming. In thiswork, we argue that incorporating visual explanations (e.g. heatmaps generatedvia GradCAM or other approaches) can boost the performance of such biasdiscovery and mitigation frameworks. To this end, we introduce VisuallyGrounded Bias Discovery and Mitigation (ViG-Bias), a simple yet effectivetechnique which can be integrated to a variety of existing frameworks toimprove both, discovery and mitigation performance. Our comprehensiveevaluation shows that incorporating visual explanations enhances existingtechniques like DOMINO, FACTS and Bias-to-Text, across several challengingdatasets, including CelebA, Waterbirds, and NICO++.</description><author>Badr-Eddine Marani, Mohamed Hanini, Nihitha Malayarukil, Stergios Christodoulidis, Maria Vakalopoulou, Enzo Ferrante</author><pubDate>Sun, 04 Aug 2024 21:56:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.01996v3</guid></item><item><title>HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections</title><link>http://arxiv.org/abs/2404.16845v2</link><description>Internet image collections containing photos captured by crowds ofphotographers show promise for enabling digital exploration of large-scaletourist landmarks. However, prior works focus primarily on geometricreconstruction and visualization, neglecting the key role of language inproviding a semantic interface for navigation and fine-grained understanding.In constrained 3D domains, recent methods have leveraged vision-and-languagemodels as a strong prior of 2D visual semantics. While these models display anexcellent understanding of broad visual semantics, they struggle withunconstrained photo collections depicting such tourist landmarks, as they lackexpert knowledge of the architectural domain. In this work, we present alocalization system that connects neural representations of scenes depictinglarge-scale landmarks with text describing a semantic region within the scene,by harnessing the power of SOTA vision-and-language models with adaptations forunderstanding landmark scene semantics. To bolster such models withfine-grained knowledge, we leverage large-scale Internet data containing imagesof similar landmarks along with weakly-related textual information. Ourapproach is built upon the premise that images physically grounded in space canprovide a powerful supervision signal for localizing new concepts, whosesemantics may be unlocked from Internet textual metadata with large languagemodels. We use correspondences between views of scenes to bootstrap spatialunderstanding of these semantics, providing guidance for 3D-compatiblesegmentation that ultimately lifts to a volumetric scene representation. Ourresults show that HaLo-NeRF can accurately localize a variety of semanticconcepts related to architectural landmarks, surpassing the results of other 3Dmodels as well as strong 2D segmentation baselines. Our project page is athttps://tau-vailab.github.io/HaLo-NeRF/.</description><author>Chen Dudai, Morris Alper, Hana Bezalel, Rana Hanocka, Itai Lang, Hadar Averbuch-Elor</author><pubDate>Sun, 04 Aug 2024 18:51:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.16845v2</guid></item><item><title>Beyond Aesthetics: Cultural Competence in Text-to-Image Models</title><link>http://arxiv.org/abs/2407.06863v4</link><description>Text-to-Image (T2I) models are being increasingly adopted in diverse globalcommunities where they create visual representations of their unique cultures.Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realismof generated images, overlooking the critical dimension of cultural competence.In this work, we introduce a framework to evaluate cultural competence of T2Imodels along two crucial dimensions: cultural awareness and cultural diversity,and present a scalable approach using a combination of structured knowledgebases and large language models to build a large dataset of cultural artifactsto enable this evaluation. In particular, we apply this approach to build CUBE(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark toevaluate cultural competence of T2I models. CUBE covers cultural artifactsassociated with 8 countries across different geo-cultural regions and along 3concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set ofhigh-quality prompts that enable the evaluation of cultural awareness, and 2)CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding toevaluate cultural diversity. We also introduce cultural diversity as a novelT2I evaluation component, leveraging quality-weighted Vendi score. Ourevaluations reveal significant gaps in the cultural awareness of existingmodels across countries and provide valuable insights into the culturaldiversity of T2I outputs for under-specified prompts. Our methodology isextendable to other cultural regions and concepts, and can facilitate thedevelopment of T2I models that better cater to the global population.</description><author>Nithish Kannen, Arif Ahmad, Marco Andreetto, Vinodkumar Prabhakaran, Utsav Prabhu, Adji Bousso Dieng, Pushpak Bhattacharyya, Shachi Dave</author><pubDate>Sun, 04 Aug 2024 08:28:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.06863v4</guid></item><item><title>MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance</title><link>http://arxiv.org/abs/2408.01869v1</link><description>In the era of Large Language Models (LLMs), given their remarkable textunderstanding and generation abilities, there is an unprecedented opportunityto develop new, LLM-based methods for trustworthy medical knowledge synthesis,extraction and summarization. This paper focuses on the problem ofPharmacovigilance (PhV), where the significance and challenges lie inidentifying Adverse Drug Events (ADEs) from diverse text sources, such asmedical literature, clinical notes, and drug labels. Unfortunately, this taskis hindered by factors including variations in the terminologies of drugs andoutcomes, and ADE descriptions often being buried in large amounts of narrativetext. We present MALADE, the first effective collaborative multi-agent systempowered by LLM with Retrieval Augmented Generation for ADE extraction from druglabel data. This technique involves augmenting a query to an LLM with relevantinformation extracted from text resources, and instructing the LLM to compose aresponse consistent with the augmented data. MALADE is a general LLM-agnosticarchitecture, and its unique capabilities are: (1) leveraging a variety ofexternal sources, such as medical literature, drug labels, and FDA tools (e.g.,OpenFDA drug information API), (2) extracting drug-outcome association in astructured format along with the strength of the association, and (3) providingexplanations for established associations. Instantiated with GPT-4 Turbo orGPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an AreaUnder ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Ourimplementation leverages the Langroid multi-agent LLM framework and can befound at https://github.com/jihyechoi77/malade.</description><author>Jihye Choi, Nils Palumbo, Prasad Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, David Page</author><pubDate>Sat, 03 Aug 2024 22:14:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01869v1</guid></item><item><title>Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models</title><link>http://arxiv.org/abs/2407.00569v4</link><description>Though advanced in understanding visual information with human languages,Large Vision-Language Models (LVLMs) still suffer from multimodalhallucinations. A natural concern is that during multimodal interaction, thegenerated hallucinations could influence the LVLMs' subsequent generation.Thus, we raise a question: When presented with a query relevant to thepreviously generated hallucination, will LVLMs be misled and respondincorrectly, even though the ground visual information exists? To answer this,we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors whenencountering generated hallucinations, where LVLMs are required to answerspecific visual questions within a curated hallucinatory conversation.Crucially, our experiment shows that the performance of open-source LVLMs dropsby at least $31\%$, indicating that LVLMs are prone to accept the generatedhallucinations and make false claims that they would not have supported withoutdistractions. We term this phenomenon Multimodal Hallucination Snowballing. Tomitigate this, we further propose a training-free method called Residual VisualDecoding, where we revise the output distribution of LVLMs with the one derivedfrom the residual visual input, providing models with direct access to thevisual information. Experiments show that our method can mitigate more than$24\%$ of the snowballed multimodal hallucination while maintainingcapabilities.</description><author>Weihong Zhong, Xiaocheng Feng, Liang Zhao, Qiming Li, Lei Huang, Yuxuan Gu, Weitao Ma, Yuan Xu, Bing Qin</author><pubDate>Sat, 03 Aug 2024 17:52:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00569v4</guid></item><item><title>Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue</title><link>http://arxiv.org/abs/2406.06399v3</link><description>We study the limitations of Large Language Models (LLMs) for the task ofresponse generation in human-machine dialogue. Several techniques have beenproposed in the literature for different dialogue types (e.g., Open-Domain).However, the evaluations of these techniques have been limited in terms of baseLLMs, dialogue types and evaluation metrics. In this work, we extensivelyanalyze different LLM adaptation techniques when applied to different dialoguetypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialoguetypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.We evaluate the performance of in-context learning and fine-tuning techniquesacross datasets selected for each dialogue type. We assess the impact ofincorporating external knowledge to ground the generation in both scenarios ofRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistentevaluation and explainability criteria for automatic metrics and humanevaluation protocols. Our analysis shows that there is no universalbest-technique for adapting large language models as the efficacy of eachtechnique depends on both the base LLM and the specific type of dialogue. Lastbut not least, the assessment of the best adaptation technique should includehuman evaluation to avoid false expectations and outcomes derived fromautomatic metrics.</description><author>Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi</author><pubDate>Sat, 03 Aug 2024 15:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06399v3</guid></item><item><title>AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models</title><link>http://arxiv.org/abs/2401.09002v5</link><description>Ensuring the security of large language models (LLMs) against attacks hasbecome increasingly urgent, with jailbreak attacks representing one of the mostsophisticated threats. To deal with such risks, we introduce an innovativeframework that can help evaluate the effectiveness of jailbreak attacks onLLMs. Unlike traditional binary evaluations focusing solely on the robustnessof LLMs, our method assesses the effectiveness of the attacking promptsthemselves. We present two distinct evaluation frameworks: a coarse-grainedevaluation and a fine-grained evaluation. Each framework uses a scoring rangefrom 0 to 1, offering unique perspectives and allowing for the assessment ofattack effectiveness in different scenarios. Additionally, we develop acomprehensive ground truth dataset specifically tailored for jailbreak prompts.This dataset serves as a crucial benchmark for our current study and provides afoundational resource for future research. By comparing with traditionalevaluation methods, our study shows that the current results align withbaseline metrics while offering a more nuanced and fine-grained assessment. Italso helps identify potentially harmful attack prompts that might appearharmless in traditional evaluations. Overall, our work establishes a solidfoundation for assessing a broader range of attack prompts in the area ofprompt injection.</description><author>Dong shu, Mingyu Jin, Chong Zhang, Liangyao Li, Zihao Zhou, Yongfeng Zhang</author><pubDate>Sat, 03 Aug 2024 06:39:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.09002v5</guid></item><item><title>SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses</title><link>http://arxiv.org/abs/2408.01669v1</link><description>Video grounding is a fundamental problem in multimodal content understanding,aiming to localize specific natural language queries in an untrimmed video.However, current video grounding datasets merely focus on simple events and areeither limited to shorter videos or brief sentences, which hinders the modelfrom evolving toward stronger multimodal understanding capabilities. To addressthese limitations, we present a large-scale video grounding dataset namedSynopGround, in which more than 2800 hours of videos are sourced from popularTV dramas and are paired with accurately localized human-written synopses. Eachparagraph in the synopsis serves as a language query and is manually annotatedwith precise temporal boundaries in the long video. These paragraph queries aretightly correlated to each other and contain a wealth of abstract expressionssummarizing video storylines and specific descriptions portraying eventdetails, which enables the model to learn multimodal perception on moreintricate concepts over longer context dependencies. Based on the dataset, wefurther introduce a more complex setting of video grounding dubbedMulti-Paragraph Video Grounding (MPVG), which takes as input multipleparagraphs and a long video for grounding each paragraph query to its temporalinterval. In addition, we propose a novel Local-Global Multimodal Reasoner(LGMR) to explicitly model the local-global structures of long-term multimodalinputs for MPVG. Our method provides an effective baseline solution to themulti-paragraph video grounding problem. Extensive experiments verify theproposed model's effectiveness as well as its superiority in long-termmulti-paragraph video grounding over prior state-of-the-arts. Dataset and codeare publicly available. Project page: https://synopground.github.io/.</description><author>Chaolei Tan, Zihang Lin, Junfu Pu, Zhongang Qi, Wei-Yi Pei, Zhi Qu, Yexin Wang, Ying Shan, Wei-Shi Zheng, Jian-Fang Hu</author><pubDate>Sat, 03 Aug 2024 05:35:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01669v1</guid></item><item><title>Infusing Environmental Captions for Long-Form Video Language Grounding</title><link>http://arxiv.org/abs/2408.02336v2</link><description>In this work, we tackle the problem of long-form video-language grounding(VLG). Given a long-form video and a natural language query, a model shouldtemporally localize the precise moment that answers the query. Humans caneasily solve VLG tasks, even with arbitrarily long videos, by discardingirrelevant moments using extensive and robust knowledge gained from experience.Unlike humans, existing VLG methods are prone to fall into superficial cueslearned from small-scale datasets, even when they are within irrelevant frames.To overcome this challenge, we propose EI-VLG, a VLG method that leveragesricher textual information provided by a Multi-modal Large Language Model(MLLM) as a proxy for human experiences, helping to effectively excludeirrelevant frames. We validate the effectiveness of the proposed method viaextensive experiments on a challenging EgoNLQ benchmark.</description><author>Hyogun Lee, Soyeon Hong, Mujeen Sung, Jinwoo Choi</author><pubDate>Tue, 06 Aug 2024 04:04:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02336v2</guid></item><item><title>Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph</title><link>http://arxiv.org/abs/2404.03623v2</link><description>Large Language Models (LLMs) demonstrate an impressive capacity to recall avast range of factual knowledge. However, understanding their underlyingreasoning and internal mechanisms in exploiting this knowledge remains a keyresearch area. This work unveils the factual information an LLM representsinternally for sentence-level claim verification. We propose an end-to-endframework to decode factual knowledge embedded in token representations from avector space to a set of ground predicates, showing its layer-wise evolutionusing a dynamic knowledge graph. Our framework employs activation patching, avector-level technique that alters a token representation during inference, toextract encoded knowledge. Accordingly, we neither rely on training norexternal models. Using factual and common-sense claims from two claimverification datasets, we showcase interpretability analyses at local andglobal levels. The local analysis highlights entity centrality in LLMreasoning, from claim-related information and multi-hop reasoning torepresentation errors causing erroneous evaluation. On the other hand, theglobal reveals trends in the underlying evolution, such as word-based knowledgeevolving into claim-related facts. By interpreting semantics from LLM latentrepresentations and enabling graph-related analyses, this work enhances theunderstanding of the factual knowledge resolution process.</description><author>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini</author><pubDate>Tue, 06 Aug 2024 15:02:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03623v2</guid></item><item><title>MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine</title><link>http://arxiv.org/abs/2408.02900v1</link><description>This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodaldataset for medicine, covering over 25 million images across 10 modalities,with multigranular annotations for more than 65 diseases. These enrichedannotations encompass both global textual information, such as disease/lesiontype, modality, region-specific descriptions, and inter-regional relationships,as well as detailed local annotations for regions of interest (ROIs), includingbounding boxes, segmentation masks. Unlike existing approach which is limitedby the availability of image-text pairs, we have developed the first automatedpipeline that scales up multimodal data by generating multigranular visual andtexual annotations (in the form of image-ROI-description triplets) without theneed for any paired text descriptions. Specifically, data from over 90different sources have been collected, preprocessed, and grounded usingdomain-specific expert models to identify ROIs related to abnormal regions. Wethen build a comprehensive knowledge base and prompt multimodal large languagemodels to perform retrieval-augmented generation with the identified ROIs asguidance, resulting in multigranular texual descriptions. Compared to existingdatasets, MedTrinity-25M provides the most enriched annotations, supporting acomprehensive range of multimodal tasks such as captioning and reportgeneration, as well as vision-centric tasks like classification andsegmentation. Pretraining on MedTrinity-25M, our model achievesstate-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodallarge language models and other representative SoTA approaches. This datasetcan also be utilized to support large-scale pre-training of multimodal medicalAI models, contributing to the development of future foundation models in themedical domain.</description><author>Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou</author><pubDate>Tue, 06 Aug 2024 02:09:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.02900v1</guid></item><item><title>Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation</title><link>http://arxiv.org/abs/2408.03735v1</link><description>This paper presents the first study to explore the potential of parameterquantization for multimodal large language models to alleviate the significantresource constraint encountered during vision-language instruction tuning. Weintroduce a Quantization-aware Scale LeArning method based on multimodalWarmup, termed QSLAW. This method is grounded in two key innovations: (1) Thelearning of group-wise scale factors for quantized LLM weights to mitigate thequantization error arising from activation outliers and achieve more effectivevision-language instruction tuning; (2) The implementation of a multimodalwarmup that progressively integrates linguistic and multimodal trainingsamples, thereby preventing overfitting of the quantized model to multimodaldata while ensuring stable adaptation of multimodal large language models todownstream vision-language tasks. Extensive experiments demonstrate that modelsquantized by QSLAW perform on par with, or even surpass, their full-precisioncounterparts, while facilitating up to 1.4 times reduction in VL tuning timeand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.</description><author>Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji</author><pubDate>Wed, 07 Aug 2024 12:42:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03735v1</guid></item><item><title>SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding from TV Dramas and Synopses</title><link>http://arxiv.org/abs/2408.01669v3</link><description>Video grounding is a fundamental problem in multimodal content understanding,aiming to localize specific natural language queries in an untrimmed video.However, current video grounding datasets merely focus on simple events and areeither limited to shorter videos or brief sentences, which hinders the modelfrom evolving toward stronger multimodal understanding capabilities. To addressthese limitations, we present a large-scale video grounding dataset namedSynopGround, in which more than 2800 hours of videos are sourced from popularTV dramas and are paired with accurately localized human-written synopses. Eachparagraph in the synopsis serves as a language query and is manually annotatedwith precise temporal boundaries in the long video. These paragraph queries aretightly correlated to each other and contain a wealth of abstract expressionssummarizing video storylines and specific descriptions portraying eventdetails, which enables the model to learn multimodal perception on moreintricate concepts over longer context dependencies. Based on the dataset, wefurther introduce a more complex setting of video grounding dubbedMulti-Paragraph Video Grounding (MPVG), which takes as input multipleparagraphs and a long video for grounding each paragraph query to its temporalinterval. In addition, we propose a novel Local-Global Multimodal Reasoner(LGMR) to explicitly model the local-global structures of long-term multimodalinputs for MPVG. Our method provides an effective baseline solution to themulti-paragraph video grounding problem. Extensive experiments verify theproposed model's effectiveness as well as its superiority in long-termmulti-paragraph video grounding over prior state-of-the-arts. Dataset and codeare publicly available. Project page: https://synopground.github.io/.</description><author>Chaolei Tan, Zihang Lin, Junfu Pu, Zhongang Qi, Wei-Yi Pei, Zhi Qu, Yexin Wang, Ying Shan, Wei-Shi Zheng, Jian-Fang Hu</author><pubDate>Thu, 08 Aug 2024 11:19:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.01669v3</guid></item><item><title>VisEval: A Benchmark for Data Visualization in the Era of Large Language Models</title><link>http://arxiv.org/abs/2407.00981v2</link><description>Translating natural language to visualization (NL2VIS) has shown greatpromise for visual data analysis, but it remains a challenging task thatrequires multiple low-level implementations, such as natural languageprocessing and visualization design. Recent advancements in pre-trained largelanguage models (LLMs) are opening new avenues for generating visualizationsfrom natural language. However, the lack of a comprehensive and reliablebenchmark hinders our understanding of LLMs' capabilities in visualizationgeneration. In this paper, we address this gap by proposing a new NL2VISbenchmark called VisEval. Firstly, we introduce a high-quality and large-scaledataset. This dataset includes 2,524 representative queries covering 146databases, paired with accurately labeled ground truths. Secondly, we advocatefor a comprehensive automated evaluation methodology covering multipledimensions, including validity, legality, and readability. By systematicallyscanning for potential issues with a number of heterogeneous checkers, VisEvalprovides reliable and trustworthy evaluation outcomes. We run VisEval on aseries of state-of-the-art LLMs. Our evaluation reveals prevalent challengesand delivers essential insights for future advancements.</description><author>Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, Yuqing Yang</author><pubDate>Wed, 07 Aug 2024 09:52:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.00981v2</guid></item><item><title>Trading Devil Final: Backdoor attack via Stock market and Bayesian Optimization</title><link>http://arxiv.org/abs/2407.14573v2</link><description>Since the advent of generative artificial intelligence, every company andresearcher has been rushing to develop their own generative models, whethercommercial or not. Given the large number of users of these powerful new tools,there is currently no intrinsically verifiable way to explain from the groundup what happens when LLMs (large language models) learn. For example, thosebased on automatic speech recognition systems, which have to rely on huge andastronomical amounts of data collected from all over the web to produce fastand efficient results, In this article, we develop a backdoor attack calledMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 ismainly based on modern stock market models. In order to show the possiblevulnerabilities of speech-based transformers that may rely on LLMs.</description><author>Orson Mengara</author><pubDate>Wed, 07 Aug 2024 08:22:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.14573v2</guid></item><item><title>Learning Fine-Grained Grounded Citations for Attributed Large Language Models</title><link>http://arxiv.org/abs/2408.04568v1</link><description>Despite the impressive performance on information-seeking tasks, largelanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,which augment generated text with in-line citations, have shown potential inmitigating hallucinations and improving verifiability. However, currentapproaches suffer from suboptimal citation quality due to their reliance onin-context learning. Furthermore, the practice of citing only coarse documentidentifiers makes it challenging for users to perform fine-grainedverification. In this work, we introduce FRONT, a training framework designedto teach LLMs to generate Fine-Grained Grounded Citations. By grounding modeloutputs in fine-grained supporting quotes, these quotes guide the generation ofgrounded and consistent responses, not only improving citation quality but alsofacilitating fine-grained verification. Experiments on the ALCE benchmarkdemonstrate the efficacy of FRONT in generating superior grounded responses andhighly supportive citations. With LLaMA-2-7B, the framework significantlyoutperforms all the baselines, achieving an average of 14.21% improvement incitation quality across all datasets, even surpassing ChatGPT.</description><author>Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, Bing Qin</author><pubDate>Thu, 08 Aug 2024 16:28:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04568v1</guid></item><item><title>Large Language Models are Capable of Offering Cognitive Reappraisal, if Guided</title><link>http://arxiv.org/abs/2404.01288v2</link><description>Large language models (LLMs) have offered new opportunities for emotionalsupport, and recent work has shown that they can produce empathic responses topeople in distress. However, long-term mental well-being requires emotionalself-regulation, where a one-time empathic response falls short. This worktakes a first step by engaging with cognitive reappraisals, a strategy frompsychology practitioners that uses language to targetedly change negativeappraisals that an individual makes of the situation; such appraisals is knownto sit at the root of human emotional experience. We hypothesize thatpsychologically grounded principles could enable such advanced psychologycapabilities in LLMs, and design RESORT which consists of a series ofreappraisal constitutions across multiple dimensions that can be used as LLMinstructions. We conduct a first-of-its-kind expert evaluation (by clinicalpsychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability togenerate cognitive reappraisal responses to medium-length social media messagesasking for support. This fine-grained evaluation showed that even LLMs at the7B scale guided by RESORT are capable of generating empathic responses that canhelp users reappraise their situations.</description><author>Hongli Zhan, Allen Zheng, Yoon Kyung Lee, Jina Suh, Junyi Jessy Li, Desmond C. Ong</author><pubDate>Thu, 08 Aug 2024 15:44:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01288v2</guid></item><item><title>Partial Experts Checkpoint: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training</title><link>http://arxiv.org/abs/2408.04307v1</link><description>As large language models continue to scale up, the imperative for faulttolerance in distributed deep learning systems intensifies, becoming a focalarea of AI infrastructure research. Checkpoint has emerged as the predominantfault tolerance strategy, with extensive studies dedicated to optimizing itsefficiency. However, the advent of the sparse Mixture-of-Experts (MoE) modelpresents new challenges for traditional checkpoint techniques due to thesubstantial increase in model size, despite comparable computational demands todense models. Breaking new ground in the realm of efficient fault tolerance forMoE model training, we introduce a novel Partial Experts Checkpoint (PEC)mechanism alongside a corresponding PEC fault-tolerant system. Our approachstrategically checkpoints a selected subset of experts, thereby significantlyreducing the checkpoint size for MoE models to a level comparable with that ofdense models. The empirical analysis on our 8-expert GPT-MoE model demonstratesthat the proposed PEC approach facilitates a substantial 54.2% decrease in thesize of non-redundant checkpoint (no data-parallel duplication), withoutcompromising the final model quality. Moreover, our PEC fault-tolerant systemachieves a 76.9% reduction in checkpoint workload per data-parallel distributedrank, thereby correspondingly diminishing the checkpointing time andfacilitating complete overlap with the training process.</description><author>Weilin Cai, Le Qin, Jiayi Huang</author><pubDate>Thu, 08 Aug 2024 08:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04307v1</guid></item><item><title>PersLLM: A Personified Training Approach for Large Language Models</title><link>http://arxiv.org/abs/2407.12393v4</link><description>Large language models exhibit aspects of human-level intelligence thatcatalyze their application as human-like agents in domains such as socialsimulations, human-machine interactions, and collaborative multi-agent systems.However, the absence of distinct personalities, such as displaying ingratiatingbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMsutility in practical applications. Addressing this, the development ofpersonality traits in LLMs emerges as a crucial area of research to unlocktheir latent potential. Existing methods to personify LLMs generally involvestrategies like employing stylized training data for instruction tuning orusing prompt engineering to simulate different personalities. These methodsonly capture superficial linguistic styles instead of the core of personalitiesand are therefore not stable. In this study, we propose PersLLM, integratingpsychology-grounded principles of personality: social practice, consistency,and dynamic development, into a comprehensive training methodology. Weincorporate personality traits directly into the model parameters, enhancingthe model's resistance to induction, promoting consistency, and supporting thedynamic evolution of personality. Single-agent evaluation validates ourmethod's superiority, as it produces responses more aligned with referencepersonalities compared to other approaches. Case studies for multi-agentcommunication highlight its benefits in enhancing opinion consistency withinindividual agents and fostering collaborative creativity among multiple agentsin dialogue contexts, potentially benefiting human simulation and multi-agentcooperation. Additionally, human-agent interaction evaluations indicate thatour personified models significantly enhance interactive experiences,underscoring the practical implications of our research.</description><author>Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhenghao Liu, Zhiyuan Liu, Maosong Sun</author><pubDate>Thu, 08 Aug 2024 06:08:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12393v4</guid></item><item><title>VideoQA in the Era of LLMs: An Empirical Study</title><link>http://arxiv.org/abs/2408.04223v1</link><description>Video Large Language Models (Video-LLMs) are flourishing and has advancedmany video-language tasks. As a golden testbed, Video Question Answering(VideoQA) plays pivotal role in Video-LLM developing. This work conducts atimely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming toelucidate their success and failure modes, and provide insights towards morehuman-like video understanding and question answering. Our analyses demonstratethat Video-LLMs excel in VideoQA; they can correlate contextual cues andgenerate plausible responses to questions about varied video contents. However,models falter in handling video temporality, both in reasoning about temporalcontent ordering and grounding QA-relevant temporal moments. Moreover, themodels behave unintuitively - they are unresponsive to adversarial videoperturbations while being sensitive to simple variations of candidate answersand questions. Also, they do not necessarily generalize better. The findingsdemonstrate Video-LLMs' QA capability in standard condition yet highlight theirsevere deficiency in robustness and interpretability, suggesting the urgentneed on rationales in Video-LLM developing.</description><author>Junbin Xiao, Nanxin Huang, Hangyu Qin, Dongyang Li, Yicong Li, Fengbin Zhu, Zhulin Tao, Jianxing Yu, Liang Lin, Tat-Seng Chua, Angela Yao</author><pubDate>Thu, 08 Aug 2024 05:14:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04223v1</guid></item><item><title>Task-oriented Sequential Grounding in 3D Scenes</title><link>http://arxiv.org/abs/2408.04034v1</link><description>Grounding natural language in physical 3D environments is essential for theadvancement of embodied artificial intelligence. Current datasets and modelsfor 3D visual grounding predominantly focus on identifying and localizingobjects from static, object-centric descriptions. These approaches do notadequately address the dynamic and sequential nature of task-oriented groundingnecessary for practical applications. In this work, we propose a new task:Task-oriented Sequential Grounding in 3D scenes, wherein an agent must followdetailed step-by-step instructions to complete daily activities by locating asequence of target objects in indoor scenes. To facilitate this task, weintroduce SG3D, a large-scale dataset containing 22,346 tasks with 112,236steps across 4,895 real-world 3D scenes. The dataset is constructed using acombination of RGB-D scans from various 3D scene datasets and an automated taskgeneration pipeline, followed by human verification for quality assurance. Weadapted three state-of-the-art 3D visual grounding models to the sequentialgrounding task and evaluated their performance on SG3D. Our results reveal thatwhile these models perform well on traditional benchmarks, they facesignificant challenges with task-oriented sequential grounding, underscoringthe need for further research in this area.</description><author>Zhuofan Zhang, Ziyu Zhu, Pengxiang Li, Tengyu Liu, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Siyuan Huang, Qing Li</author><pubDate>Wed, 07 Aug 2024 18:30:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04034v1</guid></item><item><title>Improving Large Language Model (LLM) fidelity through context-aware grounding: A systematic approach to reliability and veracity</title><link>http://arxiv.org/abs/2408.04023v1</link><description>As Large Language Models (LLMs) become increasingly sophisticated andubiquitous in natural language processing (NLP) applications, ensuring theirrobustness, trustworthiness, and alignment with human values has become acritical challenge. This paper presents a novel framework for contextualgrounding in textual models, with a particular emphasis on the ContextRepresentation stage. Our approach aims to enhance the reliability and ethicalalignment of these models through a comprehensive, context-aware methodology.By explicitly capturing and representing relevant situational, cultural, andethical contexts in a machine-readable format, we lay the foundation foranchoring a model's behavior within these contexts. Our approach leveragestechniques from knowledge representation and reasoning, such as ontologies,semantic web technologies, and logic-based formalisms. We evaluate ourframework on real-world textual datasets, demonstrating its effectiveness inimproving model performance, fairness, and alignment with human expectations,while maintaining high accuracy. Furthermore, we discuss the other keycomponents of the framework, including context-aware encoding, context-awarelearning, interpretability and explainability, and continuous monitoring andadaptation. This research contributes to the growing body of work onresponsible AI, offering a practical approach to developing more reliable,trustworthy, and ethically-aligned language models. Our findings havesignificant implications for the deployment of LLMs in sensitive domains suchas healthcare, legal systems, and social services, where contextualunderstanding is paramount.</description><author>Wrick Talukdar, Anjanava Biswas</author><pubDate>Wed, 07 Aug 2024 18:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04023v1</guid></item><item><title>DeVAn: Dense Video Annotation for Video-Language Models</title><link>http://arxiv.org/abs/2310.05060v2</link><description>We present a novel human annotated dataset for evaluating the ability forvisual-language models to generate both short and long descriptions forreal-world video clips, termed DeVAn (Dense Video Annotation). The datasetcontains 8.5K YouTube video clips of 20-60 seconds in duration and covers awide range of topics and interests. Each video clip is independently annotatedby 5 human annotators, producing both captions (1 sentence) and summaries (3-10sentences). Given any video selected from the dataset and its corresponding ASRinformation, we evaluate visuallanguage models on either caption or summarygeneration that is grounded in both the visual and auditory content of thevideo. Additionally, models are also evaluated on caption- and summary-basedretrieval tasks, where the summary-based retrieval task requires theidentification of a target video given excerpts of a given summary. Given thenovel nature of the paragraph-length video summarization task, we compareddifferent existing evaluation metrics and their alignment with humanpreferences and found that model-based evaluation metrics provide moresemantically-oriented and human-aligned evaluation. Finally, we benchmarked awide range of current video-language models on DeVAn, and we aim for DeVAn toserve as a useful evaluation set in the age of large language models andcomplex multi-modal tasks. Code is available at https://github.com/TK-21st/DeVAn.</description><author>Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang</author><pubDate>Fri, 09 Aug 2024 16:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05060v2</guid></item><item><title>How Well Do LLMs Identify Cultural Unity in Diversity?</title><link>http://arxiv.org/abs/2408.05102v1</link><description>Much work on the cultural awareness of large language models (LLMs) focuseson the models' sensitivity to geo-cultural diversity. However, in addition tocross-cultural differences, there also exists common ground across cultures.For instance, a bridal veil in the United States plays a similarcultural-relevant role as a honggaitou in China. In this study, we introduce abenchmark dataset CUNIT for evaluating decoder-only LLMs in understanding thecultural unity of concepts. Specifically, CUNIT consists of 1,425 evaluationexamples building upon 285 traditional cultural-specific concepts across 10countries. Based on a systematic manual annotation of cultural-relevantfeatures per concept, we calculate the cultural association between any pair ofcross-cultural concepts. Built upon this dataset, we design a contrastivematching task to evaluate the LLMs' capability to identify highly associatedcross-cultural concept pairs. We evaluate 3 strong LLMs, using 3 popularprompting strategies, under the settings of either giving all extracted conceptfeatures or no features at all on CUNIT Interestingly, we find that culturalassociations across countries regarding clothing concepts largely differ fromfood. Our analysis shows that LLMs are still limited to capturingcross-cultural associations between concepts compared to humans. Moreover,geo-cultural proximity shows a weak influence on model performance in capturingcross-cultural associations.</description><author>Jialin Li, Junli Wang, Junjie Hu, Ming Jiang</author><pubDate>Fri, 09 Aug 2024 14:45:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.05102v1</guid></item><item><title>ProFuser: Progressive Fusion of Large Language Models</title><link>http://arxiv.org/abs/2408.04998v1</link><description>While fusing the capacities and advantages of various large language models(LLMs) offers a pathway to construct more powerful and versatile models, afundamental challenge is to properly select advantageous model during thetraining. Existing fusion methods primarily focus on the training mode thatuses cross entropy on ground truth in a teacher-forcing setup to measure amodel's advantage, which may provide limited insight towards model advantage.In this paper, we introduce a novel approach that enhances the fusion processby incorporating both the training and inference modes. Our method evaluatesmodel advantage not only through cross entropy during training but also byconsidering inference outputs, providing a more comprehensive assessment. Tocombine the two modes effectively, we introduce ProFuser to progressivelytransition from inference mode to training mode. To validate ProFuser'seffectiveness, we fused three models, including vicuna-7b-v1.5,Llama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performancein knowledge, reasoning, and safety compared to baseline methods.</description><author>Tianyuan Shi, Fanqi Wan, Canbin Huang, Xiaojun Quan, Chenliang Li, Ming Yan, Ji Zhang</author><pubDate>Fri, 09 Aug 2024 11:18:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04998v1</guid></item><item><title>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</title><link>http://arxiv.org/abs/2408.04948v1</link><description>Extraction and interpretation of intricate information from unstructured textdata arising in financial applications, such as earnings call transcripts,present substantial challenges to large language models (LLMs) even using thecurrent best practices to use Retrieval Augmented Generation (RAG) (referred toas VectorRAG techniques which utilize vector databases for informationretrieval) due to challenges such as domain specific terminology and complexformats of the documents. We introduce a novel approach based on a combination,called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (calledGraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;A) systems forinformation extraction from financial documents that is shown to be capable ofgenerating accurate and contextually relevant answers. Using experiments on aset of financial earning call transcripts documents which come in the form ofQ&amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;As, weshow that HybridRAG which retrieves context from both vector database and KGoutperforms both traditional VectorRAG and GraphRAG individually when evaluatedat both the retrieval and generation stages in terms of retrieval accuracy andanswer generation. The proposed technique has applications beyond the financialdomain</description><author>Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, Dhagash Mehta</author><pubDate>Fri, 09 Aug 2024 09:07:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.04948v1</guid></item><item><title>Do Text-to-Vis Benchmarks Test Real Use of Visualisations?</title><link>http://arxiv.org/abs/2407.19726v2</link><description>Large language models are able to generate code for visualisations inresponse to user requests. This is a useful application, and an appealing onefor NLP research because plots of data provide grounding for language. However,there are relatively few benchmarks, and it is unknown whether those that existare representative of what people do in practice. This paper aims to answerthat question through an empirical study comparing benchmark datasets and codefrom public repositories. Our findings reveal a substantial gap in datasets,with evaluations not testing the same distribution of chart types, attributes,and the number of actions. The only representative dataset requiresmodification to become an end-to-end and practical benchmark. This shows thatnew, more benchmarks are needed to support the development of systems thattruly address users' visualisation needs. These observations will guide futuredata creation, highlighting which features hold genuine significance for users.</description><author>Hy Nguyen, Xuefei He, Andrew Reeson, Cecile Paris, Josiah Poon, Jonathan K. Kummerfeld</author><pubDate>Fri, 09 Aug 2024 00:54:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19726v2</guid></item></channel></rss>