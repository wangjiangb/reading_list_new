<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivpose estimation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 23 Jul 2023 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An Efficient Convex Hull-based Vehicle Pose Estimation Method for 3D LiDAR</title><link>http://arxiv.org/abs/2302.01034v2</link><description>Vehicle pose estimation with LiDAR is essential in the perception technologyof autonomous driving. However, due to incomplete observation measurements andsparsity of the LiDAR point cloud, it is challenging to achieve satisfactorypose extraction based on 3D LiDAR by using the existing pose estimationmethods. In addition, the requirement for real-time performance furtherincreases the difficulty of the pose estimation task. In this paper, weproposed a novel convex hull-based vehicle pose estimation method. Theextracted 3D cluster is reduced to the convex hull, reducing the computationburden and retaining contour information. Then a novel criterion based on theminimum occlusion area is developed for the search-based algorithm, which canachieve accurate pose estimation. This criterion also makes the proposedalgorithm especially suitable for obstacle avoidance. The proposed algorithm isvalidated on the KITTI dataset and a manually labeled dataset acquired at anindustrial park. The results show that our proposed method can achieve betteraccuracy than the state-of-the-art pose estimation method while maintainingreal-time speed.</description><author>Ningning Ding</author><pubDate>Sun, 02 Jul 2023 00:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01034v2</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training</title><link>http://arxiv.org/abs/2302.06019v2</link><description>Real-world robotics applications demand object pose estimation methods thatwork reliably across a variety of scenarios. Modern learning-based approachesrequire large labeled datasets and tend to perform poorly outside the trainingdomain. Our first contribution is to develop a robust corrector module thatcorrects pose estimates using depth information, thus enabling existing methodsto better generalize to new test domains; the corrector operates on semantickeypoints (but is also applicable to other pose estimators) and is fullydifferentiable. Our second contribution is an ensemble self-training approachthat simultaneously trains multiple pose estimators in a self-supervisedmanner. Our ensemble self-training architecture uses the robust corrector torefine the output of each pose estimator; then, it evaluates the quality of theoutputs using observable correctness certificates; finally, it uses theobservably correct outputs for further training, without requiring externalsupervision. As an additional contribution, we propose small improvements to aregression-based keypoint detection architecture, to enhance its robustness tooutliers; these improvements include a robust pooling scheme and a robustcentroid computation. Experiments on the YCBV and TLESS datasets show theproposed ensemble self-training outperforms fully supervised baselines whilenot requiring 3D annotations on real data.</description><author>Jingnan Shi, Rajat Talak, Dominic Maggio, Luca Carlone</author><pubDate>Thu, 11 May 2023 19:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06019v2</guid></item><item><title>Affine Correspondences between Multi-Camera Systems for Relative Pose Estimation</title><link>http://arxiv.org/abs/2306.12996v1</link><description>We present a novel method to compute the relative pose of multi-camerasystems using two affine correspondences (ACs). Existing solutions to themulti-camera relative pose estimation are either restricted to special cases ofmotion, have too high computational complexity, or require too many pointcorrespondences (PCs). Thus, these solvers impede an efficient or accuraterelative pose estimation when applying RANSAC as a robust estimator. This papershows that the 6DOF relative pose estimation problem using ACs permits afeasible minimal solution, when exploiting the geometric constraints betweenACs and multi-camera systems using a special parameterization. We present aproblem formulation based on two ACs that encompass two common types of ACsacross two views, i.e., inter-camera and intra-camera. Moreover, the frameworkfor generating the minimal solvers can be extended to solve various relativepose estimation problems, e.g., 5DOF relative pose estimation with knownrotation angle prior. Experiments on both virtual and real multi-camera systemsprove that the proposed solvers are more efficient than the state-of-the-artalgorithms, while resulting in a better relative pose accuracy. Source code isavailable at https://github.com/jizhaox/relpose-mcs-depth.</description><author>Banglei Guan, Ji Zhao</author><pubDate>Thu, 22 Jun 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12996v1</guid></item><item><title>GoalieNet: A Multi-Stage Network for Joint Goalie, Equipment, and Net Pose Estimation in Ice Hockey</title><link>http://arxiv.org/abs/2306.15853v1</link><description>In the field of computer vision-driven ice hockey analytics, one of the mostchallenging and least studied tasks is goalie pose estimation. Unlike generalhuman pose estimation, goalie pose estimation is much more complex as itinvolves not only the detection of keypoints corresponding to the joints of thegoalie concealed under thick padding and mask, but also a large number ofnon-human keypoints corresponding to the large leg pads and gloves worn, thestick, as well as the hockey net. To tackle this challenge, we introduceGoalieNet, a multi-stage deep neural network for jointly estimating the pose ofthe goalie, their equipment, and the net. Experimental results using NHLbenchmark data demonstrate that the proposed GoalieNet can achieve an averageof 84\% accuracy across all keypoints, where 22 out of 29 keypoints aredetected with more than 80\% accuracy. This indicates that such a joint poseestimation approach can be a promising research direction.</description><author>Marjan Shahi, David Clausi, Alexander Wong</author><pubDate>Wed, 28 Jun 2023 02:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15853v1</guid></item><item><title>Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video</title><link>http://arxiv.org/abs/2303.08475v2</link><description>Temporal modeling is crucial for multi-frame human pose estimation. Mostexisting methods directly employ optical flow or deformable convolution topredict full-spectrum motion fields, which might incur numerous irrelevantcues, such as a nearby person or background. Without further efforts toexcavate meaningful motion priors, their results are suboptimal, especially incomplicated spatiotemporal interactions. On the other hand, the temporaldifference has the ability to encode representative motion information whichcan potentially be valuable for pose estimation but has not been fullyexploited. In this paper, we present a novel multi-frame human pose estimationframework, which employs temporal differences across frames to model dynamiccontexts and engages mutual information objectively to facilitate useful motioninformation disentanglement. To be specific, we design a multi-stage TemporalDifference Encoder that performs incremental cascaded learning conditioned onmulti-stage feature difference sequences to derive informative motionrepresentation. We further propose a Representation Disentanglement module fromthe mutual information perspective, which can grasp discriminativetask-relevant motion signals by explicitly defining useful and noisyconstituents of the raw motion features and minimizing their mutualinformation. These place us to rank No.1 in the Crowd Pose Estimation inComplex Events Challenge on benchmark dataset HiEve, and achievestate-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018,and PoseTrack21.</description><author>Runyang Feng, Yixing Gao, Xueqing Ma, Tze Ho Elden Tse, Hyung Jin Chang</author><pubDate>Mon, 08 May 2023 14:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08475v2</guid></item><item><title>Perspective-1-Ellipsoid: Formulation, Analysis and Solutions of the Camera Pose Estimation Problem from One Ellipse-Ellipsoid Correspondence</title><link>http://arxiv.org/abs/2208.12513v3</link><description>In computer vision, camera pose estimation from correspondences between 3Dgeometric entities and their projections into the image has been a widelyinvestigated problem. Although most state-of-the-art methods exploit low-levelprimitives such as points or lines, the emergence of very effective CNN-basedobject detectors in the recent years has paved the way to the use ofhigher-level features carrying semantically meaningful information. Pioneeringworks in that direction have shown that modelling 3D objects by ellipsoids and2D detections by ellipses offers a convenient manner to link 2D and 3D data.However, the mathematical formalism most often used in the related litteraturedoes not enable to easily distinguish ellipsoids and ellipses from otherquadrics and conics, leading to a loss of specificity potentially detrimentalin some developments. Moreover, the linearization process of the projectionequation creates an over-representation of the camera parameters, also possiblycausing an efficiency loss. In this paper, we therefore introduce anellipsoid-specific theoretical framework and demonstrate its beneficialproperties in the context of pose estimation. More precisely, we first showthat the proposed formalism enables to reduce the pose estimation problem to aposition or orientation-only estimation problem in which the remaining unknownscan be derived in closed-form. Then, we demonstrate that it can be furtherreduced to a 1 Degree-of-Freedom (1DoF) problem and provide the analyticalderivations of the pose as a function of that unique scalar unknown. Weillustrate our theoretical considerations by visual examples and include adiscussion on the practical aspects. Finally, we release this paper along withthe corresponding source code in order to contribute towards more efficientresolutions of ellipsoid-related pose estimation problems.</description><author>Vincent Gaudilli√®re, Gilles Simon, Marie-Odile Berger</author><pubDate>Wed, 14 Jun 2023 13:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12513v3</guid></item><item><title>SPAC-Net: Synthetic Pose-aware Animal ControlNet for Enhanced Pose Estimation</title><link>http://arxiv.org/abs/2305.17845v1</link><description>Animal pose estimation has become a crucial area of research, but thescarcity of annotated data is a significant challenge in developing accuratemodels. Synthetic data has emerged as a promising alternative, but itfrequently exhibits domain discrepancies with real data. Style transferalgorithms have been proposed to address this issue, but they suffer frominsufficient spatial correspondence, leading to the loss of label information.In this work, we present a new approach called Synthetic Pose-aware AnimalControlNet (SPAC-Net), which incorporates ControlNet into the previouslyproposed Prior-Aware Synthetic animal data generation (PASyn) pipeline. Weleverage the plausible pose data generated by the Variational Auto-Encoder(VAE)-based data generation pipeline as input for the ControlNetHolistically-nested Edge Detection (HED) boundary task model to generatesynthetic data with pose labels that are closer to real data, making itpossible to train a high-precision pose estimation network without the need forreal data. In addition, we propose the Bi-ControlNet structure to separatelydetect the HED boundary of animals and backgrounds, improving the precision andstability of the generated data. Using the SPAC-Net pipeline, we generatesynthetic zebra and rhino images and test them on the AP10K real dataset,demonstrating superior performance compared to using only real images orsynthetic data generated by other methods. Our work demonstrates the potentialfor synthetic data to overcome the challenge of limited annotated data inanimal pose estimation.</description><author>Le Jiang, Sarah Ostadabbas</author><pubDate>Mon, 29 May 2023 02:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17845v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v3</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Wed, 17 May 2023 09:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v3</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v1</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Fri, 12 May 2023 10:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v2</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Mon, 15 May 2023 16:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v2</guid></item><item><title>CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data</title><link>http://arxiv.org/abs/2305.08042v1</link><description>This paper proposes a novel method for estimating the set of plausible posesof a rigid object from a set of points with volumetric information, such aswhether each point is in free space or on the surface of the object. Inparticular, we study how pose can be estimated from force and tactile dataarising from contact. Using data derived from contact is challenging because itis inherently less information-dense than visual data, and thus the poseestimation problem is severely under-constrained when there are few contacts.Rather than attempting to estimate the true pose of the object, which is nottractable without a large number of contacts, we seek to estimate a plausibleset of poses which obey the constraints imposed by the sensor data. Existingmethods struggle to estimate this set because they are either designed forsingle pose estimates or require informative priors to be effective. Ourapproach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL),has three key attributes: 1) It considers volumetric information, which allowsus to account for known free space; 2) It uses a novel differentiablevolumetric cost function to take advantage of powerful gradient-basedoptimization tools; and 3) It uses methods from the Quality Diversity (QD)optimization literature to produce a diverse set of high-quality poses. To ourknowledge, QD methods have not been used previously for pose registration. Wealso show how to update our plausible pose estimates online as more data isgathered by the robot. Our experiments suggest that CHSEL shows largeperformance improvements over several baseline methods for both simulated andreal-world data.</description><author>Sheng Zhong, Nima Fazeli, Dmitry Berenson</author><pubDate>Sun, 14 May 2023 02:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08042v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Relation-Based Associative Joint Location for Human Pose Estimation in Videos</title><link>http://arxiv.org/abs/2107.03591v3</link><description>Video-based human pose estimation (VHPE) is a vital yet challenging task.While deep learning methods have made significant progress for the VHPE, mostapproaches to this task implicitly model the long-range interaction betweenjoints by enlarging the receptive field of the convolution. Unlike priormethods, we design a lightweight and plug-and-play joint relation extractor(JRE) to model the associative relationship between joints explicitly andautomatically. The JRE takes the pseudo heatmaps of joints as input andcalculates the similarity between pseudo heatmaps. In this way, the JREflexibly learns the relationship between any two joints, allowing it to learnthe rich spatial configuration of human poses. Moreover, the JRE can inferinvisible joints according to the relationship between joints, which isbeneficial for the model to locate occluded joints. Then, combined withtemporal semantic continuity modeling, we propose a Relation-based PoseSemantics Transfer Network (RPSTN) for video-based human pose estimation.Specifically, to capture the temporal dynamics of poses, the pose semanticinformation of the current frame is transferred to the next with a jointrelation guided pose semantics propagator (JRPSP). The proposed model cantransfer the pose semantic features from the non-occluded frame to the occludedframe, making our method robust to the occlusion. Furthermore, the proposed JREmodule is also suitable for image-based human pose estimation. The proposedRPSTN achieves state-of-the-art results on the video-based Penn Action dataset,Sub-JHMDB dataset, and PoseTrack2018 dataset. Moreover, the proposed JREimproves the performance of backbones on the image-based COCO2017 dataset. Codeis available at https://github.com/YHDang/pose-estimation.</description><author>Yonghao Dang, Jianqin Yin, Shaojie Zhang</author><pubDate>Fri, 30 Jun 2023 10:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.03591v3</guid></item><item><title>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</title><link>http://arxiv.org/abs/2305.15727v1</link><description>Despite the significant progress in six degrees-of-freedom (6DoF) object poseestimation, existing methods have limited applicability in real-world scenariosinvolving embodied agents and downstream 3D vision tasks. These limitationsmainly come from the necessity of 3D models, closed-category detection, and alarge number of densely annotated support views. To mitigate this issue, wepropose a general paradigm for object pose estimation, called Promptable ObjectPose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoFobject pose estimation for any target object in any scene, while only a singlereference is adopted as the support view. To achieve this, POPE leverages thepower of the pre-trained large-scale 2D foundation model, employs a frameworkwith hierarchical feature representation and 3D geometry principles. Moreover,it estimates the relative camera pose between object prompts and the targetobject in new views, enabling both two-view and multi-view 6DoF pose estimationtasks. Comprehensive experimental results demonstrate that POPE exhibitsunrivaled robust performance in zero-shot settings, by achieving a significantreduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMODand OnePose datasets, respectively. We also conduct more challenging testingsin causally captured images (see Figure 1), which further demonstrates therobustness of POPE. Project page can be found withhttps://paulpanwang.github.io/POPE/.</description><author>Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, Hanwen Jiang, Zhangyang Wang</author><pubDate>Thu, 25 May 2023 06:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15727v1</guid></item><item><title>Generalizable Pose Estimation Using Implicit Scene Representations</title><link>http://arxiv.org/abs/2305.17252v1</link><description>6-DoF pose estimation is an essential component of robotic manipulationpipelines. However, it usually suffers from a lack of generalization to newinstances and object types. Most widely used methods learn to infer the objectpose in a discriminative setup where the model filters useful information toinfer the exact pose of the object. While such methods offer accurate poses,the model does not store enough information to generalize to new objects. Inthis work, we address the generalization capability of pose estimation usingmodels that contain enough information about the object to render it indifferent poses. We follow the line of work that inverts neural renderers toinfer the pose. We propose i-$\sigma$SRN to maximize the information flowingfrom the input pose to the rendered scene and invert them to infer the posegiven an input image. Specifically, we extend Scene Representation Networks(SRNs) by incorporating a separate network for density estimation and introducea new way of obtaining a weighted scene representation. We investigate severalways of initial pose estimates and losses for the neural renderer. Our finalevaluation shows a significant improvement in inference performance and speedcompared to existing approaches.</description><author>Vaibhav Saxena, Kamal Rahimi Malekshan, Linh Tran, Yotto Koga</author><pubDate>Fri, 26 May 2023 21:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17252v1</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement</title><link>http://arxiv.org/abs/2307.05561v1</link><description>As demand for robotics manipulation application increases, accuratevision-based 6D pose estimation becomes essential for autonomous operations.Convolutional Neural Networks (CNNs) based approaches for pose estimation havebeen previously introduced. However, the quest for better performance stillpersists especially for accurate robotics manipulation. This quest extends tothe Agri-robotics domain. In this paper, we propose TransPose, an improvedTransformer-based 6D pose estimation with a depth refinement module. Thearchitecture takes in only an RGB image as input with no additionalsupplementing modalities such as depth or thermal images. The architectureencompasses an innovative lighter depth estimation network that estimates depthfrom an RGB image using feature pyramid with an up-sampling method. Atransformer-based detection network with additional prediction heads isproposed to directly regress the object's centre and predict the 6D pose of thetarget. A novel depth refinement module is then used alongside the predictedcenters, 6D poses and depth patches to refine the accuracy of the estimated 6Dpose. We extensively compared our results with other state-of-the-art methodsand analysed our results for fruit-picking applications. The results weachieved show that our proposed technique outperforms the other methodsavailable in the literature.</description><author>Mahmoud Abdulsalam, Nabil Aouf</author><pubDate>Sun, 09 Jul 2023 18:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05561v1</guid></item><item><title>RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose</title><link>http://arxiv.org/abs/2303.07399v2</link><description>Recent studies on 2D pose estimation have achieved excellent performance onpublic benchmarks, yet its application in the industrial community stillsuffers from heavy model parameters and high latency. In order to bridge thisgap, we empirically explore key factors in pose estimation including paradigm,model architecture, training strategy, and deployment, and present ahigh-performance real-time multi-person pose estimation framework, RTMPose,based on MMPose. Our RTMPose-m achieves 75.8% AP on COCO with 90+ FPS on anIntel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-lachieves 67.0% AP on COCO-WholeBody with 130+ FPS. To further evaluateRTMPose's capability in critical real-time applications, we also report theperformance after deploying on the mobile device. Our RTMPose-s achieves 72.2%AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existingopen-source libraries. Code and models are released athttps://github.com/open-mmlab/mmpose/tree/1.x/projects/rtmpose.</description><author>Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, Kai Chen</author><pubDate>Mon, 03 Jul 2023 04:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07399v2</guid></item><item><title>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example</title><link>http://arxiv.org/abs/2305.12626v1</link><description>In order to meaningfully interact with the world, robot manipulators must beable to interpret objects they encounter. A critical aspect of thisinterpretation is pose estimation: inferring quantities that describe theposition and orientation of an object in 3D space. Most existing approaches topose estimation make limiting assumptions, often working only for specific,known object instances, or at best generalising to an object category usinglarge pose-labelled datasets. In this work, we present a method for achievingcategory-level pose estimation by inspection of just a single object from adesired category. We show that we can subsequently perform accurate poseestimation for unseen objects from an inspected category, and considerablyoutperform prior work by exploiting multi-view correspondences. We demonstratethat our method runs in real-time, enabling a robot manipulator equipped withan RGBD sensor to perform online 6D pose estimation for novel objects. Finally,we showcase our method in a continual learning setting, with a robot able todetermine whether objects belong to known categories, and if not, use activeperception to produce a one-shot category representation for subsequent poseestimation.</description><author>Walter Goodwin, Ioannis Havoutis, Ingmar Posner</author><pubDate>Mon, 22 May 2023 02:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12626v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human</title><link>http://arxiv.org/abs/2307.05853v1</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Wed, 12 Jul 2023 01:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v1</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v1</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Fri, 28 Apr 2023 14:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v1</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v2</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Sun, 11 Jun 2023 17:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v2</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v1</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Tue, 02 May 2023 03:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v1</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v2</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Mon, 19 Jun 2023 01:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v2</guid></item><item><title>Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects</title><link>http://arxiv.org/abs/2306.15858v1</link><description>Robotic manipulation, in particular in-hand object manipulation, oftenrequires an accurate estimate of the object's 6D pose. To improve the accuracyof the estimated pose, state-of-the-art approaches in 6D object pose estimationuse observational data from one or more modalities, e.g., RGB images, depth,and tactile readings. However, existing approaches make limited use of theunderlying geometric structure of the object captured by these modalities,thereby, increasing their reliance on visual features. This results in poorperformance when presented with objects that lack such visual features or whenvisual features are simply occluded. Furthermore, current approaches do nottake advantage of the proprioceptive information embedded in the position ofthe fingers. To address these limitations, in this paper: (1) we introduce ahierarchical graph neural network architecture for combining multimodal (visionand touch) data that allows for a geometrically informed 6D object poseestimation, (2) we introduce a hierarchical message passing operation thatflows the information within and across modalities to learn a graph-basedobject representation, and (3) we introduce a method that accounts for theproprioceptive information for in-hand object representation. We evaluate ourmodel on a diverse subset of objects from the YCB Object and Model Set, andshow that our method substantially outperforms existing state-of-the-art workin accuracy and robustness to occlusion. We also deploy our proposed frameworkon a real robot and qualitatively demonstrate successful transfer to realsettings.</description><author>Alireza Rezazadeh, Snehal Dikhale, Soshi Iba, Nawid Jamali</author><pubDate>Wed, 28 Jun 2023 02:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15858v1</guid></item><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)</title><link>http://arxiv.org/abs/2305.15873v1</link><description>Addressing accuracy limitations and pose ambiguity in 6D object poseestimation from single RGB images presents a significant challenge,particularly due to object symmetries or occlusions. In response, we introducea novel score-based diffusion method applied to the $SE(3)$ group, marking thefirst application of diffusion models to $SE(3)$ within the image domain,specifically tailored for pose estimation tasks. Extensive evaluationsdemonstrate the method's efficacy in handling pose ambiguity, mitigatingperspective-induced ambiguity, and showcasing the robustness of our surrogateStein score formulation on $SE(3)$. This formulation not only improves theconvergence of Langevin dynamics but also enhances computational efficiency.Thus, we pioneer a promising strategy for 6D object pose estimation.</description><author>Tsu-Ching Hsiao, Hao-Wei Chen, Hsuan-Kung Yang, Chun-Yi Lee</author><pubDate>Thu, 25 May 2023 10:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15873v1</guid></item><item><title>Graph-CoVis: GNN-based Multi-view Panorama Global Pose Estimation</title><link>http://arxiv.org/abs/2304.13201v1</link><description>In this paper, we address the problem of wide-baseline camera pose estimationfrom a group of 360$^\circ$ panoramas under upright-camera assumption. Recentwork has demonstrated the merit of deep-learning for end-to-end direct relativepose regression in 360$^\circ$ panorama pairs [11]. To exploit the benefits ofmulti-view logic in a learning-based framework, we introduce Graph-CoVis, whichnon-trivially extends CoVisPose [11] from relative two-view to globalmulti-view spherical camera pose estimation. Graph-CoVis is a novel GraphNeural Network based architecture that jointly learns the co-visible structureand global motion in an end-to-end and fully-supervised approach. Using theZInD [4] dataset, which features real homes presenting wide-baselines,occlusion, and limited visual overlap, we show that our model performscompetitively to state-of-the-art approaches.</description><author>Negar Nejatishahidin, Will Hutchcroft, Manjunath Narayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khosravan, Jana Kosecka, Sing Bing Kang</author><pubDate>Wed, 26 Apr 2023 01:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13201v1</guid></item><item><title>MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model</title><link>http://arxiv.org/abs/2302.08751v2</link><description>One of the major challenges in multi-person pose estimation is instance-awarekeypoint estimation. Previous methods address this problem by leveraging anoff-the-shelf detector, heuristic post-grouping process or explicit instanceidentification process, hindering further improvements in the inference speedwhich is an important factor for practical applications. From the statisticalpoint of view, those additional processes for identifying instances arenecessary to bypass learning the high-dimensional joint distribution of humankeypoints, which is a critical factor for another major challenge, theocclusion scenario. In this work, we propose a novel framework of single-stageinstance-aware pose estimation by modeling the joint distribution of humankeypoints with a mixture density model, termed as MDPose. Our MDPose estimatesthe distribution of human keypoints' coordinates using a mixture density modelwith an instance-aware keypoint head consisting simply of 8 convolutionallayers. It is trained by minimizing the negative log-likelihood of the groundtruth keypoints. Also, we propose a simple yet effective training strategy,Random Keypoint Grouping (RKG), which significantly alleviates the underflowproblem leading to successful learning of relations between keypoints. OnOCHuman dataset, which consists of images with highly occluded people, ourMDPose achieves state-of-the-art performance by successfully learning thehigh-dimensional joint distribution of human keypoints. Furthermore, our MDPoseshows significant improvement in inference speed with a competitive accuracy onMS COCO, a widely-used human keypoint dataset, thanks to the proposed muchsimpler single-stage pipeline.</description><author>Seunghyeon Seo, Jaeyoung Yoo, Jihye Hwang, Nojun Kwak</author><pubDate>Mon, 08 May 2023 13:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08751v2</guid></item><item><title>POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</title><link>http://arxiv.org/abs/2307.10387v1</link><description>The surgical usage of Mixed Reality (MR) has received growing attention inareas such as surgical navigation systems, skill assessment, and robot-assistedsurgeries. For such applications, pose estimation for hand and surgicalinstruments from an egocentric perspective is a fundamental task and has beenstudied extensively in the computer vision field in recent years. However, thedevelopment of this field has been impeded by a lack of datasets, especially inthe surgical field, where bloody gloves and reflective metallic tools make ithard to obtain 3D pose annotations for hands and objects using conventionalmethods. To address this issue, we propose POV-Surgery, a large-scale,synthetic, egocentric dataset focusing on pose estimation for hands withdifferent surgical gloves and three orthopedic surgical instruments, namelyscalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329frames, featuring high-resolution RGB-D video streams with activityannotations, accurate 3D and 2D annotations for hand-object pose, and 2Dhand-object segmentation masks. We fine-tune the current SOTA methods onPOV-Surgery and further show the generalizability when applying to real-lifecases with surgical gloves and tools by extensive evaluations. The code and thedataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.</description><author>Rui Wang, Sophokles Ktistakis, Siwei Zhang, Mirko Meboldt, Quentin Lohmeyer</author><pubDate>Wed, 19 Jul 2023 19:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10387v1</guid></item><item><title>Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning</title><link>http://arxiv.org/abs/2307.03007v1</link><description>Manual assembly workers face increasing complexity in their work.Human-centered assistance systems could help, but object recognition as anenabling technology hinders sophisticated human-centered design of thesesystems. At the same time, activity recognition based on hand poses suffersfrom poor pose estimation in complex usage scenarios, such as wearing gloves.This paper presents a self-supervised pipeline for adapting hand poseestimation to specific use cases with minimal human interaction. This enablescheap and robust hand posebased activity recognition. The pipeline consists ofa general machine learning model for hand pose estimation trained on ageneralized dataset, spatial and temporal filtering to account for anatomicalconstraints of the hand, and a retraining step to improve the model. Differentparameter combinations are evaluated on a publicly available and annotateddataset. The best parameter and model combination is then applied to unlabelledvideos from a manual assembly scenario. The effectiveness of the pipeline isdemonstrated by training an activity recognition as a downstream task in themanual assembly scenario.</description><author>Christian Jauch, Timo Leitritz, Marco F. Huber</author><pubDate>Thu, 06 Jul 2023 15:13:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03007v1</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Vision-based Target Pose Estimation with Multiple Markers for the Perching of UAVs</title><link>http://arxiv.org/abs/2304.14838v1</link><description>Autonomous Nano Aerial Vehicles have been increasingly popular insurveillance and monitoring operations due to their efficiency andmaneuverability. Once a target location has been reached, drones do not have toremain active during the mission. It is possible for the vehicle to perch andstop its motors in such situations to conserve energy, as well as maintain astatic position in unfavorable flying conditions. In the perching targetestimation phase, the steady and accuracy of a visual camera with markers is asignificant challenge. It is rapidly detectable from afar when using a largemarker, but when the drone approaches, it quickly disappears as out of cameraview. In this paper, a vision-based target poses estimation method usingmultiple markers is proposed to deal with the above-mentioned problems. First,a perching target with a small marker inside a larger one is designed toimprove detection capability at wide and close ranges. Second, the relativeposes of the flying vehicle are calculated from detected markers using amonocular camera. Next, a Kalman filter is applied to provide a more stable andreliable pose estimation, especially when the measurement data is missing dueto unexpected reasons. Finally, we introduced an algorithm for merging theposes data from multi markers. The poses are then sent to the positioncontroller to align the drone and the marker's center and steer it to perch onthe target. The experimental results demonstrated the effectiveness andfeasibility of the adopted approach. The drone can perch successfully onto thecenter of the markers with the attached 25mm-diameter rounded magnet.</description><author>Truong-Dong Do, Nguyen Xuan-Mung, Sung-Kyung Hong</author><pubDate>Tue, 25 Apr 2023 17:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14838v1</guid></item><item><title>Human Body Pose Estimation for Gait Identification: A Comprehensive Survey of Datasets and Models</title><link>http://arxiv.org/abs/2305.13765v1</link><description>Person identification is a problem that has received substantial attention,particularly in security domains. Gait recognition is one of the mostconvenient approaches enabling person identification at a distance without theneed of high-quality images. There are several review studies addressing personidentification such as the utilization of facial images, silhouette images, andwearable sensor. Despite skeleton-based person identification gainingpopularity while overcoming the challenges of traditional approaches, existingsurvey studies lack the comprehensive review of skeleton-based approaches togait identification. We present a detailed review of the human pose estimationand gait analysis that make the skeleton-based approaches possible. The studycovers various types of related datasets, tools, methodologies, and evaluationmetrics with associated challenges, limitations, and application domains.Detailed comparisons are presented for each of these aspects withrecommendations for potential research and alternatives. A common trendthroughout this paper is the positive impact that deep learning techniques arebeginning to have on topics such as human pose estimation and gaitidentification. The survey outcomes might be useful for the related researchcommunity and other stakeholders in terms of performance analysis of existingmethodologies, potential research gaps, application domains, and possiblecontributions in the future.</description><author>Luke K. Topham, Wasiq Khan, Dhiya Al-Jumeily, Abir Hussain</author><pubDate>Tue, 23 May 2023 08:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13765v1</guid></item><item><title>EgoCOL: Egocentric Camera pose estimation for Open-world 3D object Localization @Ego4D challenge 2023</title><link>http://arxiv.org/abs/2306.16606v1</link><description>We present EgoCOL, an egocentric camera pose estimation method for open-world3D object localization. Our method leverages sparse camera pose reconstructionsin a two-fold manner, video and scan independently, to estimate the camera poseof egocentric frames in 3D renders with high recall and precision. Weextensively evaluate our method on the Visual Query (VQ) 3D object localizationEgo4D benchmark. EgoCOL can estimate 62% and 59% more camera poses than theEgo4D baseline in the Ego4D Visual Queries 3D Localization challenge at CVPR2023 in the val and test sets, respectively. Our code is publicly available athttps://github.com/BCV-Uniandes/EgoCOL</description><author>Cristhian Forigua, Maria Escobar, Jordi Pont-Tuset, Kevis-Kokitsi Maninis, Pablo Arbel√°ez</author><pubDate>Thu, 29 Jun 2023 01:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16606v1</guid></item><item><title>MELON: NeRF with Unposed Images in SO(3)</title><link>http://arxiv.org/abs/2303.08096v2</link><description>Neural radiance fields enable novel-view synthesis and scene reconstructionwith photorealistic quality from a few images, but require known and accuratecamera poses. Conventional pose estimation algorithms fail on smooth orself-similar scenes, while methods performing inverse rendering from unposedviews require a rough initialization of the camera orientations. The maindifficulty of pose estimation lies in real-life objects being almost invariantunder certain transformations, making the photometric distance between renderedviews non-convex with respect to the camera parameters. Using an equivalencerelation that matches the distribution of local minima in camera space, wereduce this space to its quotient set, in which pose estimation becomes a moreconvex problem. Using a neural-network to regularize pose estimation, wedemonstrate that our method - MELON - can reconstruct a neural radiance fieldfrom unposed images with state-of-the-art accuracy while requiring ten timesfewer views than adversarial approaches.</description><author>Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, Dmitry Lagun</author><pubDate>Wed, 19 Jul 2023 09:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08096v2</guid></item><item><title>IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation</title><link>http://arxiv.org/abs/2303.13479v2</link><description>Category-level 6D pose estimation aims to predict the poses and sizes ofunseen objects from a specific category. Thanks to prior deformation, whichexplicitly adapts a category-specific 3D prior (i.e., a 3D template) to a givenobject instance, prior-based methods attained great success and have become amajor research stream. However, obtaining category-specific priors requirescollecting a large amount of 3D models, which is labor-consuming and often notaccessible in practice. This motivates us to investigate whether priors arenecessary to make prior-based methods effective. Our empirical study shows thatthe 3D prior itself is not the credit to the high performance. The keypointactually is the explicit deformation process, which aligns camera and worldcoordinates supervised by world-space 3D models (also called canonical space).Inspired by these observations, we introduce a simple prior-free implicit spacetransformation network, namely IST-Net, to transform camera-space features toworld-space counterparts and build correspondence between them in an implicitmanner without relying on 3D priors. Besides, we design camera- and world-spaceenhancers to enrich the features with pose-sensitive information andgeometrical constraints, respectively. Albeit simple, IST-Net achievesstate-of-the-art performance based-on prior-free design, with top inferencespeed on the REAL275 benchmark. Our code and models are available athttps://github.com/CVMI-Lab/IST-Net.</description><author>Jianhui Liu, Yukang Chen, Xiaoqing Ye, Xiaojuan Qi</author><pubDate>Wed, 19 Jul 2023 17:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13479v2</guid></item><item><title>A Probabilistic Relaxation of the Two-Stage Object Pose Estimation Paradigm</title><link>http://arxiv.org/abs/2306.00892v1</link><description>Existing object pose estimation methods commonly require a one-to-one pointmatching step that forces them to be separated into two consecutive stages:visual correspondence detection (e.g., by matching feature descriptors as partof a perception front-end) followed by geometric alignment (e.g., by optimizinga robust estimation objective for pointcloud registration orperspective-n-point). Instead, we propose a matching-free probabilisticformulation with two main benefits: i) it enables unified and concurrentoptimization of both visual correspondence and geometric alignment, and ii) itcan represent different plausible modes of the entire distribution of likelyposes. This in turn allows for a more graceful treatment of geometricperception scenarios where establishing one-to-one matches between points isconceptually ill-defined, such as textureless, symmetrical and/or occludedobjects and scenes where the correct pose is uncertain or there are multipleequally valid solutions.</description><author>Onur Beker</author><pubDate>Thu, 01 Jun 2023 17:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00892v1</guid></item><item><title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.14581v2</link><description>2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable tomodel the human skeletal topology. However, current GCN-based 3D HPE methodsupdate the node features by aggregating their neighbors' information withoutconsidering the interaction of joints in different motion patterns. Althoughsome studies import limb information to learn the movement patterns, the latentsynergies among joints, such as maintaining balance in the motion are seldominvestigated. We propose a hop-wise GraphFormer with intragroup jointrefinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists ofa novel Hop-wise GraphFormer(HGF) module and an Intragroup JointRefinement(IJR) module which leverages the prior limb information forperipheral joints refinement. The HGF module groups the joints by $k$-hopneighbors and utilizes a hop-wise transformer-like attention mechanism amongthese groups to discover latent joint synergy. Extensive experimental resultsshow that HopFIR outperforms the SOTA methods with a large margin (on theHuman3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).Furthermore, it is also demonstrated that previous SOTA GCN-based methods canbenefit from the proposed hop-wise attention mechanism efficiently withsignificant performance promotion, such as SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.</description><author>Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, ShanLin Yang</author><pubDate>Tue, 18 Jul 2023 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14581v2</guid></item><item><title>Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data</title><link>http://arxiv.org/abs/2307.06737v1</link><description>Human Pose Estimation is a thoroughly researched problem; however, mostdatasets focus on the side and front-view scenarios. We address the limitationby proposing a novel approach that tackles the challenges posed by extremeviewpoints and poses. We introduce a new method for synthetic data generation -RePoGen, RarE POses GENerator - with comprehensive control over pose and viewto augment the COCO dataset. Experiments on a new dataset of real images showthat adding RePoGen data to the COCO surpasses previous attempts to top-viewpose estimation and significantly improves performance on the bottom-viewdataset. Through an extensive ablation study on both the top and bottom viewdata, we elucidate the contributions of methodological choices and demonstrateimproved performance. The code and the datasets are available on the projectwebsite.</description><author>Miroslav Purkr√°bek, Ji≈ô√≠ Matas</author><pubDate>Thu, 13 Jul 2023 14:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06737v1</guid></item><item><title>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</title><link>http://arxiv.org/abs/2306.15667v2</link><description>Camera pose estimation is a long-standing computer vision problem that todate often relies on classical methods, such as handcrafted keypoint matching,RANSAC and bundle adjustment. In this paper, we propose to formulate theStructure from Motion (SfM) problem inside a probabilistic diffusion framework,modelling the conditional distribution of camera poses given input images. Thisnovel view of an old problem has several advantages. (i) The nature of thediffusion framework mirrors the iterative procedure of bundle adjustment. (ii)The formulation allows a seamless integration of geometric constraints fromepipolar geometry. (iii) It excels in typically difficult scenarios such assparse views with wide baselines. (iv) The method can predict intrinsics andextrinsics for an arbitrary amount of images. We demonstrate that our methodPoseDiffusion significantly improves over the classic SfM pipelines and thelearned approaches on two real-world datasets. Finally, it is observed that ourmethod can generalize across datasets without further training. Project page:https://posediffusion.github.io/</description><author>Jianyuan Wang, Christian Rupprecht, David Novotny</author><pubDate>Wed, 28 Jun 2023 11:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15667v2</guid></item><item><title>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling</title><link>http://arxiv.org/abs/2306.17201v1</link><description>Estimating 3D human poses only from a 2D human pose sequence is thoroughlyexplored in recent years. Yet, prior to this, no such work has attempted tounify 2D and 3D pose representations in the shared feature space. In thispaper, we propose MPM, a unified 2D-3D human pose representation framework viamasked pose modeling. We treat 2D and 3D poses as two different modalities likevision and language and build a single-stream transformer-based architecture.We apply three pretext tasks, which are masked 2D pose modeling, masked 3D posemodeling, and masked 2D pose lifting to pre-train our network and usefull-supervision to perform further fine-tuning. A high masking ratio of 72.5%in total with a spatio-temporal mask sampling strategy leading to betterrelation modeling both in spatial and temporal domains. MPM can handle multipletasks including 3D human pose estimation, 3D pose estimation from occluded 2Dpose, and 3D pose completion in a single framework. We conduct extensiveexperiments and ablation studies on several widely used human pose datasets andachieve state-of-the-art performance on Human3.6M and MPI-INF-3DHP. Codes andmodel checkpoints are available at https://github.com/vvirgooo2/MPM</description><author>Zhenyu Zhang, Wenhao Chai, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 29 Jun 2023 11:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17201v1</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach</title><link>http://arxiv.org/abs/2307.01004v1</link><description>We introduce a novel one-stage end-to-end multi-person 2D pose estimationalgorithm, known as Joint Coordinate Regression and Association (JCRA), thatproduces human pose joints and associations without requiring anypost-processing. The proposed algorithm is fast, accurate, effective, andsimple. The one-stage end-to-end network architecture significantly improvesthe inference speed of JCRA. Meanwhile, we devised a symmetric networkstructure for both the encoder and decoder, which ensures high accuracy inidentifying keypoints. It follows an architecture that directly outputs partpositions via a transformer network, resulting in a significant improvement inperformance. Extensive experiments on the MS COCO and CrowdPose benchmarksdemonstrate that JCRA outperforms state-of-the-art approaches in both accuracyand efficiency. Moreover, JCRA demonstrates 69.2 mAP and is 78\% faster atinference acceleration than previous state-of-the-art bottom-up algorithms. Thecode for this algorithm will be publicly available.</description><author>Dongyang Yu, Yunshi Xie, Wangpeng An, Li Zhang, Yufeng Yao</author><pubDate>Mon, 03 Jul 2023 14:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01004v1</guid></item><item><title>Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation</title><link>http://arxiv.org/abs/2306.17074v1</link><description>One of the mainstream schemes for 2D human pose estimation (HPE) is learningkeypoints heatmaps by a neural network. Existing methods typically improve thequality of heatmaps by customized architectures, such as high-resolutionrepresentation and vision Transformers. In this paper, we propose\textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypointsheatmaps generation problem from noised heatmaps. During training, thekeypoints are diffused to random distribution by adding noises and thediffusion model learns to recover ground-truth heatmaps from noised heatmapswith respect to conditions constructed by image feature. During inference, thediffusion model generates heatmaps from initialized heatmaps in a progressivedenoising way. Moreover, we further explore improving the performance ofDiffusionPose with conditions from human structural information. Extensiveexperiments show the prowess of our DiffusionPose, with improvements of 1.6,1.2, and 1.2 mAP on widely-used COCO, CrowdPose, and AI Challenge datasets,respectively.</description><author>Zhongwei Qiu, Qiansheng Yang, Jian Wang, Xiyu Wang, Chang Xu, Dongmei Fu, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang</author><pubDate>Thu, 29 Jun 2023 17:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17074v1</guid></item><item><title>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</title><link>http://arxiv.org/abs/2306.05410v1</link><description>A critical obstacle preventing NeRF models from being deployed broadly in thewild is their reliance on accurate camera poses. Consequently, there is growinginterest in extending NeRF models to jointly optimize camera poses and scenerepresentation, which offers an alternative to off-the-shelf SfM pipelineswhich have well-understood failure modes. Existing approaches for unposed NeRFoperate under limited assumptions, such as a prior pose distribution or coarsepose initialization, making them less effective in a general setting. In thiswork, we propose a novel approach, LU-NeRF, that jointly estimates camera posesand neural radiance fields with relaxed assumptions on pose configuration. Ourapproach operates in a local-to-global manner, where we first optimize overlocal subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose andgeometry for this challenging few-shot task. The mini-scene poses are broughtinto a global reference frame through a robust pose synchronization step, wherea final global optimization of pose and scene can be performed. We show ourLU-NeRF pipeline outperforms prior attempts at unposed NeRF without makingrestrictive assumptions on the pose prior. This allows us to operate in thegeneral SE(3) pose setting, unlike the baselines. Our results also indicate ourmodel can be complementary to feature-based SfM pipelines as it comparesfavorably to COLMAP on low-texture and low-resolution images.</description><author>Zezhou Cheng, Carlos Esteves, Varun Jampani, Abhishek Kar, Subhransu Maji, Ameesh Makadia</author><pubDate>Thu, 08 Jun 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05410v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gscho√ümann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network</title><link>http://arxiv.org/abs/2306.12525v1</link><description>In this technical report, we present the 1st place solution for the 2023Waymo Open Dataset Pose Estimation challenge. Due to the difficulty ofacquiring large-scale 3D human keypoint annotation, previous methods havecommonly relied on 2D image features and 2D sequential annotations for 3D humanpose estimation. In contrast, our proposed method, named LPFormer, uses onlyLiDAR as its input along with its corresponding 3D annotations. LPFormerconsists of two stages: the first stage detects the human bounding box andextracts multi-level feature representations, while the second stage employs atransformer-based network to regress the human keypoints using these features.Experimental results on the Waymo Open Dataset demonstrate the top performance,and improvements even compared to previous multi-modal solutions.</description><author>Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Hassan Foroosh</author><pubDate>Wed, 21 Jun 2023 20:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12525v1</guid></item><item><title>Hand Pose Estimation with Mems-Ultrasonic Sensors</title><link>http://arxiv.org/abs/2306.12652v1</link><description>Hand tracking is an important aspect of human-computer interaction and has awide range of applications in extended reality devices. However, current handmotion capture methods suffer from various limitations. For instance,visual-based hand pose estimation is susceptible to self-occlusion and changesin lighting conditions, while IMU-based tracking gloves experience significantdrift and are not resistant to external magnetic field interference. To addressthese issues, we propose a novel and low-cost hand-tracking glove that utilizesseveral MEMS-ultrasonic sensors attached to the fingers, to measure thedistance matrix among the sensors. Our lightweight deep network thenreconstructs the hand pose from the distance matrix. Our experimental resultsdemonstrate that this approach is both accurate, size-agnostic, and robust toexternal interference. We also show the design logic for the sensor selection,sensor configurations, circuit diagram, as well as model architecture.</description><author>Qiang Zhang, Yuanqiao Lin, Yubin Lin, Szymon Rusinkiewicz</author><pubDate>Thu, 22 Jun 2023 04:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12652v1</guid></item><item><title>Next-generation Surgical Navigation: Multi-view Marker-less 6DoF Pose Estimation of Surgical Instruments</title><link>http://arxiv.org/abs/2305.03535v1</link><description>State-of-the-art research of traditional computer vision is increasinglyleveraged in the surgical domain. A particular focus in computer-assistedsurgery is to replace marker-based tracking systems for instrument localizationwith pure image-based 6DoF pose estimation. However, the state of the art hasnot yet met the accuracy required for surgical navigation. In this context, wepropose a high-fidelity marker-less optical tracking system for surgicalinstrument localization. We developed a multi-view camera setup consisting ofstatic and mobile cameras and collected a large-scale RGB-D video dataset withdedicated synchronization and data fusions methods. Different state-of-the-artpose estimation methods were integrated into a deep learning pipeline andevaluated on multiple camera configurations. Furthermore, the performanceimpacts of different input modalities and camera positions, as well as trainingon purely synthetic data, were compared. The best model achieved an averageposition and orientation error of 1.3 mm and 1.0{\deg} for a surgical drill aswell as 3.8 mm and 5.2{\deg} for a screwdriver. These results significantlyoutperform related methods in the literature and are close to clinical-gradeaccuracy, demonstrating that marker-less tracking of surgical instruments isbecoming a feasible alternative to existing marker-based systems.</description><author>Jonas Hein, Nicola Cavalcanti, Daniel Suter, Lukas Zingg, Fabio Carrillo, Mazda Farshad, Marc Pollefeys, Nassir Navab, Philipp F√ºrnstahl</author><pubDate>Fri, 05 May 2023 14:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03535v1</guid></item><item><title>A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition</title><link>http://arxiv.org/abs/2307.02906v1</link><description>Sensor-based Human Activity Recognition facilitates unobtrusive monitoring ofhuman movements. However, determining the most effective sensor placement foroptimal classification performance remains challenging. This paper introduces anovel methodology to resolve this issue, using real-time 2D pose estimationsderived from video recordings of target activities. The derived skeleton dataprovides a unique strategy for identifying the optimal sensor location. Wevalidate our approach through a feasibility study, applying inertial sensors tomonitor 13 different activities across ten subjects. Our findings indicate thatthe vision-based method for sensor placement offers comparable results to theconventional deep learning approach, demonstrating its efficacy. This researchsignificantly advances the field of Human Activity Recognition by providing alightweight, on-device solution for determining the optimal sensor placement,thereby enhancing data anonymization and supporting a multimodal classificationapproach.</description><author>Orhan Konak, Alexander Wischmann, Robin van de Water, Bert Arnrich</author><pubDate>Thu, 06 Jul 2023 11:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02906v1</guid></item><item><title>6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics</title><link>http://arxiv.org/abs/2303.13241v3</link><description>We present a novel technique to estimate the 6D pose of objects from singleimages where the 3D geometry of the object is only given approximately and notas a precise 3D model. To achieve this, we employ a dense 2D-to-3Dcorrespondence predictor that regresses 3D model coordinates for every pixel.In addition to the 3D coordinates, our model also estimates the pixel-wisecoordinate error to discard correspondences that are likely wrong. This allowsus to generate multiple 6D pose hypotheses of the object, which we then refineiteratively using a highly efficient region-based approach. We also introduce anovel pixel-wise posterior formulation by which we can estimate the probabilityfor each hypothesis and select the most likely one. As we show in experiments,our approach is capable of dealing with extreme visual conditions includingoverexposure, high contrast, or low signal-to-noise ratio. This makes it apowerful technique for the particularly challenging task of estimating the poseof tumbling satellites for in-orbit robotic applications. Our method achievesstate-of-the-art performance on the SPEED+ dataset and has won the SPEC2021post-mortem competition.</description><author>Maximilian Ulmer, Maximilian Durner, Martin Sundermeyer, Manuel Stoiber, Rudolph Triebel</author><pubDate>Wed, 21 Jun 2023 15:36:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13241v3</guid></item><item><title>Towards the extraction of robust sign embeddings for low resource sign language recognition</title><link>http://arxiv.org/abs/2306.17558v1</link><description>Isolated Sign Language Recognition (SLR) has mostly been applied onrelatively large datasets containing signs executed slowly and clearly by alimited group of signers. In real-world scenarios, however, we are met withchallenging visual conditions, coarticulated signing, small datasets, and theneed for signer independent models. To tackle this difficult problem, werequire a robust feature extractor to process the sign language videos. Onecould expect human pose estimators to be ideal candidates. However, due to adomain mismatch with their training sets and challenging poses in signlanguage, they lack robustness on sign language data and image based modelsoften still outperform keypoint based models. Furthermore, whereas the commonpractice of transfer learning with image based models yields even higheraccuracy, keypoint based models are typically trained from scratch on every SLRdataset. These factors limit their usefulness for SLR. From the existingliterature, it is also not clear which, if any, pose estimator performs bestfor SLR. We compare the three most popular pose estimators for SLR: OpenPose,MMPose and MediaPipe. We show that through keypoint normalization, missingkeypoint imputation, and learning a pose embedding, we can obtain significantlybetter results and enable transfer learning. We show that keypoint-basedembeddings contain cross-lingual features: they can transfer between signlanguages and achieve competitive performance even when fine-tuning only theclassifier layer of an SLR model on a target sign language. We furthermoreachieve better performance using fine-tuned transferred embeddings than modelstrained only on the target sign language. The application of these embeddingscould prove particularly useful for low resource sign languages in the future.</description><author>Mathieu De Coster, Ellen Rushe, Ruth Holmes, Anthony Ventresque, Joni Dambre</author><pubDate>Fri, 30 Jun 2023 12:21:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17558v1</guid></item><item><title>Binarizing by Classification: Is soft function really necessary?</title><link>http://arxiv.org/abs/2205.07433v3</link><description>Binary neural networks leverage $\mathrm{Sign}$ function to binarize weightsand activations, which require gradient estimators to overcome itsnon-differentiability and will inevitably bring gradient errors duringbackpropagation. Although many hand-designed soft functions have been proposedas gradient estimators to better approximate gradients, their mechanism is notclear and there are still huge performance gaps between binary models and theirfull-precision counterparts. To address these issues and reduce gradient error,we propose to tackle network binarization as a binary classification problemand use a multi-layer perceptron (MLP) as the classifier in the forward passand gradient estimator in the backward pass. Benefiting from the MLP'stheoretical capability to fit any continuous function, it can be adaptivelylearned to binarize networks and backpropagate gradients without any priorknowledge of soft functions. From this perspective, we further empiricallyjustify that even a simple linear function can outperform previous complex softfunctions. Extensive experiments demonstrate that the proposed method yieldssurprising performance both in image classification and human pose estimationtasks. Specifically, we achieve $65.7\%$ top-1 accuracy of ResNet-34 onImageNet dataset, with an absolute improvement of $2.6\%$. Moreover, we takebinarization as a lightweighting approach for pose estimation models andpropose well-designed binary pose estimation networks SBPN and BHRNet. Whenevaluating on the challenging Microsoft COCO keypoint dataset, the proposedmethod enables binary networks to achieve a mAP of up to $60.6$ for the firsttime. Experiments conducted on real platforms demonstrate that BNN achieves abetter balance between performance and computational complexity, especiallywhen computational resources are extremely low.</description><author>Yefei He, Luoming Zhang, Weijia Wu, Hong Zhou</author><pubDate>Sun, 16 Jul 2023 08:22:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.07433v3</guid></item><item><title>IMUPoser: Full-Body Pose Estimation using IMUs in Phones, Watches, and Earbuds</title><link>http://arxiv.org/abs/2304.12518v1</link><description>Tracking body pose on-the-go could have powerful uses in fitness, mobilegaming, context-aware virtual assistants, and rehabilitation. However, usersare unlikely to buy and wear special suits or sensor arrays to achieve thisend. Instead, in this work, we explore the feasibility of estimating body poseusing IMUs already in devices that many users own -- namely smartphones,smartwatches, and earbuds. This approach has several challenges, includingnoisy data from low-cost commodity IMUs, and the fact that the number ofinstrumentation points on a users body is both sparse and in flux. Our pipelinereceives whatever subset of IMU data is available, potentially from just asingle device, and produces a best-guess pose. To evaluate our model, wecreated the IMUPoser Dataset, collected from 10 participants wearing or holdingoff-the-shelf consumer devices and across a variety of activity contexts. Weprovide a comprehensive evaluation of our system, benchmarking it on both ourown and existing IMU datasets.</description><author>Vimal Mollyn, Riku Arakawa, Mayank Goel, Chris Harrison, Karan Ahuja</author><pubDate>Tue, 25 Apr 2023 03:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12518v1</guid></item><item><title>D3L: Decomposition of 3D Rotation and Lift from 2D Joint to 3D for Human Mesh Recovery</title><link>http://arxiv.org/abs/2306.06406v1</link><description>Existing methods for 3D human mesh recovery always directly estimate SMPLparameters, which involve both joint rotations and shape parameters. However,these methods present rotation semantic ambiguity, rotation error accumulation,and shape estimation overfitting, which also leads to errors in the estimatedpose. Additionally, these methods have not efficiently leveraged theadvancements in another hot topic, human pose estimation. To address theseissues, we propose a novel approach, Decomposition of 3D Rotation and Lift from2D Joint to 3D mesh (D3L). We disentangle 3D joint rotation into bone directionand bone twist direction so that the human mesh recovery task is broken downinto estimation of pose, twist, and shape, which can be handled independently.Then we design a 2D-to-3D lifting network for estimating twist direction and 3Djoint position from 2D joint position sequences and introduce a nonlinearoptimization method for fitting shape parameters and bone directions. Ourapproach can leverage human pose estimation methods, and avoid pose errorsintroduced by shape estimation overfitting. We conduct experiments on theHuman3.6M dataset and demonstrate improved performance compared to existingmethods by a large margin.</description><author>Xiaoyang Hao, Han Li, Jun Cheng, Lei Wang</author><pubDate>Sat, 10 Jun 2023 11:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06406v1</guid></item><item><title>Digital Twin-Based 3D Map Management for Edge-Assisted Mobile Augmented Reality</title><link>http://arxiv.org/abs/2305.16571v1</link><description>In this paper, we design a 3D map management scheme for edge-assisted mobileaugmented reality (MAR) to support the pose estimation of individual MARdevice, which uploads camera frames to an edge server. Our objective is tominimize the pose estimation uncertainty of the MAR device by periodicallyselecting a proper set of camera frames for uploading to update the 3D map. Toaddress the challenges of the dynamic uplink data rate and the time-varyingpose of the MAR device, we propose a digital twin (DT)-based approach to 3D mapmanagement. First, a DT is created for the MAR device, which emulates 3D mapmanagement based on predicting subsequent camera frames. Second, a model-basedreinforcement learning (MBRL) algorithm is developed, utilizing the datacollected from both the actual and the emulated data to manage the 3D map. Withextensive emulated data provided by the DT, the MBRL algorithm can quicklyprovide an adaptive map management policy in a highly dynamic environment.Simulation results demonstrate that the proposed DT-based 3D map managementoutperforms benchmark schemes by achieving lower pose estimation uncertaintyand higher data efficiency in dynamic environments.</description><author>Conghao Zhou, Jie Gao, Mushu Li, Nan Cheng, Xuemin Shen, Weihua Zhuang</author><pubDate>Fri, 26 May 2023 02:38:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16571v1</guid></item><item><title>CLAMP: Prompt-based Contrastive Learning for Connecting Language and Animal Pose</title><link>http://arxiv.org/abs/2206.11752v3</link><description>Animal pose estimation is challenging for existing image-based methodsbecause of limited training data and large intra- and inter-species variances.Motivated by the progress of visual-language research, we propose thatpre-trained language models (e.g., CLIP) can facilitate animal pose estimationby providing rich prior knowledge for describing animal keypoints in text.However, we found that building effective connections between pre-trainedlanguage models and visual animal keypoints is non-trivial since the gapbetween text-based descriptions and keypoint-based visual features about animalpose can be significant. To address this issue, we introduce a novelprompt-based Contrastive learning scheme for connecting Language and AniMalPose (CLAMP) effectively. The CLAMP attempts to bridge the gap by adapting thetext prompts to the animal keypoints during network training. The adaptation isdecomposed into spatial-aware and feature-aware processes, and two novelcontrastive losses are devised correspondingly. In practice, the CLAMP enablesthe first cross-modal animal pose estimation paradigm. Experimental resultsshow that our method achieves state-of-the-art performance under thesupervised, few-shot, and zero-shot settings, outperforming image-based methodsby a large margin.</description><author>Xu Zhang, Wen Wang, Zhe Chen, Yufei Xu, Jing Zhang, Dacheng Tao</author><pubDate>Mon, 26 Jun 2023 01:46:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11752v3</guid></item><item><title>Decomposed Human Motion Prior for Video Pose Estimation via Adversarial Training</title><link>http://arxiv.org/abs/2305.18743v1</link><description>Estimating human pose from video is a task that receives considerableattention due to its applicability in numerous 3D fields. The complexity ofprior knowledge of human body movements poses a challenge to neural networkmodels in the task of regressing keypoints. In this paper, we address thisproblem by incorporating motion prior in an adversarial way. Different fromprevious methods, we propose to decompose holistic motion prior to joint motionprior, making it easier for neural networks to learn from prior knowledgethereby boosting the performance on the task. We also utilize a novelregularization loss to balance accuracy and smoothness introduced by motionprior. Our method achieves 9\% lower PA-MPJPE and 29\% lower acceleration errorthan previous methods tested on 3DPW. The estimator proves its robustness byachieving impressive performance on in-the-wild dataset.</description><author>Wenshuo Chen, Xiang Zhou, Zhengdi Yu, Zhaoyu Zheng, Weixi Gu, Kai Zhang</author><pubDate>Tue, 30 May 2023 05:53:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18743v1</guid></item><item><title>Full-Body Articulated Human-Object Interaction</title><link>http://arxiv.org/abs/2212.10621v2</link><description>Fine-grained capturing of 3D HOI boosts human activity understanding andfacilitates downstream visual tasks, including action recognition, holisticscene reconstruction, and human motion synthesis. Despite its significance,existing works mostly assume that humans interact with rigid objects using onlya few body parts, limiting their scope. In this paper, we address thechallenging problem of f-AHOI, wherein the whole human bodies interact witharticulated objects, whose parts are connected by movable joints. We presentCHAIRS, a large-scale motion-captured f-AHOI dataset, consisting of 16.2 hoursof versatile interactions between 46 participants and 81 articulated and rigidsittable objects. CHAIRS provides 3D meshes of both humans and articulatedobjects during the entire interactive process, as well as realistic andphysically plausible full-body interactions. We show the value of CHAIRS withobject pose estimation. By learning the geometrical relationships in HOI, wedevise the very first model that leverage human pose estimation to tackle theestimation of articulated object poses and shapes during whole-bodyinteractions. Given an image and an estimated human pose, our model firstreconstructs the pose and shape of the object, then optimizes thereconstruction according to a learned interaction prior. Under both evaluationsettings (e.g., with or without the knowledge of objects'geometries/structures), our model significantly outperforms baselines. We hopeCHAIRS will promote the community towards finer-grained interactionunderstanding. We will make the data/code publicly available.</description><author>Nan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan zhang, Yixin Chen, He Wang, Yixin Zhu, Siyuan Huang</author><pubDate>Tue, 16 May 2023 20:32:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10621v2</guid></item><item><title>HouseCat6D -- A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</title><link>http://arxiv.org/abs/2212.10428v4</link><description>Estimating the 6D pose of objects is a major 3D computer vision problem.Since the promising outcomes from instance-level approaches, research headsalso move towards category-level pose estimation for more practical applicationscenarios. However, unlike well-established instance-level pose datasets,available category-level datasets lack annotation quality and provided posequantity. We propose the new category-level 6D pose dataset HouseCat6Dfeaturing 1) Multi-modality of Polarimetric RGB and Depth (RGBD+P), 2) Highlydiverse 194 objects of 10 household object categories including 2photometrically challenging categories, 3) High-quality pose annotation with anerror range of only 1.35 mm to 1.74 mm, 4) 41 large-scale scenes with extensiveviewpoint coverage and occlusions, 5) Checkerboard-free environment throughoutthe entire scene, and 6) Additionally annotated dense 6D parallel-jaw grasps.Furthermore, we also provide benchmark results of state-of-the-artcategory-level pose estimation networks.</description><author>HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, Guangyao Zhai, Hannah Schieber, Giulia Rizzoli, Pengyuan Wang, Hongcheng Zhao, Lorenzo Garattoni, Sven Meier, Daniel Roth, Nassir Navab, Benjamin Busam</author><pubDate>Wed, 26 Apr 2023 11:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.10428v4</guid></item><item><title>Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction</title><link>http://arxiv.org/abs/2305.09510v1</link><description>Robotic manipulation systems operating in complex environments rely onperception systems that provide information about the geometry (pose and 3Dshape) of the objects in the scene along with other semantic information suchas object labels. This information is then used for choosing the feasiblegrasps on relevant objects. In this paper, we present a novel method to providethis geometric and semantic information of all objects in the scene as well asfeasible grasps on those objects simultaneously. The main advantage of ourmethod is its speed as it avoids sequential perception and grasp planningsteps. With detailed quantitative analysis, we show that our method deliverscompetitive performance compared to the state-of-the-art dedicated methods forobject shape, pose, and grasp predictions while providing fast inference at 30frames per second speed.</description><author>Shubham Agrawal, Nikhil Chavan-Dafle, Isaac Kasahara, Selim Engin, Jinwook Huh, Volkan Isler</author><pubDate>Tue, 16 May 2023 16:03:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09510v1</guid></item><item><title>Event Camera-based Visual Odometry for Dynamic Motion Tracking of a Legged Robot Using Adaptive Time Surface</title><link>http://arxiv.org/abs/2305.08962v1</link><description>Our paper proposes a direct sparse visual odometry method that combines eventand RGB-D data to estimate the pose of agile-legged robots during dynamiclocomotion and acrobatic behaviors. Event cameras offer high temporalresolution and dynamic range, which can eliminate the issue of blurred RGBimages during fast movements. This unique strength holds a potential foraccurate pose estimation of agile-legged robots, which has been a challengingproblem to tackle. Our framework leverages the benefits of both RGB-D and eventcameras to achieve robust and accurate pose estimation, even during dynamicmaneuvers such as jumping and landing a quadruped robot, the Mini-Cheetah. Ourmajor contributions are threefold: Firstly, we introduce an adaptive timesurface (ATS) method that addresses the whiteout and blackout issue inconventional time surfaces by formulating pixel-wise decay rates based on scenecomplexity and motion speed. Secondly, we develop an effective pixel selectionmethod that directly samples from event data and applies sample filteringthrough ATS, enabling us to pick pixels on distinct features. Lastly, wepropose a nonlinear pose optimization formula that simultaneously performs3D-2D alignment on both RGB-based and event-based maps and images, allowing thealgorithm to fully exploit the benefits of both data streams. We extensivelyevaluate the performance of our framework on both public datasets and our ownquadruped robot dataset, demonstrating its effectiveness in accuratelyestimating the pose of agile robots during dynamic movements.</description><author>Shifan Zhu, Zhipeng Tang, Michael Yang, Erik Learned-Miller, Donghyun Kim</author><pubDate>Mon, 15 May 2023 20:03:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08962v1</guid></item><item><title>Shape-based pose estimation for automatic standard views of the knee</title><link>http://arxiv.org/abs/2305.16717v1</link><description>Surgical treatment of complicated knee fractures is guided by real-timeimaging using a mobile C-arm. Immediate and continuous control is achieved via2D anatomy-specific standard views that correspond to a specific C-arm poserelative to the patient positioning, which is currently determined manually,following a trial-and-error approach at the cost of time and radiation dose.The characteristics of the standard views of the knee suggests that the shapeinformation of individual bones could guide an automatic positioning procedure,reducing time and the amount of unnecessary radiation during C-arm positioning.To fully automate the C-arm positioning task during knee surgeries, we proposea complete framework that enables (1) automatic laterality and standard viewclassification and (2) automatic shape-based pose regression toward the desiredstandard view based on a single initial X-ray. A suitable shape representationis proposed to incorporate semantic information into the pose regressionpipeline. The pipeline is designed to handle two distinct standard viewssimultaneously. Experiments were conducted to assess the performance of theproposed system on 3528 synthetic and 1386 real X-rays for the a.-p. andlateral standard. The view/laterality classificator resulted in an accuracy of100\%/98\% on the simulated and 99\%/98\% on the real X-rays. The poseregression performance was$d\theta_{a.-p}=5.8\pm3.3\degree,\,d\theta_{lateral}=3.7\pm2.0\degree$ on thesimulated data and$d\theta_{a.-p}=7.4\pm5.0\degree,\,d\theta_{lateral}=8.4\pm5.4\degree$ on thereal data outperforming intensity-based pose regression.</description><author>Lisa Kausch, Sarina Thomas, Holger Kunze, Jan Siad El Barbari, Klaus Maier-Hein</author><pubDate>Fri, 26 May 2023 09:03:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16717v1</guid></item><item><title>Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints</title><link>http://arxiv.org/abs/2206.12498v2</link><description>We consider a category-level perception problem, where one is given 2D or 3Dsensor data picturing an object of a given category (e.g., a car), and has toreconstruct the 3D pose and shape of the object despite intra-class variability(i.e., different car models have different shapes). We consider an active shapemodel, where -for an object category- we are given a library of potential CADmodels describing objects in that category, and we adopt a standard formulationwhere pose and shape are estimated from 2D or 3D keypoints via non-convexoptimization. Our first contribution is to develop PACE3D* and PACE2D*, thefirst certifiably optimal solvers for pose and shape estimation using 3D and 2Dkeypoints, respectively. Both solvers rely on the design of tight (i.e., exact)semidefinite relaxations. Our second contribution is to develop outlier-robustversions of both solvers, named PACE3D# and PACE2D#. Towards this goal, wepropose ROBIN, a general graph-theoretic framework to prune outliers, whichuses compatibility hypergraphs to model measurements' compatibility. We showthat in category-level perception problems these hypergraphs can be built fromthe winding orders of the keypoints (in 2D) or their convex hulls (in 3D), andmany outliers can be filtered out via maximum hyperclique computation. The lastcontribution is an extensive experimental evaluation. Besides providing anablation study on simulated datasets and on the PASCAL3D+ dataset, we combineour solver with a deep keypoint detector, and show that PACE3D# improves overthe state of the art in vehicle pose estimation in the ApolloScape datasets,and its runtime is compatible with practical applications. We release our codeat https://github.com/MIT-SPARK/PACE.</description><author>Jingnan Shi, Heng Yang, Luca Carlone</author><pubDate>Mon, 15 May 2023 04:39:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.12498v2</guid></item><item><title>Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot, Generalizable Approach using RGB Images</title><link>http://arxiv.org/abs/2306.07598v1</link><description>The accurate estimation of six degrees-of-freedom (6DoF) object poses isessential for many applications in robotics and augmented reality. However,existing methods for 6DoF pose estimation often depend on CAD templates ordense support views, restricting their usefulness in realworld situations. Inthis study, we present a new cascade framework named Cas6D for few-shot 6DoFpose estimation that is generalizable and uses only RGB images. To address thefalse positives of target object detection in the extreme few-shot setting, ourframework utilizes a selfsupervised pre-trained ViT to learn robust featurerepresentations. Then, we initialize the nearest top-K pose candidates based onsimilarity score and refine the initial poses using feature pyramids toformulate and update the cascade warped feature volume, which encodes contextat increasingly finer scales. By discretizing the pose search range usingmultiple pose bins and progressively narrowing the pose search range in eachstage using predictions from the previous stage, Cas6D can overcome the largegap between pose candidates and ground truth poses, which is a common failuremode in sparse-view scenarios. Experimental results on the LINEMOD and GenMOPdatasets demonstrate that Cas6D outperforms state-of-the-art methods by 9.2%and 3.8% accuracy (Proj-5) under the 32-shot setting compared to OnePose++ andGen6D.</description><author>Panwang Pan, Zhiwen Fan, Brandon Y. Feng, Peihao Wang, Chenxin Li, Zhangyang Wang</author><pubDate>Tue, 13 Jun 2023 08:45:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07598v1</guid></item><item><title>NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation</title><link>http://arxiv.org/abs/2305.08590v1</link><description>With the progress of 3D human pose and shape estimation, state-of-the-artmethods can either be robust to occlusions or obtain pixel-aligned accuracy innon-occlusion cases. However, they cannot obtain robustness and mesh-imagealignment at the same time. In this work, we present NIKI (Neural InverseKinematics with Invertible Neural Network), which models bi-directional errorsto improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKIcan learn from both the forward and inverse processes with invertible networks.In the inverse process, the model separates the error from the plausible 3Dpose manifold for a robust 3D human pose estimation. In the forward process, weenforce the zero-error boundary conditions to improve the sensitivity toreliable joint positions for better mesh-image alignment. Furthermore, NIKIemulates the analytical inverse kinematics algorithms with the twist-and-swingdecomposition for better interpretability. Experiments on standard andocclusion-specific benchmarks demonstrate the effectiveness of NIKI, where weexhibit robust and well-aligned results simultaneously. Code is available athttps://github.com/Jeff-sjtu/NIKI</description><author>Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang, Cewu Lu</author><pubDate>Mon, 15 May 2023 13:13:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08590v1</guid></item><item><title>Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares</title><link>http://arxiv.org/abs/2305.09527v1</link><description>We propose a differentiable nonlinear least squares framework to account foruncertainty in relative pose estimation from feature correspondences.Specifically, we introduce a symmetric version of the probabilistic normalepipolar constraint, and an approach to estimate the covariance of featurepositions by differentiating through the camera pose estimation procedure. Weevaluate our approach on synthetic, as well as the KITTI and EuRoC real-worlddatasets. On the synthetic dataset, we confirm that our learned covariancesaccurately approximate the true noise distribution. In real world experiments,we find that our approach consistently outperforms state-of-the-artnon-probabilistic and probabilistic approaches, regardless of the featureextraction algorithm of choice.</description><author>Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, Daniel Cremers</author><pubDate>Tue, 16 May 2023 16:21:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09527v1</guid></item><item><title>Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares</title><link>http://arxiv.org/abs/2305.09527v2</link><description>We propose a differentiable nonlinear least squares framework to account foruncertainty in relative pose estimation from feature correspondences.Specifically, we introduce a symmetric version of the probabilistic normalepipolar constraint, and an approach to estimate the covariance of featurepositions by differentiating through the camera pose estimation procedure. Weevaluate our approach on synthetic, as well as the KITTI and EuRoC real-worlddatasets. On the synthetic dataset, we confirm that our learned covariancesaccurately approximate the true noise distribution. In real world experiments,we find that our approach consistently outperforms state-of-the-artnon-probabilistic and probabilistic approaches, regardless of the featureextraction algorithm of choice.</description><author>Dominik Muhle, Lukas Koestler, Krishna Murthy Jatavallabhula, Daniel Cremers</author><pubDate>Thu, 18 May 2023 19:35:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.09527v2</guid></item><item><title>Poses as Queries: Image-to-LiDAR Map Localization with Transformers</title><link>http://arxiv.org/abs/2305.04298v1</link><description>High-precision vehicle localization with commercial setups is a crucialtechnique for high-level autonomous driving tasks. Localization with amonocular camera in LiDAR map is a newly emerged approach that achievespromising balance between cost and accuracy, but estimating pose by findingcorrespondences between such cross-modal sensor data is challenging, therebydamaging the localization accuracy. In this paper, we address the problem byproposing a novel Transformer-based neural network to register 2D images into3D LiDAR map in an end-to-end manner. Poses are implicitly represented ashigh-dimensional feature vectors called pose queries and can be iterativelyupdated by interacting with the retrieved relevant information from cross-modelfeatures using attention mechanism in a proposed POse Estimator Transformer(POET) module. Moreover, we apply a multiple hypotheses aggregation method thatestimates the final poses by performing parallel optimization on multiplerandomly initialized pose queries to reduce the network uncertainty.Comprehensive analysis and experimental results on public benchmark concludethat the proposed image-to-LiDAR map localization network could achievestate-of-the-art performances in challenging cross-modal localization tasks.</description><author>Jinyu Miao, Kun Jiang, Yunlong Wang, Tuopu Wen, Zhongyang Xiao, Zheng Fu, Mengmeng Yang, Maolin Liu, Diange Yang</author><pubDate>Sun, 07 May 2023 15:57:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04298v1</guid></item><item><title>BU-CVKit: Extendable Computer Vision Framework for Species Independent Tracking and Analysis</title><link>http://arxiv.org/abs/2306.04736v1</link><description>A major bottleneck of interdisciplinary computer vision (CV) research is thelack of a framework that eases the reuse and abstraction of state-of-the-art CVmodels by CV and non-CV researchers alike. We present here BU-CVKit, a computervision framework that allows the creation of research pipelines with chainableProcessors. The community can create plugins of their work for the framework,hence improving the re-usability, accessibility, and exposure of their workwith minimal overhead. Furthermore, we provide MuSeqPose Kit, a user interfacefor the pose estimation package of BU-CVKit, which automatically scans forinstalled plugins and programmatically generates an interface for them based onthe metadata provided by the user. It also provides software support forstandard pose estimation features such as annotations, 3D reconstruction,reprojection, and camera calibration. Finally, we show examples of behavioralneuroscience pipelines created through the sample plugins created for ourframework.</description><author>Mahir Patel, Lucas Carstensen, Yiwen Gu, Michael E. Hasselmo, Margrit Betke</author><pubDate>Wed, 07 Jun 2023 20:12:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04736v1</guid></item><item><title>DeepRM: Deep Recurrent Matching for 6D Pose Refinement</title><link>http://arxiv.org/abs/2205.14474v5</link><description>Precise 6D pose estimation of rigid objects from RGB images is a critical butchallenging task in robotics, augmented reality and human-computer interaction.To address this problem, we propose DeepRM, a novel recurrent networkarchitecture for 6D pose refinement. DeepRM leverages initial coarse poseestimates to render synthetic images of target objects. The rendered images arethen matched with the observed images to predict a rigid transform for updatingthe previous pose estimate. This process is repeated to incrementally refinethe estimate at each iteration. The DeepRM architecture incorporates LSTM unitsto propagate information through each refinement step, significantly improvingoverall performance. In contrast to current 2-stage Perspective-n-Point basedsolutions, DeepRM is trained end-to-end, and uses a scalable backbone that canbe tuned via a single parameter for accuracy and efficiency. During training, amulti-scale optical flow head is added to predict the optical flow between theobserved and synthetic images. Optical flow prediction stabilizes the trainingprocess, and enforces the learning of features that are relevant to the task ofpose estimation. Our results demonstrate that DeepRM achieves state-of-the-artperformance on two widely accepted challenging datasets.</description><author>Alexander Avery, Andreas Savakis</author><pubDate>Fri, 16 Jun 2023 21:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.14474v5</guid></item><item><title>Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints</title><link>http://arxiv.org/abs/2303.02885v2</link><description>Learning robust local image feature matching is a fundamental low-levelvision task, which has been widely explored in the past few years. Recently,detector-free local feature matchers based on transformers have shown promisingresults, which largely outperform pure Convolutional Neural Network (CNN) basedones. But correlations produced by transformer-based methods are spatiallylimited to the center of source views' coarse patches, because of the costlyattention learning. In this work, we rethink this issue and find that suchmatching formulation degrades pose estimation, especially for low-resolutionimages. So we propose a transformer-based cascade matching model -- Cascadefeature Matching TRansformer (CasMTR), to efficiently learn dense featurecorrelations, which allows us to choose more reliable matching pairs for therelative pose estimation. Instead of re-training a new detector, we use asimple yet effective Non-Maximum Suppression (NMS) post-process to filterkeypoints through the confidence map, and largely improve the matchingprecision. CasMTR achieves state-of-the-art performance in indoor and outdoorpose estimation as well as visual localization. Moreover, thorough ablationsshow the efficacy of the proposed components and techniques.</description><author>Chenjie Cao, Yanwei Fu</author><pubDate>Tue, 18 Jul 2023 04:35:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02885v2</guid></item><item><title>EgoHumans: An Egocentric 3D Multi-Human Benchmark</title><link>http://arxiv.org/abs/2305.16487v1</link><description>We present EgoHumans, a new multi-view multi-human video benchmark to advancethe state-of-the-art of egocentric human 3D pose estimation and tracking.Existing egocentric benchmarks either capture single subject or indoor-onlyscenarios, which limit the generalization of computer vision algorithms forreal-world applications. We propose a novel 3D capture setup to construct acomprehensive egocentric multi-human benchmark in the wild with annotations tosupport diverse tasks such as human detection, tracking, 2D/3D pose estimation,and mesh recovery. We leverage consumer-grade wearable camera-equipped glassesfor the egocentric view, which enables us to capture dynamic activities likeplaying soccer, fencing, volleyball, etc. Furthermore, our multi-view setupgenerates accurate 3D ground truth even under severe or complete occlusion. Thedataset consists of more than 125k egocentric images, spanning diverse sceneswith a particular focus on challenging and unchoreographed multi-humanactivities and fast-moving egocentric views. We rigorously evaluate existingstate-of-the-art methods and highlight their limitations in the egocentricscenario, specifically on multi-human tracking. To address such limitations, wepropose EgoFormer, a novel approach with a multi-stream transformerarchitecture and explicit 3D spatial reasoning to estimate and track the humanpose. EgoFormer significantly outperforms prior art by 13.6% IDF1 and 9.3 HOTAon the EgoHumans dataset.</description><author>Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, Kris Kitani</author><pubDate>Thu, 25 May 2023 22:37:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.16487v1</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v2</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Fri, 09 Jun 2023 19:10:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v2</guid></item><item><title>BAA-NGP: Bundle-Adjusting Accelerated Neural Graphics Primitives</title><link>http://arxiv.org/abs/2306.04166v1</link><description>Implicit neural representation has emerged as a powerful method forreconstructing 3D scenes from 2D images. Given a set of camera poses andassociated images, the models can be trained to synthesize novel, unseen views.In order to expand the use cases for implicit neural representations, we needto incorporate camera pose estimation capabilities as part of therepresentation learning, as this is necessary for reconstructing scenes fromreal-world video sequences where cameras are generally not being tracked.Existing approaches like COLMAP and, most recently, bundle-adjusting neuralradiance field methods often suffer from lengthy processing times. These delaysranging from hours to days, arise from laborious feature matching, hardwarelimitations, dense point sampling, and long training times required by amulti-layer perceptron structure with a large number of parameters. To addressthese challenges, we propose a framework called bundle-adjusting acceleratedneural graphics primitives (BAA-NGP). Our approach leverages acceleratedsampling and hash encoding to expedite both pose refinement/estimation and 3Dscene reconstruction. Experimental results demonstrate that our method achievesa more than 10 to 20 $\times$ speed improvement in novel view synthesiscompared to other bundle-adjusting neural radiance field methods withoutsacrificing the quality of pose estimation.</description><author>Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey Supikov, Michael Yip</author><pubDate>Wed, 07 Jun 2023 06:36:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04166v1</guid></item><item><title>Perpetual Humanoid Control for Real-time Simulated Avatars</title><link>http://arxiv.org/abs/2305.06456v1</link><description>We present a physics-based humanoid controller that achieves high-fidelitymotion imitation and fault-tolerant behavior in the presence of noisy input(e.g. pose estimates from video or generated from language) and unexpectedfalls. Our controller scales up to learning ten thousand motion clips withoutusing any external stabilizing forces and learns to naturally recover fromfail-state. Given reference motion, our controller can perpetually controlsimulated avatars without requiring resets. At its core, we propose theprogressive multiplicative control policy (PMCP), which dynamically allocatesnew network capacity to learn harder and harder motion sequences. PMCP allowsefficient scaling for learning from large-scale motion databases and adding newtasks, such as fail-state recovery, without catastrophic forgetting. Wedemonstrate the effectiveness of our controller by using it to imitate noisyposes from video-based pose estimators and language-based motion generators ina live and real-time multi-person avatar use case.</description><author>Zhengyi Luo, Jinkun Cao, Alexander Winkler, Kris Kitani, Weipeng Xu</author><pubDate>Wed, 10 May 2023 21:51:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06456v1</guid></item><item><title>Perpetual Humanoid Control for Real-time Simulated Avatars</title><link>http://arxiv.org/abs/2305.06456v2</link><description>We present a physics-based humanoid controller that achieves high-fidelitymotion imitation and fault-tolerant behavior in the presence of noisy input(e.g. pose estimates from video or generated from language) and unexpectedfalls. Our controller scales up to learning ten thousand motion clips withoutusing any external stabilizing forces and learns to naturally recover fromfail-state. Given reference motion, our controller can perpetually controlsimulated avatars without requiring resets. At its core, we propose theprogressive multiplicative control policy (PMCP), which dynamically allocatesnew network capacity to learn harder and harder motion sequences. PMCP allowsefficient scaling for learning from large-scale motion databases and adding newtasks, such as fail-state recovery, without catastrophic forgetting. Wedemonstrate the effectiveness of our controller by using it to imitate noisyposes from video-based pose estimators and language-based motion generators ina live and real-time multi-person avatar use case.</description><author>Zhengyi Luo, Jinkun Cao, Alexander Winkler, Kris Kitani, Weipeng Xu</author><pubDate>Wed, 24 May 2023 23:05:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06456v2</guid></item><item><title>MSQNet: Actor-agnostic Action Recognition with Multi-modal Query</title><link>http://arxiv.org/abs/2307.10763v1</link><description>Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released athttps://github.com/mondalanindya/MSQNet.</description><author>Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</author><pubDate>Thu, 20 Jul 2023 11:53:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10763v1</guid></item><item><title>On Correlated Knowledge Distillation for Monitoring Human Pose with Radios</title><link>http://arxiv.org/abs/2305.14829v1</link><description>In this work, we propose and develop a simple experimental testbed to studythe feasibility of a novel idea by coupling radio frequency (RF) sensingtechnology with Correlated Knowledge Distillation (CKD) theory towardsdesigning lightweight, near real-time and precise human pose monitoringsystems. The proposed CKD framework transfers and fuses pose knowledge from arobust "Teacher" model to a parameterized "Student" model, which can be apromising technique for obtaining accurate yet lightweight pose estimates. Toassure its efficacy, we implemented CKD for distilling logits in our integratedSoftware Defined Radio (SDR)-based experimental setup and investigated theRF-visual signal correlation. Our CKD-RF sensing technique is characterized bytwo modes -- a camera-fed Teacher Class Network (e.g., images, videos) with anSDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD modeltrains a dual multi-branch teacher and student network by distilling and fusingknowledge bases. The resulting CKD models are then subsequently used toidentify the multimodal correlation and teach the student branch in reverse.Instead of simply aggregating their learnings, CKD training comprised multipleparallel transformations with the two domains, i.e., visual images and RFsignals. Once trained, our CKD model can efficiently preserve privacy andutilize the multimodal correlated logits from the two different neural networksfor estimating poses without using visual signals/video frames (by using onlythe RF signals).</description><author>Shiva Raj Pokhrel, Jonathan Kua, Deol Satish, Phil Williams, Arkady Zaslavsky, Seng W. Loke, Jinho Choi</author><pubDate>Wed, 24 May 2023 08:34:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14829v1</guid></item><item><title>On Correlated Knowledge Distillation for Monitoring Human Pose with Radios</title><link>http://arxiv.org/abs/2305.14829v2</link><description>In this work, we propose and develop a simple experimental testbed to studythe feasibility of a novel idea by coupling radio frequency (RF) sensingtechnology with Correlated Knowledge Distillation (CKD) theory towardsdesigning lightweight, near real-time and precise human pose monitoringsystems. The proposed CKD framework transfers and fuses pose knowledge from arobust "Teacher" model to a parameterized "Student" model, which can be apromising technique for obtaining accurate yet lightweight pose estimates. Toassure its efficacy, we implemented CKD for distilling logits in our integratedSoftware Defined Radio (SDR)-based experimental setup and investigated theRF-visual signal correlation. Our CKD-RF sensing technique is characterized bytwo modes -- a camera-fed Teacher Class Network (e.g., images, videos) with anSDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD modeltrains a dual multi-branch teacher and student network by distilling and fusingknowledge bases. The resulting CKD models are then subsequently used toidentify the multimodal correlation and teach the student branch in reverse.Instead of simply aggregating their learnings, CKD training comprised multipleparallel transformations with the two domains, i.e., visual images and RFsignals. Once trained, our CKD model can efficiently preserve privacy andutilize the multimodal correlated logits from the two different neural networksfor estimating poses without using visual signals/video frames (by using onlythe RF signals).</description><author>Shiva Raj Pokhrel, Jonathan Kua, Deol Satish, Phil Williams, Arkady Zaslavsky, Seng W. Loke, Jinho Choi</author><pubDate>Tue, 30 May 2023 14:14:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14829v2</guid></item><item><title>MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose</title><link>http://arxiv.org/abs/2210.07181v2</link><description>We propose a generalizable neural radiance fields - MonoNeRF, that can betrained on large-scale monocular videos of moving in static scenes without anyground-truth annotations of depth and camera poses. MonoNeRF follows anAutoencoder-based architecture, where the encoder estimates the monocular depthand the camera pose, and the decoder constructs a Multiplane NeRFrepresentation based on the depth encoder feature, and renders the input frameswith the estimated camera. The learning is supervised by the reconstructionerror. Once the model is learned, it can be applied to multiple applicationsincluding depth estimation, camera pose estimation, and single-image novel viewsynthesis. More qualitative results are available at:https://oasisyang.github.io/mononerf .</description><author>Yang Fu, Ishan Misra, Xiaolong Wang</author><pubDate>Sun, 04 Jun 2023 08:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.07181v2</guid></item><item><title>TAPE: Temporal Attention-based Probabilistic human pose and shape Estimation</title><link>http://arxiv.org/abs/2305.00181v1</link><description>Reconstructing 3D human pose and shape from monocular videos is awell-studied but challenging problem. Common challenges include occlusions, theinherent ambiguities in the 2D to 3D mapping and the computational complexityof video processing. Existing methods ignore the ambiguities of thereconstruction and provide a single deterministic estimate for the 3D pose. Inorder to address these issues, we present a Temporal Attention basedProbabilistic human pose and shape Estimation method (TAPE) that operates on anRGB video. More specifically, we propose to use a neural network to encodevideo frames to temporal features using an attention-based neural network.Given these features, we output a per-frame but temporally-informed probabilitydistribution for the human pose using Normalizing Flows. We show that TAPEoutperforms state-of-the-art methods in standard benchmarks and serves as aneffective video-based prior for optimization-based human pose and shapeestimation. Code is available at: https: //github.com/nikosvasilik/TAPE</description><author>Nikolaos Vasilikopoulos, Nikos Kolotouros, Aggeliki Tsoli, Antonis Argyros</author><pubDate>Sat, 29 Apr 2023 07:08:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.00181v1</guid></item><item><title>HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</title><link>http://arxiv.org/abs/2305.06968v1</link><description>Monocular 3D human pose and shape estimation is an ill-posed problem sincemultiple 3D solutions can explain a 2D image of a subject. Recent approachespredict a probability distribution over plausible 3D pose and shape parametersconditioned on the image. We show that these approaches exhibit a trade-offbetween three key properties: (i) accuracy - the likelihood of the ground-truth3D solution under the predicted distribution, (ii) sample-input consistency -the extent to which 3D samples from the predicted distribution match thevisible 2D image evidence, and (iii) sample diversity - the range of plausible3D solutions modelled by the predicted distribution. Our method, HuManiFlow,predicts simultaneously accurate, consistent and diverse distributions. We usethe human kinematic tree to factorise full body pose into ancestor-conditionedper-body-part pose distributions in an autoregressive manner. Per-body-partdistributions are implemented using normalising flows that respect the manifoldstructure of SO(3), the Lie group of per-body-part poses. We show thatill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity,and employ only probabilistic training losses. Code is available at:https://github.com/akashsengupta1997/HuManiFlow.</description><author>Akash Sengupta, Ignas Budvytis, Roberto Cipolla</author><pubDate>Thu, 11 May 2023 17:49:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06968v1</guid></item></channel></rss>