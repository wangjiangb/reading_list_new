<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivpose estimation</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 17 Aug 2023 14:00:03 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An Efficient Convex Hull-based Vehicle Pose Estimation Method for 3D LiDAR</title><link>http://arxiv.org/abs/2302.01034v2</link><description>Vehicle pose estimation with LiDAR is essential in the perception technologyof autonomous driving. However, due to incomplete observation measurements andsparsity of the LiDAR point cloud, it is challenging to achieve satisfactorypose extraction based on 3D LiDAR by using the existing pose estimationmethods. In addition, the requirement for real-time performance furtherincreases the difficulty of the pose estimation task. In this paper, weproposed a novel convex hull-based vehicle pose estimation method. Theextracted 3D cluster is reduced to the convex hull, reducing the computationburden and retaining contour information. Then a novel criterion based on theminimum occlusion area is developed for the search-based algorithm, which canachieve accurate pose estimation. This criterion also makes the proposedalgorithm especially suitable for obstacle avoidance. The proposed algorithm isvalidated on the KITTI dataset and a manually labeled dataset acquired at anindustrial park. The results show that our proposed method can achieve betteraccuracy than the state-of-the-art pose estimation method while maintainingreal-time speed.</description><author>Ningning Ding</author><pubDate>Sun, 02 Jul 2023 00:47:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01034v2</guid></item><item><title>Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)</title><link>http://arxiv.org/abs/2308.00214v1</link><description>Many tasks performed in image-guided, mini-invasive, medical procedures canbe cast as pose estimation problems, where an X-ray projection is utilized toreach a target in 3D space. Recent advances in the differentiable rendering ofoptically reflective materials have enabled state-of-the-art performance in RGBcamera view synthesis and pose estimation. Expanding on these prior works, weintroduce new methods for pose estimation of radiolucent objects using X-rayprojections, and we demonstrate the critical role of optimal view synthesis inperforming this task. We first develop an algorithm (DiffDRR) that efficientlycomputes Digitally Reconstructed Radiographs (DRRs) and leverages automaticdifferentiation within TensorFlow. In conjunction with classic CBCTreconstruction algorithms, we perform pose estimation by gradient descent usinga loss function that quantifies the similarity of the DRR synthesized from arandomly initialized pose and the true fluoroscopic image at the target pose.We propose two novel methods for high-fidelity view synthesis, Neural TunedTomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods relyon classic CBCT; NeTT directly optimizes the CBCT densities, while the non-zerovalues of mNeRF are constrained by a 3D mask of the anatomic region segmentedfrom CBCT. We demonstrate that both NeTT and mNeRF distinctly improve poseestimation within our framework. By defining a successful pose estimate to be a3D angle error of less than 3 deg, we find that NeTT and mNeRF can achievesimilar results, both with overall success rates more than 93%. Furthermore, weshow that a NeTT trained for a single subject can generalize to synthesizehigh-fidelity DRRs and ensure robust pose estimations for all other subjects.Therefore, we suggest that NeTT is an attractive option for robust poseestimation using fluoroscopic projections.</description><author>Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Leon Cho, Sameer A. Ansari, Donald R. Cantrell</author><pubDate>Tue, 01 Aug 2023 02:12:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00214v1</guid></item><item><title>TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation</title><link>http://arxiv.org/abs/2307.12400v1</link><description>Transparent objects present multiple distinct challenges to visual perceptionsystems. First, their lack of distinguishing visual features makes transparentobjects harder to detect and localize than opaque objects. Even humans findcertain transparent surfaces with little specular reflection or refraction,like glass doors, difficult to perceive. A second challenge is that depthsensors typically used for opaque object perception cannot obtain accuratedepth measurements on transparent surfaces due to their unique reflectiveproperties. Stemming from these challenges, we observe that transparent objectinstances within the same category, such as cups, look more similar to eachother than to ordinary opaque objects of that same category. Given thisobservation, the present paper explores the possibility of category-leveltransparent object pose estimation rather than instance-level pose estimation.We propose \textit{\textbf{TransNet}}, a two-stage pipeline that estimatescategory-level transparent object pose using localized depth completion andsurface normal estimation. TransNet is evaluated in terms of pose estimationaccuracy on a large-scale transparent object dataset and compared to astate-of-the-art category-level pose estimation approach. Results from thiscomparison demonstrate that TransNet achieves improved pose estimation accuracyon transparent objects. Moreover, we use TransNet to build an autonomoustransparent object manipulation system for robotic pick-and-place and pouringtasks.</description><author>Huijie Zhang, Anthony Opipari, Xiaotong Chen, Jiyue Zhu, Zeren Yu, Odest Chadwicke Jenkins</author><pubDate>Sun, 23 Jul 2023 19:38:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12400v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video</title><link>http://arxiv.org/abs/2307.05853v2</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 14% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively). GitHub:https://github.com/bruceyo/GLA-GCN.</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Sat, 22 Jul 2023 02:30:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v2</guid></item><item><title>Open Challenges for Monocular Single-shot 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2302.11827v2</link><description>Object pose estimation is a non-trivial task that enables roboticmanipulation, bin picking, augmented reality, and scene understanding, to namea few use cases. Monocular object pose estimation gained considerable momentumwith the rise of high-performing deep learning-based solutions and isparticularly interesting for the community since sensors are inexpensive andinference is fast. Prior works establish the comprehensive state of the art fordiverse pose estimation problems. Their broad scopes make it difficult toidentify promising future directions. We narrow down the scope to the problemof single-shot monocular 6D object pose estimation, which is commonly used inrobotics, and thus are able to identify such trends. By reviewing recentpublications in robotics and computer vision, the state of the art isestablished at the union of both fields. Following that, we identify promisingresearch directions in order to help researchers to formulate relevant researchideas and effectively advance the state of the art. Findings include thatmethods are sophisticated enough to overcome the domain shift and thatocclusion handling is a fundamental challenge. We also highlight problems suchas novel object pose estimation and challenging materials handling as centralchallenges to advance robotics.</description><author>Stefan Thalhammer, Peter Hönig, Jean-Baptiste Weibel, Markus Vincze</author><pubDate>Thu, 20 Jul 2023 20:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11827v2</guid></item><item><title>Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training</title><link>http://arxiv.org/abs/2206.11215v4</link><description>We consider a certifiable object pose estimation problem, where -- given apartial point cloud of an object -- the goal is to not only estimate the objectpose, but also to provide a certificate of correctness for the resultingestimate. Our first contribution is a general theory of certification forend-to-end perception models. In particular, we introduce the notion of$\zeta$-correctness, which bounds the distance between an estimate and theground truth. We show that $\zeta$-correctness can be assessed by implementingtwo certificates: (i) a certificate of observable correctness, that asserts ifthe model output is consistent with the input data and prior information, (ii)a certificate of non-degeneracy, that asserts whether the input data issufficient to compute a unique estimate. Our second contribution is to applythis theory and design a new learning-based certifiable pose estimator. Wepropose C-3PO, a semantic-keypoint-based pose estimation model, augmented withthe two certificates, to solve the certifiable pose estimation problem. C-3POalso includes a keypoint corrector, implemented as a differentiableoptimization layer, that can correct large detection errors (e.g. due to thesim-to-real gap). Our third contribution is a novel self-supervised trainingapproach that uses our certificate of observable correctness to provide thesupervisory signal to C-3PO during training. In it, the model trains only onthe observably correct input-output pairs, in each training iteration. Astraining progresses, we see that the observably correct input-output pairsgrow, eventually reaching near 100% in many cases. Our experiments show that(i) standard semantic-keypoint-based methods outperform more recentalternatives, (ii) C-3PO further improves performance and significantlyoutperforms all the baselines, and (iii) C-3PO's certificates are able todiscern correct pose estimates.</description><author>Rajat Talak, Lisa Peng, Luca Carlone</author><pubDate>Fri, 28 Apr 2023 20:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.11215v4</guid></item><item><title>A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training</title><link>http://arxiv.org/abs/2302.06019v2</link><description>Real-world robotics applications demand object pose estimation methods thatwork reliably across a variety of scenarios. Modern learning-based approachesrequire large labeled datasets and tend to perform poorly outside the trainingdomain. Our first contribution is to develop a robust corrector module thatcorrects pose estimates using depth information, thus enabling existing methodsto better generalize to new test domains; the corrector operates on semantickeypoints (but is also applicable to other pose estimators) and is fullydifferentiable. Our second contribution is an ensemble self-training approachthat simultaneously trains multiple pose estimators in a self-supervisedmanner. Our ensemble self-training architecture uses the robust corrector torefine the output of each pose estimator; then, it evaluates the quality of theoutputs using observable correctness certificates; finally, it uses theobservably correct outputs for further training, without requiring externalsupervision. As an additional contribution, we propose small improvements to aregression-based keypoint detection architecture, to enhance its robustness tooutliers; these improvements include a robust pooling scheme and a robustcentroid computation. Experiments on the YCBV and TLESS datasets show theproposed ensemble self-training outperforms fully supervised baselines whilenot requiring 3D annotations on real data.</description><author>Jingnan Shi, Rajat Talak, Dominic Maggio, Luca Carlone</author><pubDate>Thu, 11 May 2023 19:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.06019v2</guid></item><item><title>YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation</title><link>http://arxiv.org/abs/2307.11550v1</link><description>6D object pose estimation is a crucial prerequisite for autonomous robotmanipulation applications. The state-of-the-art models for pose estimation areconvolutional neural network (CNN)-based. Lately, Transformers, an architectureoriginally proposed for natural language processing, is achievingstate-of-the-art results in many computer vision tasks as well. Equipped withthe multi-head self-attention mechanism, Transformers enable simplesingle-stage end-to-end architectures for learning object detection and 6Dobject pose estimation jointly. In this work, we propose YOLOPose (short formfor You Only Look Once Pose estimation), a Transformer-based multi-object 6Dpose estimation method based on keypoint regression and an improved variant ofthe YOLOPose model. In contrast to the standard heatmaps for predictingkeypoints in an image, we directly regress the keypoints. Additionally, weemploy a learnable orientation estimation module to predict the orientationfrom the keypoints. Along with a separate translation estimation module, ourmodel is end-to-end differentiable. Our method is suitable for real-timeapplications and achieves results comparable to state-of-the-art methods. Weanalyze the role of object queries in our architecture and reveal that theobject queries specialize in detecting objects in specific image regions.Furthermore, we quantify the accuracy trade-off of using datasets of smallersizes to train our model.</description><author>Arul Selvam Periyasamy, Arash Amini, Vladimir Tsaturyan, Sven Behnke</author><pubDate>Fri, 21 Jul 2023 13:53:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11550v1</guid></item><item><title>Affine Correspondences between Multi-Camera Systems for Relative Pose Estimation</title><link>http://arxiv.org/abs/2306.12996v1</link><description>We present a novel method to compute the relative pose of multi-camerasystems using two affine correspondences (ACs). Existing solutions to themulti-camera relative pose estimation are either restricted to special cases ofmotion, have too high computational complexity, or require too many pointcorrespondences (PCs). Thus, these solvers impede an efficient or accuraterelative pose estimation when applying RANSAC as a robust estimator. This papershows that the 6DOF relative pose estimation problem using ACs permits afeasible minimal solution, when exploiting the geometric constraints betweenACs and multi-camera systems using a special parameterization. We present aproblem formulation based on two ACs that encompass two common types of ACsacross two views, i.e., inter-camera and intra-camera. Moreover, the frameworkfor generating the minimal solvers can be extended to solve various relativepose estimation problems, e.g., 5DOF relative pose estimation with knownrotation angle prior. Experiments on both virtual and real multi-camera systemsprove that the proposed solvers are more efficient than the state-of-the-artalgorithms, while resulting in a better relative pose accuracy. Source code isavailable at https://github.com/jizhaox/relpose-mcs-depth.</description><author>Banglei Guan, Ji Zhao</author><pubDate>Thu, 22 Jun 2023 16:52:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12996v1</guid></item><item><title>GoalieNet: A Multi-Stage Network for Joint Goalie, Equipment, and Net Pose Estimation in Ice Hockey</title><link>http://arxiv.org/abs/2306.15853v1</link><description>In the field of computer vision-driven ice hockey analytics, one of the mostchallenging and least studied tasks is goalie pose estimation. Unlike generalhuman pose estimation, goalie pose estimation is much more complex as itinvolves not only the detection of keypoints corresponding to the joints of thegoalie concealed under thick padding and mask, but also a large number ofnon-human keypoints corresponding to the large leg pads and gloves worn, thestick, as well as the hockey net. To tackle this challenge, we introduceGoalieNet, a multi-stage deep neural network for jointly estimating the pose ofthe goalie, their equipment, and the net. Experimental results using NHLbenchmark data demonstrate that the proposed GoalieNet can achieve an averageof 84\% accuracy across all keypoints, where 22 out of 29 keypoints aredetected with more than 80\% accuracy. This indicates that such a joint poseestimation approach can be a promising research direction.</description><author>Marjan Shahi, David Clausi, Alexander Wong</author><pubDate>Wed, 28 Jun 2023 02:00:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15853v1</guid></item><item><title>Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video</title><link>http://arxiv.org/abs/2303.08475v2</link><description>Temporal modeling is crucial for multi-frame human pose estimation. Mostexisting methods directly employ optical flow or deformable convolution topredict full-spectrum motion fields, which might incur numerous irrelevantcues, such as a nearby person or background. Without further efforts toexcavate meaningful motion priors, their results are suboptimal, especially incomplicated spatiotemporal interactions. On the other hand, the temporaldifference has the ability to encode representative motion information whichcan potentially be valuable for pose estimation but has not been fullyexploited. In this paper, we present a novel multi-frame human pose estimationframework, which employs temporal differences across frames to model dynamiccontexts and engages mutual information objectively to facilitate useful motioninformation disentanglement. To be specific, we design a multi-stage TemporalDifference Encoder that performs incremental cascaded learning conditioned onmulti-stage feature difference sequences to derive informative motionrepresentation. We further propose a Representation Disentanglement module fromthe mutual information perspective, which can grasp discriminativetask-relevant motion signals by explicitly defining useful and noisyconstituents of the raw motion features and minimizing their mutualinformation. These place us to rank No.1 in the Crowd Pose Estimation inComplex Events Challenge on benchmark dataset HiEve, and achievestate-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018,and PoseTrack21.</description><author>Runyang Feng, Yixing Gao, Xueqing Ma, Tze Ho Elden Tse, Hyung Jin Chang</author><pubDate>Mon, 08 May 2023 14:43:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08475v2</guid></item><item><title>HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning</title><link>http://arxiv.org/abs/2302.00988v2</link><description>Recent advancements in 3D hand pose estimation have shown promising results,but its effectiveness has primarily relied on the availability of large-scaleannotated datasets, the creation of which is a laborious and costly process. Toalleviate the label-hungry limitation, we propose a self-supervised learningframework, HaMuCo, that learns a single-view hand pose estimator frommulti-view pseudo 2D labels. However, one of the main challenges ofself-supervised learning is the presence of noisy labels and the ``groupthink''effect from multiple views. To overcome these issues, we introduce a cross-viewinteraction network that distills the single-view estimator by utilizing thecross-view correlated features and enforcing multi-view consistency to achievecollaborative learning. Both the single-view estimator and the cross-viewinteraction network are trained jointly in an end-to-end manner. Extensiveexperiments show that our method can achieve state-of-the-art performance onmulti-view self-supervised hand pose estimation. Furthermore, the proposedcross-view interaction network can also be applied to hand pose estimation frommulti-view input and outperforms previous methods under the same settings.</description><author>Xiaozheng Zheng, Chao Wen, Zhou Xue, Pengfei Ren, Jingyu Wang</author><pubDate>Tue, 15 Aug 2023 05:51:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.00988v2</guid></item><item><title>Perspective-1-Ellipsoid: Formulation, Analysis and Solutions of the Camera Pose Estimation Problem from One Ellipse-Ellipsoid Correspondence</title><link>http://arxiv.org/abs/2208.12513v3</link><description>In computer vision, camera pose estimation from correspondences between 3Dgeometric entities and their projections into the image has been a widelyinvestigated problem. Although most state-of-the-art methods exploit low-levelprimitives such as points or lines, the emergence of very effective CNN-basedobject detectors in the recent years has paved the way to the use ofhigher-level features carrying semantically meaningful information. Pioneeringworks in that direction have shown that modelling 3D objects by ellipsoids and2D detections by ellipses offers a convenient manner to link 2D and 3D data.However, the mathematical formalism most often used in the related litteraturedoes not enable to easily distinguish ellipsoids and ellipses from otherquadrics and conics, leading to a loss of specificity potentially detrimentalin some developments. Moreover, the linearization process of the projectionequation creates an over-representation of the camera parameters, also possiblycausing an efficiency loss. In this paper, we therefore introduce anellipsoid-specific theoretical framework and demonstrate its beneficialproperties in the context of pose estimation. More precisely, we first showthat the proposed formalism enables to reduce the pose estimation problem to aposition or orientation-only estimation problem in which the remaining unknownscan be derived in closed-form. Then, we demonstrate that it can be furtherreduced to a 1 Degree-of-Freedom (1DoF) problem and provide the analyticalderivations of the pose as a function of that unique scalar unknown. Weillustrate our theoretical considerations by visual examples and include adiscussion on the practical aspects. Finally, we release this paper along withthe corresponding source code in order to contribute towards more efficientresolutions of ellipsoid-related pose estimation problems.</description><author>Vincent Gaudillière, Gilles Simon, Marie-Odile Berger</author><pubDate>Wed, 14 Jun 2023 13:09:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.12513v3</guid></item><item><title>DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16687v2</link><description>Denoising diffusion probabilistic models that were initially proposed forrealistic image generation have recently shown success in various perceptiontasks (e.g., object detection and image segmentation) and are increasinglygaining attention in computer vision. However, extending such models tomulti-frame human pose estimation is non-trivial due to the presence of theadditional temporal dimension in videos. More importantly, learningrepresentations that focus on keypoint regions is crucial for accuratelocalization of human joints. Nevertheless, the adaptation of thediffusion-based methods remains unclear on how to achieve such objective. Inthis paper, we present DiffPose, a novel diffusion architecture that formulatesvideo-based human pose estimation as a conditional heatmap generation problem.First, to better leverage temporal information, we propose SpatioTemporalRepresentation Learner which aggregates visual evidences across frames and usesthe resulting features in each denoising step as a condition. In addition, wepresent a mechanism called Lookup-based MultiScale Feature Interaction thatdetermines the correlations between local joints and global contexts acrossmultiple scales. This mechanism generates delicate representations that focuson keypoint regions. Altogether, by extending diffusion models, we show twounique characteristics from DiffPose on pose estimation task: (i) the abilityto combine multiple sets of pose estimates to improve prediction accuracy,particularly for challenging joints, and (ii) the ability to adjust the numberof iterative steps for feature refinement without retraining the model.DiffPose sets new state-of-the-art results on three benchmarks: PoseTrack2017,PoseTrack2018, and PoseTrack21.</description><author>Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing Ma, Hyung Jin Chang</author><pubDate>Sat, 05 Aug 2023 11:54:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16687v2</guid></item><item><title>KVN: Keypoints Voting Network with Differentiable RANSAC for Stereo Pose Estimation</title><link>http://arxiv.org/abs/2307.11543v1</link><description>Object pose estimation is a fundamental computer vision task exploited inseveral robotics and augmented reality applications. Many establishedapproaches rely on predicting 2D-3D keypoint correspondences using RANSAC(Random sample consensus) and estimating the object pose using the PnP(Perspective-n-Point) algorithm. Being RANSAC non-differentiable,correspondences cannot be directly learned in an end-to-end fashion. In thispaper, we address the stereo image-based object pose estimation problem by (i)introducing a differentiable RANSAC layer into a well-known monocular poseestimation network; (ii) exploiting an uncertainty-driven multi-view PnP solverwhich can fuse information from multiple views. We evaluate our approach on achallenging public stereo object pose estimation dataset, yieldingstate-of-the-art results against other recent approaches. Furthermore, in ourablation study, we show that the differentiable RANSAC layer plays asignificant role in the accuracy of the proposed method. We release with thispaper the open-source implementation of our method.</description><author>Ivano Donadi, Alberto Pretto</author><pubDate>Fri, 21 Jul 2023 13:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11543v1</guid></item><item><title>GenPose: Generative Category-level Object Pose Estimation via Diffusion Models</title><link>http://arxiv.org/abs/2306.10531v2</link><description>Object pose estimation plays a vital role in embodied AI and computer vision,enabling intelligent agents to comprehend and interact with their surroundings.Despite the practicality of category-level pose estimation, current approachesencounter challenges with partially observed point clouds, known as themultihypothesis issue. In this study, we propose a novel solution by reframingcategorylevel object pose estimation as conditional generative modeling,departing from traditional point-to-point regression. Leveraging score-baseddiffusion models, we estimate object poses by sampling candidates from thediffusion model and aggregating them through a two-step process: filtering outoutliers via likelihood estimation and subsequently mean-pooling the remainingcandidates. To avoid the costly integration process when estimating thelikelihood, we introduce an alternative method that trains an energy-basedmodel from the original score-based model, enabling end-to-end likelihoodestimation. Our approach achieves state-of-the-art performance on the REAL275dataset, surpassing 50% and 60% on strict 5d2cm and 5d5cm metrics,respectively. Furthermore, our method demonstrates strong generalizability tonovel categories sharing similar symmetric properties without fine-tuning andcan readily adapt to object pose tracking tasks, yielding comparable results tothe current state-of-the-art baselines.</description><author>Jiyao Zhang, Mingdong Wu, Hao Dong</author><pubDate>Sat, 22 Jul 2023 17:16:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10531v2</guid></item><item><title>SPAC-Net: Synthetic Pose-aware Animal ControlNet for Enhanced Pose Estimation</title><link>http://arxiv.org/abs/2305.17845v1</link><description>Animal pose estimation has become a crucial area of research, but thescarcity of annotated data is a significant challenge in developing accuratemodels. Synthetic data has emerged as a promising alternative, but itfrequently exhibits domain discrepancies with real data. Style transferalgorithms have been proposed to address this issue, but they suffer frominsufficient spatial correspondence, leading to the loss of label information.In this work, we present a new approach called Synthetic Pose-aware AnimalControlNet (SPAC-Net), which incorporates ControlNet into the previouslyproposed Prior-Aware Synthetic animal data generation (PASyn) pipeline. Weleverage the plausible pose data generated by the Variational Auto-Encoder(VAE)-based data generation pipeline as input for the ControlNetHolistically-nested Edge Detection (HED) boundary task model to generatesynthetic data with pose labels that are closer to real data, making itpossible to train a high-precision pose estimation network without the need forreal data. In addition, we propose the Bi-ControlNet structure to separatelydetect the HED boundary of animals and backgrounds, improving the precision andstability of the generated data. Using the SPAC-Net pipeline, we generatesynthetic zebra and rhino images and test them on the AP10K real dataset,demonstrating superior performance compared to using only real images orsynthetic data generated by other methods. Our work demonstrates the potentialfor synthetic data to overcome the challenge of limited annotated data inanimal pose estimation.</description><author>Le Jiang, Sarah Ostadabbas</author><pubDate>Mon, 29 May 2023 02:56:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17845v1</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v2</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Mon, 15 May 2023 16:57:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v2</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v3</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Wed, 17 May 2023 09:48:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v3</guid></item><item><title>A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects</title><link>http://arxiv.org/abs/2305.07348v1</link><description>Estimating the pose of an uncooperative spacecraft is an important computervision problem for enabling the deployment of automatic vision-based systems inorbit, with applications ranging from on-orbit servicing to space debrisremoval. Following the general trend in computer vision, more and more workshave been focusing on leveraging Deep Learning (DL) methods to address thisproblem. However and despite promising research-stage results, major challengespreventing the use of such methods in real-life missions still stand in theway. In particular, the deployment of such computation-intensive algorithms isstill under-investigated, while the performance drop when training on syntheticand testing on real images remains to mitigate. The primary goal of this surveyis to describe the current DL-based methods for spacecraft pose estimation in acomprehensive manner. The secondary goal is to help define the limitationstowards the effective deployment of DL-based spacecraft pose estimationsolutions for reliable autonomous vision-based applications. To this end, thesurvey first summarises the existing algorithms according to two approaches:hybrid modular pipelines and direct end-to-end regression methods. A comparisonof algorithms is presented not only in terms of pose accuracy but also with afocus on network architectures and models' sizes keeping potential deploymentin mind. Then, current monocular spacecraft pose estimation datasets used totrain and test these methods are discussed. The data generation methods:simulators and testbeds, the domain gap and the performance drop betweensynthetically generated and lab/space collected images and the potentialsolutions are also discussed. Finally, the paper presents open researchquestions and future directions in the field, drawing parallels with othercomputer vision applications.</description><author>Leo Pauly, Wassim Rharbaoui, Carl Shneider, Arunkumar Rathinam, Vincent Gaudilliere, Djamila Aouada</author><pubDate>Fri, 12 May 2023 10:52:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07348v1</guid></item><item><title>CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data</title><link>http://arxiv.org/abs/2305.08042v1</link><description>This paper proposes a novel method for estimating the set of plausible posesof a rigid object from a set of points with volumetric information, such aswhether each point is in free space or on the surface of the object. Inparticular, we study how pose can be estimated from force and tactile dataarising from contact. Using data derived from contact is challenging because itis inherently less information-dense than visual data, and thus the poseestimation problem is severely under-constrained when there are few contacts.Rather than attempting to estimate the true pose of the object, which is nottractable without a large number of contacts, we seek to estimate a plausibleset of poses which obey the constraints imposed by the sensor data. Existingmethods struggle to estimate this set because they are either designed forsingle pose estimates or require informative priors to be effective. Ourapproach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL),has three key attributes: 1) It considers volumetric information, which allowsus to account for known free space; 2) It uses a novel differentiablevolumetric cost function to take advantage of powerful gradient-basedoptimization tools; and 3) It uses methods from the Quality Diversity (QD)optimization literature to produce a diverse set of high-quality poses. To ourknowledge, QD methods have not been used previously for pose registration. Wealso show how to update our plausible pose estimates online as more data isgathered by the robot. Our experiments suggest that CHSEL shows largeperformance improvements over several baseline methods for both simulated andreal-world data.</description><author>Sheng Zhong, Nima Fazeli, Dmitry Berenson</author><pubDate>Sun, 14 May 2023 02:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08042v1</guid></item><item><title>EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes</title><link>http://arxiv.org/abs/2308.06493v1</link><description>Full-body ego-pose estimation from head and hand poses alone has become anactive area of research to power articulate avatar representation onheadset-based platforms. However, existing methods over-rely on the confines ofthe motion-capture spaces in which datasets were recorded, while simultaneouslyassuming continuous capture of joint motions and uniform body dimensions. Inthis paper, we propose EgoPoser, which overcomes these limitations by 1)rethinking the input representation for headset-based ego-pose estimation andintroducing a novel motion decomposition method that predicts full-body poseindependent of global positions, 2) robustly modeling body pose fromintermittent hand position and orientation tracking only when inside aheadset's field of view, and 3) generalizing across various body sizes fordifferent users. Our experiments show that EgoPoser outperformsstate-of-the-art methods both qualitatively and quantitatively, whilemaintaining a high inference speed of over 600 fps. EgoPoser establishes arobust baseline for future work, where full-body pose estimation needs nolonger rely on outside-in capture and can scale to large-scene environments.</description><author>Jiaxi Jiang, Paul Streli, Manuel Meier, Andreas Fender, Christian Holz</author><pubDate>Sat, 12 Aug 2023 08:46:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06493v1</guid></item><item><title>Neural Voting Field for Camera-Space 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2305.04328v1</link><description>We present a unified framework for camera-space 3D hand pose estimation froma single RGB image based on 3D implicit representation. As opposed to recentworks, most of which first adopt holistic or pixel-level dense regression toobtain relative 3D hand pose and then follow with complex second-stageoperations for 3D global root or scale recovery, we propose a novel unified 3Ddense regression scheme to estimate camera-space 3D hand pose via dense 3Dpoint-wise voting in camera frustum. Through direct dense modeling in 3D domaininspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction,our proposed Neural Voting Field (NVF) fully models 3D dense local evidence andhand global geometry, helping to alleviate common 2D-to-3D ambiguities.Specifically, for a 3D query point in camera frustum and its pixel-alignedimage feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) itssigned distance to the hand surface; (ii) a set of 4D offset vectors (1D votingweight and 3D directional vector to each hand joint). Following a vote-castingscheme, 4D offset vectors from near-surface points are selected to calculatethe 3D hand joint coordinates by a weighted average. Experiments demonstratethat NVF outperforms existing state-of-the-art algorithms on FreiHAND datasetfor camera-space 3D hand pose estimation. We also adapt NVF to the classic taskof root-relative 3D hand pose estimation, for which NVF also obtainsstate-of-the-art results on HO3D dataset.</description><author>Lin Huang, Chung-Ching Lin, Kevin Lin, Lin Liang, Lijuan Wang, Junsong Yuan, Zicheng Liu</author><pubDate>Sun, 07 May 2023 17:51:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04328v1</guid></item><item><title>Relation-Based Associative Joint Location for Human Pose Estimation in Videos</title><link>http://arxiv.org/abs/2107.03591v3</link><description>Video-based human pose estimation (VHPE) is a vital yet challenging task.While deep learning methods have made significant progress for the VHPE, mostapproaches to this task implicitly model the long-range interaction betweenjoints by enlarging the receptive field of the convolution. Unlike priormethods, we design a lightweight and plug-and-play joint relation extractor(JRE) to model the associative relationship between joints explicitly andautomatically. The JRE takes the pseudo heatmaps of joints as input andcalculates the similarity between pseudo heatmaps. In this way, the JREflexibly learns the relationship between any two joints, allowing it to learnthe rich spatial configuration of human poses. Moreover, the JRE can inferinvisible joints according to the relationship between joints, which isbeneficial for the model to locate occluded joints. Then, combined withtemporal semantic continuity modeling, we propose a Relation-based PoseSemantics Transfer Network (RPSTN) for video-based human pose estimation.Specifically, to capture the temporal dynamics of poses, the pose semanticinformation of the current frame is transferred to the next with a jointrelation guided pose semantics propagator (JRPSP). The proposed model cantransfer the pose semantic features from the non-occluded frame to the occludedframe, making our method robust to the occlusion. Furthermore, the proposed JREmodule is also suitable for image-based human pose estimation. The proposedRPSTN achieves state-of-the-art results on the video-based Penn Action dataset,Sub-JHMDB dataset, and PoseTrack2018 dataset. Moreover, the proposed JREimproves the performance of backbones on the image-based COCO2017 dataset. Codeis available at https://github.com/YHDang/pose-estimation.</description><author>Yonghao Dang, Jianqin Yin, Shaojie Zhang</author><pubDate>Fri, 30 Jun 2023 10:52:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2107.03591v3</guid></item><item><title>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</title><link>http://arxiv.org/abs/2305.15727v1</link><description>Despite the significant progress in six degrees-of-freedom (6DoF) object poseestimation, existing methods have limited applicability in real-world scenariosinvolving embodied agents and downstream 3D vision tasks. These limitationsmainly come from the necessity of 3D models, closed-category detection, and alarge number of densely annotated support views. To mitigate this issue, wepropose a general paradigm for object pose estimation, called Promptable ObjectPose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoFobject pose estimation for any target object in any scene, while only a singlereference is adopted as the support view. To achieve this, POPE leverages thepower of the pre-trained large-scale 2D foundation model, employs a frameworkwith hierarchical feature representation and 3D geometry principles. Moreover,it estimates the relative camera pose between object prompts and the targetobject in new views, enabling both two-view and multi-view 6DoF pose estimationtasks. Comprehensive experimental results demonstrate that POPE exhibitsunrivaled robust performance in zero-shot settings, by achieving a significantreduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMODand OnePose datasets, respectively. We also conduct more challenging testingsin causally captured images (see Figure 1), which further demonstrates therobustness of POPE. Project page can be found withhttps://paulpanwang.github.io/POPE/.</description><author>Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, Hanwen Jiang, Zhangyang Wang</author><pubDate>Thu, 25 May 2023 06:19:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15727v1</guid></item><item><title>Generalizable Pose Estimation Using Implicit Scene Representations</title><link>http://arxiv.org/abs/2305.17252v1</link><description>6-DoF pose estimation is an essential component of robotic manipulationpipelines. However, it usually suffers from a lack of generalization to newinstances and object types. Most widely used methods learn to infer the objectpose in a discriminative setup where the model filters useful information toinfer the exact pose of the object. While such methods offer accurate poses,the model does not store enough information to generalize to new objects. Inthis work, we address the generalization capability of pose estimation usingmodels that contain enough information about the object to render it indifferent poses. We follow the line of work that inverts neural renderers toinfer the pose. We propose i-$\sigma$SRN to maximize the information flowingfrom the input pose to the rendered scene and invert them to infer the posegiven an input image. Specifically, we extend Scene Representation Networks(SRNs) by incorporating a separate network for density estimation and introducea new way of obtaining a weighted scene representation. We investigate severalways of initial pose estimates and losses for the neural renderer. Our finalevaluation shows a significant improvement in inference performance and speedcompared to existing approaches.</description><author>Vaibhav Saxena, Kamal Rahimi Malekshan, Linh Tran, Yotto Koga</author><pubDate>Fri, 26 May 2023 21:42:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.17252v1</guid></item><item><title>Challenges for Monocular 6D Object Pose Estimation in Robotics</title><link>http://arxiv.org/abs/2307.12172v1</link><description>Object pose estimation is a core perception task that enables, for example,object grasping and scene understanding. The widely available, inexpensive andhigh-resolution RGB sensors and CNNs that allow for fast inference based onthis modality make monocular approaches especially well suited for roboticsapplications. We observe that previous surveys on object pose estimationestablish the state of the art for varying modalities, single- and multi-viewsettings, and datasets and metrics that consider a multitude of applications.We argue, however, that those works' broad scope hinders the identification ofopen challenges that are specific to monocular approaches and the derivation ofpromising future challenges for their application in robotics. By providing aunified view on recent publications from both robotics and computer vision, wefind that occlusion handling, novel pose representations, and formalizing andimproving category-level pose estimation are still fundamental challenges thatare highly relevant for robotics. Moreover, to further improve roboticperformance, large object sets, novel objects, refractive materials, anduncertainty estimates are central, largely unsolved open challenges. In orderto address them, ontological reasoning, deformability handling, scene-levelreasoning, realistic datasets, and the ecological footprint of algorithms needto be improved.</description><author>Stefan Thalhammer, Dominik Bauer, Peter Hönig, Jean-Baptiste Weibel, José García-Rodríguez, Markus Vincze</author><pubDate>Sat, 22 Jul 2023 22:36:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.12172v1</guid></item><item><title>Depth-based 6DoF Object Pose Estimation using Swin Transformer</title><link>http://arxiv.org/abs/2303.02133v2</link><description>Accurately estimating the 6D pose of objects is crucial for manyapplications, such as robotic grasping, autonomous driving, and augmentedreality. However, this task becomes more challenging in poor lightingconditions or when dealing with textureless objects. To address this issue,depth images are becoming an increasingly popular choice due to theirinvariance to a scene's appearance and the implicit incorporation of essentialgeometric characteristics. However, fully leveraging depth information toimprove the performance of pose estimation remains a difficult andunder-investigated problem. To tackle this challenge, we propose a novelframework called SwinDePose, that uses only geometric information from depthimages to achieve accurate 6D pose estimation. SwinDePose first calculates theangles between each normal vector defined in a depth image and the threecoordinate axes in the camera coordinate system. The resulting angles are thenformed into an image, which is encoded using Swin Transformer. Additionally, weapply RandLA-Net to learn the representations from point clouds. The resultingimage and point clouds embeddings are concatenated and fed into a semanticsegmentation module and a 3D keypoints localization module. Finally, weestimate 6D poses using a least-square fitting approach based on the targetobject's predicted semantic mask and 3D keypoints. In experiments on theLineMod and Occlusion LineMod datasets, SwinDePose outperforms existingstate-of-the-art methods for 6D object pose estimation using depth images. Thisdemonstrates the effectiveness of our approach and highlights its potential forimproving performance in real-world scenarios. Our code is athttps://github.com/zhujunli1993/SwinDePose.</description><author>Zhujun Li, Ioannis Stamos</author><pubDate>Thu, 27 Apr 2023 19:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.02133v2</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v2</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenxuan Guo, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Sun, 06 Aug 2023 15:47:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v2</guid></item><item><title>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</title><link>http://arxiv.org/abs/2308.00628v1</link><description>3D human pose estimation in outdoor environments has garnered increasingattention recently. However, prevalent 3D human pose datasets pertaining tooutdoor scenes lack diversity, as they predominantly utilize only one type ofmodality (RGB image or pointcloud), and often feature only one individualwithin each scene. This limited scope of dataset infrastructure considerablyhinders the variability of available data. In this article, we proposeHuman-M3, an outdoor multi-modal multi-view multi-person human pose databasewhich includes not only multi-view RGB videos of outdoor scenes but alsocorresponding pointclouds. In order to obtain accurate human poses, we proposean algorithm based on multi-modal data input to generate ground truthannotation. This benefits from robust pointcloud detection and tracking, whichsolves the problem of inaccurate human localization and matching ambiguity thatmay exist in previous multi-view RGB videos in outdoor multi-person scenes, andgenerates reliable ground truth annotations. Evaluation of multiple differentmodalities algorithms has shown that this database is challenging and suitablefor future research. Furthermore, we propose a 3D human pose estimationalgorithm based on multi-modal data input, which demonstrates the advantages ofmulti-modal data input for 3D human pose estimation. Code and data will bereleased on https://github.com/soullessrobot/Human-M3-Dataset.</description><author>Bohao Fan, Siqi Wang, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</author><pubDate>Tue, 01 Aug 2023 16:55:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00628v1</guid></item><item><title>TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement</title><link>http://arxiv.org/abs/2307.05561v1</link><description>As demand for robotics manipulation application increases, accuratevision-based 6D pose estimation becomes essential for autonomous operations.Convolutional Neural Networks (CNNs) based approaches for pose estimation havebeen previously introduced. However, the quest for better performance stillpersists especially for accurate robotics manipulation. This quest extends tothe Agri-robotics domain. In this paper, we propose TransPose, an improvedTransformer-based 6D pose estimation with a depth refinement module. Thearchitecture takes in only an RGB image as input with no additionalsupplementing modalities such as depth or thermal images. The architectureencompasses an innovative lighter depth estimation network that estimates depthfrom an RGB image using feature pyramid with an up-sampling method. Atransformer-based detection network with additional prediction heads isproposed to directly regress the object's centre and predict the 6D pose of thetarget. A novel depth refinement module is then used alongside the predictedcenters, 6D poses and depth patches to refine the accuracy of the estimated 6Dpose. We extensively compared our results with other state-of-the-art methodsand analysed our results for fruit-picking applications. The results weachieved show that our proposed technique outperforms the other methodsavailable in the literature.</description><author>Mahmoud Abdulsalam, Nabil Aouf</author><pubDate>Sun, 09 Jul 2023 18:33:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05561v1</guid></item><item><title>RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose</title><link>http://arxiv.org/abs/2303.07399v2</link><description>Recent studies on 2D pose estimation have achieved excellent performance onpublic benchmarks, yet its application in the industrial community stillsuffers from heavy model parameters and high latency. In order to bridge thisgap, we empirically explore key factors in pose estimation including paradigm,model architecture, training strategy, and deployment, and present ahigh-performance real-time multi-person pose estimation framework, RTMPose,based on MMPose. Our RTMPose-m achieves 75.8% AP on COCO with 90+ FPS on anIntel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-lachieves 67.0% AP on COCO-WholeBody with 130+ FPS. To further evaluateRTMPose's capability in critical real-time applications, we also report theperformance after deploying on the mobile device. Our RTMPose-s achieves 72.2%AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existingopen-source libraries. Code and models are released athttps://github.com/open-mmlab/mmpose/tree/1.x/projects/rtmpose.</description><author>Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, Kai Chen</author><pubDate>Mon, 03 Jul 2023 04:06:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07399v2</guid></item><item><title>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</title><link>http://arxiv.org/abs/2307.11934v1</link><description>Human-centric visual understanding is an important desideratum for effectivehuman-robot interaction. In order to navigate crowded public places, socialrobots must be able to interpret the activity of the surrounding humans. Thispaper addresses one key aspect of human-centric visual understanding,multi-person pose estimation. Achieving good performance on multi-person poseestimation in crowded scenes is difficult due to the challenges of occludedjoints and instance separation. In order to tackle these challenges andovercome the limitations of image features in representing invisible bodyparts, we propose a novel prompt-based pose inference strategy called LAMP(Language Assisted Multi-person Pose estimation). By utilizing the textrepresentations generated by a well-trained language model (CLIP), LAMP canfacilitate the understanding of poses on the instance and joint levels, andlearn more robust visual representations that are less susceptible toocclusion. This paper demonstrates that language-supervised training boosts theperformance of single-stage multi-person pose estimation, and bothinstance-level and joint-level prompts are valuable for training. The code isavailable at https://github.com/shengnanh20/LAMP.</description><author>Shengnan Hu, Ce Zheng, Zixiang Zhou, Chen Chen, Gita Sukthankar</author><pubDate>Sat, 22 Jul 2023 00:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11934v1</guid></item><item><title>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</title><link>http://arxiv.org/abs/2307.11934v2</link><description>Human-centric visual understanding is an important desideratum for effectivehuman-robot interaction. In order to navigate crowded public places, socialrobots must be able to interpret the activity of the surrounding humans. Thispaper addresses one key aspect of human-centric visual understanding,multi-person pose estimation. Achieving good performance on multi-person poseestimation in crowded scenes is difficult due to the challenges of occludedjoints and instance separation. In order to tackle these challenges andovercome the limitations of image features in representing invisible bodyparts, we propose a novel prompt-based pose inference strategy called LAMP(Language Assisted Multi-person Pose estimation). By utilizing the textrepresentations generated by a well-trained language model (CLIP), LAMP canfacilitate the understanding of poses on the instance and joint levels, andlearn more robust visual representations that are less susceptible toocclusion. This paper demonstrates that language-supervised training boosts theperformance of single-stage multi-person pose estimation, and bothinstance-level and joint-level prompts are valuable for training. The code isavailable at https://github.com/shengnanh20/LAMP.</description><author>Shengnan Hu, Ce Zheng, Zixiang Zhou, Chen Chen, Gita Sukthankar</author><pubDate>Wed, 26 Jul 2023 19:08:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11934v2</guid></item><item><title>You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example</title><link>http://arxiv.org/abs/2305.12626v1</link><description>In order to meaningfully interact with the world, robot manipulators must beable to interpret objects they encounter. A critical aspect of thisinterpretation is pose estimation: inferring quantities that describe theposition and orientation of an object in 3D space. Most existing approaches topose estimation make limiting assumptions, often working only for specific,known object instances, or at best generalising to an object category usinglarge pose-labelled datasets. In this work, we present a method for achievingcategory-level pose estimation by inspection of just a single object from adesired category. We show that we can subsequently perform accurate poseestimation for unseen objects from an inspected category, and considerablyoutperform prior work by exploiting multi-view correspondences. We demonstratethat our method runs in real-time, enabling a robot manipulator equipped withan RGBD sensor to perform online 6D pose estimation for novel objects. Finally,we showcase our method in a continual learning setting, with a robot able todetermine whether objects belong to known categories, and if not, use activeperception to produce a one-shot category representation for subsequent poseestimation.</description><author>Walter Goodwin, Ioannis Havoutis, Ingmar Posner</author><pubDate>Mon, 22 May 2023 02:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12626v1</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v2</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Mon, 07 Aug 2023 23:11:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v2</guid></item><item><title>Iterative Graph Filtering Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2307.16074v1</link><description>Graph convolutional networks (GCNs) have proven to be an effective approachfor 3D human pose estimation. By naturally modeling the skeleton structure ofthe human body as a graph, GCNs are able to capture the spatial relationshipsbetween joints and learn an efficient representation of the underlying pose.However, most GCN-based methods use a shared weight matrix, making itchallenging to accurately capture the different and complex relationshipsbetween joints. In this paper, we introduce an iterative graph filteringframework for 3D human pose estimation, which aims to predict the 3D jointpositions given a set of 2D joint locations in images. Our approach builds uponthe idea of iteratively solving graph filtering with Laplacian regularizationvia the Gauss-Seidel iterative method. Motivated by this iterative solution, wedesign a Gauss-Seidel network (GS-Net) architecture, which makes use of weightand adjacency modulation, skip connection, and a pure convolutional block withlayer normalization. Adjacency modulation facilitates the learning of edgesthat go beyond the inherent connections of body joints, resulting in anadjusted graph structure that reflects the human skeleton, while skipconnections help maintain crucial information from the input layer's initialfeatures as the network depth increases. We evaluate our proposed model on twostandard benchmark datasets, and compare it with a comprehensive set of strongbaseline methods for 3D human pose estimation. Our experimental resultsdemonstrate that our approach outperforms the baseline methods on bothdatasets, achieving state-of-the-art performance. Furthermore, we conductablation studies to analyze the contributions of different components of ourmodel architecture and show that the skip connection and adjacency modulationhelp improve the model performance.</description><author>Zaedul Islam, A. Ben Hamza</author><pubDate>Sat, 29 Jul 2023 21:46:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16074v1</guid></item><item><title>GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human</title><link>http://arxiv.org/abs/2307.05853v1</link><description>3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).</description><author>Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen</author><pubDate>Wed, 12 Jul 2023 01:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05853v1</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v2</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Sun, 11 Jun 2023 17:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v2</guid></item><item><title>IMP: Iterative Matching and Pose Estimation with Adaptive Pooling</title><link>http://arxiv.org/abs/2304.14837v1</link><description>Previous methods solve feature matching and pose estimation using a two-stageprocess by first finding matches and then estimating the pose. As they ignorethe geometric relationships between the two tasks, they focus on eitherimproving the quality of matches or filtering potential outliers, leading tolimited efficiency or accuracy. In contrast, we propose an iterative matchingand pose estimation framework (IMP) leveraging the geometric connectionsbetween the two tasks: a few good matches are enough for a roughly accuratepose estimation; a roughly accurate pose can be used to guide the matching byproviding geometric constraints. To this end, we implement a geometry-awarerecurrent attention-based module which jointly outputs sparse matches andcamera poses. Specifically, for each iteration, we first implicitly embedgeometric information into the module via a pose-consistency loss, allowing itto predict geometry-aware matches progressively. Second, we introduce an\textbf{e}fficient IMP, called EIMP, to dynamically discard keypoints withoutpotential matches, avoiding redundant updating and significantly reducing thequadratic time complexity of attention computation in transformers. Experimentson YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that theproposed method outperforms previous approaches in terms of accuracy andefficiency.</description><author>Fei Xue, Ignas Budvytis, Roberto Cipolla</author><pubDate>Fri, 28 Apr 2023 14:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14837v1</guid></item><item><title>HDFormer: High-order Directed Transformer for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.01825v2</link><description>Human pose estimation is a challenging task due to its structured datasequence nature. Existing methods primarily focus on pair-wise interaction ofbody joints, which is insufficient for scenarios involving overlapping jointsand rapidly changing poses. To overcome these issues, we introduce a novelapproach, the High-order Directed Transformer (HDFormer), which leverageshigh-order bone and joint relationships for improved pose estimation.Specifically, HDFormer incorporates both self-attention and high-orderattention to formulate a multi-order attention module. This module facilitatesfirst-order "joint$\leftrightarrow$joint", second-order"bone$\leftrightarrow$joint", and high-order "hyperbone$\leftrightarrow$joint"interactions, effectively addressing issues in complex and occlusion-heavysituations. In addition, modern CNN techniques are integrated into thetransformer-based architecture, balancing the trade-off between performance andefficiency. HDFormer significantly outperforms state-of-the-art (SOTA) modelson Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parametersand significantly lower computational costs. Moreover, HDFormer demonstratesbroad real-world applicability, enabling real-time, accurate 3D poseestimation. The source code is in https://github.com/hyer/HDFormer</description><author>Hanyuan Chen, Jun-Yan He, Wangmeng Xiang, Zhi-Qi Cheng, Wei Liu, Hanbing Liu, Bin Luo, Yifeng Geng, Xuansong Xie</author><pubDate>Mon, 22 May 2023 07:32:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.01825v2</guid></item><item><title>Regular Splitting Graph Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2305.05785v1</link><description>In human pose estimation methods based on graph convolutional architectures,the human skeleton is usually modeled as an undirected graph whose nodes arebody joints and edges are connections between neighboring joints. However, mostof these methods tend to focus on learning relationships between body joints ofthe skeleton using first-order neighbors, ignoring higher-order neighbors andhence limiting their ability to exploit relationships between distant joints.In this paper, we introduce a higher-order regular splitting graph network(RS-Net) for 2D-to-3D human pose estimation using matrix splitting inconjunction with weight and adjacency modulation. The core idea is to capturelong-range dependencies between body joints using multi-hop neighborhoods andalso to learn different modulation vectors for different body joints as well asa modulation matrix added to the adjacency matrix associated to the skeleton.This learnable modulation matrix helps adjust the graph structure by addingextra graph edges in an effort to learn additional connections between bodyjoints. Instead of using a shared weight matrix for all neighboring bodyjoints, the proposed RS-Net model applies weight unsharing before aggregatingthe feature vectors associated to the joints in order to capture the differentrelations between them. Experiments and ablations studies performed on twobenchmark datasets demonstrate the effectiveness of our model, achievingsuperior performance over recent state-of-the-art methods for 3D human poseestimation.</description><author>Tanvir Hassan, A. Ben Hamza</author><pubDate>Tue, 09 May 2023 23:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05785v1</guid></item><item><title>EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors</title><link>http://arxiv.org/abs/2306.09615v1</link><description>Transformer is popular in recent 3D human pose estimation, which utilizeslong-term modeling to lift 2D keypoints into the 3D space. However, currenttransformer-based methods do not fully exploit the prior knowledge of the humanskeleton provided by the kinematic structure. In this paper, we propose a noveltransformer-based model EvoPose to introduce the human body prior knowledge for3D human pose estimation effectively. Specifically, a Structural PriorsRepresentation (SPR) module represents human priors as structural featurescarrying rich body patterns, e.g. joint relationships. The structural featuresare interacted with 2D pose sequences and help the model to achieve moreinformative spatiotemporal features. Moreover, a Recursive Refinement (RR)module is applied to refine the 3D pose outputs by utilizing estimated resultsand further injects human priors simultaneously. Extensive experimentsdemonstrate the effectiveness of EvoPose which achieves a new state of the arton two most popular benchmarks, Human3.6M and MPI-INF-3DHP.</description><author>Yaqi Zhang, Yan Lu, Bin Liu, Zhiwei Zhao, Qi Chu, Nenghai Yu</author><pubDate>Fri, 16 Jun 2023 05:09:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09615v1</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v1</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Tue, 02 May 2023 03:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v1</guid></item><item><title>Hybrid model for Single-Stage Multi-Person Pose Estimation</title><link>http://arxiv.org/abs/2305.01167v2</link><description>In general, human pose estimation methods are categorized into two approachesaccording to their architectures: regression (i.e., heatmap-free) andheatmap-based methods. The former one directly estimates precise coordinates ofeach keypoint using convolutional and fully-connected layers. Although thisapproach is able to detect overlapped and dense keypoints, unexpected resultscan be obtained by non-existent keypoints in a scene. On the other hand, thelatter one is able to filter the non-existent ones out by utilizing predictedheatmaps for each keypoint. Nevertheless, it suffers from quantization errorwhen obtaining the keypoint coordinates from its heatmaps. In addition, unlikethe regression one, it is difficult to distinguish densely placed keypoints inan image. To this end, we propose a hybrid model for single-stage multi-personpose estimation, named HybridPose, which mutually overcomes each drawback ofboth approaches by maximizing their strengths. Furthermore, we introduceself-correlation loss to inject spatial dependencies between keypointcoordinates and their visibility. Therefore, HybridPose is capable of not onlydetecting densely placed keypoints, but also filtering the non-existentkeypoints in an image. Experimental results demonstrate that proposedHybridPose exhibits the keypoints visibility without performance degradation interms of the pose estimation accuracy.</description><author>Jonghyun Kim, Bosang Kim, Hyotae Lee, Jungpyo Kim, Wonhyeok Im, Lanying Jin, Dowoo Kwon, Jungho Lee</author><pubDate>Mon, 19 Jun 2023 01:58:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01167v2</guid></item><item><title>Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects</title><link>http://arxiv.org/abs/2306.15858v1</link><description>Robotic manipulation, in particular in-hand object manipulation, oftenrequires an accurate estimate of the object's 6D pose. To improve the accuracyof the estimated pose, state-of-the-art approaches in 6D object pose estimationuse observational data from one or more modalities, e.g., RGB images, depth,and tactile readings. However, existing approaches make limited use of theunderlying geometric structure of the object captured by these modalities,thereby, increasing their reliance on visual features. This results in poorperformance when presented with objects that lack such visual features or whenvisual features are simply occluded. Furthermore, current approaches do nottake advantage of the proprioceptive information embedded in the position ofthe fingers. To address these limitations, in this paper: (1) we introduce ahierarchical graph neural network architecture for combining multimodal (visionand touch) data that allows for a geometrically informed 6D object poseestimation, (2) we introduce a hierarchical message passing operation thatflows the information within and across modalities to learn a graph-basedobject representation, and (3) we introduce a method that accounts for theproprioceptive information for in-hand object representation. We evaluate ourmodel on a diverse subset of objects from the YCB Object and Model Set, andshow that our method substantially outperforms existing state-of-the-art workin accuracy and robustness to occlusion. We also deploy our proposed frameworkon a real robot and qualitatively demonstrate successful transfer to realsettings.</description><author>Alireza Rezazadeh, Snehal Dikhale, Soshi Iba, Nawid Jamali</author><pubDate>Wed, 28 Jun 2023 02:18:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15858v1</guid></item><item><title>CheckerPose: Progressive Dense Keypoint Localization for Object Pose Estimation with Graph Neural Network</title><link>http://arxiv.org/abs/2303.16874v2</link><description>Estimating the 6-DoF pose of a rigid object from a single RGB image is acrucial yet challenging task. Recent studies have shown the great potential ofdense correspondence-based solutions, yet improvements are still needed toreach practical deployment. In this paper, we propose a novel pose estimationalgorithm named CheckerPose, which improves on three main aspects. Firstly,CheckerPose densely samples 3D keypoints from the surface of the 3D object andfinds their 2D correspondences progressively in the 2D image. Compared toprevious solutions that conduct dense sampling in the image space, our strategyenables the correspondence searching in a 2D grid (i.e., pixel coordinate).Secondly, for our 3D-to-2D correspondence, we design a compact binary coderepresentation for 2D image locations. This representation not only allows forprogressive correspondence refinement but also converts the correspondenceregression to a more efficient classification problem. Thirdly, we adopt agraph neural network to explicitly model the interactions among the sampled 3Dkeypoints, further boosting the reliability and accuracy of thecorrespondences. Together, these novel components make CheckerPose a strongpose estimation algorithm. When evaluated on the popular Linemod, Linemod-O,and YCB-V object pose estimation benchmarks, CheckerPose clearly boosts theaccuracy of correspondence-based methods and achieves state-of-the-artperformances. Code is available at https://github.com/RuyiLian/CheckerPose.</description><author>Ruyi Lian, Haibin Ling</author><pubDate>Sun, 13 Aug 2023 21:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16874v2</guid></item><item><title>AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</title><link>http://arxiv.org/abs/2304.12301v1</link><description>We present AssemblyHands, a large-scale benchmark dataset with accurate 3Dhand pose annotations, to facilitate the study of egocentric activities withchallenging hand-object interactions. The dataset includes synchronizedegocentric and exocentric images sampled from the recent Assembly101 dataset,in which participants assemble and disassemble take-apart toys. To obtainhigh-quality 3D hand pose annotations for the egocentric images, we develop anefficient pipeline, where we use an initial set of manual annotations to traina model to automatically annotate a much larger dataset. Our annotation modeluses multi-view feature fusion and an iterative refinement scheme, and achievesan average keypoint error of 4.20 mm, which is 85% lower than the error of theoriginal annotations in Assembly101. AssemblyHands provides 3.0M annotatedimages, including 490K egocentric images, making it the largest existingbenchmark dataset for egocentric 3D hand pose estimation. Using this data, wedevelop a strong single-view baseline of 3D hand pose estimation fromegocentric images. Furthermore, we design a novel action classification task toevaluate predicted 3D hand poses. Our study shows that having higher-qualityhand poses directly improves the ability to recognize actions.</description><author>Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Luan Tran, Cem Keskin</author><pubDate>Mon, 24 Apr 2023 18:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12301v1</guid></item><item><title>Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation</title><link>http://arxiv.org/abs/2308.07313v1</link><description>In this paper, we study the problem of end-to-end multi-person poseestimation. State-of-the-art solutions adopt the DETR-like framework, andmainly develop the complex decoder, e.g., regarding pose estimation as keypointbox detection and combining with human detection in ED-Pose, hierarchicallypredicting with pose decoder and joint (keypoint) decoder in PETR. We present asimple yet effective transformer approach, named Group Pose. We simply regard$K$-keypoint pose estimation as predicting a set of $N\times K$ keypointpositions, each from a keypoint query, as well as representing each pose withan instance query for scoring $N$ pose predictions. Motivated by the intuitionthat the interaction, among across-instance queries of different types, is notdirectly helpful, we make a simple modification to decoder self-attention. Wereplace single self-attention over all the $N\times(K+1)$ queries with twosubsequent group self-attentions: (i) $N$ within-instance self-attention, witheach over $K$ keypoint queries and one instance query, and (ii) $(K+1)$same-type across-instance self-attention, each over $N$ queries of the sametype. The resulting decoder removes the interaction among across-instancetype-different queries, easing the optimization and thus improving theperformance. Experimental results on MS COCO and CrowdPose show that ourapproach without human box supervision is superior to previous methods withcomplex decoders, and even is slightly better than ED-Pose that uses human boxsupervision. $\href{https://github.com/Michel-liu/GroupPose-Paddle}{\rmPaddle}$ and $\href{https://github.com/Michel-liu/GroupPose}{\rm PyTorch}$ codeare available.</description><author>Huan Liu, Qiang Chen, Zichang Tan, Jiang-Jiang Liu, Jian Wang, Xiangbo Su, Xiaolong Li, Kun Yao, Junyu Han, Errui Ding, Yao Zhao, Jingdong Wang</author><pubDate>Mon, 14 Aug 2023 18:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07313v1</guid></item><item><title>Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation</title><link>http://arxiv.org/abs/2308.05438v1</link><description>One critical challenge in 6D object pose estimation from a single RGBD imageis efficient integration of two different modalities, i.e., color and depth. Inthis work, we tackle this problem by a novel Deep Fusion Transformer~(DFTr)block that can aggregate cross-modality features for improving pose estimation.Unlike existing fusion methods, the proposed DFTr can better modelcross-modality semantic correlation by leveraging their semantic similarity,such that globally enhanced features from different modalities can be betterintegrated for improved information extraction. Moreover, to further improverobustness and efficiency, we introduce a novel weighted vector-wise votingalgorithm that employs a non-iterative global optimization strategy for precise3D keypoint localization while achieving near real-time inference. Extensiveexperiments show the effectiveness and strong generalization capability of ourproposed 3D keypoint voting algorithm. Results on four widely used benchmarksalso demonstrate that our method outperforms the state-of-the-art methods bylarge margins.</description><author>Jun Zhou, Kai Chen, Linlin Xu, Qi Dou, Jing Qin</author><pubDate>Thu, 10 Aug 2023 09:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05438v1</guid></item><item><title>Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)</title><link>http://arxiv.org/abs/2305.15873v1</link><description>Addressing accuracy limitations and pose ambiguity in 6D object poseestimation from single RGB images presents a significant challenge,particularly due to object symmetries or occlusions. In response, we introducea novel score-based diffusion method applied to the $SE(3)$ group, marking thefirst application of diffusion models to $SE(3)$ within the image domain,specifically tailored for pose estimation tasks. Extensive evaluationsdemonstrate the method's efficacy in handling pose ambiguity, mitigatingperspective-induced ambiguity, and showcasing the robustness of our surrogateStein score formulation on $SE(3)$. This formulation not only improves theconvergence of Langevin dynamics but also enhances computational efficiency.Thus, we pioneer a promising strategy for 6D object pose estimation.</description><author>Tsu-Ching Hsiao, Hao-Wei Chen, Hsuan-Kung Yang, Chun-Yi Lee</author><pubDate>Thu, 25 May 2023 10:09:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15873v1</guid></item><item><title>Graph-CoVis: GNN-based Multi-view Panorama Global Pose Estimation</title><link>http://arxiv.org/abs/2304.13201v1</link><description>In this paper, we address the problem of wide-baseline camera pose estimationfrom a group of 360$^\circ$ panoramas under upright-camera assumption. Recentwork has demonstrated the merit of deep-learning for end-to-end direct relativepose regression in 360$^\circ$ panorama pairs [11]. To exploit the benefits ofmulti-view logic in a learning-based framework, we introduce Graph-CoVis, whichnon-trivially extends CoVisPose [11] from relative two-view to globalmulti-view spherical camera pose estimation. Graph-CoVis is a novel GraphNeural Network based architecture that jointly learns the co-visible structureand global motion in an end-to-end and fully-supervised approach. Using theZInD [4] dataset, which features real homes presenting wide-baselines,occlusion, and limited visual overlap, we show that our model performscompetitively to state-of-the-art approaches.</description><author>Negar Nejatishahidin, Will Hutchcroft, Manjunath Narayana, Ivaylo Boyadzhiev, Yuguang Li, Naji Khosravan, Jana Kosecka, Sing Bing Kang</author><pubDate>Wed, 26 Apr 2023 01:04:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13201v1</guid></item><item><title>MDPose: Real-Time Multi-Person Pose Estimation via Mixture Density Model</title><link>http://arxiv.org/abs/2302.08751v2</link><description>One of the major challenges in multi-person pose estimation is instance-awarekeypoint estimation. Previous methods address this problem by leveraging anoff-the-shelf detector, heuristic post-grouping process or explicit instanceidentification process, hindering further improvements in the inference speedwhich is an important factor for practical applications. From the statisticalpoint of view, those additional processes for identifying instances arenecessary to bypass learning the high-dimensional joint distribution of humankeypoints, which is a critical factor for another major challenge, theocclusion scenario. In this work, we propose a novel framework of single-stageinstance-aware pose estimation by modeling the joint distribution of humankeypoints with a mixture density model, termed as MDPose. Our MDPose estimatesthe distribution of human keypoints' coordinates using a mixture density modelwith an instance-aware keypoint head consisting simply of 8 convolutionallayers. It is trained by minimizing the negative log-likelihood of the groundtruth keypoints. Also, we propose a simple yet effective training strategy,Random Keypoint Grouping (RKG), which significantly alleviates the underflowproblem leading to successful learning of relations between keypoints. OnOCHuman dataset, which consists of images with highly occluded people, ourMDPose achieves state-of-the-art performance by successfully learning thehigh-dimensional joint distribution of human keypoints. Furthermore, our MDPoseshows significant improvement in inference speed with a competitive accuracy onMS COCO, a widely-used human keypoint dataset, thanks to the proposed muchsimpler single-stage pipeline.</description><author>Seunghyeon Seo, Jaeyoung Yoo, Jihye Hwang, Nojun Kwak</author><pubDate>Mon, 08 May 2023 13:22:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.08751v2</guid></item><item><title>POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities</title><link>http://arxiv.org/abs/2307.10387v1</link><description>The surgical usage of Mixed Reality (MR) has received growing attention inareas such as surgical navigation systems, skill assessment, and robot-assistedsurgeries. For such applications, pose estimation for hand and surgicalinstruments from an egocentric perspective is a fundamental task and has beenstudied extensively in the computer vision field in recent years. However, thedevelopment of this field has been impeded by a lack of datasets, especially inthe surgical field, where bloody gloves and reflective metallic tools make ithard to obtain 3D pose annotations for hands and objects using conventionalmethods. To address this issue, we propose POV-Surgery, a large-scale,synthetic, egocentric dataset focusing on pose estimation for hands withdifferent surgical gloves and three orthopedic surgical instruments, namelyscalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329frames, featuring high-resolution RGB-D video streams with activityannotations, accurate 3D and 2D annotations for hand-object pose, and 2Dhand-object segmentation masks. We fine-tune the current SOTA methods onPOV-Surgery and further show the generalizability when applying to real-lifecases with surgical gloves and tools by extensive evaluations. The code and thedataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.</description><author>Rui Wang, Sophokles Ktistakis, Siwei Zhang, Mirko Meboldt, Quentin Lohmeyer</author><pubDate>Wed, 19 Jul 2023 19:00:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10387v1</guid></item><item><title>Self-supervised Optimization of Hand Pose Estimation using Anatomical Features and Iterative Learning</title><link>http://arxiv.org/abs/2307.03007v1</link><description>Manual assembly workers face increasing complexity in their work.Human-centered assistance systems could help, but object recognition as anenabling technology hinders sophisticated human-centered design of thesesystems. At the same time, activity recognition based on hand poses suffersfrom poor pose estimation in complex usage scenarios, such as wearing gloves.This paper presents a self-supervised pipeline for adapting hand poseestimation to specific use cases with minimal human interaction. This enablescheap and robust hand posebased activity recognition. The pipeline consists ofa general machine learning model for hand pose estimation trained on ageneralized dataset, spatial and temporal filtering to account for anatomicalconstraints of the hand, and a retraining step to improve the model. Differentparameter combinations are evaluated on a publicly available and annotateddataset. The best parameter and model combination is then applied to unlabelledvideos from a manual assembly scenario. The effectiveness of the pipeline isdemonstrated by training an activity recognition as a downstream task in themanual assembly scenario.</description><author>Christian Jauch, Timo Leitritz, Marco F. Huber</author><pubDate>Thu, 06 Jul 2023 15:13:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.03007v1</guid></item><item><title>ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation</title><link>http://arxiv.org/abs/2305.01618v1</link><description>We propose a new dataset and a novel approach to learning hand-objectinteraction priors for hand and articulated object pose estimation. We firstcollect a dataset using visual teleoperation, where the human operator candirectly play within a physical simulator to manipulate the articulatedobjects. We record the data and obtain free and accurate annotations on objectposes and contact information from the simulator. Our system only requires aniPhone to record human hand motion, which can be easily scaled up and largelylower the costs of data and annotation collection. With this data, we learn 3Dinteraction priors including a discriminator (in a GAN) capturing thedistribution of how object parts are arranged, and a diffusion model whichgenerates the contact regions on articulated objects, guiding the hand poseestimation. Such structural and contact priors can easily transfer toreal-world data with barely any domain gap. By using our data and learnedpriors, our method significantly improves the performance on joint hand andarticulated object poses estimation over the existing state-of-the-art methods.The project is available at https://zehaozhu.github.io/ContactArt/ .</description><author>Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang</author><pubDate>Tue, 02 May 2023 18:24:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01618v1</guid></item><item><title>Vision-based Target Pose Estimation with Multiple Markers for the Perching of UAVs</title><link>http://arxiv.org/abs/2304.14838v1</link><description>Autonomous Nano Aerial Vehicles have been increasingly popular insurveillance and monitoring operations due to their efficiency andmaneuverability. Once a target location has been reached, drones do not have toremain active during the mission. It is possible for the vehicle to perch andstop its motors in such situations to conserve energy, as well as maintain astatic position in unfavorable flying conditions. In the perching targetestimation phase, the steady and accuracy of a visual camera with markers is asignificant challenge. It is rapidly detectable from afar when using a largemarker, but when the drone approaches, it quickly disappears as out of cameraview. In this paper, a vision-based target poses estimation method usingmultiple markers is proposed to deal with the above-mentioned problems. First,a perching target with a small marker inside a larger one is designed toimprove detection capability at wide and close ranges. Second, the relativeposes of the flying vehicle are calculated from detected markers using amonocular camera. Next, a Kalman filter is applied to provide a more stable andreliable pose estimation, especially when the measurement data is missing dueto unexpected reasons. Finally, we introduced an algorithm for merging theposes data from multi markers. The poses are then sent to the positioncontroller to align the drone and the marker's center and steer it to perch onthe target. The experimental results demonstrated the effectiveness andfeasibility of the adopted approach. The drone can perch successfully onto thecenter of the markers with the attached 25mm-diameter rounded magnet.</description><author>Truong-Dong Do, Nguyen Xuan-Mung, Sung-Kyung Hong</author><pubDate>Tue, 25 Apr 2023 17:51:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14838v1</guid></item><item><title>Human Body Pose Estimation for Gait Identification: A Comprehensive Survey of Datasets and Models</title><link>http://arxiv.org/abs/2305.13765v1</link><description>Person identification is a problem that has received substantial attention,particularly in security domains. Gait recognition is one of the mostconvenient approaches enabling person identification at a distance without theneed of high-quality images. There are several review studies addressing personidentification such as the utilization of facial images, silhouette images, andwearable sensor. Despite skeleton-based person identification gainingpopularity while overcoming the challenges of traditional approaches, existingsurvey studies lack the comprehensive review of skeleton-based approaches togait identification. We present a detailed review of the human pose estimationand gait analysis that make the skeleton-based approaches possible. The studycovers various types of related datasets, tools, methodologies, and evaluationmetrics with associated challenges, limitations, and application domains.Detailed comparisons are presented for each of these aspects withrecommendations for potential research and alternatives. A common trendthroughout this paper is the positive impact that deep learning techniques arebeginning to have on topics such as human pose estimation and gaitidentification. The survey outcomes might be useful for the related researchcommunity and other stakeholders in terms of performance analysis of existingmethodologies, potential research gaps, application domains, and possiblecontributions in the future.</description><author>Luke K. Topham, Wasiq Khan, Dhiya Al-Jumeily, Abir Hussain</author><pubDate>Tue, 23 May 2023 08:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13765v1</guid></item><item><title>DTF-Net: Category-Level Pose Estimation and Shape Reconstruction via Deformable Template Field</title><link>http://arxiv.org/abs/2308.02239v1</link><description>Estimating 6D poses and reconstructing 3D shapes of objects in open-worldscenes from RGB-depth image pairs is challenging. Many existing methods rely onlearning geometric features that correspond to specific templates whiledisregarding shape variations and pose differences among objects in the samecategory. As a result, these methods underperform when handling unseen objectinstances in complex environments. In contrast, other approaches aim to achievecategory-level estimation and reconstruction by leveraging normalized geometricstructure priors, but the static prior-based reconstruction struggles withsubstantial intra-class variations. To solve these problems, we propose theDTF-Net, a novel framework for pose estimation and shape reconstruction basedon implicit neural fields of object categories. In DTF-Net, we design adeformable template field to represent the general category-wise shape latentfeatures and intra-category geometric deformation features. The fieldestablishes continuous shape correspondences, deforming the category templateinto arbitrary observed instances to accomplish shape reconstruction. Weintroduce a pose regression module that shares the deformation features andtemplate codes from the fields to estimate the accurate 6D pose of each objectin the scene. We integrate a multi-modal representation extraction module toextract object features and semantic masks, enabling end-to-end inference.Moreover, during training, we implement a shape-invariant training strategy anda viewpoint sampling method to further enhance the model's capability toextract object pose features. Extensive experiments on the REAL275 and CAMERA25datasets demonstrate the superiority of DTF-Net in both synthetic and realscenes. Furthermore, we show that DTF-Net effectively supports grasping taskswith a real robot arm.</description><author>Haowen Wang, Zhipeng Fan, Zhen Zhao, Zhengping Che, Zhiyuan Xu, Dong Liu, Feifei Feng, Yakun Huang, Xiuquan Qiao, Jian Tang</author><pubDate>Fri, 04 Aug 2023 11:35:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02239v1</guid></item><item><title>EgoCOL: Egocentric Camera pose estimation for Open-world 3D object Localization @Ego4D challenge 2023</title><link>http://arxiv.org/abs/2306.16606v1</link><description>We present EgoCOL, an egocentric camera pose estimation method for open-world3D object localization. Our method leverages sparse camera pose reconstructionsin a two-fold manner, video and scan independently, to estimate the camera poseof egocentric frames in 3D renders with high recall and precision. Weextensively evaluate our method on the Visual Query (VQ) 3D object localizationEgo4D benchmark. EgoCOL can estimate 62% and 59% more camera poses than theEgo4D baseline in the Ego4D Visual Queries 3D Localization challenge at CVPR2023 in the val and test sets, respectively. Our code is publicly available athttps://github.com/BCV-Uniandes/EgoCOL</description><author>Cristhian Forigua, Maria Escobar, Jordi Pont-Tuset, Kevis-Kokitsi Maninis, Pablo Arbeláez</author><pubDate>Thu, 29 Jun 2023 01:17:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16606v1</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v1</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE,which aims to address the challenges of cross-domain learning of HPE withoutaccess to source data during the adaptation process. We further propose a novelframework that consists of three models: source model, intermediate model, andtarget model, which explores the task from both source-protect andtarget-relevant perspectives. The source-protect module preserves sourceinformation more effectively while resisting noise, and the target-relevantmodule reduces the sparsity of spatial representations by building a novelspatial probability space, and pose-specific contrastive learning andinformation maximization are proposed on the basis of this space. Comprehensiveexperiments on several domain adaptive HPE benchmarks show that the proposedmethod outperforms existing approaches by a considerable margin.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Sun, 06 Aug 2023 21:19:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v1</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v2</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task,named source-free domain adaptive HPE, which aims to address the challenges ofcross-domain learning of HPE without access to source data during theadaptation process. We further propose a novel framework that consists of threemodels: source model, intermediate model, and target model, which explores thetask from both source-protect and target-relevant perspectives. Thesource-protect module preserves source information more effectively whileresisting noise, and the target-relevant module reduces the sparsity of spatialrepresentations by building a novel spatial probability space, andpose-specific contrastive learning and information maximization are proposed onthe basis of this space. Comprehensive experiments on several domain adaptiveHPE benchmarks show that the proposed method outperforms existing approaches bya considerable margin.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Mon, 14 Aug 2023 17:33:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v2</guid></item><item><title>Source-free Domain Adaptive Human Pose Estimation</title><link>http://arxiv.org/abs/2308.03202v3</link><description>Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task,named source-free domain adaptive HPE, which aims to address the challenges ofcross-domain learning of HPE without access to source data during theadaptation process. We further propose a novel framework that consists of threemodels: source model, intermediate model, and target model, which explores thetask from both source-protect and target-relevant perspectives. Thesource-protect module preserves source information more effectively whileresisting noise, and the target-relevant module reduces the sparsity of spatialrepresentations by building a novel spatial probability space, andpose-specific contrastive learning and information maximization are proposed onthe basis of this space. Comprehensive experiments on several domain adaptiveHPE benchmarks show that the proposed method outperforms existing approaches bya considerable margin. The codes are available athttps://github.com/davidpengucf/SFDAHPE.</description><author>Qucheng Peng, Ce Zheng, Chen Chen</author><pubDate>Tue, 15 Aug 2023 16:47:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03202v3</guid></item><item><title>Of Mice and Pose: 2D Mouse Pose Estimation from Unlabelled Data and Synthetic Prior</title><link>http://arxiv.org/abs/2307.13361v1</link><description>Numerous fields, such as ecology, biology, and neuroscience, use animalrecordings to track and measure animal behaviour. Over time, a significantvolume of such data has been produced, but some computer vision techniquescannot explore it due to the lack of annotations. To address this, we proposean approach for estimating 2D mouse body pose from unlabelled images using asynthetically generated empirical pose prior. Our proposal is based on a recentself-supervised method for estimating 2D human pose that uses single images anda set of unpaired typical 2D poses within a GAN framework. We adapt this methodto the limb structure of the mouse and generate the empirical prior of 2D posesfrom a synthetic 3D mouse model, thereby avoiding manual annotation. Inexperiments on a new mouse video dataset, we evaluate the performance of theapproach by comparing pose predictions to a manually obtained ground truth. Wealso compare predictions with those from a supervised state-of-the-art methodfor animal pose estimation. The latter evaluation indicates promising resultsdespite the lack of paired training data. Finally, qualitative results using adataset of horse images show the potential of the setting to adapt to otheranimal species.</description><author>Jose Sosa, Sharn Perry, Jane Alty, David Hogg</author><pubDate>Tue, 25 Jul 2023 10:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13361v1</guid></item><item><title>MELON: NeRF with Unposed Images in SO(3)</title><link>http://arxiv.org/abs/2303.08096v2</link><description>Neural radiance fields enable novel-view synthesis and scene reconstructionwith photorealistic quality from a few images, but require known and accuratecamera poses. Conventional pose estimation algorithms fail on smooth orself-similar scenes, while methods performing inverse rendering from unposedviews require a rough initialization of the camera orientations. The maindifficulty of pose estimation lies in real-life objects being almost invariantunder certain transformations, making the photometric distance between renderedviews non-convex with respect to the camera parameters. Using an equivalencerelation that matches the distribution of local minima in camera space, wereduce this space to its quotient set, in which pose estimation becomes a moreconvex problem. Using a neural-network to regularize pose estimation, wedemonstrate that our method - MELON - can reconstruct a neural radiance fieldfrom unposed images with state-of-the-art accuracy while requiring ten timesfewer views than adversarial approaches.</description><author>Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein, Dmitry Lagun</author><pubDate>Wed, 19 Jul 2023 09:19:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.08096v2</guid></item><item><title>IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation</title><link>http://arxiv.org/abs/2303.13479v2</link><description>Category-level 6D pose estimation aims to predict the poses and sizes ofunseen objects from a specific category. Thanks to prior deformation, whichexplicitly adapts a category-specific 3D prior (i.e., a 3D template) to a givenobject instance, prior-based methods attained great success and have become amajor research stream. However, obtaining category-specific priors requirescollecting a large amount of 3D models, which is labor-consuming and often notaccessible in practice. This motivates us to investigate whether priors arenecessary to make prior-based methods effective. Our empirical study shows thatthe 3D prior itself is not the credit to the high performance. The keypointactually is the explicit deformation process, which aligns camera and worldcoordinates supervised by world-space 3D models (also called canonical space).Inspired by these observations, we introduce a simple prior-free implicit spacetransformation network, namely IST-Net, to transform camera-space features toworld-space counterparts and build correspondence between them in an implicitmanner without relying on 3D priors. Besides, we design camera- and world-spaceenhancers to enrich the features with pose-sensitive information andgeometrical constraints, respectively. Albeit simple, IST-Net achievesstate-of-the-art performance based-on prior-free design, with top inferencespeed on the REAL275 benchmark. Our code and models are available athttps://github.com/CVMI-Lab/IST-Net.</description><author>Jianhui Liu, Yukang Chen, Xiaoqing Ye, Xiaojuan Qi</author><pubDate>Wed, 19 Jul 2023 17:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13479v2</guid></item><item><title>A Probabilistic Relaxation of the Two-Stage Object Pose Estimation Paradigm</title><link>http://arxiv.org/abs/2306.00892v1</link><description>Existing object pose estimation methods commonly require a one-to-one pointmatching step that forces them to be separated into two consecutive stages:visual correspondence detection (e.g., by matching feature descriptors as partof a perception front-end) followed by geometric alignment (e.g., by optimizinga robust estimation objective for pointcloud registration orperspective-n-point). Instead, we propose a matching-free probabilisticformulation with two main benefits: i) it enables unified and concurrentoptimization of both visual correspondence and geometric alignment, and ii) itcan represent different plausible modes of the entire distribution of likelyposes. This in turn allows for a more graceful treatment of geometricperception scenarios where establishing one-to-one matches between points isconceptually ill-defined, such as textureless, symmetrical and/or occludedobjects and scenes where the correct pose is uncertain or there are multipleequally valid solutions.</description><author>Onur Beker</author><pubDate>Thu, 01 Jun 2023 17:50:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00892v1</guid></item><item><title>HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2302.14581v2</link><description>2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE). Graph Convolutional Network (GCN) has been proven inherently suitable tomodel the human skeletal topology. However, current GCN-based 3D HPE methodsupdate the node features by aggregating their neighbors' information withoutconsidering the interaction of joints in different motion patterns. Althoughsome studies import limb information to learn the movement patterns, the latentsynergies among joints, such as maintaining balance in the motion are seldominvestigated. We propose a hop-wise GraphFormer with intragroup jointrefinement (HopFIR) to tackle the 3D HPE problem. The HopFIR mainly consists ofa novel Hop-wise GraphFormer(HGF) module and an Intragroup JointRefinement(IJR) module which leverages the prior limb information forperipheral joints refinement. The HGF module groups the joints by $k$-hopneighbors and utilizes a hop-wise transformer-like attention mechanism amongthese groups to discover latent joint synergy. Extensive experimental resultsshow that HopFIR outperforms the SOTA methods with a large margin (on theHuman3.6M dataset, the mean per joint position error (MPJPE) is 32.67mm).Furthermore, it is also demonstrated that previous SOTA GCN-based methods canbenefit from the proposed hop-wise attention mechanism efficiently withsignificant performance promotion, such as SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.</description><author>Kai Zhai, Qiang Nie, Bo Ouyang, Xiang Li, ShanLin Yang</author><pubDate>Tue, 18 Jul 2023 17:07:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.14581v2</guid></item><item><title>Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation</title><link>http://arxiv.org/abs/2307.15514v1</link><description>Recent works on 6D object pose estimation focus on learning keypointcorrespondences between images and object models, and then determine the objectpose through RANSAC-based algorithms or by directly regressing the pose withend-to-end optimisations. We argue that learning point-level discriminativefeatures is overlooked in the literature. To this end, we revisit FullyConvolutional Geometric Features (FCGF) and tailor it for object 6D poseestimation to achieve state-of-the-art performance. FCGF employs sparseconvolutions and learns point-level features using a fully-convolutionalnetwork by optimising a hardest contrastive loss. We can outperform recentcompetitors on popular benchmarks by adopting key modifications to the loss andto the input data representations, by carefully tuning the training strategies,and by employing data augmentations suitable for the underlying problem. Wecarry out a thorough ablation to study the contribution of each modification.</description><author>Jaime Corsetti, Davide Boscaini, Fabio Poiesi</author><pubDate>Fri, 28 Jul 2023 13:16:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15514v1</guid></item><item><title>Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data</title><link>http://arxiv.org/abs/2307.06737v1</link><description>Human Pose Estimation is a thoroughly researched problem; however, mostdatasets focus on the side and front-view scenarios. We address the limitationby proposing a novel approach that tackles the challenges posed by extremeviewpoints and poses. We introduce a new method for synthetic data generation -RePoGen, RarE POses GENerator - with comprehensive control over pose and viewto augment the COCO dataset. Experiments on a new dataset of real images showthat adding RePoGen data to the COCO surpasses previous attempts to top-viewpose estimation and significantly improves performance on the bottom-viewdataset. Through an extensive ablation study on both the top and bottom viewdata, we elucidate the contributions of methodological choices and demonstrateimproved performance. The code and the datasets are available on the projectwebsite.</description><author>Miroslav Purkrábek, Jiří Matas</author><pubDate>Thu, 13 Jul 2023 14:17:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.06737v1</guid></item><item><title>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</title><link>http://arxiv.org/abs/2306.15667v2</link><description>Camera pose estimation is a long-standing computer vision problem that todate often relies on classical methods, such as handcrafted keypoint matching,RANSAC and bundle adjustment. In this paper, we propose to formulate theStructure from Motion (SfM) problem inside a probabilistic diffusion framework,modelling the conditional distribution of camera poses given input images. Thisnovel view of an old problem has several advantages. (i) The nature of thediffusion framework mirrors the iterative procedure of bundle adjustment. (ii)The formulation allows a seamless integration of geometric constraints fromepipolar geometry. (iii) It excels in typically difficult scenarios such assparse views with wide baselines. (iv) The method can predict intrinsics andextrinsics for an arbitrary amount of images. We demonstrate that our methodPoseDiffusion significantly improves over the classic SfM pipelines and thelearned approaches on two real-world datasets. Finally, it is observed that ourmethod can generalize across datasets without further training. Project page:https://posediffusion.github.io/</description><author>Jianyuan Wang, Christian Rupprecht, David Novotny</author><pubDate>Wed, 28 Jun 2023 11:57:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15667v2</guid></item><item><title>Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving</title><link>http://arxiv.org/abs/2307.14889v1</link><description>Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomousvehicles (AVs) to make informed decisions and respond proactively in criticalroad scenarios. Promising results of 3D HPE have been gained in several domainssuch as human-computer interaction, robotics, sports and medical analytics,often based on data collected in well-controlled laboratory environments.Nevertheless, the transfer of 3D HPE methods to AVs has received limitedresearch attention, due to the challenges posed by obtaining accurate 3D poseannotations and the limited suitability of data from other domains. We present a simple yet efficient weakly supervised approach for 3D HPE inthe AV context by employing a high-level sensor fusion between camera and LiDARdata. The weakly supervised setting enables training on the target datasetswithout any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractorand pseudo labels generated from LiDAR to image projections. Our approachoutperforms state-of-the-art results by up to $\sim$ 13% on the Waymo OpenDataset in the weakly supervised setting and achieves state-of-the-art resultsin the supervised setting.</description><author>Peter Bauer, Arij Bouazizi, Ulrich Kressel, Fabian B. Flohr</author><pubDate>Thu, 27 Jul 2023 15:28:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.14889v1</guid></item><item><title>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling</title><link>http://arxiv.org/abs/2306.17201v1</link><description>Estimating 3D human poses only from a 2D human pose sequence is thoroughlyexplored in recent years. Yet, prior to this, no such work has attempted tounify 2D and 3D pose representations in the shared feature space. In thispaper, we propose MPM, a unified 2D-3D human pose representation framework viamasked pose modeling. We treat 2D and 3D poses as two different modalities likevision and language and build a single-stream transformer-based architecture.We apply three pretext tasks, which are masked 2D pose modeling, masked 3D posemodeling, and masked 2D pose lifting to pre-train our network and usefull-supervision to perform further fine-tuning. A high masking ratio of 72.5%in total with a spatio-temporal mask sampling strategy leading to betterrelation modeling both in spatial and temporal domains. MPM can handle multipletasks including 3D human pose estimation, 3D pose estimation from occluded 2Dpose, and 3D pose completion in a single framework. We conduct extensiveexperiments and ablation studies on several widely used human pose datasets andachieve state-of-the-art performance on Human3.6M and MPI-INF-3DHP. Codes andmodel checkpoints are available at https://github.com/vvirgooo2/MPM</description><author>Zhenyu Zhang, Wenhao Chai, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang</author><pubDate>Thu, 29 Jun 2023 11:30:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17201v1</guid></item><item><title>Interweaved Graph and Attention Network for 3D Human Pose Estimation</title><link>http://arxiv.org/abs/2304.14045v1</link><description>Despite substantial progress in 3D human pose estimation from a single-viewimage, prior works rarely explore global and local correlations, leading toinsufficient learning of human skeleton representations. To address this issue,we propose a novel Interweaved Graph and Attention Network (IGANet) that allowsbidirectional communications between graph convolutional networks (GCNs) andattentions. Specifically, we introduce an IGA module, where attentions areprovided with local information from GCNs and GCNs are injected with globalinformation from attentions. Additionally, we design a simple yet effectiveU-shaped multi-layer perceptron (uMLP), which can capture multi-granularityinformation for body joints. Extensive experiments on two popular benchmarkdatasets (i.e. Human3.6M and MPI-INF-3DHP) are conducted to evaluate ourproposed method.The results show that IGANet achieves state-of-the-artperformance on both datasets. Code is available athttps://github.com/xiu-cs/IGANet.</description><author>Ti Wang, Hong Liu, Runwei Ding, Wenhao Li, Yingxuan You, Xia Li</author><pubDate>Thu, 27 Apr 2023 10:21:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.14045v1</guid></item><item><title>Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey</title><link>http://arxiv.org/abs/2206.02257v3</link><description>In this survey, we present a systematic review of 3D hand pose estimationfrom the perspective of efficient annotation and learning. 3D hand poseestimation has been an important research area owing to its potential to enablevarious applications, such as video understanding, AR/VR, and robotics.However, the performance of models is tied to the quality and quantity ofannotated 3D hand poses. Under the status quo, acquiring such annotated 3D handposes is challenging, e.g., due to the difficulty of 3D annotation and thepresence of occlusion. To reveal this problem, we review the pros and cons ofexisting annotation methods classified as manual, synthetic-model-based,hand-sensor-based, and computational approaches. Additionally, we examinemethods for learning 3D hand poses when annotated data are scarce, includingself-supervised pretraining, semi-supervised learning, and domain adaptation.Based on the study of efficient annotation and learning, we further discusslimitations and possible future directions in this field.</description><author>Takehiko Ohkawa, Ryosuke Furuta, Yoichi Sato</author><pubDate>Wed, 26 Apr 2023 07:45:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2206.02257v3</guid></item><item><title>Markerless human pose estimation for biomedical applications: a survey</title><link>http://arxiv.org/abs/2308.00519v1</link><description>Markerless Human Pose Estimation (HPE) proved its potential to supportdecision making and assessment in many fields of application. HPE is oftenpreferred to traditional marker-based Motion Capture systems due to the ease ofsetup, portability, and affordable cost of the technology. However, theexploitation of HPE in biomedical applications is still under investigation.This review aims to provide an overview of current biomedical applications ofHPE. In this paper, we examine the main features of HPE approaches and discusswhether or not those features are of interest to biomedical applications. Wealso identify those areas where HPE is already in use and present peculiaritiesand trends followed by researchers and practitioners. We include here 25approaches to HPE and more than 40 studies of HPE applied to motor developmentassessment, neuromuscolar rehabilitation, and gait &amp; posture analysis. Weconclude that markerless HPE offers great potential for extending diagnosis andrehabilitation outside hospitals and clinics, toward the paradigm of remotemedical care.</description><author>Andrea Avogaro, Federico Cunico, Bodo Rosenhahn, Francesco Setti</author><pubDate>Tue, 01 Aug 2023 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.00519v1</guid></item><item><title>Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach</title><link>http://arxiv.org/abs/2307.01004v1</link><description>We introduce a novel one-stage end-to-end multi-person 2D pose estimationalgorithm, known as Joint Coordinate Regression and Association (JCRA), thatproduces human pose joints and associations without requiring anypost-processing. The proposed algorithm is fast, accurate, effective, andsimple. The one-stage end-to-end network architecture significantly improvesthe inference speed of JCRA. Meanwhile, we devised a symmetric networkstructure for both the encoder and decoder, which ensures high accuracy inidentifying keypoints. It follows an architecture that directly outputs partpositions via a transformer network, resulting in a significant improvement inperformance. Extensive experiments on the MS COCO and CrowdPose benchmarksdemonstrate that JCRA outperforms state-of-the-art approaches in both accuracyand efficiency. Moreover, JCRA demonstrates 69.2 mAP and is 78\% faster atinference acceleration than previous state-of-the-art bottom-up algorithms. Thecode for this algorithm will be publicly available.</description><author>Dongyang Yu, Yunshi Xie, Wangpeng An, Li Zhang, Yufeng Yao</author><pubDate>Mon, 03 Jul 2023 14:40:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01004v1</guid></item><item><title>Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation</title><link>http://arxiv.org/abs/2306.17074v1</link><description>One of the mainstream schemes for 2D human pose estimation (HPE) is learningkeypoints heatmaps by a neural network. Existing methods typically improve thequality of heatmaps by customized architectures, such as high-resolutionrepresentation and vision Transformers. In this paper, we propose\textbf{DiffusionPose}, a new scheme that formulates 2D HPE as a keypointsheatmaps generation problem from noised heatmaps. During training, thekeypoints are diffused to random distribution by adding noises and thediffusion model learns to recover ground-truth heatmaps from noised heatmapswith respect to conditions constructed by image feature. During inference, thediffusion model generates heatmaps from initialized heatmaps in a progressivedenoising way. Moreover, we further explore improving the performance ofDiffusionPose with conditions from human structural information. Extensiveexperiments show the prowess of our DiffusionPose, with improvements of 1.6,1.2, and 1.2 mAP on widely-used COCO, CrowdPose, and AI Challenge datasets,respectively.</description><author>Zhongwei Qiu, Qiansheng Yang, Jian Wang, Xiyu Wang, Chang Xu, Dongmei Fu, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang</author><pubDate>Thu, 29 Jun 2023 17:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.17074v1</guid></item><item><title>Learning Better Keypoints for Multi-Object 6DoF Pose Estimation</title><link>http://arxiv.org/abs/2308.07827v1</link><description>We investigate the impact of pre-defined keypoints for pose estimation, andfound that accuracy and efficiency can be improved by training a graph networkto select a set of disperse keypoints with similarly distributed votes. Thesevotes, learned by a regression network to accumulate evidence for the keypointlocations, can be regressed more accurately compared to previous heuristickeypoint algorithms. The proposed KeyGNet, supervised by a combined lossmeasuring both Wassserstein distance and dispersion, learns the color andgeometry features of the target objects to estimate optimal keypoint locations.Experiments demonstrate the keypoints selected by KeyGNet improved the accuracyfor all evaluation metrics of all seven datasets tested, for three keypointvoting methods. The challenging Occlusion LINEMOD dataset notably improvedADD(S) by +16.4% on PVN3D, and all core BOP datasets showed an AR improvementfor all objects, of between +1% and +21.5%. There was also a notable increasein performance when transitioning from single object to multiple objecttraining using KeyGNet keypoints, essentially eliminating the SISO-MIMO gap forOcclusion LINEMOD.</description><author>Yangzheng Wu, Michael Greenspan</author><pubDate>Tue, 15 Aug 2023 16:11:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07827v1</guid></item><item><title>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</title><link>http://arxiv.org/abs/2306.05410v1</link><description>A critical obstacle preventing NeRF models from being deployed broadly in thewild is their reliance on accurate camera poses. Consequently, there is growinginterest in extending NeRF models to jointly optimize camera poses and scenerepresentation, which offers an alternative to off-the-shelf SfM pipelineswhich have well-understood failure modes. Existing approaches for unposed NeRFoperate under limited assumptions, such as a prior pose distribution or coarsepose initialization, making them less effective in a general setting. In thiswork, we propose a novel approach, LU-NeRF, that jointly estimates camera posesand neural radiance fields with relaxed assumptions on pose configuration. Ourapproach operates in a local-to-global manner, where we first optimize overlocal subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose andgeometry for this challenging few-shot task. The mini-scene poses are broughtinto a global reference frame through a robust pose synchronization step, wherea final global optimization of pose and scene can be performed. We show ourLU-NeRF pipeline outperforms prior attempts at unposed NeRF without makingrestrictive assumptions on the pose prior. This allows us to operate in thegeneral SE(3) pose setting, unlike the baselines. Our results also indicate ourmodel can be complementary to feature-based SfM pipelines as it comparesfavorably to COLMAP on low-texture and low-resolution images.</description><author>Zezhou Cheng, Carlos Esteves, Varun Jampani, Abhishek Kar, Subhransu Maji, Ameesh Makadia</author><pubDate>Thu, 08 Jun 2023 18:56:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05410v1</guid></item><item><title>Occlusion Robust 3D Human Pose Estimation with StridedPoseGraphFormer and Data Augmentation</title><link>http://arxiv.org/abs/2304.12069v1</link><description>Occlusion is an omnipresent challenge in 3D human pose estimation (HPE). Inspite of the large amount of research dedicated to 3D HPE, only a limitednumber of studies address the problem of occlusion explicitly. To fill thisgap, we propose to combine exploitation of spatio-temporal features withsynthetic occlusion augmentation during training to deal with occlusion. Tothis end, we build a spatio-temporal 3D HPE model, StridedPoseGraphFormer basedon graph convolution and transformers, and train it using occlusionaugmentation. Unlike the existing occlusion-aware methods, that are only testedfor limited occlusion, we extensively evaluate our method for varying degreesof occlusion. We show that our proposed method compares favorably with thestate-of-the-art (SoA). Our experimental results also reveal that in theabsence of any occlusion handling mechanism, the performance of SoA 3D HPEmethods degrades significantly when they encounter occlusion.</description><author>Soubarna Banik, Patricia Gschoßmann, Alejandro Mendoza Garcia, Alois Knoll</author><pubDate>Mon, 24 Apr 2023 14:05:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12069v1</guid></item><item><title>LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network</title><link>http://arxiv.org/abs/2306.12525v1</link><description>In this technical report, we present the 1st place solution for the 2023Waymo Open Dataset Pose Estimation challenge. Due to the difficulty ofacquiring large-scale 3D human keypoint annotation, previous methods havecommonly relied on 2D image features and 2D sequential annotations for 3D humanpose estimation. In contrast, our proposed method, named LPFormer, uses onlyLiDAR as its input along with its corresponding 3D annotations. LPFormerconsists of two stages: the first stage detects the human bounding box andextracts multi-level feature representations, while the second stage employs atransformer-based network to regress the human keypoints using these features.Experimental results on the Waymo Open Dataset demonstrate the top performance,and improvements even compared to previous multi-modal solutions.</description><author>Dongqiangzi Ye, Yufei Xie, Weijia Chen, Zixiang Zhou, Hassan Foroosh</author><pubDate>Wed, 21 Jun 2023 20:20:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12525v1</guid></item><item><title>Hand Pose Estimation with Mems-Ultrasonic Sensors</title><link>http://arxiv.org/abs/2306.12652v1</link><description>Hand tracking is an important aspect of human-computer interaction and has awide range of applications in extended reality devices. However, current handmotion capture methods suffer from various limitations. For instance,visual-based hand pose estimation is susceptible to self-occlusion and changesin lighting conditions, while IMU-based tracking gloves experience significantdrift and are not resistant to external magnetic field interference. To addressthese issues, we propose a novel and low-cost hand-tracking glove that utilizesseveral MEMS-ultrasonic sensors attached to the fingers, to measure thedistance matrix among the sensors. Our lightweight deep network thenreconstructs the hand pose from the distance matrix. Our experimental resultsdemonstrate that this approach is both accurate, size-agnostic, and robust toexternal interference. We also show the design logic for the sensor selection,sensor configurations, circuit diagram, as well as model architecture.</description><author>Qiang Zhang, Yuanqiao Lin, Yubin Lin, Szymon Rusinkiewicz</author><pubDate>Thu, 22 Jun 2023 04:41:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12652v1</guid></item><item><title>Next-generation Surgical Navigation: Multi-view Marker-less 6DoF Pose Estimation of Surgical Instruments</title><link>http://arxiv.org/abs/2305.03535v1</link><description>State-of-the-art research of traditional computer vision is increasinglyleveraged in the surgical domain. A particular focus in computer-assistedsurgery is to replace marker-based tracking systems for instrument localizationwith pure image-based 6DoF pose estimation. However, the state of the art hasnot yet met the accuracy required for surgical navigation. In this context, wepropose a high-fidelity marker-less optical tracking system for surgicalinstrument localization. We developed a multi-view camera setup consisting ofstatic and mobile cameras and collected a large-scale RGB-D video dataset withdedicated synchronization and data fusions methods. Different state-of-the-artpose estimation methods were integrated into a deep learning pipeline andevaluated on multiple camera configurations. Furthermore, the performanceimpacts of different input modalities and camera positions, as well as trainingon purely synthetic data, were compared. The best model achieved an averageposition and orientation error of 1.3 mm and 1.0{\deg} for a surgical drill aswell as 3.8 mm and 5.2{\deg} for a screwdriver. These results significantlyoutperform related methods in the literature and are close to clinical-gradeaccuracy, demonstrating that marker-less tracking of surgical instruments isbecoming a feasible alternative to existing marker-based systems.</description><author>Jonas Hein, Nicola Cavalcanti, Daniel Suter, Lukas Zingg, Fabio Carrillo, Mazda Farshad, Marc Pollefeys, Nassir Navab, Philipp Fürnstahl</author><pubDate>Fri, 05 May 2023 14:42:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03535v1</guid></item><item><title>A Real-time Human Pose Estimation Approach for Optimal Sensor Placement in Sensor-based Human Activity Recognition</title><link>http://arxiv.org/abs/2307.02906v1</link><description>Sensor-based Human Activity Recognition facilitates unobtrusive monitoring ofhuman movements. However, determining the most effective sensor placement foroptimal classification performance remains challenging. This paper introduces anovel methodology to resolve this issue, using real-time 2D pose estimationsderived from video recordings of target activities. The derived skeleton dataprovides a unique strategy for identifying the optimal sensor location. Wevalidate our approach through a feasibility study, applying inertial sensors tomonitor 13 different activities across ten subjects. Our findings indicate thatthe vision-based method for sensor placement offers comparable results to theconventional deep learning approach, demonstrating its efficacy. This researchsignificantly advances the field of Human Activity Recognition by providing alightweight, on-device solution for determining the optimal sensor placement,thereby enhancing data anonymization and supporting a multimodal classificationapproach.</description><author>Orhan Konak, Alexander Wischmann, Robin van de Water, Bert Arnrich</author><pubDate>Thu, 06 Jul 2023 11:38:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02906v1</guid></item><item><title>6D Object Pose Estimation from Approximate 3D Models for Orbital Robotics</title><link>http://arxiv.org/abs/2303.13241v3</link><description>We present a novel technique to estimate the 6D pose of objects from singleimages where the 3D geometry of the object is only given approximately and notas a precise 3D model. To achieve this, we employ a dense 2D-to-3Dcorrespondence predictor that regresses 3D model coordinates for every pixel.In addition to the 3D coordinates, our model also estimates the pixel-wisecoordinate error to discard correspondences that are likely wrong. This allowsus to generate multiple 6D pose hypotheses of the object, which we then refineiteratively using a highly efficient region-based approach. We also introduce anovel pixel-wise posterior formulation by which we can estimate the probabilityfor each hypothesis and select the most likely one. As we show in experiments,our approach is capable of dealing with extreme visual conditions includingoverexposure, high contrast, or low signal-to-noise ratio. This makes it apowerful technique for the particularly challenging task of estimating the poseof tumbling satellites for in-orbit robotic applications. Our method achievesstate-of-the-art performance on the SPEED+ dataset and has won the SPEC2021post-mortem competition.</description><author>Maximilian Ulmer, Maximilian Durner, Martin Sundermeyer, Manuel Stoiber, Rudolph Triebel</author><pubDate>Wed, 21 Jun 2023 15:36:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.13241v3</guid></item></channel></rss>