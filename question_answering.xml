<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 24 Sep 2025 13:00:07 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An Empirical Study on How Video-LLMs Answer Video Questions</title><link>http://arxiv.org/abs/2508.15360v1</link><description>Taking advantage of large-scale data and pretrained language models, VideoLarge Language Models (Video-LLMs) have shown strong capabilities in answeringvideo questions. However, most existing efforts focus on improving performance,with limited attention to understanding their internal mechanisms. This paperaims to bridge this gap through a systematic empirical study. To interpretexisting VideoLLMs, we adopt attention knockouts as our primary analytical tooland design three variants: Video Temporal Knockout, Video Spatial Knockout, andLanguage-to-Video Knockout. Then, we apply these three knockouts on differentnumbers of layers (window of layers). By carefully controlling the window oflayers and types of knockouts, we provide two settings: a global setting and afine-grained setting. Our study reveals three key findings: (1) Global settingindicates Video information extraction primarily occurs in early layers,forming a clear two-stage process -- lower layers focus on perceptual encoding,while higher layers handle abstract reasoning; (2) In the fine-grained setting,certain intermediate layers exert an outsized impact on video questionanswering, acting as critical outliers, whereas most other layers contributeminimally; (3) In both settings, we observe that spatial-temporal modelingrelies more on language-guided retrieval than on intra- and inter-frameself-attention among video tokens, despite the latter's high computationalcost. Finally, we demonstrate that these insights can be leveraged to reduceattention computation in Video-LLMs. To our knowledge, this is the first workto systematically uncover how Video-LLMs internally process and understandvideo content, offering interpretability and efficiency perspectives for futureresearch.</description><author>Chenhui Gou, Ziyu Ma, Zicheng Duan, Haoyu He, Feng Chen, Akide Liu, Bohan Zhuang, Jianfei Cai, Hamid Rezatofighi</author><pubDate>Thu, 21 Aug 2025 08:42:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15360v1</guid></item><item><title>Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding</title><link>http://arxiv.org/abs/2410.01671v3</link><description>Large language models (LLMs) have shown remarkable capabilities in naturallanguage processing; however, they still face difficulties when tasked withunderstanding lengthy contexts and executing effective question answering.These challenges often arise due to the complexity and ambiguity present inlonger texts. To enhance the performance of LLMs in such scenarios, weintroduce the Long Question Coreference Adaptation (LQCA) method. Thisinnovative framework focuses on coreference resolution tailored to longcontexts, allowing the model to identify and manage references effectively. TheLQCA method encompasses four key steps: resolving coreferences withinsub-documents, computing the distances between mentions, defining arepresentative mention for coreference, and answering questions through mentionreplacement. By processing information systematically, the framework provideseasier-to-handle partitions for LLMs, promoting better understanding.Experimental evaluations on a range of LLMs and datasets have yielded positiveresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,highlighting the effectiveness of leveraging coreference resolution to bridgecontext gaps in question answering. Our code is public athttps://github.com/OceannTwT/LQCA.</description><author>Yanming Liu, Xinyue Peng, Jiannan Cao, Yanxin Shen, Tianyu Du, Sheng Cheng, Xun Wang, Jianwei Yin, Xuhong Zhang</author><pubDate>Fri, 15 Aug 2025 05:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01671v3</guid></item><item><title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title><link>http://arxiv.org/abs/2507.22752v2</link><description>We introduce CUS-QA, a benchmark for open-ended regional question answeringthat encompasses both textual and visual modalities. We also provide strongbaselines using state-of-the-art large language models (LLMs). Our datasetconsists of manually curated questions and answers grounded in Wikipedia,created by native speakers from Czechia, Slovakia, and Ukraine, withaccompanying English translations. It includes both purely textual questionsand those requiring visual understanding. We evaluate state-of-the-art LLMsthrough prompting and complement this with human judgments of answercorrectness. Using these human evaluations, we analyze the reliability ofexisting automatic evaluation metrics. Our baseline results show that even thebest open-weight LLMs achieve only around 50% accuracy on textual questions andbelow 30% on visual questions. LLM-based evaluation metrics show strongcorrelation with human judgment, while traditional string-overlap metricsperform surprisingly well due to the prevalence of named entities in answers.</description><author>Jindřich Libovický, Jindřich Helcl, Andrei Manea, Gianluca Vico</author><pubDate>Thu, 21 Aug 2025 12:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22752v2</guid></item><item><title>Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset</title><link>http://arxiv.org/abs/2508.11058v1</link><description>The advancement of 3D vision-language (3D VL) learning is hindered by severallimitations in existing 3D VL datasets: they rarely necessitate reasoningbeyond a close range of objects in single viewpoint, and annotations often linkinstructions to single objects, missing richer contextual alignments betweenmultiple objects. This significantly curtails the development of models capableof deep, multi-view 3D scene understanding over distant objects. To addressthese challenges, we introduce MV-ScanQA, a novel 3D question answering datasetwhere 68% of questions explicitly require integrating information from multipleviews (compared to less than 7% in existing datasets), thereby rigorouslytesting multi-view compositional reasoning. To facilitate the training ofmodels for such demanding scenarios, we present TripAlign dataset, alarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M &lt;2Dview, set of 3D objects, text&gt; triplets that explicitly aligns groups ofcontextually related objects with text, providing richer, view-groundedmulti-object multimodal alignment signals than previous single-objectannotations. We further develop LEGO, a baseline method for the multi-viewreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2DLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlignachieves state-of-the-art performance not only on the proposed MV-ScanQA, butalso on existing benchmarks for 3D dense captioning and question answering.Datasets and code are available athttps://matthewdm0816.github.io/tripalign-mvscanqa.</description><author>Wentao Mo, Qingchao Chen, Yuxin Peng, Siyuan Huang, Yang Liu</author><pubDate>Thu, 14 Aug 2025 20:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11058v1</guid></item><item><title>Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs</title><link>http://arxiv.org/abs/2508.19111v1</link><description>Large vision-language models (LVLMs) demonstrate strong visual questionanswering (VQA) capabilities but are shown to hallucinate. A reliable modelshould perceive its knowledge boundaries-knowing what it knows and what it doesnot. This paper investigates LVLMs' perception of their knowledge boundaries byevaluating three types of confidence signals: probabilistic confidence, answerconsistency-based confidence, and verbalized confidence. Experiments on threeLVLMs across three VQA datasets show that, although LVLMs possess a reasonableperception level, there is substantial room for improvement. Among the threeconfidences, probabilistic and consistency-based signals are more reliableindicators, while verbalized confidence often leads to overconfidence. Toenhance LVLMs' perception, we adapt several established confidence calibrationmethods from Large Language Models (LLMs) and propose three effective methods.Additionally, we compare LVLMs with their LLM counterparts, finding thatjointly processing visual and textual inputs decreases question-answeringperformance but reduces confidence, resulting in an improved perception levelcompared to LLMs.</description><author>Zhikai Ding, Shiyu Ni, Keping Bi</author><pubDate>Tue, 26 Aug 2025 15:14:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19111v1</guid></item><item><title>AirRAG: Autonomous Strategic Planning and Reasoning Steer Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2501.10053v3</link><description>Leveraging the autonomous decision-making capabilities of large languagemodels (LLMs) has demonstrated superior performance in reasoning tasks.However, despite the success of iterative or agentic retrieval-augmentedgeneration (RAG) techniques, these methods are often constrained to a singlesolution space when confronted with complex problems. In this paper, we proposea novel thinking pattern in RAG that integrates autonomous strategic planningwith efficient reasoning actions, significantly activating intrinsic reasoningcapabilities and expanding the solution space of specific tasks via Monte CarloTree Search (MCTS), which we refer to as AirRAG. Specifically, our approachdesigns five fundamental reasoning actions, which are expanded to a broadtree-based reasoning space using MCTS. The approach also incorporatesself-consistency verification to explore potential reasoning paths andinference scaling law. Additionally, computationally optimal strategies areemployed to allocate more inference resources to key actions, thereby enhancingoverall performance. Experimental results demonstrate the effectiveness ofAirRAG, showing significant performance gains on complex question-answeringdatasets. Furthermore, AirRAG is flexible and lightweight, making it easy tointegrate with other advanced technologies and models.</description><author>Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Guochao Jiang, Jingyi Song, Hao Wang</author><pubDate>Wed, 27 Aug 2025 07:38:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.10053v3</guid></item><item><title>Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for Multimodal Medical VQA</title><link>http://arxiv.org/abs/2507.05520v3</link><description>Dermatological care via telemedicine often lacks the rich context ofin-person visits. Clinicians must make diagnoses based on a handful of imagesand brief descriptions, without the benefit of physical exams, second opinions,or reference materials. While many medical AI systems attempt to bridge thesegaps with domain-specific fine-tuning, this work hypothesized that mimickingclinical reasoning processes could offer a more effective path forward. Thisstudy tested seven vision-language models on medical visual question answeringacross six configurations: baseline models, fine-tuned variants, and bothaugmented with either reasoning layers that combine multiple modelperspectives, analogous to peer consultation, or retrieval-augmented generationthat incorporates medical literature at inference time, serving a role similarto reference-checking. While fine-tuning degraded performance in four of sevenmodels with an average 30% decrease, baseline models collapsed on test data.Clinical-inspired architectures, meanwhile, achieved up to 70% accuracy,maintaining performance on unseen data while generating explainable,literature-grounded outputs critical for clinical adoption. These findingsdemonstrate that medical AI succeeds by reconstructing the collaborative andevidence-based practices fundamental to clinical diagnosis.</description><author>Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar</author><pubDate>Tue, 26 Aug 2025 14:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05520v3</guid></item><item><title>Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for Multimodal Medical VQA</title><link>http://arxiv.org/abs/2507.05520v2</link><description>Dermatological care via telemedicine often lacks the rich context ofin-person visits. Clinicians must make diagnoses based on a handful of imagesand brief descriptions, without the benefit of physical exams, second opinions,or reference materials. While many medical AI systems attempt to bridge thesegaps with domain-specific fine-tuning, this work hypothesized that mimickingclinical reasoning processes could offer a more effective path forward. Thisstudy tested seven vision-language models on medical visual question answeringacross six configurations: baseline models, fine-tuned variants, and bothaugmented with either reasoning layers that combine multiple modelperspectives, analogous to peer consultation, or retrieval-augmented generationthat incorporates medical literature at inference time, serving a role similarto reference-checking. While fine-tuning degraded performance in four of sevenmodels with an average 30\% decrease, baseline models collapsed on test data.Clinical-inspired architectures, meanwhile, achieved up to 70\% accuracy,maintaining performance on unseen data while generating explainable,literature-grounded outputs critical for clinical adoption. These findingsdemonstrate that medical AI succeeds by reconstructing the collaborative andevidence-based practices fundamental to clinical diagnosis.</description><author>Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar</author><pubDate>Mon, 25 Aug 2025 14:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05520v2</guid></item><item><title>Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs</title><link>http://arxiv.org/abs/2405.14862v2</link><description>Decoder-only large language models typically rely solely on masked causalattention, which limits their expressiveness by restricting information flow toone direction. We propose Bitune, a method that enhances pretraineddecoder-only LLMs by incorporating bidirectional attention into promptprocessing. We evaluate Bitune in instruction-tuning and question-answeringsettings, showing significant improvements in performance on commonsensereasoning, arithmetic, and language understanding tasks. Furthermore, extensiveablation studies validate the role of each component of the method, anddemonstrate that Bitune is compatible with various parameter-efficientfinetuning techniques and full model finetuning.</description><author>Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano</author><pubDate>Thu, 28 Aug 2025 17:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.14862v2</guid></item><item><title>General Table Question Answering via Answer-Formula Joint Generation</title><link>http://arxiv.org/abs/2503.12345v3</link><description>Advanced table question answering (TableQA) methods prompt large languagemodels (LLMs) to generate answer text, SQL query, Python code, or customoperation, which impressively improve the complex reasoning problems in theTableQA task. However, these methods lack the versatility to cope with specificquestion types or table structures. In contrast, the Spreadsheet Formula, thewidely used and well-defined operation language for tabular data, has not beenthoroughly explored to solve TableQA. In this paper, we first attempt to usethe Formula as the executable representation for solving complex reasoning ontables with different structures. Specifically, we construct\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existingdatasets. In addition, we propose \texttt{TabAF}, a general table answeringframework to solve multiple types of tasks over multiple types of tablessimultaneously, which decodes answers and Formulas with a single LLM backbone.Extensive experiments demonstrate the versatility and generalization of\texttt{TabAF}. Under the same model size, \texttt{TabAF} achieves newstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.</description><author>Zhongyuan Wang, Richong Zhang, Zhijie Nie, Hangyu Mao</author><pubDate>Sun, 31 Aug 2025 14:11:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.12345v3</guid></item><item><title>Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2508.11247v1</link><description>Multi-hop question answering (MHQA) requires integrating knowledge scatteredacross multiple passages to derive the correct answer. Traditionalretrieval-augmented generation (RAG) methods primarily focus on coarse-grainedtextual semantic similarity and ignore structural associations among dispersedknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methodsaddress this by leveraging knowledge graphs (KGs) to capture structuralassociations, but they tend to overly rely on structural information andfine-grained word- or phrase-level retrieval, resulting in an underutilizationof textual semantics. In this paper, we propose a novel RAG approach calledHGRAG for MHQA that achieves cross-granularity integration of structural andsemantic information via hypergraphs. Structurally, we construct an entityhypergraph where fine-grained entities serve as nodes and coarse-grainedpassages as hyperedges, and establish knowledge association through sharedentities. Semantically, we design a hypergraph retrieval method that integratesfine-grained entity similarity and coarse-grained passage similarity viahypergraph diffusion. Finally, we employ a retrieval enhancement module, whichfurther refines the retrieved results both semantically and structurally, toobtain the most relevant passages as context for answer generation with theLLM. Experimental results on benchmark datasets demonstrate that our approachoutperforms state-of-the-art methods in QA performance, and achieves a6$\times$ speedup in retrieval efficiency.</description><author>Changjian Wang, Weihong Deng, Weili Guan, Quan Lu, Ning Jiang</author><pubDate>Fri, 15 Aug 2025 06:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11247v1</guid></item><item><title>SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation with Casual Inference</title><link>http://arxiv.org/abs/2403.07088v7</link><description>Large language models(LLMs) have shown its outperforming ability on varioustasks and question answering. However, LLMs require substantial memory storageon low-resource devices. More critically, the computational speed on thesedevices is also severely limited. In this paper, we propose SPA(Side PluginAdaption), a lightweight architecture for fast on-devices inference on theconstraints of strict on-devices computation and memory constraints. Comparedwith other on-devices seq2seq generation, SPA could make a fast and stableinference on low-resource constraints, allowing it to obtain cost effiency. Ourmethod establish an interaction between a pretrained LLMs on-cloud and additiveparameters on-devices, which could provide the knowledge on both pretrainedLLMs and featured personal feature. Further more, SPA provides a framework tokeep feature-base parameters on low computational devices while leave theparameters containing general information on the high computational devices.</description><author>Yanming Liu, Xinyue Peng, Ningjing Sang, Yafeng Yan, Xiaolan Ke, Zhiting Zheng, Shaobo Liu, Songhang Deng, Jiannan Cao, Le Dai, Xingzu Liu, Ruilin Nong, Weihao Liu</author><pubDate>Fri, 15 Aug 2025 04:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07088v7</guid></item><item><title>PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging</title><link>http://arxiv.org/abs/2505.11872v3</link><description>Recent advancements in prompt-based medical image segmentation have enabledclinicians to identify tumors using simple input like bounding boxes or textprompts. However, existing methods face challenges when doctors need tointeract through natural language or when position reasoning is required -understanding spatial relationships between anatomical structures andpathologies. We present PRS-Med, a framework that integrates vision-languagemodels with segmentation capabilities to generate both accurate segmentationmasks and corresponding spatial reasoning outputs. Additionally, we introducethe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),which provides diverse, spatially-grounded question-answer pairs to address thelack of position reasoning data in medical imaging. PRS-Med demonstratessuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,endoscopy, RGB), significantly outperforming state-of-the-art methods in bothsegmentation accuracy and position reasoning. Our approach enables intuitivedoctor-system interaction through natural language, facilitating more efficientdiagnoses. Our dataset pipeline, model, and codebase will be released to fosterfurther research in spatially-aware multimodal reasoning for medicalapplications.</description><author>Quoc-Huy Trinh, Minh-Van Nguyen, Jung Zeng, Ulas Bagci, Debesh Jha</author><pubDate>Fri, 15 Aug 2025 02:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11872v3</guid></item><item><title>MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering</title><link>http://arxiv.org/abs/2508.11163v1</link><description>This paper presents MobQA, a benchmark dataset designed to evaluate thesemantic understanding capabilities of large language models (LLMs) for humanmobility data through natural language question answering. While existing models excel at predicting human movement patterns, it remainsunobvious how much they can interpret the underlying reasons or semanticmeaning of those patterns. MobQA provides a comprehensive evaluation frameworkfor LLMs to answer questions about diverse human GPS trajectories spanningdaily to weekly granularities. It comprises 5,800 high-quality question-answerpairs across three complementary question types: factual retrieval (precisedata extraction), multiple-choice reasoning (semantic inference), and free-formexplanation (interpretive description), which all require spatial, temporal,and semantic reasoning. Our evaluation of major LLMs reveals strong performanceon factual retrieval but significant limitations in semantic reasoning andexplanation question answering, with trajectory length substantially impactingmodel effectiveness. These findings demonstrate the achievements andlimitations of state-of-the-art LLMs for semantic mobilityunderstanding.\footnote{MobQA dataset is available athttps://github.com/CyberAgentAILab/mobqa.}</description><author>Hikaru Asano, Hiroki Ouchi, Akira Kasuga, Ryo Yonetani</author><pubDate>Fri, 15 Aug 2025 02:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11163v1</guid></item><item><title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title><link>http://arxiv.org/abs/2508.11133v1</link><description>Large language models (LLMs) are emerging as a go-to tool for queryinginformation. However, current LLM benchmarks rarely feature natural questionsthat are both information-seeking as well as genuinely time-consuming forhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 naturaland complex questions that require dozens, and at times hundreds, ofintermediate steps to solve -- far more than any existing QA benchmark. Tobuild MoNaCo, we developed a decomposed annotation pipeline to elicit andmanually answer natural time-consuming questions at scale. Frontier LLMsevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall andhallucinations. Our results underscore the need for reasoning models thatbetter handle the complexity and sheer breadth of real-worldinformation-seeking questions -- with MoNaCo providing an effective resourcefor tracking such progress. The MONACO benchmark, codebase, prompts and modelspredictions are publicly available at: https://tomerwolgithub.github.io/monaco</description><author>Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty</author><pubDate>Fri, 15 Aug 2025 00:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11133v1</guid></item><item><title>Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?</title><link>http://arxiv.org/abs/2508.11011v1</link><description>Construction safety inspections typically involve a human inspectoridentifying safety concerns on-site. With the rise of powerful Vision LanguageModels (VLMs), researchers are exploring their use for tasks such as detectingsafety rule violations from on-site images. However, there is a lack of opendatasets to comprehensively evaluate and further fine-tune VLMs in constructionsafety inspection. Current applications of VLMs use small, supervised datasets,limiting their applicability in tasks they are not directly trained for. Inthis paper, we propose the ConstructionSite 10k, featuring 10,000 constructionsite images with annotations for three inter-connected tasks, including imagecaptioning, safety rule violation visual question answering (VQA), andconstruction element visual grounding. Our subsequent evaluation of currentstate-of-the-art large pre-trained VLMs shows notable generalization abilitiesin zero-shot and few-shot settings, while additional training is needed to makethem applicable to actual construction sites. This dataset allows researchersto train and evaluate their own VLMs with new architectures and techniques,providing a valuable benchmark for construction safety inspection.</description><author>Xuezheng Chen, Zhengbo Zou</author><pubDate>Thu, 14 Aug 2025 18:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11011v1</guid></item><item><title>All for law and law for all: Adaptive RAG Pipeline for Legal Research</title><link>http://arxiv.org/abs/2508.13107v1</link><description>Retrieval-Augmented Generation (RAG) mitigates hallucinations by groundinglarge language model outputs in cited sources, a capability that is especiallycritical in the legal domain. We present an end-to-end RAG pipeline thatrevisits and extends the LegalBenchRAG baseline with three targetedenhancements: (i) a context-aware query translator that disentangles documentreferences from natural-language questions and adapts retrieval depth andresponse style based on expertise and specificity, (ii) open-source retrievalstrategies using SBERT and GTE embeddings that achieve substantial performancegains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for$K&gt;4$) while remaining cost-efficient, and (iii) a comprehensive evaluation andgeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall toassess semantic alignment and faithfulness across models and prompt designs.Our results show that carefully designed open-source pipelines can rival oroutperform proprietary approaches in retrieval quality, while a customlegal-grounded prompt consistently produces more faithful and contextuallyrelevant answers than baseline prompting. Taken together, these contributionsdemonstrate the potential of task-aware, component-level tuning to deliverlegally grounded, reproducible, and cost-effective RAG systems for legalresearch assistance.</description><author>Figarri Keisha, Prince Singh, Pallavi, Dion Fernandes, Aravindh Manivannan, Ilham Wicaksono, Faisal Ahmad</author><pubDate>Mon, 18 Aug 2025 17:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13107v1</guid></item><item><title>Checkmate: interpretable and explainable RSVQA is the endgame</title><link>http://arxiv.org/abs/2508.13086v1</link><description>Remote Sensing Visual Question Answering (RSVQA) presents unique challengesin ensuring that model decisions are both understandable and grounded in visualcontent. Current models often suffer from a lack of interpretability andexplainability, as well as from biases in dataset distributions that lead toshortcut learning. In this work, we tackle these issues by introducing a novelRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253questions and a balanced answer distribution. Each answer is linked to one ormore cells within the image, enabling fine-grained visual reasoning. Building on this dataset, we develop an explainable and interpretable modelcalled Checkmate that identifies the image cells most relevant to itsdecisions. Through extensive experiments across multiple model architectures,we show that our approach improves transparency and supports more trustworthydecision-making in RSVQA systems.</description><author>Lucrezia Tosato, Christel Tartini Chappuis, Syrielle Montariol, Flora Weissgerber, Sylvain Lobry, Devis Tuia</author><pubDate>Mon, 18 Aug 2025 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13086v1</guid></item><item><title>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review</title><link>http://arxiv.org/abs/2410.21673v3</link><description>Public Code Review (PCR) is developed in the Software Question Answering(SQA) community, assisting developers in exploring high-quality and efficientreview services. Current methods on PCR mainly focus on the reviewer'sperspective, including finding a capable reviewer, predicting comment quality,and recommending/generating review comments. However, it is not well studiedthat how to satisfy the review necessity requests posted by developers whichcan increase their visibility, which in turn acts as a prerequisite for betterreview responses. To this end, we propose K nowledge-guided P rompt learningfor P ublic Code Review (KP-PCR) to achieve developer-based code review requestquality assurance (i.e., predicting request necessity and recommending tagssubtask). Specifically, we reformulate the two subtasks via 1) text prompttuning which converts both of them into a Masked Language Model (MLM) byconstructing prompt templates using hard prompt; and 2) knowledge and codeprefix tuning which introduces knowledge guidance from fine-tuned largelanguage models by soft prompt, and uses program dependence graph tocharacterize code snippets. Finally, both of the request necessity predictionand tag recommendation subtasks output predicted results through an answerengineering module. In addition, we further analysis the time complexity of ourKP-PCR that has lightweight prefix based the operation of introducing knowledgeguidance. Experimental results on the PCR dataset for the period 2011-2023demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the requestnecessity prediction and by 1.4%-6.9% in the tag recommendation. The codeimplementation is released at https://github.com/WUT-IDEA/KP-PCR.</description><author>Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang</author><pubDate>Thu, 21 Aug 2025 17:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21673v3</guid></item><item><title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title><link>http://arxiv.org/abs/2508.15717v1</link><description>Multimodal large language models (MLLMs) have made significant progress invisual-language reasoning, but their ability to efficiently handle long videosremains limited. Despite recent advances in long-context MLLMs, storing andattending to the key-value (KV) cache for long visual contexts incurssubstantial memory and computational overhead. Existing visual compressionmethods require either encoding the entire visual context before compression orhaving access to the questions in advance, which is impractical for long videounderstanding and multi-turn conversational settings. In this work, we proposeStreamMem, a query-agnostic KV cache memory mechanism for streaming videounderstanding. Specifically, StreamMem encodes new video frames in a streamingmanner, compressing the KV cache using attention scores between visual tokensand generic query tokens, while maintaining a fixed-size KV memory to enableefficient question answering (QA) in memory-constrained, long-video scenarios.Evaluation on three long video understanding and two streaming video questionanswering benchmarks shows that StreamMem achieves state-of-the-art performancein query-agnostic KV cache compression and is competitive with query-awarecompression approaches.</description><author>Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren</author><pubDate>Thu, 21 Aug 2025 16:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15717v1</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title><link>http://arxiv.org/abs/2508.15690v1</link><description>GRAFT is a structured multimodal benchmark for evaluating models oninstruction-following, visual reasoning, and visual-textual alignment tasks. Itfeatures programmatically generated charts and synthetically rendered tables,created with Python visualization libraries to ensure control over datasemantics, structure, and clarity. Each GRAFT instance pairs a chart or tableimage with a systematically generated, multi-step analytical question basedsolely on visual content. Answers are provided in structured formats such asJSON or YAML, supporting consistent evaluation of both reasoning and outputformat. The benchmark introduces a taxonomy of reasoning types includingcomparison, trend identification, ranking, aggregation, proportion estimation,and anomaly detection to enable comprehensive assessment. Reference answersfollow strict factual and formatting guidelines for precise, aspect-basedevaluation. GRAFT offers a unified, scalable framework for fine-grainedbenchmarking of multimodal models on visually grounded, structured reasoningtasks, setting a new evaluation standard in this field.</description><author>Abhigya Verma, Sriram Puttagunta, Seganrasan Subramanian, Sravan Ramachandran</author><pubDate>Thu, 21 Aug 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15690v1</guid></item><item><title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title><link>http://arxiv.org/abs/2508.15641v1</link><description>Understanding videos requires more than answering open ended questions, itdemands the ability to pinpoint when events occur and how entities interactacross time. While recent Video LLMs have achieved remarkable progress inholistic reasoning, they remain coarse in temporal perception: timestamps areencoded only implicitly, frame level features are weak in capturing continuity,and language vision alignment often drifts from the entities of interest. Inthis paper, we present Grounded VideoDiT, a Video LLM designed to overcomethese limitations by introducing three key innovations. First, a DiffusionTemporal Latent (DTL) encoder enhances boundary sensitivity and maintainstemporal consistency. Second, object grounded representations explicitly bindquery entities to localized visual evidence, strengthening alignment. Third, amixed token scheme with discrete temporal tokens provides explicit timestampmodeling, enabling fine grained temporal reasoning. Together, these designsequip Grounded VideoDiT with robust grounding capabilities, as validated bystate of the art results on Charades STA, NExT GQA, and multiple VideoQAbenchmarks.</description><author>Pengcheng Fang, Yuxia Chen, Rui Guo</author><pubDate>Thu, 21 Aug 2025 15:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15641v1</guid></item><item><title>Using a cognitive architecture to consider antiBlackness in design and development of AI systems</title><link>http://arxiv.org/abs/2207.00644v3</link><description>How might we use cognitive modeling to consider the ways in whichantiblackness, and racism more broadly, impact the design and development of AIsystems? We provide a discussion and an example towards an answer to thisquestion. We use the ACT-R/{\Phi} cognitive architecture and an existingknowledge graph system, ConceptNet, to consider this question not only from acognitive and sociocultural perspective, but also from a physiologicalperspective. In addition to using a cognitive modeling as a means to explorehow antiblackness may manifest in the design and development of AI systems(particularly from a software engineering perspective), we also introduceconnections between antiblackness, the Human, and computational cognitivemodeling. We argue that the typical eschewing of sociocultural processes andknowledge structures in cognitive architectures and cognitive modelingimplicitly furthers a colorblind approach to cognitive modeling and hidessociocultural context that is always present in human behavior and affectscognitive processes.</description><author>Christopher L. Dancy</author><pubDate>Thu, 21 Aug 2025 12:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00644v3</guid></item><item><title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title><link>http://arxiv.org/abs/2508.14052v2</link><description>Accurate information retrieval (IR) is critical in the financial domain,where investors must identify relevant information from large collections ofdocuments. Traditional IR methods-whether sparse or dense-often fall short inretrieval accuracy, as it requires not only capturing semantic similarity butalso performing fine-grained reasoning over document structure anddomain-specific knowledge. Recent advances in large language models (LLMs) haveopened up new opportunities for retrieval with multi-step reasoning, where themodel ranks passages through iterative reasoning about which information ismost relevant to a given query. However, there exists no benchmark to evaluatesuch capabilities in the financial domain. To address this gap, we introduceFinAgentBench, the first large-scale benchmark for evaluating retrieval withmulti-step reasoning in finance -- a setting we term agentic retrieval. Thebenchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firmsand assesses whether LLM agents can (1) identify the most relevant documenttype among candidates, and (2) pinpoint the key passage within the selecteddocument. Our evaluation framework explicitly separates these two reasoningsteps to address context limitations. This design enables to provide aquantitative basis for understanding retrieval-centric LLM behavior in finance.We evaluate a suite of state-of-the-art models and further demonstrated howtargeted fine-tuning can significantly improve agentic retrieval performance.Our benchmark provides a foundation for studying retrieval-centric LLM behaviorin complex, domain-specific tasks for finance. We will release the datasetpublicly upon acceptance of the paper and plan to expand and share dataset forthe full S&amp;P 500 and beyond.</description><author>Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</author><pubDate>Thu, 21 Aug 2025 09:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14052v2</guid></item><item><title>Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering</title><link>http://arxiv.org/abs/2503.14957v2</link><description>We introduce \dataset (Procedural Knowledge Reasoning Question Answering), anew benchmark for question answering over procedural tasks that requirestructured reasoning. PKR-QA is constructed semi-automatically using aprocedural knowledge graph (PKG), which encodes task-specific knowledge acrossdiverse domains. The PKG is built by curating and linking information from theCOIN instructional video dataset and the ontology, enriched with commonsenseknowledge from ConceptNet and structured outputs from Large Language Models(LLMs), followed by manual verification. To generate question-answer pairs, wedesign graph traversal templates where each template is applied systematicallyover PKG. To enable interpretable reasoning, we propose a neurosymbolicapproach called Knowledge Module Learning (KML), which learns proceduralrelations via neural modules and composes them for structured reasoning withLLMs. Experiments demonstrate that this paradigm improves reasoning performanceon our dataset and enables step-by-step reasoning traces that facilitateinterpretability. Our theoretical analysis on KML learning shows that ourtrained models satisfy near optimal conditions for learning KG relations asneural network mapping models. Code and dataset will be released soon.</description><author>Thanh-Son Nguyen, Hong Yang, Tzeh Yuan Neoh, Hao Zhang, Ee Yeo Keat, Basura Fernando</author><pubDate>Thu, 21 Aug 2025 09:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.14957v2</guid></item><item><title>Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering</title><link>http://arxiv.org/abs/2502.11491v2</link><description>Large language models (LLMs) have shown remarkable capabilities in naturallanguage processing. However, in knowledge graph question answering tasks(KGQA), there remains the issue of answering questions that require multi-hopreasoning. Existing methods rely on entity vector matching, but the purpose ofthe question is abstract and difficult to match with specific entities. As aresult, it is difficult to establish reasoning paths to the purpose, whichleads to information loss and redundancy. To address this issue, inspired byhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), anovel framework that constructs reasoning paths from purposes back toconditions. ORT operates in three key phases: (1) using LLM to extract purposelabels and condition labels, (2) constructing label reasoning paths based onthe KG ontology, and (3) using the label reasoning paths to guide knowledgeretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achievesstate-of-the-art performance and significantly enhances the capability of LLMsfor KGQA.</description><author>Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin</author><pubDate>Thu, 21 Aug 2025 07:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.11491v2</guid></item><item><title>TComQA: Extracting Temporal Commonsense from Text</title><link>http://arxiv.org/abs/2508.15274v1</link><description>Understanding events necessitates grasping their temporal context, which isoften not explicitly stated in natural language. For example, it is not atrivial task for a machine to infer that a museum tour may last for a fewhours, but can not take months. Recent studies indicate that even advancedlarge language models (LLMs) struggle in generating text that require reasoningwith temporal commonsense due to its infrequent explicit mention in text.Therefore, automatically mining temporal commonsense for events enables thecreation of robust language models. In this work, we investigate the capacityof LLMs to extract temporal commonsense from text and evaluate multipleexperimental setups to assess their effectiveness. Here, we propose a temporalcommonsense extraction pipeline that leverages LLMs to automatically minetemporal commonsense and use it to construct TComQA, a dataset derived fromSAMSum and RealNews corpora. TComQA has been validated through crowdsourcingand achieves over 80\% precision in extracting temporal commonsense. The modeltrained with TComQA also outperforms an LLM fine-tuned on existing dataset oftemporal question answering task.</description><author>Lekshmi R Nair, Arun Sankar, Koninika Pal</author><pubDate>Thu, 21 Aug 2025 06:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15274v1</guid></item><item><title>Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering</title><link>http://arxiv.org/abs/2508.15213v1</link><description>Large Language Models (LLMs) perform well in general QA but often struggle indomain-specific scenarios. Retrieval-Augmented Generation (RAG) introducesexternal knowledge but suffers from hallucinations and latency due to noisyretrievals. Continued pretraining internalizes domain knowledge but is costlyand lacks cross-domain flexibility. We attribute this challenge to thelong-tail distribution of domain knowledge, which leaves partial yet usefulinternal knowledge underutilized. We further argue that knowledge acquisitionshould be progressive, mirroring human learning: first understanding concepts,then applying them to complex reasoning. To address this, we propose Selct2Know(S2K), a cost-effective framework that internalizes domain knowledge through aninternal-external knowledge self-selection strategy and selective supervisedfine-tuning. We also introduce a structured reasoning data generation pipelineand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,and financial QA benchmarks show that S2K consistently outperforms existingmethods and matches domain-pretrained LLMs with significantly lower cost.</description><author>Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling</author><pubDate>Thu, 21 Aug 2025 03:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15213v1</guid></item><item><title>LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</title><link>http://arxiv.org/abs/2508.15192v1</link><description>While large language models (LLMs) have shown promise in healthcare, theirapplication for rare medical conditions is still hindered by scarce andunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causingexcessive sweating beyond physiological needs, is one such rare disorder,affecting 2-3% of the population and significantly impacting both physicalcomfort and psychosocial well-being. To date, no work has tailored LLMs toadvance the diagnosis or care of hyperhidrosis. To address this gap, we presentLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy andempathetic hyperhidrosis support. The system follows a three-stage pipeline. Inthe data augmentation stage, a frontier LLM generates medically plausiblesynthetic vignettes from curated open-source data to create a diverse andbalanced question-answer dataset. In the fine-tuning stage, an open-sourcefoundation model is fine-tuned on the dataset to provide diagnosis,personalized treatment recommendations, and empathetic psychological support.In the inference and expert evaluation stage, clinical and psychologicalspecialists assess accuracy, appropriateness, and empathy, with validatedresponses iteratively enriching the dataset. Experiments show that LLM4Sweatoutperforms baselines and delivers the first open-source LLM framework forhyperhidrosis, offering a generalizable approach for other rare diseases withsimilar data and trustworthiness challenges.</description><author>Wenjie Lin, Jin Wei-Kocsis</author><pubDate>Thu, 21 Aug 2025 03:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15192v1</guid></item><item><title>Identifying and Answering Questions with False Assumptions: An Interpretable Approach</title><link>http://arxiv.org/abs/2508.15139v1</link><description>People often ask questions with false assumptions, a type of question thatdoes not have regular answers. Answering such questions require firstidentifying the false assumptions. Large Language Models (LLMs) often generatemisleading answers because of hallucinations. In this paper, we focus onidentifying and answering questions with false assumptions in several domains.We first investigate to reduce the problem to fact verification. Then, wepresent an approach leveraging external evidence to mitigate hallucinations.Experiments with five LLMs demonstrate that (1) incorporating retrievedevidence is beneficial and (2) generating and validating atomic assumptionsyields more improvements and provides an interpretable answer by specifying thefalse assumptions.</description><author>Zijie Wang, Eduardo Blanco</author><pubDate>Thu, 21 Aug 2025 00:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15139v1</guid></item><item><title>LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text</title><link>http://arxiv.org/abs/2508.15085v1</link><description>LongRecall. The completeness of machine-generated text, ensuring that itcaptures all relevant information, is crucial in domains such as medicine andlaw and in tasks like list-based question answering (QA), where omissions canhave serious consequences. However, existing recall metrics often depend onlexical overlap, leading to errors with unsubstantiated entities andparaphrased answers, while LLM-as-a-Judge methods with long holistic promptscapture broader semantics but remain prone to misalignment and hallucinationswithout structured verification. We introduce LongRecall, a general three-stagerecall evaluation framework that decomposes answers into self-contained facts,successively narrows plausible candidate matches through lexical and semanticfiltering, and verifies their alignment through structured entailment checks.This design reduces false positives and false negatives while accommodatingdiverse phrasings and contextual variations, serving as a foundational buildingblock for systematic recall assessment. We evaluate LongRecall on threechallenging long-form QA benchmarks using both human annotations and LLM-basedjudges, demonstrating substantial improvements in recall accuracy over stronglexical and LLM-as-a-Judge baselines.</description><author>MohamamdJavad Ardestani, Ehsan Kamalloo, Davood Rafiei</author><pubDate>Wed, 20 Aug 2025 21:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15085v1</guid></item><item><title>Don't Think Twice! Over-Reasoning Impairs Confidence Calibration</title><link>http://arxiv.org/abs/2508.15050v1</link><description>Large Language Models deployed as question answering tools require robustcalibration to avoid overconfidence. We systematically evaluate how reasoningcapabilities and budget affect confidence assessment accuracy, using theClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetaryhealth. Our key finding challenges the "test-time scaling" paradigm: whilerecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,increasing reasoning budgets consistently impairs rather than improvescalibration. Extended reasoning leads to systematic overconfidence that worsenswith longer thinking budgets, producing diminishing and negative returns beyondmodest computational investments. Conversely, search-augmented generationdramatically outperforms pure reasoning, achieving 89.3% accuracy by retrievingrelevant evidence. Our results suggest that information access, rather thanreasoning depth or inference budget, may be the critical bottleneck forimproved confidence calibration of knowledge-intensive tasks.</description><author>Romain Lacombe, Kerrie Wu, Eddie Dilworth</author><pubDate>Wed, 20 Aug 2025 20:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15050v1</guid></item><item><title>PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data</title><link>http://arxiv.org/abs/2502.20616v2</link><description>Personalization is critical in AI assistants, particularly in the context ofprivate AI models that work with individual users. A key scenario in thisdomain involves enabling AI models to access and interpret a user's privatedata (e.g., conversation history, user-AI interactions, app usage) tounderstand personal details such as biographical information, preferences, andsocial connections. However, due to the sensitive nature of such data, thereare no publicly available datasets that allow us to assess an AI model'sability to understand users through direct access to personal information. To address this gap, we introduce a synthetic data generation pipeline thatcreates diverse, realistic user profiles and private documents simulating humanactivities. Leveraging this synthetic data, we present PersonaBench, abenchmark designed to evaluate AI models' performance in understanding personalinformation derived from simulated private user data. We evaluate Retrieval-Augmented Generation (RAG) pipelines using questionsdirectly related to a user's personal information, supported by the relevantprivate documents provided to the models. Our results reveal that currentretrieval-augmented AI models struggle to answer private questions byextracting personal information from user documents, highlighting the need forimproved methodologies to enhance personalization capabilities in AI.</description><author>Juntao Tan, Liangwei Yang, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Tulika Manoj Awalgaonkar, Jianguo Zhang, Weiran Yao, Ming Zhu, Shirley Kokane, Silvio Savarese, Huan Wang, Caiming Xiong, Shelby Heinecke</author><pubDate>Wed, 20 Aug 2025 18:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.20616v2</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v2</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</author><pubDate>Thu, 21 Aug 2025 18:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v2</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model</title><link>http://arxiv.org/abs/2312.11370v2</link><description>Large language models (LLMs) have shown remarkable proficiency in human-levelreasoning and generation capabilities, which encourages extensive research ontheir application in mathematical problem solving. However, current work hasbeen largely focused on text-based mathematical problems, with limitedinvestigation in problems involving geometric information. Addressing this gap,we aim to enable LLMs to solve geometric problems by understanding image input.We first analyze the limitations of current Multimodal Large Language Models(MLLMs) in this area: they struggle to accurately comprehending basic geometricelements and their relationships. To overcome these challenges, we takeadvantage of the unique characteristics of geometric problems (such as uniquegeometric logical form, and geometric scalability) and the capacity of thetextual LLMs to build an enriched multimodal geometry dataset based on existingdata. The augmented dataset, Geo170K, contains more than 170K geometricimage-caption and question-answer pairs. Utilizing our constructed Geo170Kdataset, we develop G-LLaVA, which demonstrates exceptional performance insolving geometric problems, significantly outperforming GPT-4-V on theMathVista benchmark with only 7B parameters.</description><author>Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong</author><pubDate>Wed, 20 Aug 2025 15:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11370v2</guid></item><item><title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title><link>http://arxiv.org/abs/2504.19254v3</link><description>Hallucinations are a persistent problem with Large Language Models (LLMs). Asthese models become increasingly used in high-stakes domains, such ashealthcare and finance, the need for effective hallucination detection iscrucial. To this end, we outline a versatile framework for zero-resourcehallucination detection that practitioners can apply to real-world use cases.To achieve this, we adapt a variety of existing uncertainty quantification (UQ)techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,transforming them as necessary into standardized response-level confidencescores ranging from 0 to 1. To enhance flexibility, we propose a tunableensemble approach that incorporates any combination of the individualconfidence scores. This approach enables practitioners to optimize the ensemblefor a specific use case for improved performance. To streamline implementation,the full suite of scorers is offered in this paper's companion Python toolkit,UQLM. To evaluate the performance of the various scorers, we conduct anextensive set of experiments using several LLM question-answering benchmarks.We find that our tunable ensemble typically surpasses its individual componentsand outperforms existing hallucination detection methods. Our resultsdemonstrate the benefits of customized hallucination detection strategies forimproving the accuracy and reliability of LLMs.</description><author>Dylan Bouchard, Mohit Singh Chauhan</author><pubDate>Wed, 20 Aug 2025 14:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19254v3</guid></item><item><title>Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems</title><link>http://arxiv.org/abs/2508.14553v1</link><description>Over time, software systems have reached a level of complexity that makes itdifficult for their developers and users to explain particular decisions madeby them. In this paper, we focus on the explainability of component-basedsystems for Question Answering (QA). These components often conduct processesdriven by AI methods, in which behavior and decisions cannot be clearlyexplained or justified, s.t., even for QA experts interpreting the executedprocess and its results is hard. To address this challenge, we present anapproach that considers the components' input and output data flows as a sourcefor representing the behavior and provide explanations for the components,enabling users to comprehend what happened. In the QA framework used here, thedata flows of the components are represented as SPARQL queries (inputs) and RDFtriples (outputs). Hence, we are also providing valuable insights onverbalization regarding these data types. In our experiments, the approachgenerates explanations while following template-based settings (baseline) orvia the use of Large Language Models (LLMs) with different configurations(automatic generation). Our evaluation shows that the explanations generatedvia LLMs achieve high quality and mostly outperform template-based approachesaccording to the users' ratings. Therefore, it enables us to automaticallyexplain the behavior and decisions of QA components to humans while using RDFand SPARQL as a context for explanations.</description><author>Dennis Schiese, Aleksandr Perevalov, Andreas Both</author><pubDate>Wed, 20 Aug 2025 09:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14553v1</guid></item><item><title>Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2508.14427v1</link><description>This paper addresses the problems of missing reasoning chains andinsufficient entity-level semantic understanding in large language models whendealing with tasks that require structured knowledge. It proposes a fine-tuningalgorithm framework based on knowledge graph injection. The method builds onpretrained language models and introduces structured graph information forauxiliary learning. A graph neural network is used to encode entities and theirrelations, constructing a graph-based semantic representation. A fusionmechanism is then designed to jointly model the knowledge graph embeddings withthe contextual representations from the language model. To enhance therobustness of knowledge integration, a gating mechanism is introduced todynamically balance the contributions of linguistic semantics and structuralknowledge. This effectively mitigates conflicts between differentrepresentational spaces. During training, a joint loss function is constructedto account for both task performance and structural alignment objectives. Thishelps improve the accuracy of entity prediction and semantic reasoning. Thestudy also includes a series of systematic sensitivity experiments. Itevaluates the effects of learning rate, graph coverage, and structuralperturbations on model performance. The results further validate theeffectiveness and stability of the proposed method across tasks such as entityrecognition, question answering, and language generation. Experimental findingsshow that the proposed structure-aware fine-tuning framework significantlyenhances the model's ability to represent complex semantic units. Itdemonstrates better semantic consistency and contextual logic modeling inscenarios involving structural reasoning and entity extraction.</description><author>Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du</author><pubDate>Wed, 20 Aug 2025 04:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14427v1</guid></item><item><title>A Little Human Data Goes A Long Way</title><link>http://arxiv.org/abs/2410.13098v3</link><description>Faced with an expensive human annotation process, creators of NLP systemsincreasingly turn to synthetic data generation. While this method showspromise, the extent to which synthetic data can replace human annotation ispoorly understood. We investigate the use of synthetic data in FactVerification (FV) and Question Answering (QA) by studying the effects ofincrementally replacing human generated data with synthetic points on eightdiverse datasets. Strikingly, replacing up to 90% of the training data onlymarginally decreases performance, but replacing the final 10% leads to severedeclines. We find that models trained on purely synthetic data can be reliablyimproved by including as few as 125 human generated data points. We show thatmatching the performance gain of just a little additional human data (only 200points) requires an order of magnitude more synthetic data and estimate priceratios at which human annotation would be a more cost-effective solution. Ourresults suggest that even when human annotation at scale is infeasible, thereis great value to having a small proportion of the dataset being humangenerated.</description><author>Dhananjay Ashok, Jonathan May</author><pubDate>Wed, 20 Aug 2025 01:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13098v3</guid></item><item><title>PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</title><link>http://arxiv.org/abs/2508.16439v1</link><description>Large language models (LLMs) and vision-augmented LLMs (VLMs) havesignificantly advanced medical informatics, diagnostics, and decision support.However, these models exhibit systematic biases, particularly age bias,compromising their reliability and equity. This is evident in their poorerperformance on pediatric-focused text and visual question-answering tasks. Thisbias reflects a broader imbalance in medical research, where pediatric studiesreceive less funding and representation despite the significant disease burdenin children. To address these issues, a new comprehensive multi-modal pediatricquestion-answering benchmark, PediatricsMQA, has been introduced. It consistsof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatrictopics across seven developmental stages (prenatal to adolescent) and 2,067vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256anatomical regions. The dataset was developed using a hybrid manual-automaticpipeline, incorporating peer-reviewed pediatric literature, validated questionbanks, existing benchmarks, and existing QA resources. Evaluatingstate-of-the-art open models, we find dramatic performance drops in youngercohorts, highlighting the need for age-aware methods to ensure equitable AIsupport in pediatric care.</description><author>Adil Bahaj, Mounir Ghogho</author><pubDate>Fri, 22 Aug 2025 14:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16439v1</guid></item><item><title>Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish</title><link>http://arxiv.org/abs/2508.16431v1</link><description>We introduce Cetvel, a comprehensive benchmark designed to evaluate largelanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lackeither task diversity or culturally relevant content, or both. Cetvel addressesthese gaps by combining a broad range of both discriminative and generativetasks ensuring content that reflects the linguistic and cultural richness ofTurkish language. Cetvel covers 23 tasks grouped into seven categories,including tasks such as grammatical error correction, machine translation, andquestion answering rooted in Turkish history and idiomatic language. Weevaluate 33 open-weight LLMs (up to 70B parameters) covering different modelfamilies and instruction paradigms. Our experiments reveal that Turkish-centricinstruction-tuned models generally underperform relative to multilingual orgeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored forthe language. Moreover, we show that tasks such as grammatical error correctionand extractive question answering are particularly discriminative indifferentiating model capabilities. Cetvel offers a comprehensive andculturally grounded evaluation suite for advancing the development andassessment of LLMs in Turkish.</description><author>Yakup Abrek Er, Ilker Kesen, Gözde Gül Şahin, Aykut Erdem</author><pubDate>Fri, 22 Aug 2025 14:42:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16431v1</guid></item><item><title>RoMedQA: The First Benchmark for Romanian Medical Question Answering</title><link>http://arxiv.org/abs/2508.16390v1</link><description>Question answering (QA) is an actively studied topic, being a core naturallanguage processing (NLP) task that needs to be addressed before achievingArtificial General Intelligence (AGI). However, the lack of QA datasets inspecific domains and languages hinders the development of robust AI models ableto generalize across various domains and languages. To this end, we introduceRoMedQA, the first Romanian QA benchmark for the medical domain, alongside acomprehensive evaluation of state-of-the-art large language models (LLMs). Weconstruct a high-quality and large-scale dataset comprising 102,646 QA pairsrelated to cancer patients. The questions regard medical case summaries of1,011 patients, requiring either keyword extraction or reasoning to be answeredcorrectly. RoMedQA is the result of a time-consuming manual annotation processcarried out by seven physicians specialized in oncology or radiotherapy, whospent a total of about 2,100 work hours to generate the QA pairs. We experimentwith four LLMs from distinct families of models on RoMedQA. Each model isemployed in two scenarios, namely one based on zero-shot prompting and onebased on supervised fine-tuning. Our results show that fine-tuned modelssignificantly outperform their zero-shot counterparts, clearly indicating thatpretrained models fail to generalize on RoMedQA. Our findings demonstrate theimportance of both domain-specific and language-specific fine-tuning forreliable clinical QA in Romanian. We publicly release our dataset and code athttps://github.com/ana-rogoz/RoMedQA.</description><author>Ana-Cristina Rogoz, Radu Tudor Ionescu, Alexandra-Valentina Anghel, Ionut-Lucian Antone-Iordache, Simona Coniac, Andreea Iuliana Ionescu</author><pubDate>Fri, 22 Aug 2025 13:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16390v1</guid></item><item><title>MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains</title><link>http://arxiv.org/abs/2508.18260v1</link><description>Large reasoning models (LRMs) have shown significant progress in test-timescaling through chain-of-thought prompting. Current approaches like search-o1integrate retrieval augmented generation (RAG) into multi-step reasoningprocesses but rely on a single, linear reasoning chain while incorporatingunstructured textual information in a flat, context-agnostic manner. As aresult, these approaches can lead to error accumulation throughout thereasoning chain, which significantly limits its effectiveness in medicalquestion-answering (QA) tasks where both accuracy and traceability are criticalrequirements. To address these challenges, we propose MIRAGE (Multi-chainInference with Retrieval-Augmented Graph Exploration), a novel test-timescalable reasoning framework that performs dynamic multi-chain inference overstructured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complexqueries into entity-grounded sub-questions, 2) executes parallel inferencechains, 3) retrieves evidence adaptively via neighbor expansion and multi-hoptraversal, and 4) integrates answers using cross-chain verification to resolvecontradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,Tree-of-Thought variants, and other retrieval-augmented baselines in bothautomatic and human evaluations. Additionally, MIRAGE improves interpretabilityby generating explicit reasoning chains that trace each factual claim toconcrete chains within the knowledge graph, making it well-suited for complexmedical reasoning scenarios. The code will be available for further research.</description><author>Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong</author><pubDate>Mon, 25 Aug 2025 17:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18260v1</guid></item><item><title>Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering</title><link>http://arxiv.org/abs/2508.18093v1</link><description>We present a case study evaluating large language models (LLMs) with128K-token context windows on a technical question answering (QA) task. Ourbenchmark is built on a user manual for an agricultural machine, available inEnglish, French, and German. It simulates a cross-lingual information retrievalscenario where questions are posed in English against all three languageversions of the manual. The evaluation focuses on realistic"needle-in-a-haystack" challenges and includes unanswerable questions to testfor hallucinations. We compare nine long-context LLMs using direct promptingagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for thisspecific manual show that Hybrid RAG consistently outperforms directlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.57B achieve high accuracy (over 85%) across all languages with RAG. This papercontributes a detailed analysis of LLM performance in a specialized industrialdomain and an open framework for similar evaluations, highlighting practicaltrade-offs and challenges.</description><author>Julius Gun, Timo Oksanen</author><pubDate>Mon, 25 Aug 2025 14:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18093v1</guid></item><item><title>See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops</title><link>http://arxiv.org/abs/2508.17932v1</link><description>Human video comprehension demonstrates dynamic coordination between reasoningand visual attention, adaptively focusing on query-relevant details. However,current long-form video question answering systems employ rigid pipelines thatdecouple reasoning from perception, leading to either information loss throughpremature visual abstraction or computational inefficiency through exhaustiveprocessing. The core limitation lies in the inability to adapt visualextraction to specific reasoning requirements, different queries demandfundamentally different visual evidence from the same video content. In thiswork, we present CAVIA, a training-free framework that revolutionizes videounderstanding through reasoning, perception coordination. Unlike conventionalapproaches where visual processing operates independently of reasoning, CAVIAcreates a closed-loop system where reasoning continuously guides visualextraction based on identified information gaps. CAVIA introduces threeinnovations: (1) hierarchical reasoning, guided localization to precise frames;(2) cross-modal semantic bridging for targeted extraction; (3)confidence-driven iterative synthesis. CAVIA achieves state-of-the-artperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamicreasoning-perception coordination provides a scalable paradigm for videounderstanding.</description><author>Zixuan Dong, Baoyun Peng, Yufei Wang, Lin Liu, Xinxin Dong, Yunlong Cao, Xiaodong Wang</author><pubDate>Mon, 25 Aug 2025 12:00:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17932v1</guid></item><item><title>The Ramon Llull's Thinking Machine for Automated Ideation</title><link>http://arxiv.org/abs/2508.19200v1</link><description>This paper revisits Ramon Llull's Ars combinatoria - a medieval framework forgenerating knowledge through symbolic recombination - as a conceptualfoundation for building a modern Llull's thinking machine for researchideation. Our approach defines three compositional axes: Theme (e.g.,efficiency, adaptivity), Domain (e.g., question answering, machinetranslation), and Method (e.g., adversarial training, linear attention). Theseelements represent high-level abstractions common in scientific work -motivations, problem settings, and technical approaches - and serve as buildingblocks for LLM-driven exploration. We mine elements from human experts orconference papers and show that prompting LLMs with curated combinationsproduces research ideas that are diverse, relevant, and grounded in currentliterature. This modern thinking machine offers a lightweight, interpretabletool for augmenting scientific creativity and suggests a path towardcollaborative ideation between humans and AI.</description><author>Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu</author><pubDate>Tue, 26 Aug 2025 17:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19200v1</guid></item><item><title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2505.24073v2</link><description>Large Vision-Language Models (LVLMs) have made remarkable strides inmultimodal tasks such as visual question answering, visual grounding, andcomplex reasoning. However, they remain limited by static training data,susceptibility to hallucinations, and inability to verify claims againstup-to-date, external evidence, compromising their performance in dynamicreal-world applications. Retrieval-Augmented Generation (RAG) offers apractical solution to mitigate these challenges by allowing the LVLMs to accesslarge-scale knowledge databases via retrieval mechanisms, thereby groundingmodel outputs in factual, contextually relevant information. Here in thispaper, we conduct the first systematic dissection of the multimodal RAGpipeline for LVLMs, explicitly investigating (1) the retrieval phase: on themodality configurations and retrieval strategies, (2) the re-ranking stage: onstrategies to mitigate positional biases and improve the relevance of retrievedevidence, and (3) the generation phase: we further investigate how to bestintegrate retrieved candidates into the final generation process. Finally, weextend to explore a unified agentic framework that integrates re-ranking andgeneration through self-reflection, enabling LVLMs to select relevant evidenceand suppress irrelevant context dynamically. Our full-stack exploration of RAGfor LVLMs yields substantial insights, resulting in an average performanceboost of 5% without any fine-tuning.</description><author>Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Suofei Feng, Ryan Rossi, Zhengzhong Tu</author><pubDate>Tue, 26 Aug 2025 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24073v2</guid></item><item><title>Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis</title><link>http://arxiv.org/abs/2406.12719v4</link><description>Large Language Models (LLMs), already shown to ace various unstructured textcomprehension tasks, have also remarkably been shown to tackle table(structured) comprehension tasks without specific training. Building on earlierstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),model scale, instruction tuning, and domain bias affect Tabular QA (TQA)robustness by testing LLMs, under diverse augmentations and perturbations, ondiverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$,and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newerLLMs deliver stronger, more robust TQA performance, data contamination andreliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through anin-depth attention analysis, we reveal a strong correlation betweenperturbation-induced shifts in attention dispersion and the drops inperformance, with sensitivity peaking in the model's middle layers. Wehighlight the need for improved interpretable methodologies to develop morereliable LLMs for table comprehension. Through an in-depth attention analysis,we reveal a strong correlation between perturbation-induced shifts in attentiondispersion and performance drops, with sensitivity peaking in the model'smiddle layers. Based on these findings, we argue for the development ofstructure-aware self-attention mechanisms and domain-adaptive processingtechniques to improve the transparency, generalization, and real-worldreliability of LLMs on tabular data.</description><author>Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao</author><pubDate>Tue, 26 Aug 2025 15:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12719v4</guid></item><item><title>Trustworthy Agents for Electronic Health Records through Confidence Estimation</title><link>http://arxiv.org/abs/2508.19096v1</link><description>Large language models (LLMs) show promise for extracting information fromElectronic Health Records (EHR) and supporting clinical decisions. However,deployment in clinical settings faces challenges due to hallucination risks. Wepropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metricquantifying the accuracy-reliability trade-off at varying confidencethresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporatingstepwise confidence estimation for clinical question answering. Experiments onMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines understrict reliability constraints, achieving improvements of 44.23%p and 25.34%pat HCAcc@70% while baseline methods fail at these thresholds. These resultshighlight limitations of traditional accuracy metrics in evaluating healthcareAI agents. Our work contributes to developing trustworthy clinical agents thatdeliver accurate information or transparently express uncertainty whenconfidence is low.</description><author>Yongwoo Song, Minbyul Jeong, Mujeen Sung</author><pubDate>Tue, 26 Aug 2025 14:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19096v1</guid></item><item><title>Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</title><link>http://arxiv.org/abs/2508.03337v4</link><description>The practical application of Multimodal Large Language Models (MLLMs) toVideo Question Answering (Video-QA) is severely hindered by the high token costof processing numerous video frames. While increasing the number of sampledframes is a common strategy, we observe a "less is more" phenomenon whereexcessive frames can paradoxically degrade performance due to context dilution.Concurrently, state-of-the-art keyframe selection methods, while effective,still yield significant temporal redundancy, which we term 'visual echoes'. Toaddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novelpost-processing method that intelligently prunes the selected keyframes. AFPemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 andCLIP feature space to identify and merge these echoes into singlerepresentatives. To compensate for information loss, we then introduce alightweight, text-based semantic graph that provides critical context withminimal token overhead. Conducting extensive experiments on the LongVideoBenchand VideoMME benchmarks across multiple leading MLLMs, our full approachdemonstrates a drastic reduction in required frames by up to 86.9% and totalinput tokens by up to 83.2%. Crucially, by providing a concise, high-qualityset of frames, our method not only enhances efficiency but often improvesaccuracy over baselines that use more frames. The code will be released uponpublication.</description><author>Shaoguang Wang, Ziyang Chen, Yijie Xu, Weiyu Guo, Hui Xiong</author><pubDate>Tue, 26 Aug 2025 14:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03337v4</guid></item><item><title>MovieCORE: COgnitive REasoning in Movies</title><link>http://arxiv.org/abs/2508.19026v1</link><description>This paper introduces MovieCORE, a novel video question answering (VQA)dataset designed to probe deeper cognitive understanding of movie content.Unlike existing datasets that focus on surface-level comprehension, MovieCOREemphasizes questions that engage System-2 thinking while remaining specific tothe video material. We present an innovative agentic brainstorming approach,utilizing multiple large language models (LLMs) as thought agents to generateand refine high-quality question-answer pairs. To evaluate dataset quality, wedevelop a set of cognitive tests assessing depth, thought-provocationpotential, and syntactic complexity. We also propose a comprehensive evaluationscheme for assessing VQA model performance on deeper cognitive tasks. Toaddress the limitations of existing video-language models (VLMs), we introducean agentic enhancement module, Agentic Choice Enhancement (ACE), which improvesmodel reasoning capabilities post-training by up to 25%. Our work contributesto advancing movie understanding in AI systems and provides valuable insightsinto the capabilities and limitations of current VQA models when faced withmore challenging, nuanced questions about cinematic content. Our project page,dataset and code can be found athttps://joslefaure.github.io/assets/html/moviecore.html.</description><author>Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu</author><pubDate>Tue, 26 Aug 2025 13:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19026v1</guid></item><item><title>Enhancing Document VQA Models via Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2508.18984v1</link><description>Document Visual Question Answering (Document VQA) must cope with documentsthat span dozens of pages, yet leading systems still concatenate every page orrely on very large vision-language models, both of which are memory-hungry.Retrieval-Augmented Generation (RAG) offers an attractive alternative, firstretrieving a concise set of relevant segments before generating answers fromthis selected evidence. In this paper, we systematically evaluate the impact ofincorporating RAG into Document VQA through different retrieval variants -text-based retrieval using OCR tokens and purely visual retrieval without OCR -across multiple models and benchmarks. Evaluated on the multi-page datasetsMP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the"concatenate-all-pages" baseline by up to +22.5 ANLS, while the visual variantachieves +5.0 ANLS improvement without requiring any text extraction. Anablation confirms that retrieval and reranking components drive most of thegain, whereas the layout-guided chunking strategy - proposed in several recentworks to leverage page structure - fails to help on these datasets. Ourexperiments demonstrate that careful evidence selection consistently boostsaccuracy across multiple model sizes and multi-page benchmarks, underscoringits practical value for real-world Document VQA.</description><author>Eric López, Artemis Llabrés, Ernest Valveny</author><pubDate>Tue, 26 Aug 2025 12:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18984v1</guid></item><item><title>PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations</title><link>http://arxiv.org/abs/2508.18982v1</link><description>Time series forecasting has seen considerable improvement during the lastyears, with transformer models and large language models driving advancementsof the state of the art. Modern forecasting models are generally opaque and donot provide explanations for their forecasts, while well-known post-hocexplainability methods like LIME are not suitable for the forecasting context.We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time seriesforecasting models and their forecasts. Our method is based on localized inputperturbations and results in multi-granular explanations. Further, it is ableto characterize cross-channel correlations for multivariate time seriesforecasts. We clearly outline the algorithmic procedure behind PAX-TS,demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,compare it with two other state-of-the-art explanation algorithms, and presentthe different explanation types of the method. We found that the explanationsof high-performing and low-performing algorithms differ on the same datasets,highlighting that the explanations of PAX-TS effectively capture a model'sbehavior. Based on time step correlation matrices resulting from the benchmark,we identify 6 classes of patterns that repeatedly occur across differentdatasets and algorithms. We found that the patterns are indicators ofperformance, with noticeable differences in forecasting error between theclasses. Lastly, we outline a multivariate example where PAX-TS demonstrateshow the forecasting model takes cross-channel correlations into account. WithPAX-TS, time series forecasting models' mechanisms can be illustrated indifferent levels of detail, and its explanations can be used to answerpractical questions on forecasts.</description><author>Tim Kreuzer, Jelena Zdravkovic, Panagiotis Papapetrou</author><pubDate>Tue, 26 Aug 2025 12:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18982v1</guid></item><item><title>Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs</title><link>http://arxiv.org/abs/2508.17334v2</link><description>We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)on cricket scorecards, designed to evaluate large vision-language models(LVLMs) on complex numerical and cross-lingual reasoning over semi-structuredtabular images. MMCRICBENCH-3K comprises 1,463 synthetically generatedscorecard images from ODI, T20, and Test formats, accompanied by 1,500 EnglishQA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring Englishscorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindiscorecards, with all questions and answers kept in English to enable controlledcross-script evaluation. The task demands reasoning over structured numericaldata, multi-image context, and implicit domain knowledge. Empirical resultsshow that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggleon the English subset despite it being their primary training language andexhibit a further drop in performance on the Hindi subset. This reveals keylimitations in structure-aware visual text understanding, numerical reasoning,and cross-lingual generalization. The dataset is publicly available via HuggingFace at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLMresearch in this direction.</description><author>Somraj Gautam, Abhirama Subramanyam Penamakuri, Abhishek Bhandari, Gaurav Harit</author><pubDate>Tue, 26 Aug 2025 12:16:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17334v2</guid></item><item><title>From Confidence to Collapse in LLM Factual Robustness</title><link>http://arxiv.org/abs/2508.16267v2</link><description>Ensuring the robustness of factual knowledge in LLMs is critical for reliableapplications in tasks such as question answering and reasoning. However,existing evaluation methods predominantly focus on performance-based metrics,often investigating from the perspective of prompt perturbations, whichcaptures only the externally triggered side of knowledge robustness. To bridgethis gap, we introduce a principled approach to measure factual robustness fromthe perspective of the generation process by analyzing token distributionentropy in combination with temperature scaling sensitivity. These two factorsbuild the Factual Robustness Score (FRS), a novel metric which quantifies thestability of a fact against perturbations in decoding conditions, given itsinitial uncertainty. To validate our approach, we conduct extensive experimentson 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). Weshow that factual robustness varies significantly -- smaller models report anFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ underincreased uncertainty. These insights demonstrate how entropy and temperaturescaling impact factual accuracy, and lay a foundation for developing morerobust knowledge retention and retrieval in future models.</description><author>Alina Fastowski, Bardh Prenkaj, Gjergji Kasneci</author><pubDate>Tue, 26 Aug 2025 11:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16267v2</guid></item><item><title>ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</title><link>http://arxiv.org/abs/2508.18190v2</link><description>Semi-structured tables, widely used in real-world applications (e.g.,financial reports, medical records, transactional orders), often involveflexible and complex layouts (e.g., hierarchical headers and merged cells).These tables generally rely on human analysts to interpret table layouts andanswer relevant natural language questions, which is costly and inefficient. Toautomate the procedure, existing methods face significant challenges. First,methods like NL2SQL require converting semi-structured tables into structuredones, which often causes substantial information loss. Second, methods likeNL2Code and multi-modal LLM QA struggle to understand the complex layouts ofsemi-structured tables and cannot accurately answer corresponding questions. Tothis end, we propose ST-Raptor, a tree-based framework for semi-structuredtable question answering using large language models. First, we introduce theHierarchical Orthogonal Tree (HO-Tree), a structural model that capturescomplex semi-structured table layouts, along with an effective algorithm forconstructing the tree. Second, we define a set of basic tree operations toguide LLMs in executing common QA tasks. Given a user question, ST-Raptordecomposes it into simpler sub-questions, generates corresponding treeoperation pipelines, and conducts operation-table alignment for accuratepipeline execution. Third, we incorporate a two-stage verification mechanism:forward validation checks the correctness of execution steps, while backwardvalidation evaluates answer reliability by reconstructing queries frompredicted answers. To benchmark the performance, we present SSTQA, a dataset of764 questions over 102 real-world semi-structured tables. Experiments show thatST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The codeis available at https://github.com/weAIDB/ST-Raptor.</description><author>Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu</author><pubDate>Tue, 26 Aug 2025 08:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18190v2</guid></item><item><title>Beyond the Textual: Generating Coherent Visual Options for MCQs</title><link>http://arxiv.org/abs/2508.18772v1</link><description>Multiple-choice questions (MCQs) play a crucial role in fostering deepthinking and knowledge integration in education. However, previous research hasprimarily focused on generating MCQs with textual options, but it largelyoverlooks the visual options. Moreover, generating high-quality distractorsremains a major challenge due to the high cost and limited scalability ofmanual authoring. To tackle these problems, we propose a Cross-modal OptionsSynthesis (CmOS), a novel framework for generating educational MCQs with visualoptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoningprocess and Retrieval-Augmented Generation (RAG) to produce semanticallyplausible and visually similar answer and distractors. It also includes adiscrimination module to identify content suitable for visual options.Experimental results on test tasks demonstrate the superiority of CmOS incontent discrimination, question generation and visual option generation overexisting methods across various subjects and educational levels.</description><author>Wanqiang Wang, Longzhu He, Wei Zheng</author><pubDate>Tue, 26 Aug 2025 07:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18772v1</guid></item><item><title>Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</title><link>http://arxiv.org/abs/2508.03337v5</link><description>The practical application of Multimodal Large Language Models (MLLMs) toVideo Question Answering (Video-QA) is severely hindered by the high token costof processing numerous video frames. While increasing the number of sampledframes is a common strategy, we observe a "less is more" phenomenon whereexcessive frames can paradoxically degrade performance due to context dilution.Concurrently, state-of-the-art keyframe selection methods, while effective,still yield significant temporal redundancy, which we term 'visual echoes'. Toaddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novelpost-processing method that intelligently prunes the selected keyframes. AFPemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 andCLIP feature space to identify and merge these echoes into singlerepresentatives. To compensate for information loss, we then introduce alightweight, text-based semantic graph that provides critical context withminimal token overhead. Conducting extensive experiments on the LongVideoBenchand VideoMME benchmarks across multiple leading MLLMs, our full approachdemonstrates a drastic reduction in required frames by up to 86.9% and totalinput tokens by up to 83.2%. Crucially, by providing a concise, high-qualityset of frames, our method not only enhances efficiency but often improvesaccuracy over baselines that use more frames. The code will be released uponpublication.</description><author>Shaoguang Wang, Ziyang Chen, Yijie Xu, Weiyu Guo, Hui Xiong</author><pubDate>Wed, 27 Aug 2025 01:38:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03337v5</guid></item><item><title>AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering</title><link>http://arxiv.org/abs/2508.20047v2</link><description>We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health QuestionAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-locatedwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabicmedical QA resources by offering two complementary tracks: {MentalQA}, focusingon Arabic mental health Q\&amp;A (e.g., anxiety, depression, stigma reduction), and{MedArabiQ}, covering broader medical domains such as internal medicine,pediatrics, and clinical decision making. Each track comprises multiplesubtasks, evaluation datasets, and standardized metrics, facilitating fairbenchmarking. The task was structured to promote modeling under realistic,multilingual, and culturally nuanced healthcare contexts. We outline thedataset creation, task design and evaluation framework, participationstatistics, baseline systems, and summarize the overall outcomes. We concludewith reflections on the performance trends observed and prospects for futureiterations in Arabic health QA.</description><author>Hassan Alhuzali, Farah Shamout, Muhammad Abdul-Mageed, Chaimae Abouzahir, Mouath Abu-Daoud, Ashwag Alasmari, Walid Al-Eisawi, Renad Al-Monef, Ali Alqahtani, Lama Ayash, Nizar Habash, Leen Kharouf</author><pubDate>Thu, 28 Aug 2025 07:48:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20047v2</guid></item><item><title>GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2410.05229v2</link><description>Recent advancements in Large Language Models (LLMs) have sparked interest intheir formal reasoning capabilities, particularly in mathematics. The GSM8Kbenchmark is widely used to assess the mathematical reasoning of models ongrade-school-level questions. While the performance of LLMs on GSM8K hassignificantly improved in recent years, it remains unclear whether theirmathematical reasoning capabilities have genuinely advanced, raising questionsabout the reliability of the reported metrics. To address these concerns, weconduct a large-scale study on several SOTA open and closed models. To overcomethe limitations of existing evaluations, we introduce GSM-Symbolic, an improvedbenchmark created from symbolic templates that allow for the generation of adiverse set of questions. GSM-Symbolic enables more controllable evaluations,providing key insights and more reliable metrics for measuring the reasoningcapabilities of models.Our findings reveal that LLMs exhibit noticeablevariance when responding to different instantiations of the same question.Specifically, the performance of all models declines when only the numericalvalues in the question are altered in the GSM-Symbolic benchmark. Furthermore,we investigate the fragility of mathematical reasoning in these models and showthat their performance significantly deteriorates as the number of clauses in aquestion increases. We hypothesize that this decline is because current LLMscannot perform genuine logical reasoning; they replicate reasoning steps fromtheir training data. Adding a single clause that seems relevant to the questioncauses significant performance drops (up to 65%) across all state-of-the-artmodels, even though the clause doesn't contribute to the reasoning chain neededfor the final answer. Overall, our work offers a more nuanced understanding ofLLMs' capabilities and limitations in mathematical reasoning.</description><author>Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar</author><pubDate>Wed, 27 Aug 2025 16:24:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05229v2</guid></item><item><title>mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks</title><link>http://arxiv.org/abs/2506.08400v3</link><description>Large Language models (LLMs) have demonstrated impressive performance on awide range of tasks, including in multimodal settings such as speech. However,their evaluation is often limited to English and a few high-resource languages.For low-resource languages, there is no standardized evaluation benchmark. Inthis paper, we address this gap by introducing mSTEB, a new benchmark toevaluate the performance of LLMs on a wide range of tasks covering languageidentification, text classification, question answering, and translation taskson both speech and text modalities. We evaluated the performance of leadingLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art openmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap inperformance between high-resource and low-resource languages, especially forlanguages spoken in Africa and Americas/Oceania. Our findings show that moreinvestment is needed to address their under-representation in LLMs coverage.</description><author>Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, David Ifeoluwa Adelani</author><pubDate>Wed, 27 Aug 2025 15:27:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.08400v3</guid></item><item><title>KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts</title><link>http://arxiv.org/abs/2508.19944v1</link><description>Understanding and reasoning over text within visual contexts poses asignificant challenge for Vision-Language Models (VLMs), given the complexityand diversity of real-world scenarios. To address this challenge, text-richVisual Question Answering (VQA) datasets and benchmarks have emerged forhigh-resource languages like English. However, a critical gap persists forlow-resource languages such as Korean, where the lack of comprehensivebenchmarks hinders robust model evaluation and comparison. To bridge this gap,we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-richVQA Attuned to diverse visual contexts. KRETA facilitates an in-depthevaluation of both visual text understanding and reasoning capabilities, whilealso supporting a multifaceted assessment across 15 domains and 26 image types.Additionally, we introduce a semi-automated VQA generation pipelinespecifically optimized for text-rich settings, leveraging refined stepwiseimage decomposition and a rigorous seven-metric evaluation protocol to ensuredata quality. While KRETA is tailored for Korean, we hope our adaptable andextensible pipeline will facilitate the development of similar benchmarks inother languages, thereby accelerating multilingual VLM research. The code anddataset for KRETA are available at https://github.com/tabtoyou/KRETA.</description><author>Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, Hyunjun Eun</author><pubDate>Wed, 27 Aug 2025 15:01:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19944v1</guid></item><item><title>Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement</title><link>http://arxiv.org/abs/2508.19887v1</link><description>In this paper, we introduce Bangla-Bayanno, an open-ended Visual QuestionAnswering (VQA) Dataset in Bangla, a widely used, low-resource language inmultimodal AI research. The majority of existing datasets are either manuallyannotated with an emphasis on a specific domain, query type, or answer type orare constrained by niche answer formats. In order to mitigate human-inducederrors and guarantee lucidity, we implemented a multilingual LLM-assistedtranslation refinement pipeline. This dataset overcomes the issues oflow-quality translations from multilingual sources. The dataset comprises52,650 question-answer pairs across 4750+ images. Questions are classified intothree distinct answer types: nominal (short descriptive), quantitative(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensiveopen-source, high-quality VQA benchmark in Bangla, aiming to advance researchin low-resource multimodal learning and facilitate the development of moreinclusive AI systems.</description><author>Mohammed Rakibul Hasan, Rafi Majid, Ahanaf Tahmid</author><pubDate>Wed, 27 Aug 2025 13:48:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19887v1</guid></item><item><title>General agents contain world models</title><link>http://arxiv.org/abs/2506.01622v3</link><description>Are world models a necessary ingredient for flexible, goal-directedbehaviour, or is model-free learning sufficient? We provide a formal answer tothis question, showing that any agent capable of generalizing to multi-stepgoal-directed tasks must have learned a predictive model of its environment. Weshow that this model can be extracted from the agent's policy, and thatincreasing the agents performance or the complexity of the goals it can achieverequires learning increasingly accurate world models. This has a number ofconsequences: from developing safe and general agents, to bounding agentcapabilities in complex environments, and providing new algorithms foreliciting world models from agents.</description><author>Jonathan Richens, David Abel, Alexis Bellot, Tom Everitt</author><pubDate>Wed, 27 Aug 2025 13:09:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.01622v3</guid></item><item><title>Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning</title><link>http://arxiv.org/abs/2508.19828v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities acrossa wide range of NLP tasks, but they remain fundamentally stateless, constrainedby limited context windows that hinder long-horizon reasoning. Recent effortsto address this limitation often augment LLMs with an external memory bank, yetmost existing pipelines are static and heuristic-driven, lacking any learnedmechanism for deciding what to store, update, or retrieve. We presentMemory-R1, a reinforcement learning (RL) framework that equips LLMs with theability to actively manage and utilize external memory through two specializedagents: a Memory Manager that learns to perform structured memory operations{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevantentries and reasons over them to produce an answer. Both agents are fine-tunedwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management anduse with minimal supervision. With as few as 152 question-answer pairs and acorresponding temporal memory bank for training, Memory-R1 outperforms the mostcompetitive existing baseline and demonstrates strong generalization acrossdiverse question types and LLM backbones. Beyond presenting an effectiveapproach, this work provides insights into how RL can unlock more agentic,memory-aware behaviors in LLMs, pointing toward richer, more persistentreasoning systems.</description><author>Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Schütze, Volker Tresp, Yunpu Ma</author><pubDate>Wed, 27 Aug 2025 12:26:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19828v1</guid></item><item><title>NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks</title><link>http://arxiv.org/abs/2508.19724v1</link><description>Commonsense visual-question answering often hinges on knowledge that ismissing from the image or the question. Small vision-language models (sVLMs)such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generativecounterparts. To study the effect of careful commonsense knowledge integrationon sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves naturallanguage facts, (ii) prompts an LLM to craft natural language explanations, and(iii) feeds both signals to sVLMs respectively across two commonsense VQAdatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Factsretrieved using a fine-tuned ColBERTv2 and an object information-enrichedprompt yield explanations that largely cut down hallucinations, while liftingthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVAand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2Band SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additionalfinetuning using noise-robust losses (such as symmetric cross entropy andgeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Ourfindings expose when LLM-based commonsense knowledge beats retrieval fromcommonsense knowledge bases, how noise-aware training stabilises small modelsin the context of external knowledge augmentation, and why parameter-efficientcommonsense reasoning is now within reach for 250M models.</description><author>Aritra Dutta, Swapnanil Mukherjee, Deepanway Ghosal, Somak Aditya</author><pubDate>Wed, 27 Aug 2025 09:34:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19724v1</guid></item><item><title>PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</title><link>http://arxiv.org/abs/2508.16439v3</link><description>Large language models (LLMs) and vision-augmented LLMs (VLMs) havesignificantly advanced medical informatics, diagnostics, and decision support.However, these models exhibit systematic biases, particularly age bias,compromising their reliability and equity. This is evident in their poorerperformance on pediatric-focused text and visual question-answering tasks. Thisbias reflects a broader imbalance in medical research, where pediatric studiesreceive less funding and representation despite the significant disease burdenin children. To address these issues, a new comprehensive multi-modal pediatricquestion-answering benchmark, PediatricsMQA, has been introduced. It consistsof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatrictopics across seven developmental stages (prenatal to adolescent) and 2,067vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256anatomical regions. The dataset was developed using a hybrid manual-automaticpipeline, incorporating peer-reviewed pediatric literature, validated questionbanks, existing benchmarks, and existing QA resources. Evaluatingstate-of-the-art open models, we find dramatic performance drops in youngercohorts, highlighting the need for age-aware methods to ensure equitable AIsupport in pediatric care.</description><author>Adil Bahaj, Oumaima Fadi, Mohamed Chetouani, Mounir Ghogho</author><pubDate>Wed, 27 Aug 2025 08:33:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16439v3</guid></item><item><title>CoCoA: Confidence and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models</title><link>http://arxiv.org/abs/2508.17670v2</link><description>Faithful generation in large language models (LLMs) is challenged byknowledge conflicts between parametric memory and external context. Existingcontrastive decoding methods tuned specifically to handle conflict often lackadaptability and can degrade performance in low conflict settings. We introduceCoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-levelalgorithm for principled conflict resolution and enhanced faithfulness. CoCoAresolves conflict by utilizing confidence-aware measures (entropy gap andcontextual peakedness) and the generalized divergence between the parametricand contextual distributions. Crucially, CoCoA maintains strong performanceeven in low conflict settings. Extensive experiments across multiple LLMs ondiverse Question Answering (QA), Summarization, and Long-Form QuestionAnswering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performanceover strong baselines like AdaCAD. It yields significant gains in QA accuracy,up to 9.2 points on average compared to the strong baseline AdaCAD, andimproves factuality in summarization and LFQA by up to 2.5 points on averageacross key benchmarks. Additionally, it demonstrates superior sensitivity toconflict variations. CoCoA enables more informed, context-aware, and ultimatelymore faithful token generation.</description><author>Anant Khandelwal, Manish Gupta, Puneet Agrawal</author><pubDate>Wed, 27 Aug 2025 08:29:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17670v2</guid></item><item><title>Pixel-Optimization-Free Patch Attack on Stereo Depth Estimation</title><link>http://arxiv.org/abs/2506.17632v2</link><description>Stereo Depth Estimation (SDE) is essential for scene perception invision-based systems such as autonomous driving. Prior work shows SDE isvulnerable to pixel-optimization attacks, but these methods are limited todigital, static, and view-specific settings, making them impractical. Thisraises a central question: how to design deployable, adaptive, and transferableattacks under realistic constraints? We present two contributions to answer it.First, we build a unified framework that extends pixel-optimization attacks tofour stereo-matching stages: feature extraction, cost-volume construction, costaggregation, and disparity regression. Through systematic evaluation acrossnine SDE models with realistic constraints like photometric consistency, weshow existing attacks suffer from poor transferability. Second, we proposePatchHunter, the first pixel-optimization-free attack. PatchHunter casts patchgeneration as a search in a structured space of visual patterns that disruptcore SDE assumptions, and uses a reinforcement learning policy to discovereffective and transferable patterns efficiently. We evaluate PatchHunter onthree levels: autonomous driving dataset, high-fidelity simulator, andreal-world deployment. On KITTI, PatchHunter outperforms pixel-level attacks inboth effectiveness and black-box transferability. Tests in CARLA and onvehicles with industrial-grade stereo cameras confirm robustness to physicalvariations. Even under challenging conditions such as low lighting, PatchHunterachieves a D1-all error above 0.4, while pixel-level attacks remain near 0.</description><author>Hangcheng Liu, Xu Kuang, Xingshuo Han, Xingwan Wu, Haoran Ou, Shangwei Guo, Xingyi Huang, Tao Xiang, Tianwei Zhang</author><pubDate>Wed, 27 Aug 2025 03:35:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.17632v2</guid></item><item><title>CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning</title><link>http://arxiv.org/abs/2508.19542v1</link><description>While multimodal large language models (MLLMs) exhibit strong performance onsingle-video tasks (e.g., video question answering), their ability acrossmultiple videos remains critically underexplored. However, this capability isessential for real-world applications, including multi-camera surveillance andcross-video procedural learning. To bridge this gap, we present CVBench, thefirst comprehensive benchmark designed to assess cross-video relationalreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanningthree hierarchical tiers: cross-video object association (identifying sharedentities), cross-video event association (linking temporal or causal eventchains), and cross-video complex reasoning (integrating commonsense and domainknowledge). Built from five domain-diverse video clusters (e.g., sports, liferecords), the benchmark challenges models to synthesise information acrossdynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (includingGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thoughtprompting paradigms. Key findings reveal stark performance gaps: even topmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,compared to the 91% accuracy of human performance. Crucially, our analysisreveals fundamental bottlenecks inherent in current MLLM architectures, notablydeficient inter-video context retention and poor disambiguation of overlappingentities. CVBench establishes a rigorous framework for diagnosing and advancingmulti-video reasoning, offering architectural insights for next-generationMLLMs.The data and evaluation code are available athttps://github.com/Hokhim2/CVBench.</description><author>Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, Xiongfei Yao, Shuaiwei Jiao</author><pubDate>Wed, 27 Aug 2025 03:29:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19542v1</guid></item><item><title>MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning</title><link>http://arxiv.org/abs/2507.16812v2</link><description>Scientific reasoning is critical for developing AI scientists and supportinghuman researchers in advancing the frontiers of natural science discovery.However, the open-source community has primarily focused on mathematics andcoding while neglecting the scientific domain, largely due to the absence ofopen, large-scale, high-quality, verifiable scientific reasoning datasets. Tobridge this gap, we first present TextbookReasoning, an open dataset featuringtruthful reference answers extracted from 12k university-level scientifictextbooks, comprising 650k reasoning questions spanning 7 scientificdisciplines. We further introduce MegaScience, a large-scale mixture ofhigh-quality open-source datasets totaling 1.25 million instances, developedthrough systematic ablation studies that evaluate various data selectionmethodologies to identify the optimal subset for each publicly availablescientific dataset. Meanwhile, we build a comprehensive evaluation systemcovering diverse subjects and question types across 15 benchmarks,incorporating comprehensive answer extraction strategies to ensure accurateevaluation metrics. Our experiments demonstrate that our datasets achievesuperior performance and training efficiency with more concise response lengthscompared to existing open-source scientific datasets. Furthermore, we trainLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, whichsignificantly outperform the corresponding official instruct models in averageperformance. In addition, MegaScience exhibits greater effectiveness for largerand stronger models, suggesting a scaling benefit for scientific tuning. Werelease our data curation pipeline, evaluation system, datasets, and seventrained models to the community to advance scientific reasoning research.</description><author>Run-Ze Fan, Zengzhi Wang, Pengfei Liu</author><pubDate>Wed, 27 Aug 2025 03:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.16812v2</guid></item><item><title>Reference-Aligned Retrieval-Augmented Question Answering over Heterogeneous Proprietary Documents</title><link>http://arxiv.org/abs/2502.19596v5</link><description>Proprietary corporate documents contain rich domain-specific knowledge, buttheir overwhelming volume and disorganized structure make it difficult even foremployees to access the right information when needed. For example, in theautomotive industry, vehicle crash-collision tests, each costing hundreds ofthousands of dollars, produce highly detailed documentation. However,retrieving relevant content during decision-making remains time-consuming dueto the scale and complexity of the material. While Retrieval-AugmentedGeneration (RAG)-based Question Answering (QA) systems offer a promisingsolution, building an internal RAG-QA system poses several challenges: (1)handling heterogeneous multi-modal data sources, (2) preserving dataconfidentiality, and (3) enabling traceability between each piece ofinformation in the generated answer and its original source document. Toaddress these, we propose a RAG-QA framework for internal enterprise use,consisting of: (1) a data pipeline that converts raw multi-modal documents intoa structured corpus and QA pairs, (2) a fully on-premise, privacy-preservingarchitecture, and (3) a lightweight reference matcher that links answersegments to supporting content. Applied to the automotive domain, our systemimproves factual correctness (+1.79, +1.94), informativeness (+1.33, +1.16),and helpfulness (+1.08, +1.67) over a non-RAG baseline, based on 1-5 scaleratings from both human and LLM judge.</description><author>Nayoung Choi, Grace Byun, Andrew Chung, Ellie S. Paek, Shinsun Lee, Jinho D. Choi</author><pubDate>Wed, 27 Aug 2025 03:09:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.19596v5</guid></item><item><title>Caught in the Act: a mechanistic approach to detecting deception</title><link>http://arxiv.org/abs/2508.19505v1</link><description>Sophisticated instrumentation for AI systems might have indicators thatsignal misalignment from human values, not unlike a "check engine" light incars. One such indicator of misalignment is deceptiveness in generatedresponses. Future AI instrumentation may have the ability to detect when an LLMgenerates deceptive responses while reasoning about seemingly plausible butincorrect answers to factual questions. In this work, we demonstrate thatlinear probes on LLMs internal activations can detect deception in theirresponses with extremely high accuracy. Our probes reach a maximum of greaterthan 90% accuracy in distinguishing between deceptive and non-deceptivearguments generated by llama and qwen models ranging from 1.5B to 14Bparameters, including their DeepSeek-r1 finetuned variants. We observe thatprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,while larger models (greater than 7B) reach 70-80%, with their reasoningcounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stagepattern across layers: near-random (50%) in early layers, peaking in middlelayers, and slightly declining in later layers. Furthermore, using an iterativenull space projection approach, we find multitudes of linear directions thatencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B andQwen 14B models.</description><author>Gerard Boxo, Ryan Socha, Daniel Yoo, Shivam Raval</author><pubDate>Wed, 27 Aug 2025 01:29:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19505v1</guid></item><item><title>The Ramon Llull's Thinking Machine for Automated Ideation</title><link>http://arxiv.org/abs/2508.19200v2</link><description>This paper revisits Ramon Llull's Ars combinatoria - a medieval framework forgenerating knowledge through symbolic recombination - as a conceptualfoundation for building a modern Llull's thinking machine for researchideation. Our approach defines three compositional axes: Theme (e.g.,efficiency, adaptivity), Domain (e.g., question answering, machinetranslation), and Method (e.g., adversarial training, linear attention). Theseelements represent high-level abstractions common in scientific work -motivations, problem settings, and technical approaches - and serve as buildingblocks for LLM-driven exploration. We mine elements from human experts orconference papers and show that prompting LLMs with curated combinationsproduces research ideas that are diverse, relevant, and grounded in currentliterature. This modern thinking machine offers a lightweight, interpretabletool for augmenting scientific creativity and suggests a path towardcollaborative ideation between humans and AI.</description><author>Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu</author><pubDate>Thu, 28 Aug 2025 17:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19200v2</guid></item><item><title>ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</title><link>http://arxiv.org/abs/2508.21010v1</link><description>Existing Causal-Why Video Question Answering (VideoQA) models often strugglewith higher-order reasoning, relying on opaque, monolithic pipelines thatentangle video understanding, causal inference, and answer generation. Theseblack-box approaches offer limited interpretability and tend to depend onshallow heuristics. We propose a novel, modular framework that explicitlydecouples causal reasoning from answer generation, introducing natural languagecausal chains as interpretable intermediate representations. Inspired by humancognitive models, these structured cause-effect sequences bridge low-levelvideo content with high-level causal reasoning, enabling transparent andlogically coherent inference. Our two-stage architecture comprises a CausalChain Extractor (CCE) that generates causal chains from video-question pairs,and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded inthese chains. To address the lack of annotated reasoning traces, we introduce ascalable method for generating high-quality causal chains from existingdatasets using large language models. We also propose CauCo, a new evaluationmetric for causality-oriented captioning. Experiments on three large-scalebenchmarks demonstrate that our approach not only outperforms state-of-the-artmodels, but also yields substantial gains in explainability, user trust, andgeneralization -- positioning the CCE as a reusable causal reasoning engineacross diverse domains. Project page:https://paritoshparmar.github.io/chainreaction/</description><author>Paritosh Parmar, Eric Peh, Basura Fernando</author><pubDate>Thu, 28 Aug 2025 17:10:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21010v1</guid></item><item><title>InterAct-Video: Reasoning-Rich Video QA for Urban Traffic</title><link>http://arxiv.org/abs/2507.14743v3</link><description>Traffic monitoring is crucial for urban mobility, road safety, andintelligent transportation systems (ITS). Deep learning has advancedvideo-based traffic monitoring through video question answering (VideoQA)models, enabling structured insight extraction from traffic videos. However,existing VideoQA models struggle with the complexity of real-world trafficscenes, where multiple concurrent events unfold across spatiotemporaldimensions. To address these challenges, this paper introduces \textbf{InterActVideoQA}, a curated dataset designed to benchmark and enhance VideoQA modelsfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours ofreal-world traffic footage collected from diverse intersections, segmented into10-second video clips, with over 25,000 question-answer (QA) pairs coveringspatiotemporal dynamics, vehicle interactions, incident detection, and othercritical traffic attributes. State-of-the-art VideoQA models are evaluated onInterAct VideoQA, exposing challenges in reasoning over fine-grainedspatiotemporal dependencies within complex traffic scenarios. Additionally,fine-tuning these models on InterAct VideoQA yields notable performanceimprovements, demonstrating the necessity of domain-specific datasets forVideoQA. InterAct VideoQA is publicly available as a benchmark dataset tofacilitate future research in real-world deployable VideoQA models forintelligent transportation systems. GitHub Repo:https://github.com/joe-rabbit/InterAct_VideoQA</description><author>Joseph Raj Vishal, Divesh Basina, Rutuja Patil, Manas Srinivas Gowda, Katha Naik, Yezhou Yang, Bharatesh Chakravarthi</author><pubDate>Thu, 28 Aug 2025 16:45:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14743v3</guid></item><item><title>MSRS: Evaluating Multi-Source Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2508.20867v1</link><description>Retrieval-augmented systems are typically evaluated in settings whereinformation required to answer the query can be found within a single source orthe answer is short-form or factoid-based. However, many real-worldapplications demand the ability to integrate and summarize informationscattered across multiple sources, where no single source is sufficient torespond to the user's question. In such settings, the retrieval component of aRAG pipeline must recognize a variety of relevance signals, and the generationcomponent must connect and synthesize information across multiple sources. Wepresent a scalable framework for constructing evaluation benchmarks thatchallenge RAG systems to integrate information across distinct sources andgenerate long-form responses. Using our framework, we build two new benchmarkson Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representingnarrative synthesis and summarization tasks, respectively, that requireretrieval from large collections. Our extensive experiments with various RAGpipelines -- including sparse and dense retrievers combined with frontier LLMs-- reveal that generation quality is highly dependent on retrievaleffectiveness, which varies greatly by task. While multi-source synthesisproves challenging even in an oracle retrieval setting, we find that reasoningmodels significantly outperform standard LLMs at this distinct step.</description><author>Rohan Phanse, Yijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, Arman Cohan</author><pubDate>Thu, 28 Aug 2025 14:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20867v1</guid></item><item><title>JADES: A Universal Framework for Jailbreak Assessment via Decompositional Scoring</title><link>http://arxiv.org/abs/2508.20848v1</link><description>Accurately determining whether a jailbreak attempt has succeeded is afundamental yet unresolved challenge. Existing evaluation methods rely onmisaligned proxy indicators or naive holistic judgments. They frequentlymisinterpret model responses, leading to inconsistent and subjectiveassessments that misalign with human perception. To address this gap, weintroduce JADES (Jailbreak Assessment via Decompositional Scoring), a universaljailbreak evaluation framework. Its key mechanism is to automatically decomposean input harmful question into a set of weighted sub-questions, score eachsub-answer, and weight-aggregate the sub-scores into a final decision. JADESalso incorporates an optional fact-checking module to strengthen the detectionof hallucinations in jailbreak responses. We validate JADES on JailbreakQR, anewly introduced benchmark proposed in this work, consisting of 400 pairs ofjailbreak prompts and responses, each meticulously annotated by humans. In abinary setting (success/failure), JADES achieves 98.5% agreement with humanevaluators, outperforming strong baselines by over 9%. Re-evaluating fivepopular attacks on four LLMs reveals substantial overestimation (e.g., LAA'sattack success rate on GPT-3.5-Turbo drops from 93% to 69%). Our results showthat JADES could deliver accurate, consistent, and interpretable evaluations,providing a reliable basis for measuring future jailbreak attacks.</description><author>Junjie Chu, Mingjie Li, Ziqing Yang, Ye Leng, Chenhao Lin, Chao Shen, Michael Backes, Yun Shen, Yang Zhang</author><pubDate>Thu, 28 Aug 2025 14:40:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20848v1</guid></item><item><title>CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning</title><link>http://arxiv.org/abs/2508.19542v2</link><description>While multimodal large language models (MLLMs) exhibit strong performance onsingle-video tasks (e.g., video question answering), their ability acrossmultiple videos remains critically underexplored. However, this capability isessential for real-world applications, including multi-camera surveillance andcross-video procedural learning. To bridge this gap, we present CVBench, thefirst comprehensive benchmark designed to assess cross-video relationalreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanningthree hierarchical tiers: cross-video object association (identifying sharedentities), cross-video event association (linking temporal or causal eventchains), and cross-video complex reasoning (integrating commonsense and domainknowledge). Built from five domain-diverse video clusters (e.g., sports, liferecords), the benchmark challenges models to synthesise information acrossdynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (includingGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thoughtprompting paradigms. Key findings reveal stark performance gaps: even topmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,compared to the 91% accuracy of human performance. Crucially, our analysisreveals fundamental bottlenecks inherent in current MLLM architectures, notablydeficient inter-video context retention and poor disambiguation of overlappingentities. CVBench establishes a rigorous framework for diagnosing and advancingmulti-video reasoning, offering architectural insights for next-generationMLLMs. The data and evaluation code are available athttps://github.com/Hokhim2/CVBench.</description><author>Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, Xiongfei Yao, Shuaiwei Jiao</author><pubDate>Thu, 28 Aug 2025 14:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19542v2</guid></item><item><title>A Graph-Based Test-Harness for LLM Evaluation</title><link>http://arxiv.org/abs/2508.20810v1</link><description>We present a first known prototype of a dynamic, systematic benchmark ofmedical guidelines for 400+ questions, with 3.3+ trillion possiblecombinations, covering 100\% of guideline relationships. We transformed the WHOIMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,treatments, follow-ups, severities) and 300+ edges, then used graph traversalto generate questions that incorporated age-specific scenarios and contextualdistractors to ensure clinical relevance. Our graph-based approach enablessystematic evaluation across clinical tasks (45-67\% accuracy), and we findmodels excel at symptom recognition but struggle with triaging severity,treatment protocols and follow-up care, demonstrating how customized benchmarkscan identify specific capability gaps that general-domain evaluations miss.Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training(supervised finetuning, GRPO, DPO), where correct answers provide high-rewardsamples without expensive human annotation. The graph-based approachsuccessfully addresses the coverage limitations of manually curated benchmarks.This methodology is a step toward scalable, contamination-resistant solutionfor creating comprehensive benchmarks that can be dynamically generated,including when the guidelines are updated. Code and datasets are available athttps://github.com/jessicalundin/graph_testing_harness</description><author>Jessica Lundin, Guillaume Chabot-Couture</author><pubDate>Thu, 28 Aug 2025 14:10:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20810v1</guid></item><item><title>Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey</title><link>http://arxiv.org/abs/2308.09388v2</link><description>Image restoration (IR) has been an indispensable and challenging task in thelow-level vision field, which strives to improve the subjective quality ofimages distorted by various forms of degradation. Recently, the diffusion modelhas achieved significant advancements in the visual generation of AIGC, therebyraising an intuitive question, "whether diffusion model can boost imagerestoration". To answer this, some pioneering studies attempt to integratediffusion models into the image restoration task, resulting in superiorperformances than previous GAN-based methods. Despite that, a comprehensive andenlightening survey on diffusion model-based image restoration remains scarce.In this paper, we are the first to present a comprehensive review of recentdiffusion model-based methods on image restoration, encompassing the learningparadigm, conditional strategy, framework design, modeling strategy, andevaluation. Concretely, we first introduce the background of the diffusionmodel briefly and then present two prevalent workflows that exploit diffusionmodels in image restoration. Subsequently, we classify and emphasize theinnovative designs using diffusion models for both IR and blind/real-world IR,intending to inspire future development. To evaluate existing methodsthoroughly, we summarize the commonly-used dataset, implementation details, andevaluation metrics. Additionally, we present the objective comparison foropen-sourced methods across three tasks, including image super-resolution,deblurring, and inpainting. Ultimately, informed by the limitations in existingworks, we propose five potential and challenging directions for the futureresearch of diffusion model-based IR, including sampling efficiency, modelcompression, distortion simulation and estimation, distortion invariantlearning, and framework design.</description><author>Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang, Wenjun Zeng, Xinchao Wang, Zhibo Chen</author><pubDate>Thu, 28 Aug 2025 13:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09388v2</guid></item><item><title>NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks</title><link>http://arxiv.org/abs/2508.19724v2</link><description>Commonsense visual-question answering often hinges on knowledge that ismissing from the image or the question. Small vision-language models (sVLMs)such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generativecounterparts. To study the effect of careful commonsense knowledge integrationon sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves naturallanguage facts, (ii) prompts an LLM to craft natural language explanations, and(iii) feeds both signals to sVLMs respectively across two commonsense VQAdatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Factsretrieved using a fine-tuned ColBERTv2 and an object information-enrichedprompt yield explanations that largely cut down hallucinations, while liftingthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVAand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2Band SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additionalfinetuning using noise-robust losses (such as symmetric cross entropy andgeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Ourfindings expose when LLM-based commonsense knowledge beats retrieval fromcommonsense knowledge bases, how noise-aware training stabilises small modelsin the context of external knowledge augmentation, and why parameter-efficientcommonsense reasoning is now within reach for 250M models.</description><author>Aritra Dutta, Swapnanil Mukherjee, Deepanway Ghosal, Somak Aditya</author><pubDate>Thu, 28 Aug 2025 12:05:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19724v2</guid></item><item><title>Humans Perceive Wrong Narratives from AI Reasoning Texts</title><link>http://arxiv.org/abs/2508.16599v2</link><description>A new generation of AI models generates step-by-step reasoning text beforeproducing an answer. This text appears to offer a human-readable window intotheir computation process, and is increasingly relied upon for transparency andinterpretability. However, it is unclear whether human understanding of thistext matches the model's actual computational process. In this paper, weinvestigate a necessary condition for correspondence: the ability of humans toidentify which steps in a reasoning text causally influence later steps. Weevaluated humans on this ability by composing questions based on counterfactualmeasurements and found a significant discrepancy: participant accuracy was only29%, barely above chance (25%), and remained low (42%) even when evaluating themajority vote on questions with high agreement. Our results reveal afundamental gap between how humans interpret reasoning texts and how models useit, challenging its utility as a simple interpretability tool. We argue thatreasoning texts should be treated as an artifact to be investigated, not takenat face value, and that understanding the non-human ways these models uselanguage is a critical research direction.</description><author>Mosh Levy, Zohar Elyoseph, Yoav Goldberg</author><pubDate>Thu, 28 Aug 2025 11:53:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16599v2</guid></item><item><title>See then Tell: Enhancing Key Information Extraction with Vision Grounding</title><link>http://arxiv.org/abs/2409.19573v2</link><description>In the digital era, the ability to understand visually rich documents thatintegrate text, complex layouts, and imagery is critical. Traditional KeyInformation Extraction (KIE) methods primarily rely on Optical CharacterRecognition (OCR), which often introduces significant latency, computationaloverhead, and errors. Current advanced image-to-text approaches, which bypassOCR, typically yield plain text outputs without corresponding vision grounding.In this paper, we introduce STNet (See then Tell Net), a novel end-to-end modeldesigned to deliver precise answers with relevant vision grounding.Distinctively, STNet utilizes a unique &lt;see&gt; token to observe pertinent imageareas, aided by a decoder that interprets physical coordinates linked to thistoken. Positioned at the outset of the answer text, the &lt;see&gt; token allows themodel to first see-observing the regions of the image related to the inputquestion-and then tell-providing articulated textual responses. To enhance themodel's seeing capabilities, we collect extensive structured table recognitiondatasets. Leveraging the advanced text processing prowess of GPT-4, we developthe TVG (TableQA with Vision Grounding) dataset, which not only providestext-based Question Answering (QA) pairs but also incorporates precise visiongrounding for these pairs. Our approach demonstrates substantial advancementsin KIE performance, achieving state-of-the-art results on publicly availabledatasets such as CORD, SROIE, and DocVQA. The code will also be made publiclyavailable.</description><author>Shuhang Liu, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Jun Du, Qing Wang, Jianshu Zhang, Chenyu Liu</author><pubDate>Thu, 28 Aug 2025 11:02:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.19573v2</guid></item><item><title>GDS Agent: A Graph Algorithmic Reasoning Agent</title><link>http://arxiv.org/abs/2508.20637v1</link><description>Large language models (LLMs) have shown remarkable multimodal informationprocessing and reasoning ability. When equipped with tools through functioncalling and enhanced with retrieval-augmented techniques, compound LLM-basedsystems can access closed data sources and answer questions about them.However, they still struggle to process and reason over large-scalegraph-structure data. We introduce the GDS (Graph Data Science) agent in thistechnical report. The GDS agent introduces a comprehensive set of graphalgorithms as tools, together with preprocessing (retrieval) and postprocessingof algorithm results, in a model context protocol (MCP) server. The server canbe used with any modern LLM out-of-the-box. GDS agent allows users to ask anyquestion that implicitly and intrinsically requires graph algorithmic reasoningabout their data, and quickly obtain accurate and grounded answers. We alsointroduce a new benchmark that evaluates intermediate tool calls as well asfinal responses. The results indicate that GDS agent is able to solve a widespectrum of graph tasks. We also provide detailed case studies for moreopen-ended tasks and study scenarios where the agent struggles. Finally, wediscuss the remaining challenges and the future roadmap.</description><author>Borun Shi, Ioannis Panagiotas</author><pubDate>Thu, 28 Aug 2025 10:35:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20637v1</guid></item><item><title>DriveQA: Passing the Driving Knowledge Test</title><link>http://arxiv.org/abs/2508.21824v1</link><description>If a Large Language Model (LLM) were to take a driving knowledge test today,would it pass? Beyond standard spatial and visual question-answering (QA) taskson current autonomous driving benchmarks, driving knowledge tests require acomplete understanding of all traffic rules, signage, and right-of-wayprinciples. To pass this test, human drivers must discern various edge casesthat rarely appear in real-world datasets. In this work, we present DriveQA, anextensive open-source text and vision-based benchmark that exhaustively coverstraffic regulations and scenarios. Through our experiments using DriveQA, weshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well onbasic traffic rules but exhibit significant weaknesses in numerical reasoningand complex right-of-way scenarios, traffic sign variations, and spatiallayouts, (2) fine-tuning on DriveQA improves accuracy across multiplecategories, particularly in regulatory sign recognition and intersectiondecision-making, (3) controlled variations in DriveQA-V provide insights intomodel sensitivity to environmental factors such as lighting, perspective,distance, and weather conditions, and (4) pretraining on DriveQA enhancesdownstream driving task performance, leading to improved results on real-worlddatasets such as nuScenes and BDD, while also demonstrating that models caninternalize text and synthetic traffic knowledge to generalize effectivelyacross downstream QA tasks.</description><author>Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar</author><pubDate>Fri, 29 Aug 2025 17:59:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21824v1</guid></item><item><title>How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images</title><link>http://arxiv.org/abs/2508.21565v1</link><description>Effectively understanding urban scenes requires fine-grained spatialreasoning about objects, layouts, and depth cues. However, how well currentvision-language models (VLMs), pretrained on general scenes, transfer theseabilities to urban domain remains underexplored. To address this gap, weconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,and LLaVA-1.5-evaluating both zero-shot performance and the effects offine-tuning with a synthetic VQA dataset specific to urban scenes. We constructsuch dataset from segmentation, depth, and object detection predictions ofstreet-view images, pairing each question with LLM-generated Chain-of-Thought(CoT) answers for step-by-step reasoning supervision. Results show that whileVLMs perform reasonably well in zero-shot settings, fine-tuning with oursynthetic CoT-supervised dataset substantially boosts performance, especiallyfor challenging question types such as negation and counterfactuals. This studyintroduces urban spatial reasoning as a new challenge for VLMs and demonstratessynthetic dataset construction as a practical path for adapting general-purposemodels to specialized domains.</description><author>Juneyoung Ro, Namwoo Kim, Yoonjin Yoon</author><pubDate>Fri, 29 Aug 2025 12:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21565v1</guid></item><item><title>MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents</title><link>http://arxiv.org/abs/2508.21475v1</link><description>Large multimodal language models (MLLMs) are increasingly deployed as webagents, yet many multimodal browsing benchmarks can be solved by shallow, fixedworkflows that lean on high-recall image search and nearby text-masking thegenuinely multimodal challenges of fine-grained visual reasoning, provenanceverification, and long-horizon tool use. We introduce MMSearch-Plus, abenchmark of 311 tasks that highly demand multimodal understanding whilepreserving the difficulty profile of strong text-only browsing suites. Eachitem is constructed to contain multiple weak, localized visual signals thatmust be extracted, propagated through iterative text-image search, andcross-validated under retrieval noise before answering. Our curation procedure,Spatial-Temporal Extrapolation, seeds questions whose answers requireextrapolating from spatial cues (micro-text, part-level appearance, layouts,signage) and temporal traces (broadcast overlays, seasonal context) toout-of-image facts such as events, dates, and venues. We provide amodel-agnostic agent framework with browsing tools and evaluate a range ofclosed and open MLLMs. The strongest agent (o3) attains 15.1% without searchand 36.0% accuracy with rollout under our framework, while a strong open-sourcemodel (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20rounds of search. Beyond answer accuracy, we assess bounding-box production andcropped-image search, and conduct an error analysis that surfaces failures insource verification, part-based reasoning, and long-horizon planning.</description><author>Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao, Ziru Liu, Haoli Bai, Rui Liu, Lingpeng Kong</author><pubDate>Fri, 29 Aug 2025 09:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21475v1</guid></item><item><title>Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models</title><link>http://arxiv.org/abs/2412.06748v2</link><description>A key component of building safe and reliable language models is enabling themodels to appropriately refuse to follow certain instructions or answer certainquestions. We may want models to output refusal messages for various categoriesof user queries, for example, ill-posed questions, instructions for committingillegal acts, or queries which require information past the model's knowledgehorizon. Engineering models that refuse to answer such questions is complicatedby the fact that an individual may want their model to exhibit varying levelsof sensitivity for refusing queries of various categories, and different usersmay want different refusal rates. The current default approach involvestraining multiple models with varying proportions of refusal messages from eachcategory to achieve the desired refusal rates, which is computationallyexpensive and may require training a new model to accommodate each user'sdesired preference over refusal rates. To address these challenges, we proposerefusal tokens, one such token for each refusal category or a single refusaltoken, which are prepended to the model's responses during training. We thenshow how to increase or decrease the probability of generating the refusaltoken for each category during inference to steer the model's refusal behavior.Refusal tokens enable controlling a single model's refusal rates without theneed of any further fine-tuning, but only by selectively intervening duringgeneration.</description><author>Neel Jain, Aditya Shrivastava, Chenyang Zhu, Daben Liu, Alfy Samuel, Ashwinee Panda, Anoop Kumar, Micah Goldblum, Tom Goldstein</author><pubDate>Fri, 29 Aug 2025 17:08:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06748v2</guid></item><item><title>What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge</title><link>http://arxiv.org/abs/2508.08344v2</link><description>Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is anincreasingly explored approach for combining the reasoning capabilities oflarge language models with the structured evidence of knowledge graphs.However, current evaluation practices fall short: existing benchmarks ofteninclude questions that can be directly answered using existing triples in KG,making it unclear whether models perform reasoning or simply retrieve answersdirectly. Moreover, inconsistent evaluation metrics and lenient answer matchingcriteria further obscure meaningful comparisons. In this work, we introduce ageneral method for constructing benchmarks, together with an evaluationprotocol, to systematically assess KG-RAG methods under knowledgeincompleteness. Our empirical results show that current KG-RAG methods havelimited reasoning ability under missing knowledge, often rely on internalmemorization, and exhibit varying degrees of generalization depending on theirdesign.</description><author>Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Hongkuan Zhou, Yuan He, Jiaoyan Chen, Steffen Staab, Evgeny Kharlamov</author><pubDate>Fri, 29 Aug 2025 16:43:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.08344v2</guid></item><item><title>Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness</title><link>http://arxiv.org/abs/2504.05163v2</link><description>Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a techniquethat enhances Large Language Model (LLM) inference in tasks like QuestionAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).However, real-world KGs are often incomplete, meaning that essentialinformation for answering questions may be missing. Existing benchmarks do notadequately capture the impact of KG incompleteness on KG-RAG performance. Inthis paper, we systematically evaluate KG-RAG methods under incomplete KGs byremoving triples using different methods and analyzing the resulting effects.We demonstrate that KG-RAG methods are sensitive to KG incompleteness,highlighting the need for more robust approaches in realistic settings.</description><author>Dongzhuoran Zhou, Yuqicheng Zhu, Xiaxia Wang, Yuan He, Jiaoyan Chen, Steffen Staab, Evgeny Kharlamov</author><pubDate>Fri, 29 Aug 2025 16:38:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.05163v2</guid></item><item><title>CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</title><link>http://arxiv.org/abs/2508.21732v1</link><description>Recent advancements in Large Vision-Language Models (LVLMs) have demonstratedimpressive capabilities across various multimodal tasks. They continue,however, to struggle with trivial scenarios such as reading values from DigitalMeasurement Devices (DMDs), particularly in real-world conditions involvingclutter, occlusions, extreme viewpoints, and motion blur; common inhead-mounted cameras and Augmented Reality (AR) applications. Motivated bythese limitations, this work introduces CAD2DMD-SET, a synthetic datageneration tool designed to support visual question answering (VQA) tasksinvolving DMDs. By leveraging 3D CAD models, advanced rendering, andhigh-fidelity image composition, our tool produces diverse, VQA-labelledsynthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we presentDMDBench, a curated validation set of 1,000 annotated real-world imagesdesigned to evaluate model performance under practical constraints.Benchmarking three state-of-the-art LVLMs using Average Normalised LevenshteinSimilarity (ANLS) and further fine-tuning LoRA's of these models withCAD2DMD-SET's generated dataset yielded substantial improvements, with InternVLshowcasing a score increase of 200% without degrading on other tasks. Thisdemonstrates that the CAD2DMD-SET training dataset substantially improves therobustness and performance of LVLMs when operating under the previously statedchallenging conditions. The CAD2DMD-SET tool is expected to be released asopen-source once the final version of this manuscript is prepared, allowing thecommunity to add different measurement devices and generate their own datasets.</description><author>João Valente, Atabak Dehban, Rodrigo Ventura</author><pubDate>Fri, 29 Aug 2025 15:57:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.21732v1</guid></item><item><title>ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</title><link>http://arxiv.org/abs/2508.18190v3</link><description>Semi-structured tables, widely used in real-world applications (e.g.,financial reports, medical records, transactional orders), often involveflexible and complex layouts (e.g., hierarchical headers and merged cells).These tables generally rely on human analysts to interpret table layouts andanswer relevant natural language questions, which is costly and inefficient. Toautomate the procedure, existing methods face significant challenges. First,methods like NL2SQL require converting semi-structured tables into structuredones, which often causes substantial information loss. Second, methods likeNL2Code and multi-modal LLM QA struggle to understand the complex layouts ofsemi-structured tables and cannot accurately answer corresponding questions. Tothis end, we propose ST-Raptor, a tree-based framework for semi-structuredtable question answering using large language models. First, we introduce theHierarchical Orthogonal Tree (HO-Tree), a structural model that capturescomplex semi-structured table layouts, along with an effective algorithm forconstructing the tree. Second, we define a set of basic tree operations toguide LLMs in executing common QA tasks. Given a user question, ST-Raptordecomposes it into simpler sub-questions, generates corresponding treeoperation pipelines, and conducts operation-table alignment for accuratepipeline execution. Third, we incorporate a two-stage verification mechanism:forward validation checks the correctness of execution steps, while backwardvalidation evaluates answer reliability by reconstructing queries frompredicted answers. To benchmark the performance, we present SSTQA, a dataset of764 questions over 102 real-world semi-structured tables. Experiments show thatST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The codeis available at https://github.com/weAIDB/ST-Raptor.</description><author>Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu</author><pubDate>Tue, 02 Sep 2025 02:30:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18190v3</guid></item><item><title>Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring</title><link>http://arxiv.org/abs/2507.22369v3</link><description>Classroom behavior monitoring is a critical aspect of educational research,with significant implications for student engagement and learning outcomes.Recent advancements in Visual Question Answering (VQA) models offer promisingtools for automatically analyzing complex classroom interactions from videorecordings. In this paper, we investigate the applicability of severalstate-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, andNVILA, in the context of classroom behavior analysis. To facilitate rigorousevaluation, we introduce our BAV-Classroom-VQA dataset derived from real-worldclassroom video recordings at the Banking Academy of Vietnam. We present themethodology for data collection, annotation, and benchmark the performance ofthe selected VQA models on this dataset. Our initial experimental resultsdemonstrate that all four models achieve promising performance levels inanswering behavior-related visual questions, showcasing their potential infuture classroom analytics and intervention systems.</description><author>Sinh Trong Vu, Hieu Trung Pham, Dung Manh Nguyen, Hieu Minh Hoang, Nhu Hoang Le, Thu Ha Pham, Tai Tan Mai</author><pubDate>Tue, 02 Sep 2025 02:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22369v3</guid></item><item><title>VSLLaVA: a pipeline of large multimodal foundation model for industrial vibration signal analysis</title><link>http://arxiv.org/abs/2409.07482v2</link><description>While Large Multimodal Models (LMMs) excel in general multimodal tasks, theylack the domain-specific knowledge for industrial vibration signal analysis.This paper introduces VSLLaVA, a comprehensive pipeline that utilizes expertknowledge-guided instruction tuning and evaluation to create an end-to-end LMMfor signal analysis. To achieve this, we construct a novelSignal-Question-Answer (SQA) dataset using an expert rule-based signalgenerator. This dataset facilitates a two-stage learning procedure. The firststep is efficient instruction fine-tuning with Low-Rank Adaptation (LoRA),which imparts specialized signal identification capabilities. Subsequently, wedesigned a tailored Group Relative Policy Optimization (GRPO) to refine thereasoning capabilities and enhance classification robustness. Then, a dual-modeevaluation framework is proposed, combining an LLM referee with expert rulesfor semantic assessment using quantitative metrics for numerical and textualaccuracy, which reveals that VSLLaVA significantly improves performance insignal type identification and parameter analysis, and makes progress in theidentification and parameter analysis of fault-related signals. This researchdemonstrates a viable approach for developing specialized foundational modelsfor complex industrial applications and marks a transition from conventionaltask-specific systems to a cohesive, interactive foundational model.</description><author>Qi Li, Xinran Zhang, Jinfeng Huang, Hongliang He, Feibin Zhang, Zhaoye Qin, Fulei Chu</author><pubDate>Mon, 01 Sep 2025 21:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07482v2</guid></item><item><title>Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need</title><link>http://arxiv.org/abs/2507.13966v2</link><description>Language models traditionally used for cross-domain generalization haverecently demonstrated task-specific reasoning. However, their top-down trainingapproach on general corpora is insufficient for acquiring abstractions neededfor deep domain expertise. This may require a bottom-up approach that acquiresexpertise by learning to compose simple domain concepts into more complex ones.A knowledge graph (KG) provides this compositional structure, where domainprimitives are represented as head-relation-tail edges and their paths encodehigher-level concepts. We present a task generation pipeline that synthesizestasks directly from KG primitives, enabling models to acquire and compose themfor reasoning. We fine-tune language models on the resultant KG-groundedcurriculum to demonstrate domain-specific superintelligence. While broadlyapplicable, we validate our approach in medicine, where reliable KGs exist.Using a medical KG, we curate 24,000 reasoning tasks paired with thinkingtraces derived from diverse medical primitives. We fine-tune the QwQ-32B modelon this curriculum to obtain QwQ-Med-3 that takes a step towards medicalsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantifyreasoning abilities across 15 medical domains. Our experiments demonstrate thatQwQ-Med-3 significantly outperforms state-of-the-art reasoning models onICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquiredprimitives to widen the performance gap on the hardest tasks of ICD-Bench.Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3transfers acquired expertise to enhance the base model's performance. While theindustry's approach to artificial general intelligence (AGI) emphasizes broadexpertise, we envision a future in which AGI emerges from the composableinteraction of efficient domain-specific superintelligent agents.</description><author>Bhishma Dedhia, Yuval Kansal, Niraj K. Jha</author><pubDate>Mon, 01 Sep 2025 20:28:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.13966v2</guid></item><item><title>ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation</title><link>http://arxiv.org/abs/2507.14201v2</link><description>We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x onthe task of Cyber Threat Investigation through security questions derived frominvestigation graphs. Real-world security analysts must sift through a largenumber of heterogeneous alert signals and security logs, follow multi-hopchains of evidence, and compile an incident report. With the developments ofLLMs, building LLM-based agents for automatic thread investigation is apromising direction. To assist the development and evaluation of LLM agents, weconstruct a dataset from a controlled Azure tenant that covers 8 simulatedreal-world multi-step attacks, 57 log tables from Microsoft Sentinel andrelated services, and 589 automatically generated questions. We leveragesecurity logs extracted with expert-crafted detection logic to build threatinvestigation graphs, and then generate questions with LLMs using paired nodeson the graph, taking the start node as background context and the end node asanswer. Anchoring each question to these explicit nodes and edges not onlyprovides automatic, explainable ground truth answers but also makes thepipeline reusable and readily extensible to new logs. This also enables theautomatic generation of procedural tasks with verifiable rewards, which can benaturally extended to training agents via reinforcement learning. Ourcomprehensive experiments with different models confirm the difficulty of thetask: with the base setting, the average reward across all evaluated models is0.249, and the best achieved is 0.368, leaving substantial headroom for futureresearch. Code and data are coming soon!</description><author>Yiran Wu, Mauricio Velazco, Andrew Zhao, Manuel Raúl Meléndez Luján, Srisuma Movva, Yogesh K Roy, Quang Nguyen, Roberto Rodriguez, Qingyun Wu, Michael Albada, Julia Kiseleva, Anand Mudgerikar</author><pubDate>Mon, 01 Sep 2025 20:02:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14201v2</guid></item><item><title>UniBERT: Adversarial Training for Language-Universal Representations</title><link>http://arxiv.org/abs/2503.12608v3</link><description>This paper presents UniBERT, a compact multilingual language model that usesan innovative training framework that integrates three components: maskedlanguage modeling, adversarial training, and knowledge distillation.Pre-trained on a meticulously curated Wikipedia corpus spanning 107 languages,UniBERT is designed to reduce the computational demands of large-scale modelswhile maintaining competitive performance across various natural languageprocessing tasks. Comprehensive evaluations on four tasks - named entityrecognition, natural language inference, question answering, and semantictextual similarity - demonstrate that our multilingual training strategyenhanced by an adversarial objective significantly improves cross-lingualgeneralization. Specifically, UniBERT models show an average relativeimprovement of 7.72% over traditional baselines, which achieved an averagerelative improvement of only 1.17%, and statistical analysis confirms thesignificance of these gains (p-value = 0.0181). This work highlights thebenefits of combining adversarial training and knowledge distillation to buildscalable and robust language models, thus advancing the field of multilingualand cross-lingual natural language processing.</description><author>Andrei-Marius Avram, Marian Lupaşcu, Dumitru-Clementin Cercel, Ionuţ Mironică, Ştefan Trăuşan-Matu</author><pubDate>Mon, 01 Sep 2025 18:01:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.12608v3</guid></item></channel></rss>