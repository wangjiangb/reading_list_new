<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sat, 24 Feb 2024 06:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>PokeMQA: Programmable knowledge editing for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2312.15194v2</link><description>Multi-hop question answering (MQA) is one of the challenging tasks toevaluate machine's comprehension and reasoning abilities, where large languagemodels (LLMs) have widely achieved the human-comparable performance. Due to thedynamics of knowledge facts in real world, knowledge editing has been exploredto update model with the up-to-date facts while avoiding expensive re-trainingor fine-tuning. Starting from the edited fact, the updated model needs toprovide cascading changes in the chain of MQA. The previous art simply adopts amix-up prompt to instruct LLMs conducting multiple reasoning taskssequentially, including question decomposition, answer generation, and conflictchecking via comparing with edited facts. However, the coupling of thesefunctionally-diverse reasoning tasks inhibits LLMs' advantages in comprehendingand answering questions while disturbing them with the unskilled task ofconflict checking. We thus propose a framework, Programmable knowledge editingfor Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically,we prompt LLMs to decompose knowledge-augmented multi-hop question, whileinteracting with a detached trainable scope detector to modulate LLMs behaviordepending on external conflict signal. The experiments on three LLM backbonesand two benchmark datasets validate our superiority in knowledge editing ofMQA, outperforming all competitors by a large margin in almost all settings andconsistently producing reliable reasoning process.</description><author>Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, Xin Wang</author><pubDate>Thu, 15 Feb 2024 03:10:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15194v2</guid></item><item><title>Unsupervised LLM Adaptation for Question Answering</title><link>http://arxiv.org/abs/2402.12170v1</link><description>Large language models (LLM) learn diverse knowledge present in thelarge-scale training dataset via self-supervised training. Followed byinstruction-tuning, LLM acquires the ability to return correct information fordiverse questions. However, adapting these pre-trained LLMs to new targetdomains, such as different organizations or periods, for the question-answering(QA) task incurs a substantial annotation cost. To tackle this challenge, wepropose a novel task, unsupervised LLM adaptation for question answering. Inthis task, we leverage a pre-trained LLM, a publicly available QA dataset(source data), and unlabeled documents from the target domain. Our goal is tolearn LLM that can answer questions about the target domain. We introduce onesynthetic and two real datasets to evaluate models fine-tuned on the source andtarget data, and reveal intriguing insights; (i) fine-tuned models exhibit theability to provide correct answers for questions about the target domain eventhough they do not see any questions about the information described in theunlabeled documents, but (ii) they have difficulties in accessing informationlocated in the middle or at the end of documents, and (iii) this challenge canbe partially mitigated by replacing input tokens with random ones duringadaptation.</description><author>Kuniaki Saito, Kihyuk Sohn, Chen-Yu Lee, Yoshitaka Ushiku</author><pubDate>Fri, 16 Feb 2024 06:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12170v1</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v2</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Tue, 20 Feb 2024 05:04:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v2</guid></item><item><title>Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi</title><link>http://arxiv.org/abs/2308.09862v3</link><description>The recent advances in deep-learning have led to the development of highlysophisticated systems with an unquenchable appetite for data. On the otherhand, building good deep-learning models for low-resource languages remains achallenging task. This paper focuses on developing a Question Answering datasetfor two such languages- Hindi and Marathi. Despite Hindi being the 3rd mostspoken language worldwide, with 345 million speakers, and Marathi being the11th most spoken language globally, with 83.2 million speakers, both languagesface limited resources for building efficient Question Answering systems. Totackle the challenge of data scarcity, we have developed a novel approach fortranslating the SQuAD 2.0 dataset into Hindi and Marathi. We release thelargest Question-Answering dataset available for these languages, with eachdataset containing 28,000 samples. We evaluate the dataset on variousarchitectures and release the best-performing models for both Hindi andMarathi, which will facilitate further research in these languages. Leveragingsimilarity tools, our method holds the potential to create datasets in diverselanguages, thereby enhancing the understanding of natural language acrossvaried linguistic contexts. Our fine-tuned models, code, and dataset will bemade publicly available.</description><author>Maithili Sabane, Onkar Litake, Aman Chadha</author><pubDate>Sat, 17 Feb 2024 07:02:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09862v3</guid></item><item><title>CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering</title><link>http://arxiv.org/abs/2401.13170v2</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current evaluation metrics to determine answer equivalence (AE) often do notalign with human judgments, particularly more verbose, free-form answers fromlarge language models (LLM). There are two challenges: a lack of data and thatmodels are too big: LLM-based scorers can correlate better with human judges,but this task has only been tested on limited QA datasets, and even whenavailable, update of the model is limited because LLMs are large and oftenexpensive. We rectify both of these issues by providing clear and consistentguidelines for evaluating AE in machine QA adopted from professional human QAcontests. We also introduce a combination of standard evaluation and a moreefficient, robust, and lightweight discriminate AE classifier-based matchingmethod (CFMatch, smaller than 1 MB), trained and validated to more accuratelyevaluate answer correctness in accordance with adopted expert AE rules that aremore aligned with human judgments.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Boyd-Graber</author><pubDate>Tue, 20 Feb 2024 19:37:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13170v2</guid></item><item><title>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</title><link>http://arxiv.org/abs/2402.08966v1</link><description>Difference visual question answering (diff-VQA) is a challenging task thatrequires answering complex questions based on differences between a pair ofimages. This task is particularly important in reading chest X-ray imagesbecause radiologists often compare multiple images of the same patient taken atdifferent times to track disease progression and changes in its severity intheir clinical practice. However, previous works focused on designing specificnetwork architectures for the diff-VQA task, missing opportunities to enhancethe model's performance using a pretrained vision-language model (VLM). Here,we introduce a novel VLM called PLURAL, which is pretrained on natural andlongitudinal chest X-ray data for the diff-VQA task. The model is developedusing a step-by-step approach, starting with being pretrained on natural imagesand texts, followed by being trained using longitudinal chest X-ray data. Thelongitudinal data consist of pairs of X-ray images, along with question-answersets and radiologist's reports that describe the changes in lung abnormalitiesand diseases over time. Our experimental results show that the PLURAL modeloutperforms state-of-the-art methods not only in diff-VQA for longitudinalX-rays but also in conventional VQA for a single X-ray image. Through extensiveexperiments, we demonstrate the effectiveness of the proposed VLM architectureand pretraining method in improving the model's performance.</description><author>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin</author><pubDate>Wed, 14 Feb 2024 06:20:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08966v1</guid></item><item><title>Question Answering Over Spatio-Temporal Knowledge Graph</title><link>http://arxiv.org/abs/2402.11542v1</link><description>Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledgegraphs (KGs) by incorporating time and location information. While the researchcommunity's focus on Knowledge Graph Question Answering (KGQA), the field ofanswering questions incorporating both spatio-temporal information based onSTKGs remains largely unexplored. Furthermore, a lack of comprehensive datasetsalso has hindered progress in this area. To address this issue, we presentSTQAD, a dataset comprising 10,000 natural language questions forspatio-temporal knowledge graph question answering (STKGQA). Unfortunately,various state-of-the-art KGQA approaches fall far short of achievingsatisfactory performance on our dataset. In response, we propose STCQA, a newspatio-temporal KGQA approach that utilizes a novel STKG embedding method namedSTComplEx. By extracting temporal and spatial information from a question, ourQA model can better comprehend the question and retrieve accurate answers fromthe STKG. Through extensive experiments, we demonstrate the quality of ourdataset and the effectiveness of our STKGQA method.</description><author>Xinbang Dai, Huiying Li, Guilin Qi</author><pubDate>Sun, 18 Feb 2024 10:44:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11542v1</guid></item><item><title>PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering</title><link>http://arxiv.org/abs/2402.11034v1</link><description>Existing work on Temporal Question Answering (TQA) has predominantly focusedon questions anchored to specific timestamps or events (e.g. "Who was the USpresident in 1970?"). Little work has studied questions whose temporal contextis relative to the present time (e.g. "Who was the previous US president?"). Werefer to this problem as Present-Anchored Temporal QA (PATQA). PATQA posesunique challenges: (1) large language models (LLMs) may have outdatedknowledge, (2) complex temporal relationships (e.g. 'before', 'previous') arehard to reason, (3) multi-hop reasoning may be required, and (4) the goldanswers of benchmarks must be continuously updated. To address thesechallenges, we introduce the PAT-Questions benchmark, which includes single andmulti-hop temporal questions. The answers in PAT-Questions can be automaticallyrefreshed by re-running SPARQL queries on a knowledge graph, if available. Weevaluate several state-of-the-art LLMs and a SOTA temporal reasoning model(TEMPREASON-T5) on PAT-Questions through direct prompting andretrieval-augmented generation (RAG). The results highlight the limitations ofexisting solutions in PATQA and motivate the need for new methods to improvePATQA reasoning capabilities.</description><author>Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis</author><pubDate>Fri, 16 Feb 2024 19:26:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11034v1</guid></item><item><title>Exploring Hybrid Question Answering via Program-based Prompting</title><link>http://arxiv.org/abs/2402.10812v1</link><description>Question answering over heterogeneous data requires reasoning over diversesources of data, which is challenging due to the large scale of information andorganic coupling of heterogeneous data. Various approaches have been proposedto address these challenges. One approach involves training specializedretrievers to select relevant information, thereby reducing the input length.Another approach is to transform diverse modalities of data into a singlemodality, simplifying the task difficulty and enabling more straightforwardprocessing. In this paper, we propose HProPro, a novel program-based promptingframework for the hybrid question answering task. HProPro follows the codegeneration and execution paradigm. In addition, HProPro integrates variousfunctions to tackle the hybrid reasoning scenario. Specifically, HProProcontains function declaration and function implementation to perform hybridinformation-seeking over data from various sources and modalities, whichenables reasoning over such data without training specialized retrievers orperforming modal transformations. Experimental results on two typical hybridquestion answering benchmarks HybridQA and MultiModalQA demonstrate theeffectiveness of HProPro: it surpasses all baseline systems and achieves thebest performances in the few-shot settings on both datasets.</description><author>Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu</author><pubDate>Fri, 16 Feb 2024 16:35:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10812v1</guid></item><item><title>Question Calibration and Multi-Hop Modeling for Temporal Question Answering</title><link>http://arxiv.org/abs/2402.13188v1</link><description>Many models that leverage knowledge graphs (KGs) have recently demonstratedremarkable success in question answering (QA) tasks. In the real world, manyfacts contained in KGs are time-constrained thus temporal KGQA has receivedincreasing attention. Despite the fruitful efforts of previous models intemporal KGQA, they still have several limitations. (I) They adopt pre-trainedlanguage models (PLMs) to obtain question representations, while PLMs tend tofocus on entity information and ignore entity transfer caused by temporalconstraints, and finally fail to learn specific temporal representations ofentities. (II) They neither emphasize the graph structure between entities norexplicitly model the multi-hop relationship in the graph, which will make itdifficult to solve complex multi-hop question answering. To alleviate thisproblem, we propose a novel Question Calibration and Multi-Hop Modeling(QC-MHM) approach. Specifically, We first calibrate the question representationby fusing the question and the time-constrained concepts in KG. Then, weconstruct the GNN layer to complete multi-hop message passing. Finally, thequestion representation is combined with the embedding output by the GNN togenerate the final prediction. Empirical results verify that the proposed modelachieves better performance than the state-of-the-art models in the benchmarkdataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestionsdataset's complex questions are absolutely improved by 5.1% and 1.2% comparedto the best-performing baseline. Moreover, QC-MHM can generate interpretableand trustworthy predictions.</description><author>Chao Xue, Di Liang, Pengfei Wang, Jing Zhang</author><pubDate>Tue, 20 Feb 2024 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13188v1</guid></item><item><title>Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports</title><link>http://arxiv.org/abs/2401.01505v3</link><description>Reasoning over sports videos for question answering is an important task withnumerous applications, such as player training and information retrieval.However, this task has not been explored due to the lack of relevant datasetsand the challenging nature it presents. Most datasets for video questionanswering (VideoQA) focus mainly on general and coarse-grained understanding ofdaily-life videos, which is not applicable to sports scenarios requiringprofessional action understanding and fine-grained motion analysis. In thispaper, we introduce the first dataset, named Sports-QA, specifically designedfor the sports VideoQA task. The Sports-QA dataset includes various types ofquestions, such as descriptions, chronologies, causalities, and counterfactualconditions, covering multiple sports. Furthermore, to address thecharacteristics of the sports VideoQA task, we propose a new Auto-FocusTransformer (AFT) capable of automatically focusing on particular scales oftemporal information for question answering. We conduct extensive experimentson Sports-QA, including baseline studies and the evaluation of differentmethods. The results demonstrate that our AFT achieves state-of-the-artperformance.</description><author>Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen</author><pubDate>Wed, 14 Feb 2024 23:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.01505v3</guid></item><item><title>Unifying Image Processing as Visual Prompting Question Answering</title><link>http://arxiv.org/abs/2310.10513v2</link><description>Image processing is a fundamental task in computer vision, which aims atenhancing image quality and extracting essential features for subsequent visionapplications. Traditionally, task-specific models are developed for individualtasks and designing such models requires distinct expertise. Building upon thesuccess of large language models (LLMs) in natural language processing (NLP),there is a similar trend in computer vision, which focuses on developinglarge-scale models through pretraining and in-context learning. This paradigmshift reduces the reliance on task-specific models, yielding a powerful unifiedmodel to deal with various tasks. However, these advances have predominantlyconcentrated on high-level vision tasks, with less attention paid to low-levelvision tasks. To address this issue, we propose a universal model for generalimage processing that covers image restoration, image enhancement, imagefeature extraction tasks, etc. Our proposed framework, named PromptGIP, unifiesthese diverse image processing tasks within a universal framework. Inspired byNLP question answering (QA) techniques, we employ a visual prompting questionanswering paradigm. Specifically, we treat the input-output image pair as astructured question-answer sentence, thereby reprogramming the image processingtask as a prompting QA problem. PromptGIP can undertake diverse cross-domaintasks using provided visual prompts, eliminating the need for task-specificfinetuning. Our methodology offers a universal and adaptive solution to generalimage processing. While PromptGIP has demonstrated a certain degree ofout-of-domain task generalization capability, further research is expected tofully explore its more powerful emergent generalization.</description><author>Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</author><pubDate>Wed, 21 Feb 2024 03:31:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10513v2</guid></item><item><title>II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering</title><link>http://arxiv.org/abs/2402.11058v1</link><description>Visual Question Answering (VQA) often involves diverse reasoning scenariosacross Vision and Language (V&amp;L). Most prior VQA studies, however, have merelyfocused on assessing the model's overall accuracy without evaluating it ondifferent reasoning cases. Furthermore, some recent works observe thatconventional Chain-of-Thought (CoT) prompting fails to generate effectivereasoning for VQA, especially for complex scenarios requiring multi-hopreasoning. In this paper, we propose II-MMR, a novel idea to identify andimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQAquestion with an image and finds a reasoning path to reach its answer using twonovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)knowledge triplet-guided prompt. II-MMR then analyzes this path to identifydifferent reasoning cases in current VQA benchmarks by estimating how many hopsand what types (i.e., visual or beyond-visual) of reasoning are required toanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMRobserves that most of their VQA questions are easy to answer, simply demanding"single-hop" reasoning, whereas only a few questions require "multi-hop"reasoning. Moreover, while the recent V&amp;L model struggles with such complexmulti-hop reasoning questions even using the traditional CoT method, II-MMRshows its effectiveness across all reasoning cases in both zero-shot andfine-tuning settings.</description><author>Jihyung Kil, Farideh Tavazoee, Dongyeop Kang, Joo-Kyung Kim</author><pubDate>Fri, 16 Feb 2024 20:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11058v1</guid></item><item><title>Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering</title><link>http://arxiv.org/abs/2402.12728v1</link><description>Knowledge-based visual question answering (KVQA) has been extensively studiedto answer visual questions with external knowledge, e.g., knowledge graphs(KGs). While several attempts have been proposed to leverage large languagemodels (LLMs) as an implicit knowledge source, it remains challenging sinceLLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,images, KGs and LLMs, cannot be readily aligned for complex scenarios. Totackle these, we present a novel modality-aware integration with LLMs for KVQA(MAIL). It carefully leverages multimodal knowledge for both imageunderstanding and knowledge reasoning. Specifically, (i) we propose a two-stageprompting strategy with LLMs to densely embody the image into a scene graphwith detailed visual features; (ii) We construct a coupled concept graph bylinking the mentioned entities with external facts. (iii) A tailoredpseudo-siamese graph medium fusion is designed for sufficient multimodalfusion. We utilize the shared mentioned entities in two graphs as mediums tobridge a tight inter-modal exchange, while maximally preserving insightfulintra-modal learning by constraining the fusion within mediums. Extensiveexperiments on two benchmark datasets show the superiority of MAIL with 24xless resources.</description><author>Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang</author><pubDate>Tue, 20 Feb 2024 05:32:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12728v1</guid></item><item><title>Silver Retriever: Advancing Neural Passage Retrieval for Polish Question Answering</title><link>http://arxiv.org/abs/2309.08469v2</link><description>Modern open-domain question answering systems often rely on accurate andefficient retrieval components to find passages containing the facts necessaryto answer the question. Recently, neural retrievers have gained popularity overlexical alternatives due to their superior performance. However, most of thework concerns popular languages such as English or Chinese. For others, such asPolish, few models are available. In this work, we present Silver Retriever, aneural retriever for Polish trained on a diverse collection of manually orweakly labeled datasets. Silver Retriever achieves much better results thanother Polish models and is competitive with larger multilingual models.Together with the model, we open-source five new passage retrieval datasets.</description><author>Piotr Rybak, Maciej Ogrodniczuk</author><pubDate>Thu, 22 Feb 2024 13:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.08469v2</guid></item><item><title>PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation</title><link>http://arxiv.org/abs/2402.11161v1</link><description>Question answering (QA) can only make progress if we know if an answer iscorrect, but for many of the most challenging and interesting QA examples,current answer correctness (AC) metrics do not align with human judgments,particularly verbose, free form answers from large language models (LLM). Thereare two challenges: a lack of data and that models are too big. LLM basedscorers correlate better with humans, but this expensive task has only beentested on limited QA datasets. We rectify these issues by providing clearguidelines for evaluating machine QA adopted from human QA contests. We alsointroduce Precise ANswer correctness Determination and Adjudication (PANDA), asmall, efficient, deterministic AC classifier (812 KB) that more accuratelyevaluates answer correctness.</description><author>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Lee Boyd-Graber</author><pubDate>Sat, 17 Feb 2024 01:56:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11161v1</guid></item><item><title>Robust Visual Question Answering: Datasets, Methods, and Future Challenges</title><link>http://arxiv.org/abs/2307.11471v2</link><description>Visual question answering requires a system to provide an accurate naturallanguage answer given an image and a natural language question. However, it iswidely recognized that previous generic VQA methods often exhibit a tendency tomemorize biases present in the training data rather than learning properbehaviors, such as grounding images before predicting answers. Therefore, thesemethods usually achieve high in-distribution but poor out-of-distributionperformance. In recent years, various datasets and debiasing methods have beenproposed to evaluate and enhance the VQA robustness, respectively. This paperprovides the first comprehensive survey focused on this emerging fashion.Specifically, we first provide an overview of the development process ofdatasets from in-distribution and out-of-distribution perspectives. Then, weexamine the evaluation metrics employed by these datasets. Thirdly, we proposea typology that presents the development process, similarities and differences,robustness comparison, and technical features of existing debiasing methods.Furthermore, we analyze and discuss the robustness of representativevision-and-language pre-training models on VQA. Finally, through a thoroughreview of the available literature and experimental analysis, we discuss thekey areas for future research from various viewpoints.</description><author>Jie Ma, Pinghui Wang, Dechen Kong, Zewei Wang, Jun Liu, Hongbin Pei, Junzhou Zhao</author><pubDate>Sun, 18 Feb 2024 08:00:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.11471v2</guid></item><item><title>PolQA: Polish Question Answering Dataset</title><link>http://arxiv.org/abs/2212.08897v2</link><description>Recently proposed systems for open-domain question answering (OpenQA) requirelarge amounts of training data to achieve state-of-the-art performance.However, data annotation is known to be time-consuming and therefore expensiveto acquire. As a result, the appropriate datasets are available only for ahandful of languages (mainly English and Chinese). In this work, we introduceand publicly release PolQA, the first Polish dataset for OpenQA. It consists of7,000 questions, 87,525 manually labeled evidence passages, and a corpus ofover 7,097,322 candidate passages. Each question is classified according to itsformulation, type, as well as entity type of the answer. This resource allowsus to evaluate the impact of different annotation choices on the performance ofthe QA system and propose an efficient annotation strategy that increases thepassage retrieval accuracy@10 by 10.55 p.p. while reducing the annotation costby 82%.</description><author>Piotr Rybak, Piotr Przyby≈Ça, Maciej Ogrodniczuk</author><pubDate>Thu, 22 Feb 2024 13:24:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.08897v2</guid></item><item><title>VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models</title><link>http://arxiv.org/abs/2402.11083v1</link><description>Visual Question Answering (VQA) is a fundamental task in computer vision andnatural language process fields. Although the ``pre-training &amp; finetuning''learning paradigm significantly improves the VQA performance, the adversarialrobustness of such a learning paradigm has not been explored. In this paper, wedelve into a new problem: using a pre-trained multimodal source model to createadversarial image-text pairs and then transferring them to attack the targetVQA models. Correspondingly, we propose a novel VQAttack model, which caniteratively generate both image and text perturbations with the designedmodules: the large language model (LLM)-enhanced image attack and thecross-modal joint attack module. At each iteration, the LLM-enhanced imageattack module first optimizes the latent representation-based loss to generatefeature-level image perturbations. Then it incorporates an LLM to furtherenhance the image perturbations by optimizing the designed masked answeranti-recovery loss. The cross-modal joint attack module will be triggered at aspecific iteration, which updates the image and text perturbationssequentially. Notably, the text perturbation updates are based on both thelearned gradients in the word embedding space and word synonym-basedsubstitution. Experimental results on two VQA datasets with five validatedmodels demonstrate the effectiveness of the proposed VQAttack in thetransferable attack setting, compared with state-of-the-art baselines. Thiswork reveals a significant blind spot in the ``pre-training &amp; fine-tuning''paradigm on VQA tasks. Source codes will be released.</description><author>Ziyi Yin, Muchao Ye, Tianrong Zhang, Jiaqi Wang, Han Liu, Jinghui Chen, Ting Wang, Fenglong Ma</author><pubDate>Fri, 16 Feb 2024 21:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11083v1</guid></item><item><title>Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation</title><link>http://arxiv.org/abs/2310.13505v3</link><description>Models for conversational question answering (ConvQA) over knowledge graphs(KGs) are usually trained and tested on benchmarks of gold QA pairs. Thisimplies that training is limited to surface forms seen in the respectivedatasets, and evaluation is on a small set of held-out questions. Through ourproposed framework REIGN, we take several steps to remedy this restrictedlearning setup. First, we systematically generate reformulations of trainingquestions to increase robustness of models to surface form variations. This isa particularly challenging problem, given the incomplete nature of suchquestions. Second, we guide ConvQA models towards higher performance by feedingit only those reformulations that help improve their answering quality, usingdeep reinforcement learning. Third, we demonstrate the viability of trainingmajor model components on one benchmark and applying them zero-shot to another.Finally, for a rigorous evaluation of robustness for trained models, we use andrelease large numbers of diverse reformulations generated by prompting GPT forbenchmark test sets (resulting in 20x increase in sizes). Our findings showthat ConvQA models with robust training via reformulations, significantlyoutperform those with standard training from gold QA pairs only.</description><author>Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum</author><pubDate>Fri, 16 Feb 2024 19:15:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.13505v3</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>A Dataset of Open-Domain Question Answering with Multiple-Span Answers</title><link>http://arxiv.org/abs/2402.09923v1</link><description>Multi-span answer extraction, also known as the task of multi-span questionanswering (MSQA), is critical for real-world applications, as it requiresextracting multiple pieces of information from a text to answer complexquestions. Despite the active studies and rapid progress in English MSQAresearch, there is a notable lack of publicly available MSQA benchmark inChinese. Previous efforts for constructing MSQA datasets predominantlyemphasized entity-centric contextualization, resulting in a bias towardscollecting factoid questions and potentially overlooking questions requiringmore detailed descriptive responses. To overcome these limitations, we presentCLEAN, a comprehensive Chinese multi-span question answering dataset thatinvolves a wide range of open-domain subjects with a substantial number ofinstances requiring descriptive answers. Additionally, we provide establishedmodels from relevant literature as baselines for CLEAN. Experimental resultsand analysis show the characteristics and challenge of the newly proposed CLEANdataset for the community. Our dataset, CLEAN, will be publicly released atzhiyiluo.site/misc/clean_v1.0_ sample.json.</description><author>Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu</author><pubDate>Thu, 15 Feb 2024 13:03:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09923v1</guid></item><item><title>PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models</title><link>http://arxiv.org/abs/2402.13653v1</link><description>We introduce the novel task of zero-shot Protein Question Answering (PQA) forfree-form scientific enquiry. Given a previously unseen protein sequence and anatural language question, the task is to deliver a scientifically accurateanswer. This task not only supports future biological research, but could alsoprovide a test bed for assessing the scientific precision of large languagemodels (LLMs). We contribute the first specialized dataset for PQA modeltraining, containing 257K protein sequences annotated with 1.97M scientificquestion-answer pairs. Additionally, we propose and study several novelbiologically relevant benchmarks for scientific PQA. Employing two robustmulti-modal architectures, we establish an initial state-of-the-art performancefor PQA and reveal key performance factors through ablation studies. Ourcomprehensive PQA framework, named Pika, including dataset, code, modelcheckpoints, and a user-friendly demo, is openly accessible ongithub.com/EMCarrami/Pika, promoting wider research and application in thefield.</description><author>Eli M Carrami, Sahand Sharifzadeh</author><pubDate>Wed, 21 Feb 2024 09:38:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13653v1</guid></item><item><title>Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2402.05128v2</link><description>Textbook question answering (TQA) is a challenging task in artificialintelligence due to the complex nature of context and multimodal data. Althoughprevious research has significantly improved the task, there are still somelimitations including the models' weak reasoning and inability to capturecontextual information in the lengthy context. The introduction of largelanguage models (LLMs) has revolutionized the field of AI, however, directlyapplying LLMs often leads to inaccurate answers. This paper proposes amethodology that handle the out-of-domain scenario in TQA where concepts arespread across different lessons by incorporating the retrieval augmentedgeneration (RAG) technique and utilize transfer learning to handle the longcontext and enhance reasoning abilities. Through supervised fine-tuning of theLLM model Llama-2 and the incorporation of RAG, our architecture outperformsthe baseline, achieving a 4.12% accuracy improvement on validation set and9.84% on test set for non-diagram multiple-choice questions.</description><author>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</author><pubDate>Wed, 14 Feb 2024 10:06:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.05128v2</guid></item><item><title>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</title><link>http://arxiv.org/abs/2402.12869v1</link><description>Augmenting Large Language Models (LLMs) for Question Answering (QA) withdomain specific data has attracted wide attention. However, domain data oftenexists in a hybrid format, including text and semi-structured tables, posingchallenges for the seamless integration of information. Table-to-TextGeneration is a promising solution by facilitating the transformation of hybriddata into a uniformly text-formatted corpus. Although this technique has beenwidely studied by the NLP community, there is currently no comparative analysison how corpora generated by different table-to-text methods affect theperformance of QA systems. In this paper, we address this research gap in twosteps. First, we innovatively integrate table-to-text generation into theframework of enhancing LLM-based QA systems with domain hybrid data. Then, weutilize this framework in real-world industrial data to conduct extensiveexperiments on two types of QA systems (DSFT and RAG frameworks) with fourrepresentative methods: Markdown format, Template serialization, TPLM-basedmethod, and LLM-based method. Based on the experimental results, we draw someempirical findings and explore the underlying reasons behind the success ofsome methods. We hope the findings of this work will provide a valuablereference for the academic and industrial communities in developing robust QAsystems.</description><author>Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang</author><pubDate>Tue, 20 Feb 2024 10:00:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12869v1</guid></item><item><title>A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction</title><link>http://arxiv.org/abs/2402.11177v1</link><description>Electronic health records (EHRs) hold significant value for research andapplications. As a new way of information extraction, question answering (QA)can extract more flexible information than conventional methods and is moreaccessible to clinical researchers, but its progress is impeded by the scarcityof annotated data. In this paper, we propose a novel approach thatautomatically generates training data for transfer learning of QA models. Ourpipeline incorporates a preprocessing module to handle challenges posed byextraction types that are not readily compatible with extractive QA frameworks,including cases with discontinuous answers and many-to-one relationships. Theobtained QA model exhibits excellent performance on subtasks of informationextraction in EHRs, and it can effectively handle few-shot or zero-shotsettings involving yes-no questions. Case studies and ablation studiesdemonstrate the necessity of each component in our design, and the resultingmodel is deemed suitable for practical use.</description><author>Huaiyuan Ying, Sheng Yu</author><pubDate>Sat, 17 Feb 2024 02:55:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11177v1</guid></item><item><title>Prompt-based Personalized Federated Learning for Medical Visual Question Answering</title><link>http://arxiv.org/abs/2402.09677v1</link><description>We present a novel prompt-based personalized federated learning (pFL) methodto address data heterogeneity and privacy concerns in traditional medicalvisual question answering (VQA) methods. Specifically, we regard medicaldatasets from different organs as clients and use pFL to train personalizedtransformer-based VQA models for each client. To address the high computationalcomplexity of client-to-client communication in previous pFL methods, wepropose a succinct information sharing system by introducing prompts that aresmall learnable parameters. In addition, the proposed method introduces areliability parameter to prevent the negative effects of low performance andirrelevant clients. Finally, extensive evaluations on various heterogeneousmedical datasets attest to the effectiveness of our proposed method.</description><author>He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama</author><pubDate>Thu, 15 Feb 2024 03:09:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09677v1</guid></item><item><title>Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering</title><link>http://arxiv.org/abs/2402.09911v1</link><description>Mitigating the hallucinations of Large Language Models (LLMs) and enhancingthem is a crucial task. Although some existing methods employ modelself-enhancement techniques, they fall short of effectively addressing unknownfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches failsto address the generalization across different KG sources and the enhancementof open-ended answer questions simultaneously. To tackle these limitations,there is a framework that combines Pseudo-Graph Generation and Atomic KnowledgeVerification proposed. The enhancement of LLM using KG in an open-endedquestion-answering setting is implemented by leveraging the Pseudo-GraphGeneration. Atomic Knowledge Verification utilizes atomic-level knowledgequerying and verification to achieve generalizability under different KGsources. Compared to the baseline, this approach yields a minimum improvementof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,we observe a minimum accuracy improvement of 7.5. Moreover, there is alsodemonstration that this framework exhibits generalizability across different KGsources. In summary, our results pave the way for enhancing LLMs byincorporating Pseudo- and Multisource-KGs, particularly in the context ofopen-ended questions.</description><author>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao</author><pubDate>Thu, 15 Feb 2024 12:20:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09911v1</guid></item><item><title>Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering</title><link>http://arxiv.org/abs/2402.11194v1</link><description>Large Language Models (LLMs), excel in natural language understanding, buttheir capability for complex mathematical reasoning with an amalgamation ofstructured tables and unstructured text is uncertain. This study explores LLMs'mathematical reasoning on four financial tabular question-answering datasets:TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments withvarious models and prompting techniques, we assess how LLMs adapt to complextables and mathematical tasks. We focus on sensitivity to table complexity andperformance variations with an increasing number of arithmetic reasoning steps.The results provide insights into LLMs' capabilities and limitations inhandling complex mathematical scenarios for semi-structured tables. Ultimately,we introduce a novel prompting technique tailored to semi-structured documents,matching or outperforming other baselines in performance while providing anuanced understanding of LLMs abilities for such a task.</description><author>Pragya Srivastava, Manuj Malik, Tanuja Ganu</author><pubDate>Sat, 17 Feb 2024 05:10:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11194v1</guid></item><item><title>FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning</title><link>http://arxiv.org/abs/2402.12692v1</link><description>The application of formulas is a fundamental ability of humans whenaddressing numerical reasoning problems. However, existing numerical reasoningdatasets seldom explicitly indicate the formulas employed during the reasoningsteps. To bridge this gap, we propose a question answering dataset forformula-based numerical reasoning called FormulaQA, from junior high schoolphysics examinations. We further conduct evaluations on LLMs with size rangingfrom 7B to over 100B parameters utilizing zero-shot and few-shotchain-of-thoughts methods and we explored the approach of usingretrieval-augmented LLMs when providing an external formula database. We alsofine-tune on smaller models with size not exceeding 2B. Our empirical findingsunderscore the significant potential for improvement in existing models whenapplied to our complex, formula-driven FormulaQA.</description><author>Xiao Li, Sichen Liu, Bolin Zhu, Yin Zhu, Yiwei liu, Gong Cheng</author><pubDate>Tue, 20 Feb 2024 03:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12692v1</guid></item><item><title>FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning</title><link>http://arxiv.org/abs/2402.12692v2</link><description>The application of formulas is a fundamental ability of humans whenaddressing numerical reasoning problems. However, existing numerical reasoningdatasets seldom explicitly indicate the formulas employed during the reasoningsteps. To bridge this gap, we propose a question answering dataset forformula-based numerical reasoning called FormulaQA, from junior high schoolphysics examinations. We further conduct evaluations on LLMs with size rangingfrom 7B to over 100B parameters utilizing zero-shot and few-shotchain-of-thoughts methods and we explored the approach of usingretrieval-augmented LLMs when providing an external formula database. We alsofine-tune on smaller models with size not exceeding 2B. Our empirical findingsunderscore the significant potential for improvement in existing models whenapplied to our complex, formula-driven FormulaQA.</description><author>Xiao Li, Sichen Liu, Bolin Zhu, Yin Zhu, Yiwei Liu, Gong Cheng</author><pubDate>Wed, 21 Feb 2024 02:17:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12692v2</guid></item><item><title>VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering</title><link>http://arxiv.org/abs/2312.08367v2</link><description>In this work, we propose an efficient Video-Language Alignment viaFrame-Prompting and Distilling (VLAP) network. Our VLAP model addresses bothefficient frame sampling and effective cross-modal alignment in a unified way.In our VLAP network, we design a new learnable question-aware Frame-Promptertogether with a new cross-modal distillation (QFormer-Distiller) module.Pre-trained large image-language models have shown promising results onproblems such as visual question answering. However, how to efficiently andeffectively sample image frames when adapting pre-trained large image-languagemodel to video-language alignment is still the major challenge. Compared withprior work, our VLAP model demonstrates the capability of selecting key frameswith critical contents, thus improving the video-language alignment accuracywhile reducing the inference latency (+3.3% on NExT-QA Temporal with 3.0X speedup). Overall, our VLAP network outperforms (e.g. +4.6% on STAR Interaction and+2.2% on STAR average with 3.0X speed up, ours 2-frames out-perform SeViLA4-frames on VLEP with 4.2X speed up) the state-of-the-art methods on the videoquestion-answering benchmarks.</description><author>Xijun Wang, Junbang Liang, Chun-Kai Wang, Kenan Deng, Yu Lou, Ming Lin, Shan Yang</author><pubDate>Thu, 15 Feb 2024 10:57:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.08367v2</guid></item><item><title>Zero-shot sampling of adversarial entities in biomedical question answering</title><link>http://arxiv.org/abs/2402.10527v1</link><description>The increasing depth of parametric domain knowledge in large language models(LLMs) is fueling their rapid deployment in real-world applications. Inhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilitiesis essential for quantifying the trustworthiness of model predictions andregulating their use. The recent discovery of named entities as adversarialexamples in natural language processing tasks raises questions about theirpotential guises in other settings. Here, we propose a powerscaleddistance-weighted sampling scheme in embedding space to discover diverseadversarial entities as distractors. We demonstrate its advantage over randomsampling in adversarial question answering on biomedical topics. Our approachenables the exploration of different regions on the attack surface, whichreveals two regimes of adversarial entities that markedly differ in theircharacteristics. Moreover, we show that the attacks successfully manipulatetoken-wise Shapley value explanations, which become deceptive in theadversarial setting. Our investigations illustrate the brittleness of domainknowledge in LLMs and reveal a shortcoming of standard evaluations forhigh-capacity models.</description><author>R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</author><pubDate>Fri, 16 Feb 2024 09:29:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10527v1</guid></item><item><title>Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering</title><link>http://arxiv.org/abs/2309.02233v2</link><description>Large-scale language models (LLMs) like ChatGPT have demonstrated impressiveabilities in generating responses based on human instructions. However, theiruse in the medical field can be challenging due to their lack of specific,in-depth knowledge. In this study, we present a system called LLMs Augmentedwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs inspecialized domains. LLM-AMT integrates authoritative medical textbooks intothe LLMs' framework using plug-and-play modules. These modules include a QueryAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,they incorporate authoritative medical knowledge. Additionally, an LLM Readeraids in contextual understanding. Our experimental results on three medical QAtasks demonstrate that LLMAMT significantly improves response quality, withaccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as thebase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained ona massive amount of medical corpus by 2-3%. We found that despite being 100xsmaller in size, medical textbooks as a retrieval corpus is proven to be a moreeffective knowledge database than Wikipedia in the medical domain, boostingperformance by 7.8%-13.7%.</description><author>Yubo Wang, Xueguang Ma, Wenhu Chen</author><pubDate>Thu, 22 Feb 2024 16:32:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.02233v2</guid></item><item><title>Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</title><link>http://arxiv.org/abs/2402.08277v2</link><description>Advances towards more faithful and traceable answers of Large Language Models(LLMs) are crucial for various research and practical endeavors. One avenue inreaching this goal is basing the answers on reliable sources. However, thisEvidence-Based QA has proven to work insufficiently with LLMs in terms ofciting the correct sources (source quality) and truthfully representing theinformation within sources (answer attributability). In this work, wesystematically investigate how to robustly fine-tune LLMs for better sourcequality and answer attributability. Specifically, we introduce a datageneration pipeline with automated data quality filters, which can synthesizediversified high-quality training and testing data at scale. We furtherintroduce four test sets to benchmark the robustness of fine-tuned specialistmodels. Extensive evaluation shows that fine-tuning on synthetic data improvesperformance on both in- and out-of-distribution. Furthermore, we show that dataquality, which can be drastically improved by proposed quality filters, mattersmore than quantity in improving Evidence-Based QA.</description><author>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold</author><pubDate>Fri, 16 Feb 2024 11:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08277v2</guid></item><item><title>Grounded Question-Answering in Long Egocentric Videos</title><link>http://arxiv.org/abs/2312.06505v3</link><description>Existing approaches to video understanding, mainly designed for short videosfrom a third-person perspective, are limited in their applicability in certainfields, such as robotics. In this paper, we delve into open-endedquestion-answering (QA) in long, egocentric videos, which allows individuals orrobots to inquire about their own past visual experiences. This task presentsunique challenges, including the complexity of temporally grounding querieswithin extensive video content, the high resource demands for precise dataannotation, and the inherent difficulty of evaluating open-ended answers due totheir ambiguous nature. Our proposed approach tackles these challenges by (i)integrating query grounding and answering within a unified model to reduceerror propagation; (ii) employing large language models for efficient andscalable data synthesis; and (iii) introducing a close-ended QA task forevaluation, to manage answer ambiguity. Extensive experiments demonstrate theeffectiveness of our method, which also achieves state-of-the-art performanceon the QAEgo4D and Ego4D-NLQ benchmarks. Code, data, and models are availableat https://github.com/Becomebright/GroundVQA.</description><author>Shangzhe Di, Weidi Xie</author><pubDate>Thu, 15 Feb 2024 15:18:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06505v3</guid></item><item><title>Answer is All You Need: Instruction-following Text Embedding via Answering the Question</title><link>http://arxiv.org/abs/2402.09642v1</link><description>This work aims to build a text embedder that can capture characteristics oftexts specified by user instructions. Despite its tremendous potential todeploy user-oriented embeddings, none of previous approaches provides aconcrete solution for it. This paper offers a new viewpoint, which treats theinstruction as a question about the input text and encodes the expected answersto obtain the representation accordingly. Intuitively, texts with the same(implicit) semantics would share similar answers following the instruction,thus leading to more similar embeddings. Specifically, we propose InBedder thatinstantiates this embed-via-answering idea by only fine-tuning language modelson abstractive question answering tasks. InBedder demonstrates significantlyimproved instruction-following capabilities according to our proposedinstruction awareness tests and instruction robustness tests, when applied toboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-basedLMs (e.g., roberta-large). Additionally, our qualitative analysis of clusteringoutcomes, achieved by applying different instructions to the same corpus,demonstrates a high degree of interpretability.</description><author>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang</author><pubDate>Thu, 15 Feb 2024 01:02:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09642v1</guid></item><item><title>Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models</title><link>http://arxiv.org/abs/2402.13492v1</link><description>While large language models (LMs) demonstrate remarkable performance, theyencounter challenges in providing accurate responses when queried forinformation beyond their pre-trained memorization. Although augmenting themwith relevant external information can mitigate these issues, failure toconsider the necessity of retrieval may adversely affect overall performance.Previous research has primarily focused on examining how entities influenceretrieval models and knowledge recall in LMs, leaving other aspects relativelyunexplored. In this work, our goal is to offer a more detailed, fact-centricanalysis by exploring the effects of combinations of entities and relations. Tofacilitate this, we construct a new question answering (QA) dataset calledWiTQA (Wikipedia Triple Question Answers). This dataset includes questionsabout entities and relations of various popularity levels, each accompanied bya supporting passage. Our extensive experiments with diverse LMs and retrieversreveal when retrieval does not consistently enhance LMs from the viewpoints offact-centric popularity.Confirming earlier findings, we observe that larger LMsexcel in recalling popular facts. However, they notably encounter difficultywith infrequent entity-relation pairs compared to retrievers. Interestingly,they can effectively retain popular relations of less common entities. Wedemonstrate the efficacy of our finer-grained metric and insights through anadaptive retrieval system that selectively employs retrieval and recall basedon the frequencies of entities and relations in the question.</description><author>Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani</author><pubDate>Wed, 21 Feb 2024 03:05:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13492v1</guid></item><item><title>BiMediX: Bilingual Medical Mixture of Experts LLM</title><link>http://arxiv.org/abs/2402.13253v1</link><description>In this paper, we introduce BiMediX, the first bilingual medical mixture ofexperts LLM designed for seamless interaction in both English and Arabic. Ourmodel facilitates a wide range of medical interactions in English and Arabic,including multi-turn chats to inquire about additional details such as patientsymptoms and medical history, multiple-choice question answering, andopen-ended question answering. We propose a semi-automated English-to-Arabictranslation pipeline with human refinement to ensure high-quality translations.We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingualinstruction set covering 1.3 Million diverse medical interactions, resulting inover 632 million healthcare specialized tokens for instruction tuning. OurBiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats andmaintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-artMed42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively,computed across multiple medical evaluation benchmarks in English, whileoperating at 8-times faster inference. Moreover, our BiMediX outperforms thegeneric Arabic-English bilingual LLM, Jais-30B, by average absolute gains of10% on our Arabic medical benchmark and 15% on bilingual evaluations acrossmultiple datasets. Our project page with source code and trained model isavailable at https://github.com/mbzuai-oryx/BiMediX .</description><author>Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal</author><pubDate>Tue, 20 Feb 2024 18:59:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13253v1</guid></item><item><title>An Evaluation of GPT-4V and Gemini in Online VQA</title><link>http://arxiv.org/abs/2312.10637v2</link><description>While there is much excitement about the potential of large multimodal models(LMM), a comprehensive evaluation is critical to establish their truecapabilities and limitations. In support of this aim, we evaluate twostate-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answeringdataset sourced from an authentic online question answering community. Weconduct fine-grained analysis by generating seven types of metadata for nearly2,000 visual questions, such as image type and the required image processingcapabilities. Our zero-shot performance analysis highlights the types ofquestions that are most challenging for both models, including questionsrelated to "puzzling" topic, with "Identification" user intention, with "SheetMusic" image type, or labeled as "hard" by GPT-4.</description><author>Mengchen Liu, Chongyan Chen, Danna Gurari</author><pubDate>Wed, 14 Feb 2024 03:49:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10637v2</guid></item><item><title>Debating with More Persuasive LLMs Leads to More Truthful Answers</title><link>http://arxiv.org/abs/2402.06782v2</link><description>Common methods for aligning large language models (LLMs) with desiredbehaviour heavily rely on human-labelled data. However, as models growincreasingly sophisticated, they will surpass human expertise, and the role ofhuman evaluation will evolve into non-experts overseeing experts. Inanticipation of this, we ask: can weaker models assess the correctness ofstronger models? We investigate this question in an analogous setting, wherestronger models (experts) possess the necessary information to answer questionsand weaker models (non-experts) lack this information. The method we evaluateis \textit{debate}, where two LLM experts each argue for a different answer,and a non-expert selects the answer. We find that debate consistently helpsboth non-expert models and humans answer questions, achieving 76\% and 88\%accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore,optimising expert debaters for persuasiveness in an unsupervised mannerimproves non-expert ability to identify the truth in debates. Our resultsprovide encouraging empirical evidence for the viability of aligning modelswith debate in the absence of ground truth.</description><author>Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rockt√§schel, Ethan Perez</author><pubDate>Thu, 15 Feb 2024 22:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06782v2</guid></item><item><title>SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning</title><link>http://arxiv.org/abs/2401.13246v2</link><description>Elucidating the reasoning process with structured explanations from questionto answer is crucial, as it significantly enhances the interpretability,traceability, and trustworthiness of question-answering (QA) systems. However,structured explanations demand models to perform intricately structuredreasoning, which poses great challenges. Most existing methods focus onsingle-step reasoning through supervised learning, ignoring logicaldependencies between steps. Moreover, existing reinforcement learning (RL)based methods overlook the structured relationships, underutilizing thepotential of RL in structured reasoning. In this paper, we propose SEER, anovel method that maximizes a structure-based return to facilitate structuredreasoning and explanation. Our proposed structure-based return preciselydescribes the hierarchical and branching structure inherent in structuredreasoning, effectively capturing the intricate relationships between differentreasoning steps. In addition, we introduce a fine-grained reward function tometiculously delineate diverse reasoning steps. Extensive experiments show thatSEER significantly outperforms state-of-the-art methods, achieving an absoluteimprovement of 6.9% over RL-based methods on EntailmentBank, a 4.4% averageimprovement on STREET benchmark, and exhibiting outstanding efficiency andcross-dataset generalization performance.</description><author>Guoxin Chen, Kexin Tang, Chao Yang, Fuying Ye, Yu Qiao, Yiming Qian</author><pubDate>Fri, 16 Feb 2024 14:16:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.13246v2</guid></item><item><title>Slot-VLM: SlowFast Slots for Video-Language Modeling</title><link>http://arxiv.org/abs/2402.13088v1</link><description>Video-Language Models (VLMs), powered by the advancements in Large LanguageModels (LLMs), are charting new frontiers in video understanding. A pivotalchallenge is the development of an efficient method to encapsulate videocontent into a set of representative tokens to align with LLMs. In this work,we introduce Slot-VLM, a novel framework designed to generate semanticallydecomposed video tokens, in terms of object-wise and event-wise visualrepresentations, to facilitate LLM inference. Particularly, we design aSlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the densevideo tokens from the CLIP vision encoder to a set of representative slots. Inorder to take into account both the spatial object details and the variedtemporal dynamics, SF-Slots is built with a dual-branch structure. TheSlow-Slots branch focuses on extracting object-centric slots from features athigh spatial resolution but low (slow) frame sample rate, emphasizing detailedobject information. Conversely, Fast-Slots branch is engineered to learnevent-centric slots from high temporal sample rate but low spatial resolutionfeatures. These complementary slots are combined to form the vision context,serving as the input to the LLM for efficient question answering. Ourexperimental results demonstrate the effectiveness of our Slot-VLM, whichachieves the state-of-the-art performance on video question-answering.</description><author>Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu</author><pubDate>Tue, 20 Feb 2024 15:30:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13088v1</guid></item><item><title>MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning</title><link>http://arxiv.org/abs/2402.11260v1</link><description>Adapting large language models (LLMs) to new domains/tasks and enabling themto be efficient lifelong learners is a pivotal challenge. In this paper, wepropose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation forLifelong Learning. MoRAL combines the multi-tasking abilities of MoE with thefine-tuning abilities of LoRA for effective life-long learning of LLMs. Incontrast to the conventional approaches that use factual triplets as inputsMoRAL relies on simple question-answer pairs, which is a more practical andeffective strategy for robust and efficient learning. Owing to new datasettings, we introduce a new evaluation benchmark namely: Life Long Learning ofLLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,and a set of evaluation metrics for rigorous evaluation of MoRAL in open-bookand closed-book settings. Experimental evaluation shows (i) LLMs learn fast inopen-book settings with up to 30.15% improvement in "RA" for Phi-2-2.7Bcompared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL showshigher performance improvement for models with a greater number of parameters;(iii) MoRAL is robust to catastrophic forgetting offering better knowledgeretention compared to baselines.</description><author>Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, Di Wang</author><pubDate>Sat, 17 Feb 2024 12:25:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11260v1</guid></item><item><title>ScreenAI: A Vision-Language Model for UI and Infographics Understanding</title><link>http://arxiv.org/abs/2402.04615v2</link><description>Screen user interfaces (UIs) and infographics, sharing similar visuallanguage and design principles, play important roles in human communication andhuman-machine interaction. We introduce ScreenAI, a vision-language model thatspecializes in UI and infographics understanding. Our model improves upon thePaLI architecture with the flexible patching strategy of pix2struct and istrained on a unique mixture of datasets. At the heart of this mixture is anovel screen annotation task in which the model has to identify the type andlocation of UI elements. We use these text annotations to describe screens toLarge Language Models and automatically generate question-answering (QA), UInavigation, and summarization training datasets at scale. We run ablationstudies to demonstrate the impact of these design choices. At only 5Bparameters, ScreenAI achieves new state-of-the-artresults on UI- andinfographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and WidgetCaptioning), and new best-in-class performance on others (Chart QA, DocVQA, andInfographicVQA) compared to models of similar size. Finally, we release threenew datasets: one focused on the screen annotation task and two others focusedon question answering.</description><author>Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor CƒÉrbune, Jason Lin, Jindong Chen, Abhanshu Sharma</author><pubDate>Mon, 19 Feb 2024 17:03:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.04615v2</guid></item><item><title>Instruction-tuned Language Models are Better Knowledge Learners</title><link>http://arxiv.org/abs/2402.12847v1</link><description>In order for large language model (LLM)-based assistants to effectively adaptto evolving information needs, it must be possible to update their factualknowledge through continued training on new data. The standard recipe for doingso involves continued pre-training on new documents followed byinstruction-tuning on question-answer (QA) pairs. However, we find that LLMstrained with this recipe struggle to answer questions, even though theperplexity of documents is minimized. We found that QA pairs are generallystraightforward, while documents are more complex, weaving many factualstatements together in an intricate manner. Therefore, we hypothesize that itis beneficial to expose LLMs to QA pairs before continued pre-training ondocuments so that the process of encoding knowledge from complex documentstakes into account how this knowledge is accessed through questions. Based onthis, we propose pre-instruction-tuning (PIT), a method that instruction-tuneson questions prior to training on documents. This contrasts with standardinstruction-tuning, which learns how to extract knowledge after training ondocuments. Extensive experiments and ablation studies demonstrate that PITsignificantly enhances the ability of LLMs to absorb knowledge from newdocuments, outperforming standard instruction-tuning by 17.8%.</description><author>Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer</author><pubDate>Tue, 20 Feb 2024 09:20:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12847v1</guid></item><item><title>Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?</title><link>http://arxiv.org/abs/2402.12483v1</link><description>Multiple-choice question answering (MCQA) is often used to evaluate largelanguage models (LLMs). To see if MCQA assesses LLMs as intended, we probe ifLLMs can perform MCQA with choices-only prompts, where models must select thecorrect answer only from the choices. In three MCQA datasets and four LLMs,this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracygain. To help explain this behavior, we conduct an in-depth, black-box analysison memorization, choice dynamics, and question inference. Our key findings arethreefold. First, we find no evidence that the choices-only accuracy stems frommemorization alone. Second, priors over individual choices do not fully explainchoices-only accuracy, hinting that LLMs use the group dynamics of choices.Third, LLMs have some ability to infer a relevant question from choices, andsurprisingly can sometimes even match the original question. We hope tomotivate the use of stronger baselines in MCQA benchmarks, the design of robustMCQA datasets, and further efforts to explain LLM decision-making.</description><author>Nishant Balepur, Abhilasha Ravichander, Rachel Rudinger</author><pubDate>Mon, 19 Feb 2024 19:38:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12483v1</guid></item><item><title>GenDec: A robust generative Question-decomposition method for Multi-hop reasoning</title><link>http://arxiv.org/abs/2402.11166v1</link><description>Multi-hop QA (MHQA) involves step-by-step reasoning to answer complexquestions and find multiple relevant supporting facts. However, Existing largelanguage models'(LLMs) reasoning ability in multi-hop question answeringremains exploration, which is inadequate in answering multi-hop questions.Moreover, it is unclear whether LLMs follow a desired reasoning chain to reachthe right final answer. In this paper, we propose a \textbf{gen}erativequestion \textbf{dec}omposition method (GenDec) from the perspective ofexplainable QA by generating independent and complete sub-questions based onincorporating additional extracted evidence for enhancing LLMs' reasoningability in RAG. To demonstrate the impact, generalization, and robustness ofGendec, we conduct two experiments, the first is combining GenDec with small QAsystems on paragraph retrieval and QA tasks. We secondly examine the reasoningcapabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA,MuSiQue, and PokeMQA datasets.</description><author>Jian Wu, Linyi Yang, Yuliang Ji, Wenhao Huang, B√∂rje F. Karlsson, Manabu Okumura</author><pubDate>Sat, 17 Feb 2024 02:21:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11166v1</guid></item><item><title>MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition</title><link>http://arxiv.org/abs/2402.11924v1</link><description>Although Large Language Models (LLMs) have shown strong performance inMulti-hop Question Answering (MHQA) tasks, their real reasoning ability remainsexploration. Current LLM QA evaluation benchmarks have shown limitations,including 1) data contamination, the evaluation data are potentially exposed toLLMs during the pretraining stage; and 2) ignoration of the reasoning chainevaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QAbenchmark based on the new, unprecedented knowledge by editing theoff-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate thereasoning chain in the form of sub-questions and intermediate answerscorresponding to the multi-hop questions. Specifically, based on theobservation, 1) LLMs show a performance gap between the original HotpotQA andour edited data, deeming that current MHQA benchmarks have the potential riskof data contamination that hard to evaluate LLMs' performance objectively andscientifically; 2) LLMs only get a small percentage of the right reasoningchain, e.g. GPT-4 only gets 36.3\% right reasoning chain. We believe this newMulti-hop QA evaluation benchmark and novel evaluation methods will facilitatethe development of trustworthy LLM evaluation on the MHQA task.</description><author>Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang</author><pubDate>Mon, 19 Feb 2024 08:12:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11924v1</guid></item><item><title>KnowGPT: Black-Box Knowledge Injection for Large Language Models</title><link>http://arxiv.org/abs/2312.06185v2</link><description>Generative Large Language Models (LLMs), such as ChatGPT, offer interactiveAPIs that can answer common questions at a human-expert level. However, thesemodels often give inaccurate or incorrect responses when faced with questionsrequiring domain-specific or professional-specific knowledge not covered intheir training corpus. Furthermore, many state-of-the-art LLMs are notopen-source, making it challenging to inject knowledge with model APIs only. Inthis work, we introduce KnowGPT, a black-box knowledge injection framework forLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-ArmedBandit (MAB) to construct the most suitable prompt for each question. Ourextensive experiments on three benchmark datasets showcase that KnowGPTsignificantly enhances the existing methods. Notably, KnowGPT achieves anaverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%over GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQAofficial leaderboard, which is comparable to human-level performance.</description><author>Qinggang Zhang, Junnan Dong, Hao Chen, Xiao Huang, Daochen Zha, Zailiang Yu</author><pubDate>Mon, 19 Feb 2024 09:28:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06185v2</guid></item><item><title>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</title><link>http://arxiv.org/abs/2402.11651v1</link><description>Large language models (LLMs) have achieved success in acting as agents, whichinteract with environments through tools like search engines. However, LLMs arenot optimized specifically for tool use during training or alignment, limitingtheir effectiveness as agents. To resolve this problem, previous work hascollected interaction trajectories between GPT-4 and environments, andfine-tuned smaller models with them. As part of this, the standard approach hasbeen to simply discard trajectories that do not finish the task successfully,which, on the one hand, leads to a significant waste of data and resources, andon the other hand, has the potential to limit the possible optimization pathsduring fine-tuning. In this paper, we contend that large language models canlearn from failures through appropriate data cleaning and fine-tuningstrategies. We conduct experiments on mathematical reasoning, multi-hopquestion answering, and strategic question answering tasks. Experimentalresults demonstrate that compared to solely using positive examples,incorporating negative examples enhances model performance by a large margin.</description><author>Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, Timothy Baldwin</author><pubDate>Sun, 18 Feb 2024 17:10:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11651v1</guid></item><item><title>Plausible Extractive Rationalization through Semi-Supervised Entailment Signal</title><link>http://arxiv.org/abs/2402.08479v3</link><description>The increasing use of complex and opaque black box models requires theadoption of interpretable measures, one such option is extractive rationalizingmodels, which serve as a more interpretable alternative. These models, alsoknown as Explain-Then-Predict models, employ an explainer model to extractrationales and subsequently condition the predictor with the extractedinformation. Their primary objective is to provide precise and faithfulexplanations, represented by the extracted rationales. In this paper, we take asemi-supervised approach to optimize for the plausibility of extractedrationales. We adopt a pre-trained natural language inference (NLI) model andfurther fine-tune it on a small set of supervised rationales ($10\%$). The NLIpredictor is leveraged as a source of supervisory signals to the explainer viaentailment alignment. We show that, by enforcing the alignment agreementbetween the explanation and answer in a question-answering task, theperformance can be improved without access to ground truth labels. We evaluateour approach on the ERASER dataset and show that our approach achievescomparable results with supervised extractive models and outperformsunsupervised approaches by $&gt; 100\%$.</description><author>Yeo Wei Jie, Ranjan Satapathy, Erik Cambria</author><pubDate>Fri, 16 Feb 2024 09:57:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.08479v3</guid></item><item><title>$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning</title><link>http://arxiv.org/abs/2402.10150v1</link><description>In self-supervised contrastive learning, a widely-adopted objective functionis InfoNCE, which uses the heuristic cosine similarity for the representationcomparison, and is closely related to maximizing the Kullback-Leibler(KL)-based mutual information. In this paper, we aim at answering twointriguing questions: (1) Can we go beyond the KL-based objective? (2) Besidesthe popular cosine similarity, can we design a better similarity function? Weprovide answers to both questions by generalizing the KL-based mutualinformation to the $f$-Mutual Information in Contrastive Learning ($f$-MICL)using the $f$-divergences. To answer the first question, we provide a widerange of $f$-MICL objectives which share the nice properties of InfoNCE (e.g.,alignment and uniformity), and meanwhile result in similar or even superiorperformance. For the second question, assuming that the joint featuredistribution is proportional to the Gaussian kernel, we derive an $f$-Gaussiansimilarity with better interpretability and empirical performance. Finally, weidentify close relationships between the $f$-MICL objective and several popularInfoNCE-based objectives. Using benchmark tasks from both vision and naturallanguage, we empirically evaluate $f$-MICL with different $f$-divergences onvarious architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that$f$-MICL generally outperforms the benchmarks and the best-performing$f$-divergence is task and dataset dependent.</description><author>Yiwei Lu, Guojun Zhang, Sun Sun, Hongyu Guo, Yaoliang Yu</author><pubDate>Thu, 15 Feb 2024 17:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10150v1</guid></item><item><title>Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA</title><link>http://arxiv.org/abs/2401.15847v2</link><description>Multipanel images, commonly seen as web screenshots, posters, etc., pervadeour daily lives. These images, characterized by their composition of multiplesubfigures in distinct layouts, effectively convey information to people.Toward building advanced multimodal AI applications, such as agents thatunderstand complex scenes and navigate through webpages, the skill ofmultipanel visual reasoning is essential, and a comprehensive evaluation ofmodels in this regard is important. Therefore, we introduce Multipanel VisualQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 tripletsof questions, answers, and multipanel images that specifically challenge modelsin comprehending multipanel images. Our evaluation shows that questions in theMultipanelVQA benchmark pose significant challenges to the state-of-the-artLarge Vision Language Models (LVLMs) tested, even though humans can attainapproximately 99\% accuracy on these questions. Distinctively, theMultipanelVQA benchmark features synthetically generated multipanel imagesspecifically crafted to isolate and assess the impact of various factors, suchas the layout, on LVLMs' multipanel image comprehension abilities. As a result,in addition to benchmarking the capabilities of LVLMs in understandingmultipanel images, we analyze the potential causes for LVLMs' performance andoffer insights for enhancement with the synthetic data. Code and data arereleased at https://sites.google.com/view/multipanelvqa/home.</description><author>Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang</author><pubDate>Mon, 19 Feb 2024 05:14:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.15847v2</guid></item><item><title>Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games</title><link>http://arxiv.org/abs/2310.01468v3</link><description>Large language models (LLMs) are effective at answering questions that areclearly asked. However, when faced with ambiguous queries they can actunpredictably and produce incorrect outputs. This underscores the need for thedevelopment of intelligent agents capable of asking clarification questions toresolve ambiguities effectively. This capability requires complexunderstanding, state tracking, reasoning and planning over multipleconversational turns. However, directly measuring this can be challenging. Inthis paper, we offer a surrogate problem which assesses an LLMs's capability todeduce an entity unknown to itself, but revealed to a judge, by asking thejudge a series of queries. This \textit{entity-deducing game} can serve as anevaluation framework to probe the conversational reasoning and planningcapabilities of language models. We systematically evaluate various LLMs anddiscover significant differences in their performance on this task. We findthat strong LLMs like GPT-4 outperform human players by a large margin. Wefurther employ Behavior Cloning (BC) to examine whether a weaker model iscapable of imitating a stronger model and generalizing to data or domains,using only the demonstrations from a stronger model. We finally propose to useReinforcement Learning to enhance reasoning and planning capacity of Vicunamodels through episodes of game playing, which lead to significant performanceimprovement. We hope that this problem offers insights into how autonomousagents could be trained to behave more intelligently in ambiguouscircumstances.</description><author>Yizhe Zhang, Jiarui Lu, Navdeep Jaitly</author><pubDate>Tue, 20 Feb 2024 21:24:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.01468v3</guid></item><item><title>GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding</title><link>http://arxiv.org/abs/2402.06764v2</link><description>Integrating large language models (LLMs) with knowledge graphs derived fromdomain-specific data represents an important advancement towards more powerfuland factual reasoning. As these models grow more capable, it is crucial toenable them to perform multi-step inferences over real-world knowledge graphswhile minimizing hallucination. While large language models excel atconversation and text generation, their ability to reason overdomain-specialized graphs of interconnected entities remains limited. Forexample, can we query a LLM to identify the optimal contact in a professionalnetwork for a specific goal, based on relationships and attributes in a privatedatabase? The answer is no--such capabilities lie beyond current methods.However, this question underscores a critical technical gap that must beaddressed. Many high-value applications in areas such as science, security, ande-commerce rely on proprietary knowledge graphs encoding unique structures,relationships, and logical constraints. We introduce a fine-tuning frameworkfor developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledgegraph into an alternate text representation with labeled question-answer pairs.We demonstrate that grounding the models in specific graph-based knowledgeexpands the models' capacity for structure-based reasoning. Our methodologyleverages the large-language model's generative capabilities to create thedataset and proposes an efficient alternate to retrieval-augmented generationstyled methods.</description><author>Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, Sutanay Choudhury</author><pubDate>Fri, 16 Feb 2024 17:23:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06764v2</guid></item><item><title>Tokenization Preference for Human and Machine Learning Model: An Annotation Study</title><link>http://arxiv.org/abs/2304.10813v3</link><description>Is preferred tokenization for humans also preferred for machine-learning (ML)models? This study examines the relations between preferred tokenization forhumans (appropriateness and readability) and one for ML models (performance onan NLP task). The question texts of the Japanese commonsense question-answeringdataset are tokenized with six different tokenizers, and the performances ofhuman annotators and ML models were compared. Furthermore, we analyze relationsamong performance of answers by human and ML model, the appropriateness oftokenization for human, and response time to questions by human. This studyprovides a quantitative investigation result that shows that preferredtokenizations for humans and ML models are not necessarily always the same. Theresult also implies that existing methods using language models fortokenization could be a good compromise both for human and ML models.</description><author>Tatsuya Hiraoka, Tomoya Iwakura</author><pubDate>Fri, 16 Feb 2024 07:55:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.10813v3</guid></item><item><title>Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions</title><link>http://arxiv.org/abs/2402.13514v1</link><description>Retrieve-then-read and generate-then-read are two typical solutions to handleunknown and known questions in open-domain question-answering, while the formerretrieves necessary external knowledge and the later prompt the large languagemodels to generate internal known knowledge encoded in the parameters. However,few of previous works consider the compositional unknown questions, whichconsist of several known or unknown sub-questions. Thus, simple binaryclassification (known or unknown) becomes sub-optimal and inefficient since itwill call external retrieval excessively for each compositional unknownquestion. To this end, we propose the first Compositional unknownQuestion-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer(Self-DC) framework to empower LLMs to adaptively call different methodson-demand, resulting in better performance and efficiency. Experimental resultson two datasets (CuQA and FreshQA) demonstrate that Self-DC can achievecomparable or even better performance with much more less retrieval timescompared with several strong baselines.</description><author>Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Guanhua Chen, Huimin Wang, Kam-fai Wong</author><pubDate>Wed, 21 Feb 2024 03:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13514v1</guid></item><item><title>Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation</title><link>http://arxiv.org/abs/2402.11493v1</link><description>In recent years, substantial advancements have been made in the developmentof large language models, achieving remarkable performance across diversetasks. To evaluate the knowledge ability of language models, previous studieshave proposed lots of benchmarks based on question-answering pairs. We arguethat it is not reliable and comprehensive to evaluate language models with afixed question or limited paraphrases as the query, since language models aresensitive to prompt. Therefore, we introduce a novel concept named knowledgeboundary to encompass both prompt-agnostic and prompt-sensitive knowledgewithin language models. Knowledge boundary avoids prompt sensitivity inlanguage model evaluations, rendering them more dependable and robust. Toexplore the knowledge boundary for a given model, we propose projected gradientdescent method with semantic constraints, a new algorithm designed to identifythe optimal prompt for each piece of knowledge. Experiments demonstrate asuperior performance of our algorithm in computing the knowledge boundarycompared to existing methods. Furthermore, we evaluate the ability of multiplelanguage models in several domains with knowledge boundary.</description><author>Xunjian Yin, Xu Zhang, Jie Ruan, Xiaojun Wan</author><pubDate>Sun, 18 Feb 2024 07:48:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11493v1</guid></item><item><title>Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs</title><link>http://arxiv.org/abs/2402.11199v1</link><description>Large language models (LLMs) demonstrate strong reasoning abilities whenprompted to generate chain-of-thought (CoT) explanations alongside answers.However, previous research on evaluating LLMs has solely focused on answeraccuracy, neglecting the correctness of the generated CoT. In this paper, wedelve deeper into the CoT reasoning capabilities of LLMs in multi-hop questionanswering by utilizing knowledge graphs (KGs). We propose a noveldiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledgeof reasoning and the accuracy of the generated CoT. Through experimentsconducted on 5 different families of LLMs across 2 multi-hop question-answeringdatasets, we find that LLMs possess sufficient knowledge to perform reasoning.However, there exists a significant disparity between answer accuracy andfaithfulness of the CoT reasoning generated by LLMs, indicating that they oftenarrive at correct answers through incorrect reasoning.</description><author>Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari</author><pubDate>Sat, 17 Feb 2024 05:22:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11199v1</guid></item><item><title>BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence</title><link>http://arxiv.org/abs/2402.12174v1</link><description>Retrieval-augmented large language models (LLMs) have demonstrated efficacyin knowledge-intensive tasks such as open-domain QA, addressing inherentchallenges in knowledge update and factual inadequacy. However, inconsistenciesbetween retrieval knowledge and the necessary knowledge for LLMs, leading to adecline in LLM's answer quality. This paper introduces BIDER, an approach thatrefines retrieval documents into Key Supporting Evidence (KSE) throughknowledge synthesis, supervised fine-tuning (SFT), and preference alignment. Wetrain BIDER by learning from crafting KSE, while maximizing its output to alignwith LLM's information acquisition preferences through reinforcement learning.Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7%while reducing input content length in retrieval documents by 80%,outperforming existing methods. The proposed KSE simulation effectively equipsLLMs with essential information for accurate question answering.</description><author>Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou</author><pubDate>Mon, 19 Feb 2024 14:28:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12174v1</guid></item><item><title>OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models</title><link>http://arxiv.org/abs/2310.07637v3</link><description>Information Technology (IT) Operations (Ops), particularly ArtificialIntelligence for IT Operations (AIOps), is the guarantee for maintaining theorderly and stable operation of existing information systems. According toGartner's prediction, the use of AI technology for automated IT operations hasbecome a new trend. Large language models (LLMs) that have exhibited remarkablecapabilities in NLP-related tasks, are showing great potential in the field ofAIOps, such as in aspects of root cause analysis of failures, generation ofoperations and maintenance scripts, and summarizing of alert information.Nevertheless, the performance of current LLMs in Ops tasks is yet to bedetermined. In this paper, we present OpsEval, a comprehensive task-orientedOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs'proficiency in various crucial scenarios at different ability levels. Thebenchmark includes 7184 multi-choice questions and 1736 question-answering (QA)formats in English and Chinese. By conducting a comprehensive performanceevaluation of the current leading large language models, we show how variousLLM techniques can affect the performance of Ops, and discussed findingsrelated to various topics, including model quantification, QA evaluation, andhallucination issues. To ensure the credibility of our evaluation, we invitedozens of domain experts to manually review our questions. At the same time, wehave open-sourced 20% of the test QA to assist current researchers inpreliminary evaluations of their OpsLLM models. The remaining 80% of the data,which is not disclosed, is used to eliminate the issue of the test set leakage.Additionally, we have constructed an online leaderboard that is updated inreal-time and will continue to be updated, ensuring that any newly emergingLLMs will be evaluated promptly. Both our dataset and leaderboard have beenmade public.</description><author>Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li, Gaogang Xie, Xidao Wen, Xiaohui Nie, Minghua Ma, Dan Pei</author><pubDate>Fri, 16 Feb 2024 08:17:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.07637v3</guid></item><item><title>Benchmarking Retrieval-Augmented Generation for Medicine</title><link>http://arxiv.org/abs/2402.13178v1</link><description>While large language models (LLMs) have achieved state-of-the-art performanceon a wide range of medical question answering (QA) tasks, they still facechallenges with hallucinations and outdated knowledge. Retrieval-augmentedgeneration (RAG) is a promising solution and has been widely adopted. However,a RAG system can involve multiple flexible components, and there is a lack ofbest practices regarding the optimal RAG setting for various medical purposes.To systematically evaluate such systems, we propose the Medical InformationRetrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kindbenchmark including 7,663 questions from five medical QA datasets. UsingMIRAGE, we conducted large-scale experiments with over 1.8 trillion prompttokens on 41 combinations of different corpora, retrievers, and backbone LLMsthrough the MedRAG toolkit introduced in this work. Overall, MedRAG improvesthe accuracy of six different LLMs by up to 18% over chain-of-thoughtprompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Ourresults show that the combination of various medical corpora and retrieversachieves the best performance. In addition, we discovered a log-linear scalingproperty and the "lost-in-the-middle" effects in medical RAG. We believe ourcomprehensive evaluations can serve as practical guidelines for implementingRAG systems for medicine.</description><author>Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang</author><pubDate>Tue, 20 Feb 2024 17:44:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13178v1</guid></item><item><title>CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</title><link>http://arxiv.org/abs/2401.17043v2</link><description>Retrieval-Augmented Generation (RAG) is a technique that enhances thecapabilities of large language models (LLMs) by incorporating externalknowledge sources. This method addresses common LLM limitations, includingoutdated information and the tendency to produce inaccurate "hallucinated"content. However, the evaluation of RAG systems is challenging, as existingbenchmarks are limited in scope and diversity. Most of the current benchmarkspredominantly assess question-answering applications, overlooking the broaderspectrum of situations where RAG could prove advantageous. Moreover, they onlyevaluate the performance of the LLM component of the RAG pipeline in theexperiments, and neglect the influence of the retrieval component and theexternal knowledge database. To address these issues, this paper constructs alarge-scale and more comprehensive benchmark, and evaluates all the componentsof RAG systems in various RAG application scenarios. Specifically, we havecategorized the range of RAG applications into four distinct types-Create,Read, Update, and Delete (CRUD), each representing a unique use case. "Create"refers to scenarios requiring the generation of original, varied content."Read" involves responding to intricate questions in knowledge-intensivesituations. "Update" focuses on revising and rectifying inaccuracies orinconsistencies in pre-existing texts. "Delete" pertains to the task ofsummarizing extensive texts into more concise forms. For each of these CRUDcategories, we have developed comprehensive datasets to evaluate theperformance of RAG systems. We also analyze the effects of various componentsof the RAG system, such as the retriever, the context length, the knowledgebase construction, and the LLM. Finally, we provide useful insights foroptimizing the RAG technology for different scenarios.</description><author>Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen, Yi Luo, Peng Cheng, Haiying Deng, Zhonghao Wang, Zijia Lu</author><pubDate>Mon, 19 Feb 2024 03:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.17043v2</guid></item><item><title>Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?</title><link>http://arxiv.org/abs/2402.14453v1</link><description>Education that suits the individual learning level is necessary to improvestudents' understanding. The first step in achieving this purpose by usinglarge language models (LLMs) is to adjust the textual difficulty of theresponse to students. This work analyzes how LLMs can implicitly adjust textdifficulty between user input and its generated text. To conduct theexperiments, we created a new dataset from Stack-Overflow to explore theperformance of question-answering-based conversation. Experimental results onthe Stack-Overflow dataset and the TSCC dataset, including multi-turnconversation show that LLMs can implicitly handle text difficulty between userinput and its generated response. We also observed that some LLMs can surpasshumans in handling text difficulty and the importance of instruction-tuning.</description><author>Seiji Gobara, Hidetaka Kamigaito, Taro Watanabe</author><pubDate>Thu, 22 Feb 2024 11:16:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14453v1</guid></item><item><title>Uncovering the Full Potential of Visual Grounding Methods in VQA</title><link>http://arxiv.org/abs/2401.07803v2</link><description>Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt toimprove VQA performance by strengthening a model's reliance onquestion-relevant visual information. The presence of such relevant informationin the visual input is typically assumed in training and testing. Thisassumption, however, is inherently flawed when dealing with imperfect imagerepresentations common in large-scale VQA, where the information carried byvisual features frequently deviates from expected ground-truth contents. As aresult, training and testing of VG-methods is performed with largely inaccuratedata, which obstructs proper assessment of their potential benefits. In thisstudy, we demonstrate that current evaluation schemes for VG-methods areproblematic due to the flawed assumption of availability of relevant visualinformation. Our experiments show that these methods can be much more effectivewhen evaluation conditions are corrected. Code is provided on GitHub.</description><author>Daniel Reich, Tanja Schultz</author><pubDate>Thu, 15 Feb 2024 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.07803v2</guid></item><item><title>AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis</title><link>http://arxiv.org/abs/2402.09742v1</link><description>The incorporation of Large Language Models (LLMs) in healthcare marks asignificant advancement. However, the application has predominantly beenlimited to discriminative and question-answering tasks, which does not fullyleverage their interactive potential. To address this limitation, our paperpresents AI Hospital, a framework designed to build a real-time interactivediagnosis environment. To simulate the procedure, we collect high-qualitymedical records to create patient, examiner, and medical director agents. AIHospital is then utilized for the interactive evaluation and collaboration ofLLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmarkwhere various LLMs serve as intern doctors for interactive diagnosis.Subsequently, to improve diagnostic accuracy, we introduce a collaborativemechanism that involves iterative discussions and a dispute resolution processunder the supervision of the medical director. In our experiments, we validatethe reliability of AI Hospital. The results not only explore the feasibility ofapply LLMs in clinical consultation but also confirm the effectiveness of thedispute resolution focused collaboration method.</description><author>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou</author><pubDate>Thu, 15 Feb 2024 06:46:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09742v1</guid></item><item><title>Probabilistic Reasoning in Generative Large Language Models</title><link>http://arxiv.org/abs/2402.09614v1</link><description>This paper considers the challenges that Large Language Models (LLMs) facewhen reasoning over text that includes information involving uncertaintyexplicitly quantified via probability values. This type of reasoning isrelevant to a variety of contexts ranging from everyday conversations tomedical decision-making. Despite improvements in the mathematical reasoningcapabilities of LLMs, they still exhibit significant difficulties when it comesto probabilistic reasoning. To deal with this problem, we first introduce theBayesian Linguistic Inference Dataset (BLInD), a new dataset specificallydesigned to test the probabilistic reasoning capabilities of LLMs. We thenleverage this new dataset to thoroughly illustrate the specific limitations ofLLMs for tasks involving probabilistic reasoning and present several strategiesthat map the problem to different formal representations, including Pythoncode, probabilistic inference algorithms, and probabilistic logicalprogramming. We conclude by providing an evaluation of our methods on BLInD andon an adaptation of a causal reasoning question-answering dataset, whichfurther shows their practical effectiveness.</description><author>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</author><pubDate>Wed, 14 Feb 2024 23:05:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09614v1</guid></item><item><title>OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM</title><link>http://arxiv.org/abs/2402.09181v1</link><description>Large Vision-Language Models (LVLMs) have demonstrated remarkablecapabilities in various multimodal tasks. However, their potential in themedical domain remains largely unexplored. A significant challenge arises fromthe scarcity of diverse medical images spanning various modalities andanatomical regions, which is essential in real-world medical applications. Tosolve this problem, in this paper, we introduce OmniMedVQA, a novelcomprehensive medical Visual Question Answering (VQA) benchmark. This benchmarkis collected from 75 different medical datasets, including 12 differentmodalities and covering more than 20 distinct anatomical regions. Importantly,all images in this benchmark are sourced from authentic medical scenarios,ensuring alignment with the requirements of the medical field and suitabilityfor evaluating LVLMs. Through our extensive experiments, we have found thatexisting LVLMs struggle to address these medical VQA problems effectively.Moreover, what surprises us is that medical-specialized LVLMs even exhibitinferior performance to those general-domain models, calling for a moreversatile and robust LVLM in the biomedical field. The evaluation results notonly reveal the current limitations of LVLM in understanding real medicalimages but also highlight our dataset's significance. Our dataset will be madepublicly available.</description><author>Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo</author><pubDate>Wed, 14 Feb 2024 13:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09181v1</guid></item><item><title>Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks</title><link>http://arxiv.org/abs/2402.09177v1</link><description>Large Language Models (LLMs) are susceptible to Jailbreaking attacks, whichaim to extract harmful information by subtly modifying the attack query. Asdefense mechanisms evolve, directly obtaining harmful information becomesincreasingly challenging for Jailbreaking attacks. In this work, inspired byhuman practices of indirect context to elicit harmful information, we focus ona new attack form called Contextual Interaction Attack. The idea relies on theautoregressive nature of the generation process in LLMs. We contend that theprior context--the information preceding the attack query--plays a pivotal rolein enabling potent Jailbreaking attacks. Specifically, we propose an approachthat leverages preliminary question-answer pairs to interact with the LLM. Bydoing so, we guide the responses of the model toward revealing the 'desired'harmful information. We conduct experiments on four different LLMs anddemonstrate the efficacy of this attack, which is black-box and can alsotransfer across LLMs. We believe this can lead to further developments andunderstanding of the context vector in LLMs.</description><author>Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos</author><pubDate>Wed, 14 Feb 2024 13:45:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.09177v1</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item><item><title>Multi-modal preference alignment remedies regression of visual instruction tuning on language model</title><link>http://arxiv.org/abs/2402.10884v1</link><description>In production, multi-modal large language models (MLLMs) are expected tosupport multi-turn queries of interchanging image and text modalities. However,the current MLLMs trained with visual-question-answering (VQA) datasets couldsuffer from degradation, as VQA datasets lack the diversity and complexity ofthe original text instruction datasets which the underlying language model hadbeen trained with. To address this challenging degradation, we first collect alightweight (6k entries) VQA preference dataset where answers were annotated byGemini for 5 quality metrics in a granular fashion, and investigate standardSupervised Fine-tuning, rejection sampling, Direct Preference Optimization(DPO), and SteerLM. Our findings indicate that the with DPO we are able tosurpass instruction-following capabilities of the language model, achieving a6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despitesmall data scale. This enhancement in textual instruction proficiencycorrelates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\%on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarkscompared to previous RLHF approach. In conclusion, we propose adistillation-based multi-modal alignment model with fine-grained annotations ona small dataset that reconciles the textual and visual performance of MLLMs,restoring and boosting language capability after visual instruction tuning.</description><author>Shengzhi Li, Rongyu Lin, Shichao Pei</author><pubDate>Fri, 16 Feb 2024 18:42:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10884v1</guid></item><item><title>Anchor-based Large Language Models</title><link>http://arxiv.org/abs/2402.07616v2</link><description>Large language models (LLMs) predominantly employ decoder-only transformerarchitectures, necessitating the retention of keys/values information forhistorical tokens to provide contextual information and avoid redundantcomputation. However, the substantial size and parameter volume of these LLMsrequire massive GPU memory. This memory demand increases with the length of theinput text, leading to an urgent need for more efficient methods of informationstorage and processing. This study introduces Anchor-based LLMs (AnLLMs), whichutilize an innovative anchor-based self-attention network (AnSAN) and also ananchor-based inference strategy. This approach enables LLMs to compresssequence information into an anchor token, reducing the keys/values cache andenhancing inference efficiency. Experiments on question-answering benchmarksreveal that AnLLMs maintain similar accuracy levels while achieving up to 99%keys/values cache reduction and up to 3.5 times faster inference. Despite aminor compromise in accuracy, the substantial enhancements of AnLLMs employingthe AnSAN technique in resource utilization and computational efficiencyunderscore their potential for practical LLM applications.</description><author>Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang</author><pubDate>Fri, 16 Feb 2024 16:58:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07616v2</guid></item><item><title>Inference to the Best Explanation in Large Language Models</title><link>http://arxiv.org/abs/2402.10767v1</link><description>While Large Language Models (LLMs) have found success in real-worldapplications, their underlying explanatory process is still poorly understood.This paper proposes IBE-Eval, a framework inspired by philosophical accounts onInference to the Best Explanation (IBE) to advance the interpretation andevaluation of LLMs' explanations. IBE-Eval estimates the plausibility ofnatural language explanations through a combination of explicit logical andlinguistic features including: consistency, parsimony, coherence, anduncertainty. Extensive experiments are conducted on Causal Question Answering(CQA), where \textit{IBE-Eval} is tasked to select the most plausible causalexplanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama2). The experiments reveal that IBE-Eval can successfully identify the bestexplanation with up to 77\% accuracy ($\approx 27\%$ above random), improvingupon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsicallymore efficient and interpretable. Additional analyses suggest that, despitemodel-specific variances, LLM-generated explanations tend to conform to IBEcriteria and that IBE-Eval is significantly correlated with human judgment,opening up opportunities for future development of automated explanationverification tools.</description><author>Dhairya Dalal, Marco Valentino, Andr√© Freitas, Paul Buitelaar</author><pubDate>Fri, 16 Feb 2024 15:41:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10767v1</guid></item><item><title>Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research</title><link>http://arxiv.org/abs/2402.10743v1</link><description>Entity and relationship extraction is a crucial component in natural languageprocessing tasks such as knowledge graph construction, question answeringsystem design, and semantic analysis. Most of the information of the Yishuischool of traditional Chinese Medicine (TCM) is stored in the form ofunstructured classical Chinese text. The key information extraction of TCMtexts plays an important role in mining and studying the academic schools ofTCM. In order to solve these problems efficiently using artificial intelligencemethods, this study constructs a word segmentation and entity relationshipextraction model based on conditional random fields under the framework ofnatural language processing technology to identify and extract the entityrelationship of traditional Chinese medicine texts, and uses the commonweighting technology of TF-IDF information retrieval and data mining to extractimportant key entity information in different ancient books. The dependencysyntactic parser based on neural network is used to analyze the grammaticalrelationship between entities in each ancient book article, and it isrepresented as a tree structure visualization, which lays the foundation forthe next construction of the knowledge graph of Yishui school and the use ofartificial intelligence methods to carry out the research of TCM academicschools.</description><author>Hanqing Zhao, Yuehan Li</author><pubDate>Fri, 16 Feb 2024 14:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10743v1</guid></item><item><title>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</title><link>http://arxiv.org/abs/2305.11738v3</link><description>Recent developments in large language models (LLMs) have been impressive.However, these models sometimes show inconsistencies and problematic behavior,such as hallucinating facts, generating flawed code, or creating offensive andtoxic content. Unlike these models, humans typically utilize external tools tocross-check and refine their initial content, like using a search engine forfact-checking, or a code interpreter for debugging. Inspired by thisobservation, we introduce a framework called CRITIC that allows LLMs, which areessentially "black boxes" to validate and progressively amend their own outputsin a manner similar to human interaction with tools. More specifically,starting with an initial output, CRITIC interacts with appropriate tools toevaluate certain aspects of the text, and then revises the output based on thefeedback obtained during this validation process. Comprehensive evaluationsinvolving free-form question answering, mathematical program synthesis, andtoxicity reduction demonstrate that CRITIC consistently enhances theperformance of LLMs. Meanwhile, our research highlights the crucial importanceof external feedback in promoting the ongoing self-improvement of LLMs.</description><author>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</author><pubDate>Fri, 16 Feb 2024 08:17:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11738v3</guid></item><item><title>BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains</title><link>http://arxiv.org/abs/2402.10373v1</link><description>Large Language Models (LLMs) have demonstrated remarkable versatility inrecent years, offering potential applications across specialized domains suchas healthcare and medicine. Despite the availability of various open-sourceLLMs tailored for health contexts, adapting general-purpose LLMs to the medicaldomain presents significant challenges. In this paper, we introduce BioMistral,an open-source LLM tailored for the biomedical domain, utilizing Mistral as itsfoundation model and further pre-trained on PubMed Central. We conduct acomprehensive evaluation of BioMistral on a benchmark comprising 10 establishedmedical question-answering (QA) tasks in English. We also explore lightweightmodels obtained through quantization and model merging approaches. Our resultsdemonstrate BioMistral's superior performance compared to existing open-sourcemedical models and its competitive edge against proprietary counterparts.Finally, to address the limited availability of data beyond English and toassess the multilingual generalization of medical LLMs, we automaticallytranslated and evaluated this benchmark into 7 other languages. This marks thefirst large-scale multilingual evaluation of LLMs in the medical domain.Datasets, multilingual evaluation benchmarks, scripts, and all the modelsobtained during our experiments are freely released.</description><author>Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, Richard Dufour</author><pubDate>Thu, 15 Feb 2024 23:39:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10373v1</guid></item><item><title>Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge</title><link>http://arxiv.org/abs/2402.12352v1</link><description>Large language models (LLMs) are transforming the way information isretrieved with vast amounts of knowledge being summarized and presented vianatural language conversations. Yet, LLMs are prone to highlight the mostfrequently seen pieces of information from the training set and to neglect therare ones. In the field of biomedical research, latest discoveries are key toacademic and industrial actors and are obscured by the abundance of anever-increasing literature corpus (the information overload problem). Surfacingnew associations between biomedical entities, e.g., drugs, genes, diseases,with LLMs becomes a challenge of capturing the long-tail knowledge of thebiomedical scientific production. To overcome this challenge, RetrievalAugmented Generation (RAG) has been proposed to alleviate some of theshortcomings of LLMs by augmenting the prompts with context retrieved fromexternal datasets. RAG methods typically select the context via maximumsimilarity search over text embeddings. In this study, we show that RAG methodsleave out a significant proportion of relevant information due to clusters ofover-represented concepts in the biomedical literature. We introduce a novelinformation-retrieval method that leverages a knowledge graph to downsamplethese clusters and mitigate the information overload problem. Its retrievalperformance is about twice better than embedding similarity alternatives onboth precision and recall. Finally, we demonstrate that both embeddingsimilarity and knowledge graph retrieval methods can be advantageously combinedinto a hybrid model that outperforms both, enabling potential improvements tobiomedical question-answering models.</description><author>Julien Delile, Srayanta Mukherjee, Anton Van Pamel, Leonid Zhukov</author><pubDate>Mon, 19 Feb 2024 18:31:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12352v1</guid></item><item><title>Survey of Hallucination in Natural Language Generation</title><link>http://arxiv.org/abs/2202.03629v6</link><description>Natural Language Generation (NLG) has improved exponentially in recent yearsthanks to the development of sequence-to-sequence deep learning technologiessuch as Transformer-based language models. This advancement has led to morefluent and coherent NLG, leading to improved development in downstream taskssuch as abstractive summarization, dialogue generation and data-to-textgeneration. However, it is also apparent that deep learning based generation isprone to hallucinate unintended text, which degrades the system performance andfails to meet user expectations in many real-world scenarios. To address thisissue, many studies have been presented in measuring and mitigatinghallucinated texts, but these have never been reviewed in a comprehensivemanner before. In this survey, we thus provide a broad overview of the researchprogress and challenges in the hallucination problem in NLG. The survey isorganized into two parts: (1) a general overview of metrics, mitigationmethods, and future directions; (2) an overview of task-specific researchprogress on hallucinations in the following downstream tasks, namelyabstractive summarization, dialogue generation, generative question answering,data-to-text generation, machine translation, and visual-language generation;and (3) hallucinations in large language models (LLMs). This survey serves tofacilitate collaborative efforts among researchers in tackling the challenge ofhallucinated texts in NLG.</description><author>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Delong Chen, Ho Shu Chan, Wenliang Dai, Andrea Madotto, Pascale Fung</author><pubDate>Mon, 19 Feb 2024 14:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2202.03629v6</guid></item><item><title>Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2402.12048v1</link><description>Catastrophic forgetting emerges as a critical challenge when fine-tuningmulti-modal large language models (MLLMs), where improving performance onunseen tasks often leads to a significant performance drop on the originaltasks. This paper presents a comprehensive analysis of catastrophic forgettingin MLLMs and introduces a post-training adjustment method called Model Tailor.Our method primarily preserves the pre-trained parameters while replacing asmall number ($\leq$ 10\%) of fine-tuned parameters, maintaining $\sim$ 99\%effectiveness on original tasks versus pre-training, and achieving $\sim$ 97\%on new tasks compared to standard fine-tuning. Specifically, we derive a sparsemask to identify the "model patch", based on a fusion strategy that integratessalience and sensitivity analysis. Subsequently, a compensation mechanism isintroduced to "decorate the patch", enhancing the model's performance on bothtarget and original tasks. Additionally, our method is adaptable to multi-taskscenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in bothimage captioning and visual question answering tasks, our approach demonstratessignificant task adaptability while preserving inherent pre-trainedcapabilities.</description><author>Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Kun Kuang, Chao Wu</author><pubDate>Mon, 19 Feb 2024 11:02:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12048v1</guid></item><item><title>Learning to Edit: Aligning LLMs with Knowledge Editing</title><link>http://arxiv.org/abs/2402.11905v1</link><description>Knowledge editing techniques, aiming to efficiently modify a minor proportionof knowledge in large language models (LLMs) without negatively impactingperformance across other inputs, have garnered widespread attention. However,existing methods predominantly rely on memorizing the updated knowledge,impeding LLMs from effectively combining the new knowledge with their inherentknowledge when answering questions. To this end, we propose a Learning to Edit(LTE) framework, focusing on teaching LLMs to apply updated knowledge intoinput questions, inspired by the philosophy of "Teach a man to fish." LTEfeatures a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs ona meticulously curated parallel dataset to make reliable, in-scope edits whilepreserving out-of-scope information and linguistic proficiency; and (ii) theInference Phase, which employs a retrieval-based mechanism for real-time andmass knowledge editing. By comparing our approach with seven advanced baselinesacross four popular knowledge editing benchmarks and two LLM architectures, wedemonstrate LTE's superiority in knowledge editing performance, robustness inboth batch and sequential editing, minimal interference on general tasks, andrapid editing speeds. The data and code are available athttps://github.com/YJiangcm/LTE.</description><author>Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang</author><pubDate>Mon, 19 Feb 2024 07:45:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11905v1</guid></item><item><title>LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation</title><link>http://arxiv.org/abs/2402.11485v1</link><description>Adapting English-based large language models (LLMs) to other languages hasbecome increasingly popular due to the efficiency and potential ofcross-lingual transfer. However, existing language adaptation methods oftenoverlook the benefits of cross-lingual supervision. In this study, we introduceLEIA, a language adaptation tuning method that utilizes Wikipedia entity namesaligned across languages. This method involves augmenting the target languagecorpus with English entity names and training the model using left-to-rightlanguage modeling. We assess LEIA on diverse question answering datasets using7B-parameter LLMs, demonstrating significant performance gains across variousnon-English languages. The source code is available athttps://github.com/studio-ousia/leia.</description><author>Ikuya Yamada, Ryokan Ri</author><pubDate>Sun, 18 Feb 2024 07:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11485v1</guid></item><item><title>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</title><link>http://arxiv.org/abs/2402.11770v1</link><description>We introduce a structured chain-of-thought (SCoT) prompting approach togenerating content-grounded multi-turn question-answer conversations using apre-trained large language model (LLM). At the core of our proposal is astructured breakdown of the complex task into a number of states in a statemachine, so that actions corresponding to various subtasks, e.g., contentreading and utterance generation, can be executed in their own dedicatedstates. Each state leverages a unique set of resources including prompts and(optionally) additional tools to augment the generation process. Ourexperimental results show that SCoT prompting with designated states forhallucination mitigation increases agent faithfulness to grounding documents byup to 16.8%. When used as training data, our open-domain conversationssynthesized from only 6 Wikipedia-based seed demonstrations train strongconversational QA agents; in out-of-domain evaluation, for example, we observeimprovements of up to 13.9% over target domain gold data when the latter isaugmented with our generated examples.</description><author>Md Arafat Sultan, Jatin Ganhotra, Ram√≥n Fernandez Astudillo</author><pubDate>Mon, 19 Feb 2024 01:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11770v1</guid></item><item><title>MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs</title><link>http://arxiv.org/abs/2402.11756v1</link><description>Generative Large Language Models (LLMs) are widely utilized for theirexcellence in various tasks. However, their tendency to produce inaccurate ormisleading outputs poses a potential risk, particularly in high-stakesenvironments. Therefore, estimating the correctness of generative LLM outputsis an important task for enhanced reliability. Uncertainty Estimation (UE) ingenerative LLMs is an evolving domain, where SOTA probability-based methodscommonly employ length-normalized scoring. In this work, we proposeMeaning-Aware Response Scoring (MARS) as an alternative to length-normalizedscoring for UE methods. MARS is a novel scoring function that considers thesemantic contribution of each token in the generated sequence in the context ofthe question. We demonstrate that integrating MARS into UE methods results in auniversal and significant improvement in UE performance. We conduct experimentsusing three distinct closed-book question-answering datasets across fivepopular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a MedicalQA dataset. Code can be foundhttps://anonymous.4open.science/r/LLM_Uncertainity-309B.</description><author>Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr</author><pubDate>Mon, 19 Feb 2024 01:04:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11756v1</guid></item><item><title>Solving Data-centric Tasks using Large Language Models</title><link>http://arxiv.org/abs/2402.11734v1</link><description>Large language models (LLMs) are rapidly replacing help forums likeStackOverflow, and are especially helpful for non-professional programmers andend users. These users are often interested in data-centric tasks, such asspreadsheet manipulation and data wrangling, which are hard to solve if theintent is only communicated using a natural-language description, withoutincluding the data. But how do we decide how much data and which data toinclude in the prompt? This paper makes two contributions towards answeringthis question. First, we create a dataset of real-world NL-to-code tasksmanipulating tabular data, mined from StackOverflow posts. Second, we introducea cluster-then-select prompting technique, which adds the most representativerows from the input data to the LLM prompt. Our experiments show that LLMperformance is indeed sensitive to the amount of data passed in the prompt, andthat for tasks with a lot of syntactic variation in the input table, ourcluster-then-select technique outperforms a random selection baseline.</description><author>Shraddha Barke, Christian Poelitz, Carina Suzana Negreanu, Benjamin Zorn, Jos√© Cambronero, Andrew D. Gordon, Vu Le, Elnaz Nouri, Nadia Polikarpova, Advait Sarkar, Brian Slininger, Neil Toronto, Jack Williams</author><pubDate>Sun, 18 Feb 2024 23:19:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11734v1</guid></item><item><title>LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration</title><link>http://arxiv.org/abs/2402.11550v1</link><description>Large language models (LLMs) have demonstrated impressive performance inunderstanding language and executing complex reasoning tasks. However, LLMswith long context windows have been notorious for their expensive trainingcosts and high inference latency. Even the most advanced models such as GPT-4and Claude2 often make mistakes when processing inputs of over $100k$ tokens, aphenomenon also known as \textit{lost in the middle}. In this paper, we propose\textsc{LongAgent}, a method based on multi-agent collaboration, which scalesLLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiorityin long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader isresponsible for understanding user intent and directing team members to acquireinformation from documents. Due to members' hallucinations, it is non-trivialfor a leader to obtain accurate information from the responses of dozens tohundreds of members. To address this, we develop an \textit{inter-membercommunication} mechanism to resolve response conflicts caused by hallucinationsthrough information sharing. Our experimental results indicate that\textsc{LongAgent} offers a promising alternative for long-text processing. Theagent team instantiated with LLaMA-7B achieves significant improvements intasks such as 128k-long text retrieval, multi-hop question answering, comparedto GPT-4.</description><author>Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, Xuanjing Huang</author><pubDate>Sun, 18 Feb 2024 11:46:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11550v1</guid></item><item><title>Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought</title><link>http://arxiv.org/abs/2402.11541v1</link><description>Although the method of enhancing large language models' (LLMs') reasoningability and reducing their hallucinations through the use of knowledge graphs(KGs) has received widespread attention, the exploration of how to enable LLMsto integrate the structured knowledge in KGs on-the-fly remains inadequate.Researchers often co-train KG embeddings and LLM parameters to equip LLMs withthe ability of comprehending KG knowledge. However, this resource-hungrytraining paradigm significantly increases the model learning cost and is alsounsuitable for non-open-source, black-box LLMs. In this paper, we employcomplex question answering (CQA) as a task to assess the LLM's ability ofcomprehending KG knowledge. We conducted a comprehensive comparison of KGknowledge injection methods (from triples to natural language text), aiming toexplore the optimal prompting method for supplying KG knowledge to LLMs,thereby enhancing their comprehension of KG. Contrary to our initialexpectations, our analysis revealed that LLMs effectively handle messy, noisy,and linearized KG knowledge, outperforming methods that employ well-designednatural language (NL) textual prompts. This counter-intuitive finding providessubstantial insights for future research on LLMs' comprehension of structuredknowledge.</description><author>Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Guilin Qi</author><pubDate>Sun, 18 Feb 2024 10:44:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11541v1</guid></item><item><title>SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency</title><link>http://arxiv.org/abs/2311.01740v2</link><description>Hallucination detection is a critical step toward understanding thetrustworthiness of modern language models (LMs). To achieve this goal, were-examine existing detection approaches based on the self-consistency of LMsand uncover two types of hallucinations resulting from 1) question-level and 2)model-level, which cannot be effectively identified through self-consistencycheck alone. Building upon this discovery, we propose a novel sampling-basedmethod, i.e., semantic-aware cross-check consistency (SAC3) that expands on theprinciple of self-consistency checking. Our SAC3 approach incorporatesadditional mechanisms to detect both question-level and model-levelhallucinations by leveraging advances including semantically equivalentquestion perturbation and cross-model response consistency checking. Throughextensive and systematic empirical analysis, we demonstrate that SAC3outperforms the state of the art in detecting both non-factual and factualstatements across multiple question-answering and open-domain generationbenchmarks.</description><author>Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, Sricharan Kumar</author><pubDate>Sun, 18 Feb 2024 06:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.01740v2</guid></item><item><title>Faithful Knowledge Graph Explanations for Commonsense Reasoning</title><link>http://arxiv.org/abs/2310.04910v3</link><description>While fusing language models and knowledge graphs has become common incommonsense question answering research, enabling faithful chain-of-thoughtexplanations in these models remains an open problem. Our analysis reveals thatone major weakness of current KG-based explanation methodologies lies inoverlooking the faithfulness of path decoding during evaluation. This oversightleads to the distribution of the graph encoder often diverging from theoriginal model predictions. To address this gap, we present two maincontributions: (1) We propose and validate Text-GNN Fidelity in this specificcontext, to assess the reliability of the graph representation. (2) Weintroduce TeGDA (Text-Graph Distribution-aware Alignment), a novel algorithmthat aligns the graph encoder with the target model to improve the faithfulnessof subsequent explanations and that can be easily integrated into existingapproaches. Our experiments and analysis show its potential to produce morefaithful systems. Concretely, our work emphasises the neglected distributionalmisalignment problem in LM-KG reasoning models, which has been a latent sourceof spurious explanations.</description><author>Weihe Zhai, Arkaitz Zubiaga, Bingquan Liu, Chengjie Sun, Yalong Zhao</author><pubDate>Sat, 17 Feb 2024 14:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04910v3</guid></item><item><title>Visual Hallucinations of Multi-modal Large Language Models</title><link>http://arxiv.org/abs/2402.14683v1</link><description>Visual hallucination (VH) means that a multi-modal LLM (MLLM) imaginesincorrect details about an image in visual question answering. Existing studiesfind VH instances only in existing image datasets, which results in biasedunderstanding of MLLMs' performance under VH due to limited diversity of suchVH instances. In this work, we propose a tool called VHTest to generate adiverse set of VH instances. Specifically, VHTest finds some initial VHinstances in existing image datasets (e.g., COCO), generates a text descriptionfor each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) togenerate VH images based on the text descriptions. We collect a benchmarkdataset with 1,200 VH instances in 8 VH modes using VHTest. We find thatexisting MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for alarge fraction of the instances in our benchmark. Moreover, we find thatfine-tuning an MLLM using our benchmark dataset reduces its likelihood tohallucinate without sacrificing its performance on other benchmarks. Ourbenchmarks are publicly available: https://github.com/wenhuang2000/VHTest.</description><author>Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong</author><pubDate>Thu, 22 Feb 2024 16:40:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.14683v1</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v1</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Tuo Zhao, Jing Gao</author><pubDate>Fri, 16 Feb 2024 23:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v1</guid></item><item><title>VideoPrism: A Foundational Visual Encoder for Video Understanding</title><link>http://arxiv.org/abs/2402.13217v1</link><description>We introduce VideoPrism, a general-purpose video encoder that tackles diversevideo understanding tasks with a single frozen model. We pretrain VideoPrism ona heterogeneous corpus containing 36M high-quality video-caption pairs and 582Mvideo clips with noisy parallel text (e.g., ASR transcripts). The pretrainingapproach improves upon masked autoencoding by global-local distillation ofsemantic video embeddings and a token shuffling scheme, enabling VideoPrism tofocus primarily on the video modality while leveraging the invaluable textassociated with videos. We extensively test VideoPrism on four broad groups ofvideo understanding tasks, from web video question answering to CV for science,achieving state-of-the-art performance on 30 out of 33 video understandingbenchmarks.</description><author>Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong</author><pubDate>Tue, 20 Feb 2024 18:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13217v1</guid></item><item><title>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks</title><link>http://arxiv.org/abs/2402.07197v3</link><description>Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot andinstruction-following capabilities, have catalyzed a revolutionarytransformation across diverse research fields of artificial intelligence,especially for open-ended tasks. While the idea is less explored in the graphdomain, despite the availability of numerous powerful graph models (GMs), theyare restricted to tasks in a pre-defined form. Although several methodsapplying LLMs to graphs have been proposed, they fail to simultaneously handlethe pre-defined and open-ended tasks, with LLM as a node feature enhancer or asa standalone predictor. To break this dilemma, we propose to bridge thepretrained GM and LLM by a Translator, named GraphTranslator, aiming toleverage GM to handle the pre-defined tasks effectively and utilize theextended interface of LLMs to offer various open-ended tasks for GM. To trainsuch Translator, we propose a Producer capable of constructing the graph-textalignment data along node information, neighbor information and modelinformation. By treating the node representation as a type of language, theproposed GraphTranslator empowers an LLM to make predictions based on noderepresentation and language instructions, providing a unified perspective forboth pre-defined and open-ended tasks. Extensive results show that the proposedGraphTranslator effectively improves the results of zero-shot nodeclassification. The graph question answering experiments reveal ourGraphTranslator potential across a broad spectrum of open-ended applicationsthrough language instructions.</description><author>Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi</author><pubDate>Tue, 20 Feb 2024 08:34:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07197v3</guid></item><item><title>On Context Utilization in Summarization with Large Language Models</title><link>http://arxiv.org/abs/2310.10570v3</link><description>Large language models (LLMs) excel in abstractive summarization tasks,delivering fluent and pertinent summaries. Recent advancements have extendedtheir capabilities to handle long-input contexts, exceeding 100k tokens.However, in question answering, language models exhibit uneven utilization oftheir input context. They tend to favor the initial and final segments,resulting in a U-shaped performance pattern concerning where the answer islocated within the input. This bias raises concerns, particularly insummarization where crucial content may be dispersed throughout the sourcedocument(s). Besides, in summarization, mapping facts from the source to thesummary is not trivial as salient content is usually re-phrased. In this paper,we conduct the first comprehensive study on context utilization and positionbias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5evaluation metrics. We introduce a new evaluation benchmark called MiddleSum onthe which we benchmark two alternative inference methods to alleviate positionbias: hierarchical summarization and incremental summarization.</description><author>Mathieu Ravaut, Aixin Sun, Nancy F. Chen, Shafiq Joty</author><pubDate>Tue, 20 Feb 2024 05:14:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10570v3</guid></item><item><title>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</title><link>http://arxiv.org/abs/2402.11770v2</link><description>We introduce a structured chain-of-thought (SCoT) prompting approach togenerating content-grounded multi-turn question-answer conversations using apre-trained large language model (LLM). At the core of our proposal is astructured breakdown of the complex task into a number of states in a statemachine, so that actions corresponding to various subtasks, e.g., contentreading and utterance generation, can be executed in their own dedicatedstates. Each state leverages a unique set of resources including prompts and(optionally) additional tools to augment the generation process. Ourexperimental results show that SCoT prompting with designated states forhallucination mitigation increases agent faithfulness to grounding documents byup to 16.8%. When used as training data, our open-domain conversationssynthesized from only 6 Wikipedia-based seed demonstrations train strongconversational QA agents; in out-of-domain evaluation, for example, we observeimprovements of up to 13.9% over target domain gold data when the latter isaugmented with our generated examples.</description><author>Md Arafat Sultan, Jatin Ganhotra, Ram√≥n Fernandez Astudillo</author><pubDate>Tue, 20 Feb 2024 02:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11770v2</guid></item><item><title>MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs</title><link>http://arxiv.org/abs/2402.11756v2</link><description>Generative Large Language Models (LLMs) are widely utilized for theirexcellence in various tasks. However, their tendency to produce inaccurate ormisleading outputs poses a potential risk, particularly in high-stakesenvironments. Therefore, estimating the correctness of generative LLM outputsis an important task for enhanced reliability. Uncertainty Estimation (UE) ingenerative LLMs is an evolving domain, where SOTA probability-based methodscommonly employ length-normalized scoring. In this work, we proposeMeaning-Aware Response Scoring (MARS) as an alternative to length-normalizedscoring for UE methods. MARS is a novel scoring function that considers thesemantic contribution of each token in the generated sequence in the context ofthe question. We demonstrate that integrating MARS into UE methods results in auniversal and significant improvement in UE performance. We conduct experimentsusing three distinct closed-book question-answering datasets across fivepopular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a MedicalQA dataset. Code can be found https://github.com/Ybakman/LLM_Uncertainity.</description><author>Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr</author><pubDate>Tue, 20 Feb 2024 02:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11756v2</guid></item><item><title>TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness</title><link>http://arxiv.org/abs/2402.12545v1</link><description>Large Language Models (LLMs) have demonstrated impressive capabilities acrossvarious domains, prompting a surge in their practical applications. However,concerns have arisen regarding the trustworthiness of LLMs outputs,particularly in closed-book question-answering tasks, where non-experts maystruggle to identify inaccuracies due to the absence of contextual or groundtruth information. This paper introduces TrustScore, a framework based on theconcept of Behavioral Consistency, which evaluates whether an LLMs responsealigns with its intrinsic knowledge. Additionally, TrustScore can seamlesslyintegrate with fact-checking methods, which assesses alignment with externalknowledge sources. The experimental results show that TrustScore achievesstrong correlations with human judgments, surpassing existing reference-freemetrics, and achieving results on par with reference-based metrics.</description><author>Danna Zheng, Danyang Liu, Mirella Lapata, Jeff Z. Pan</author><pubDate>Mon, 19 Feb 2024 21:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12545v1</guid></item><item><title>Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data</title><link>http://arxiv.org/abs/2402.12424v1</link><description>In this paper, we investigate the effectiveness of various LLMs ininterpreting tabular data through different prompting strategies and dataformats. Our analysis extends across six benchmarks for table-related taskssuch as question-answering and fact-checking. We introduce for the first timethe assessment of LLMs' performance on image-based table representations.Specifically, we compare five text-based and three image-based tablerepresentations, demonstrating the influence of representation and prompting onLLM performance. Our study provides insights into the effective use of LLMs ontable-related tasks.</description><author>Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, Rada Mihalcea</author><pubDate>Mon, 19 Feb 2024 16:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12424v1</guid></item><item><title>Towards Building Multilingual Language Model for Medicine</title><link>http://arxiv.org/abs/2402.13963v1</link><description>In this paper, we aim to develop an open-source, multilingual language modelfor medicine, that the benefits a wider, linguistically diverse audience fromdifferent regions. In general, we present the contribution from the followingaspects: first, for multilingual medical-specific adaptation, we construct anew multilingual medical corpus, that contains approximately 25.5B tokensencompassing 6 main languages, termed as MMedC, that enables auto-regressivetraining for existing general LLMs. second, to monitor the development ofmultilingual LLMs in medicine, we propose a new multilingual medicalmulti-choice question-answering benchmark with rationale, termed as MMedBench;third, we have assessed a number of popular, opensource large language models(LLMs) on our benchmark, along with those further auto-regressive trained onMMedC, as a result, our final model, termed as MMedLM 2, with only 7Bparameters, achieves superior performance compared to all other open-sourcemodels, even rivaling GPT-4 on MMedBench. We will make the resources publiclyavailable, including code, model weights, and datasets.</description><author>Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie</author><pubDate>Wed, 21 Feb 2024 17:47:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13963v1</guid></item><item><title>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</title><link>http://arxiv.org/abs/2305.11738v4</link><description>Recent developments in large language models (LLMs) have been impressive.However, these models sometimes show inconsistencies and problematic behavior,such as hallucinating facts, generating flawed code, or creating offensive andtoxic content. Unlike these models, humans typically utilize external tools tocross-check and refine their initial content, like using a search engine forfact-checking, or a code interpreter for debugging. Inspired by thisobservation, we introduce a framework called CRITIC that allows LLMs, which areessentially "black boxes" to validate and progressively amend their own outputsin a manner similar to human interaction with tools. More specifically,starting with an initial output, CRITIC interacts with appropriate tools toevaluate certain aspects of the text, and then revises the output based on thefeedback obtained during this validation process. Comprehensive evaluationsinvolving free-form question answering, mathematical program synthesis, andtoxicity reduction demonstrate that CRITIC consistently enhances theperformance of LLMs. Meanwhile, our research highlights the crucial importanceof external feedback in promoting the ongoing self-improvement of LLMs.</description><author>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</author><pubDate>Wed, 21 Feb 2024 12:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11738v4</guid></item></channel></rss>