<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 25 Aug 2025 01:00:41 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An Empirical Study on How Video-LLMs Answer Video Questions</title><link>http://arxiv.org/abs/2508.15360v1</link><description>Taking advantage of large-scale data and pretrained language models, VideoLarge Language Models (Video-LLMs) have shown strong capabilities in answeringvideo questions. However, most existing efforts focus on improving performance,with limited attention to understanding their internal mechanisms. This paperaims to bridge this gap through a systematic empirical study. To interpretexisting VideoLLMs, we adopt attention knockouts as our primary analytical tooland design three variants: Video Temporal Knockout, Video Spatial Knockout, andLanguage-to-Video Knockout. Then, we apply these three knockouts on differentnumbers of layers (window of layers). By carefully controlling the window oflayers and types of knockouts, we provide two settings: a global setting and afine-grained setting. Our study reveals three key findings: (1) Global settingindicates Video information extraction primarily occurs in early layers,forming a clear two-stage process -- lower layers focus on perceptual encoding,while higher layers handle abstract reasoning; (2) In the fine-grained setting,certain intermediate layers exert an outsized impact on video questionanswering, acting as critical outliers, whereas most other layers contributeminimally; (3) In both settings, we observe that spatial-temporal modelingrelies more on language-guided retrieval than on intra- and inter-frameself-attention among video tokens, despite the latter's high computationalcost. Finally, we demonstrate that these insights can be leveraged to reduceattention computation in Video-LLMs. To our knowledge, this is the first workto systematically uncover how Video-LLMs internally process and understandvideo content, offering interpretability and efficiency perspectives for futureresearch.</description><author>Chenhui Gou, Ziyu Ma, Zicheng Duan, Haoyu He, Feng Chen, Akide Liu, Bohan Zhuang, Jianfei Cai, Hamid Rezatofighi</author><pubDate>Thu, 21 Aug 2025 08:42:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15360v1</guid></item><item><title>Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding</title><link>http://arxiv.org/abs/2410.01671v3</link><description>Large language models (LLMs) have shown remarkable capabilities in naturallanguage processing; however, they still face difficulties when tasked withunderstanding lengthy contexts and executing effective question answering.These challenges often arise due to the complexity and ambiguity present inlonger texts. To enhance the performance of LLMs in such scenarios, weintroduce the Long Question Coreference Adaptation (LQCA) method. Thisinnovative framework focuses on coreference resolution tailored to longcontexts, allowing the model to identify and manage references effectively. TheLQCA method encompasses four key steps: resolving coreferences withinsub-documents, computing the distances between mentions, defining arepresentative mention for coreference, and answering questions through mentionreplacement. By processing information systematically, the framework provideseasier-to-handle partitions for LLMs, promoting better understanding.Experimental evaluations on a range of LLMs and datasets have yielded positiveresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,highlighting the effectiveness of leveraging coreference resolution to bridgecontext gaps in question answering. Our code is public athttps://github.com/OceannTwT/LQCA.</description><author>Yanming Liu, Xinyue Peng, Jiannan Cao, Yanxin Shen, Tianyu Du, Sheng Cheng, Xun Wang, Jianwei Yin, Xuhong Zhang</author><pubDate>Fri, 15 Aug 2025 05:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01671v3</guid></item><item><title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title><link>http://arxiv.org/abs/2507.22752v2</link><description>We introduce CUS-QA, a benchmark for open-ended regional question answeringthat encompasses both textual and visual modalities. We also provide strongbaselines using state-of-the-art large language models (LLMs). Our datasetconsists of manually curated questions and answers grounded in Wikipedia,created by native speakers from Czechia, Slovakia, and Ukraine, withaccompanying English translations. It includes both purely textual questionsand those requiring visual understanding. We evaluate state-of-the-art LLMsthrough prompting and complement this with human judgments of answercorrectness. Using these human evaluations, we analyze the reliability ofexisting automatic evaluation metrics. Our baseline results show that even thebest open-weight LLMs achieve only around 50% accuracy on textual questions andbelow 30% on visual questions. LLM-based evaluation metrics show strongcorrelation with human judgment, while traditional string-overlap metricsperform surprisingly well due to the prevalence of named entities in answers.</description><author>Jindřich Libovický, Jindřich Helcl, Andrei Manea, Gianluca Vico</author><pubDate>Thu, 21 Aug 2025 12:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22752v2</guid></item><item><title>Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset</title><link>http://arxiv.org/abs/2508.11058v1</link><description>The advancement of 3D vision-language (3D VL) learning is hindered by severallimitations in existing 3D VL datasets: they rarely necessitate reasoningbeyond a close range of objects in single viewpoint, and annotations often linkinstructions to single objects, missing richer contextual alignments betweenmultiple objects. This significantly curtails the development of models capableof deep, multi-view 3D scene understanding over distant objects. To addressthese challenges, we introduce MV-ScanQA, a novel 3D question answering datasetwhere 68% of questions explicitly require integrating information from multipleviews (compared to less than 7% in existing datasets), thereby rigorouslytesting multi-view compositional reasoning. To facilitate the training ofmodels for such demanding scenarios, we present TripAlign dataset, alarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M &lt;2Dview, set of 3D objects, text&gt; triplets that explicitly aligns groups ofcontextually related objects with text, providing richer, view-groundedmulti-object multimodal alignment signals than previous single-objectannotations. We further develop LEGO, a baseline method for the multi-viewreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2DLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlignachieves state-of-the-art performance not only on the proposed MV-ScanQA, butalso on existing benchmarks for 3D dense captioning and question answering.Datasets and code are available athttps://matthewdm0816.github.io/tripalign-mvscanqa.</description><author>Wentao Mo, Qingchao Chen, Yuxin Peng, Siyuan Huang, Yang Liu</author><pubDate>Thu, 14 Aug 2025 20:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11058v1</guid></item><item><title>Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis</title><link>http://arxiv.org/abs/2503.20047v2</link><description>Vision-language models (VLMs) have shown promise in 2D medical imageanalysis, but extending them to 3D remains challenging due to the highcomputational demands of volumetric data and the difficulty of aligning 3Dspatial features with clinical text. We present Med3DVLM, a 3D VLM designed toaddress these challenges through three key innovations: (1) DCFormer, anefficient encoder that uses decomposed 3D convolutions to capture fine-grainedspatial features at scale; (2) SigLIP, a contrastive learning strategy withpairwise sigmoid loss that improves image-text alignment without relying onlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuseslow- and high-level image features with text embeddings for richer multi-modalrepresentations. We evaluate our model on the M3D dataset, which includesradiology reports and VQA data for 120,084 3D medical images. Results show thatMed3DVLM achieves superior performance across multiple benchmarks. Forimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantlyoutperforming the current state-of-the-art M3D model (19.10%). For reportgeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-endedvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and inclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These resultshighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,enabling scalable, multi-task reasoning across clinical applications. Our codeis publicly available at https://github.com/mirthAI/Med3DVLM.</description><author>Yu Xin, Gorkem Can Ates, Kuang Gong, Wei Shao</author><pubDate>Fri, 15 Aug 2025 13:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.20047v2</guid></item><item><title>Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions</title><link>http://arxiv.org/abs/2508.11414v1</link><description>Large language models implicitly encode preferences over human values, yetsteering them often requires large training data. In this work, we investigatea simple approach: Can we reliably modify a model's value system in downstreambehavior by training it to answer value survey questions accordingly? We firstconstruct value profiles of several open-source LLMs by asking them to rate aseries of value-related descriptions spanning 20 distinct human values, whichwe use as a baseline for subsequent experiments. We then investigate whetherthe value system of a model can be governed by fine-tuning on the valuesurveys. We evaluate the effect of finetuning on the model's behavior in twoways; first, we assess how answers change on in-domain, held-out surveyquestions. Second, we evaluate whether the model's behavior changes inout-of-domain settings (situational scenarios). To this end, we construct acontextualized moral judgment dataset based on Reddit posts and evaluatechanges in the model's behavior in text-based adventure games. We demonstratethat our simple approach can not only change the model's answers to in-domainsurvey questions, but also produces substantial shifts (value alignment) inimplicit downstream task behavior.</description><author>Shangrui Nie, Florian Mai, David Kaczér, Charles Welch, Zhixue Zhao, Lucie Flek</author><pubDate>Fri, 15 Aug 2025 11:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11414v1</guid></item><item><title>Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries</title><link>http://arxiv.org/abs/2502.16636v2</link><description>Retrieval-augmented generation (RAG) is a paradigm that augments largelanguage models (LLMs) with external knowledge to tackle knowledge-intensivequestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)under Multimodal RAG settings, they predominantly retrieve from textual corporaand do not explicitly assess how models exploit visual evidence duringgeneration. Consequently, there still lacks benchmark that isolates andmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,a question-answering benchmark that targets visually grounded,knowledge-intensive questions. Unlike prior work, Visual-RAG requirestext-to-image retrieval and the integration of retrieved clue images to extractvisual evidence for answer generation. With Visual-RAG, we evaluate 5open-source and 3 proprietary MLLMs, showcasing that images provide strongevidence in augmented generation. However, even state-of-the-art modelsstruggle to efficiently extract and utilize visual knowledge. Our resultshighlight the need for improved visual retrieval, grounding, and attribution inmultimodal RAG systems.</description><author>Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, Wenya Wang</author><pubDate>Fri, 15 Aug 2025 09:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.16636v2</guid></item><item><title>LLM Compression: How Far Can We Go in Balancing Size and Performance?</title><link>http://arxiv.org/abs/2508.11318v1</link><description>Quantization is an essential and popular technique for improving theaccessibility of large language models (LLMs) by reducing memory usage andcomputational costs while maintaining performance. In this study, we apply4-bit Group Scaling Quantization (GSQ) and Generative Pretrained TransformerQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating theirimpact across multiple NLP tasks. We benchmark these models on MS MARCO(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K(Mathematical Reasoning) datasets, assessing both accuracy and efficiencyacross various tasks. The study measures the trade-offs between modelcompression and task performance, analyzing key evaluation metrics, namelyaccuracy, inference latency, and throughput (total output tokens generated persecond), providing insights into the suitability of low-bit quantization forreal-world deployment. Using the results, users can then make suitabledecisions based on the specifications that need to be met. We discuss the prosand cons of GSQ and GPTQ techniques on models of different sizes, which alsoserve as a benchmark for future experiments.</description><author>Sahil Sk, Debasish Dhal, Sonal Khosla, Sk Shahid, Sambit Shekhar, Akash Dhaka, Shantipriya Parida, Dilip K. Prasad, Ondřej Bojar</author><pubDate>Fri, 15 Aug 2025 08:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11318v1</guid></item><item><title>MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks</title><link>http://arxiv.org/abs/2507.18932v2</link><description>Environmental, Social, and Governance (ESG) reports are essential forevaluating sustainability practices, ensuring regulatory compliance, andpromoting financial transparency. However, these documents are often lengthy,structurally diverse, and multimodal, comprising dense text, structured tables,complex figures, and layout-dependent semantics. Existing AI systems oftenstruggle to perform reliable document-level reasoning in such settings, and nodedicated benchmark currently exists in ESG domain. To fill the gap, weintroduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targetedto evaluate multimodal understanding and complex reasoning across structurallydiverse and multi-source ESG documents. This dataset is constructed via ahuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generatescandidate question-answer (QA) pairs by jointly interpreting rich textual,tabular, and visual information from layout-aware document pages. Second, anLLM verifies the semantic accuracy, completeness, and reasoning complexity ofeach QA pair. This automated process is followed by an expert-in-the-loopvalidation, where domain specialists validate and calibrate QA pairs to ensurequality, relevance, and diversity. MMESGBench comprises 933 validated QA pairsderived from 45 ESG documents, spanning across seven distinct document typesand three major ESG source categories. Questions are categorized assingle-page, cross-page, or unanswerable, with each accompanied by fine-grainedmultimodal evidence. Initial experiments validate that multimodal andretrieval-augmented models substantially outperform text-only baselines,particularly on visually grounded and cross-page tasks. MMESGBench is publiclyavailable as an open-source dataset athttps://github.com/Zhanglei1103/MMESGBench.</description><author>Lei Zhang, Xin Zhou, Chaoyue He, Di Wang, Yi Wu, Hong Xu, Wei Liu, Chunyan Miao</author><pubDate>Fri, 15 Aug 2025 08:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.18932v2</guid></item><item><title>AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries</title><link>http://arxiv.org/abs/2508.11285v1</link><description>Depression, anxiety, and stress are widespread mental health concerns thatincreasingly drive individuals to seek information from Large Language Models(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, GeminiPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twentypragmatic questions about depression, anxiety, and stress when those questionsare framed for six user profiles (baseline, woman, man, young, old, anduniversity student). The models generated 2,880 answers, which we scored forsentiment and emotions using state-of-the-art tools. Our analysis revealed thatoptimism, fear, and sadness dominated the emotional landscape across alloutputs, with neutral sentiment maintaining consistently high values.Gratitude, joy, and trust appeared at moderate levels, while emotions such asanger, disgust, and love were rarely expressed. The choice of LLM significantlyinfluenced emotional expression patterns. Mixtral exhibited the highest levelsof negative emotions including disapproval, annoyance, and sadness, while Llamademonstrated the most optimistic and joyful responses. The type of mentalhealth condition dramatically shaped emotional responses: anxiety promptselicited extraordinarily high fear scores (0.974), depression prompts generatedelevated sadness (0.686) and the highest negative sentiment, whilestress-related queries produced the most optimistic responses (0.755) withelevated joy and trust. In contrast, demographic framing of queries producedonly marginal variations in emotional tone. Statistical analyses confirmedsignificant model-specific and condition-specific differences, whiledemographic influences remained minimal. These findings highlight the criticalimportance of model selection in mental health applications, as each LLMexhibits a distinct emotional signature that could significantly impact userexperience and outcomes.</description><author>Arya VarastehNezhad, Reza Tavasoli, Soroush Elyasi, MohammadHossein LotfiNia, Hamed Farbeh</author><pubDate>Fri, 15 Aug 2025 07:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11285v1</guid></item><item><title>Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2508.11247v1</link><description>Multi-hop question answering (MHQA) requires integrating knowledge scatteredacross multiple passages to derive the correct answer. Traditionalretrieval-augmented generation (RAG) methods primarily focus on coarse-grainedtextual semantic similarity and ignore structural associations among dispersedknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methodsaddress this by leveraging knowledge graphs (KGs) to capture structuralassociations, but they tend to overly rely on structural information andfine-grained word- or phrase-level retrieval, resulting in an underutilizationof textual semantics. In this paper, we propose a novel RAG approach calledHGRAG for MHQA that achieves cross-granularity integration of structural andsemantic information via hypergraphs. Structurally, we construct an entityhypergraph where fine-grained entities serve as nodes and coarse-grainedpassages as hyperedges, and establish knowledge association through sharedentities. Semantically, we design a hypergraph retrieval method that integratesfine-grained entity similarity and coarse-grained passage similarity viahypergraph diffusion. Finally, we employ a retrieval enhancement module, whichfurther refines the retrieved results both semantically and structurally, toobtain the most relevant passages as context for answer generation with theLLM. Experimental results on benchmark datasets demonstrate that our approachoutperforms state-of-the-art methods in QA performance, and achieves a6$\times$ speedup in retrieval efficiency.</description><author>Changjian Wang, Weihong Deng, Weili Guan, Quan Lu, Ning Jiang</author><pubDate>Fri, 15 Aug 2025 06:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11247v1</guid></item><item><title>SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation with Casual Inference</title><link>http://arxiv.org/abs/2403.07088v7</link><description>Large language models(LLMs) have shown its outperforming ability on varioustasks and question answering. However, LLMs require substantial memory storageon low-resource devices. More critically, the computational speed on thesedevices is also severely limited. In this paper, we propose SPA(Side PluginAdaption), a lightweight architecture for fast on-devices inference on theconstraints of strict on-devices computation and memory constraints. Comparedwith other on-devices seq2seq generation, SPA could make a fast and stableinference on low-resource constraints, allowing it to obtain cost effiency. Ourmethod establish an interaction between a pretrained LLMs on-cloud and additiveparameters on-devices, which could provide the knowledge on both pretrainedLLMs and featured personal feature. Further more, SPA provides a framework tokeep feature-base parameters on low computational devices while leave theparameters containing general information on the high computational devices.</description><author>Yanming Liu, Xinyue Peng, Ningjing Sang, Yafeng Yan, Xiaolan Ke, Zhiting Zheng, Shaobo Liu, Songhang Deng, Jiannan Cao, Le Dai, Xingzu Liu, Ruilin Nong, Weihao Liu</author><pubDate>Fri, 15 Aug 2025 04:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07088v7</guid></item><item><title>PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging</title><link>http://arxiv.org/abs/2505.11872v3</link><description>Recent advancements in prompt-based medical image segmentation have enabledclinicians to identify tumors using simple input like bounding boxes or textprompts. However, existing methods face challenges when doctors need tointeract through natural language or when position reasoning is required -understanding spatial relationships between anatomical structures andpathologies. We present PRS-Med, a framework that integrates vision-languagemodels with segmentation capabilities to generate both accurate segmentationmasks and corresponding spatial reasoning outputs. Additionally, we introducethe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),which provides diverse, spatially-grounded question-answer pairs to address thelack of position reasoning data in medical imaging. PRS-Med demonstratessuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,endoscopy, RGB), significantly outperforming state-of-the-art methods in bothsegmentation accuracy and position reasoning. Our approach enables intuitivedoctor-system interaction through natural language, facilitating more efficientdiagnoses. Our dataset pipeline, model, and codebase will be released to fosterfurther research in spatially-aware multimodal reasoning for medicalapplications.</description><author>Quoc-Huy Trinh, Minh-Van Nguyen, Jung Zeng, Ulas Bagci, Debesh Jha</author><pubDate>Fri, 15 Aug 2025 02:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11872v3</guid></item><item><title>MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering</title><link>http://arxiv.org/abs/2508.11163v1</link><description>This paper presents MobQA, a benchmark dataset designed to evaluate thesemantic understanding capabilities of large language models (LLMs) for humanmobility data through natural language question answering. While existing models excel at predicting human movement patterns, it remainsunobvious how much they can interpret the underlying reasons or semanticmeaning of those patterns. MobQA provides a comprehensive evaluation frameworkfor LLMs to answer questions about diverse human GPS trajectories spanningdaily to weekly granularities. It comprises 5,800 high-quality question-answerpairs across three complementary question types: factual retrieval (precisedata extraction), multiple-choice reasoning (semantic inference), and free-formexplanation (interpretive description), which all require spatial, temporal,and semantic reasoning. Our evaluation of major LLMs reveals strong performanceon factual retrieval but significant limitations in semantic reasoning andexplanation question answering, with trajectory length substantially impactingmodel effectiveness. These findings demonstrate the achievements andlimitations of state-of-the-art LLMs for semantic mobilityunderstanding.\footnote{MobQA dataset is available athttps://github.com/CyberAgentAILab/mobqa.}</description><author>Hikaru Asano, Hiroki Ouchi, Akira Kasuga, Ryo Yonetani</author><pubDate>Fri, 15 Aug 2025 02:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11163v1</guid></item><item><title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title><link>http://arxiv.org/abs/2508.11133v1</link><description>Large language models (LLMs) are emerging as a go-to tool for queryinginformation. However, current LLM benchmarks rarely feature natural questionsthat are both information-seeking as well as genuinely time-consuming forhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 naturaland complex questions that require dozens, and at times hundreds, ofintermediate steps to solve -- far more than any existing QA benchmark. Tobuild MoNaCo, we developed a decomposed annotation pipeline to elicit andmanually answer natural time-consuming questions at scale. Frontier LLMsevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall andhallucinations. Our results underscore the need for reasoning models thatbetter handle the complexity and sheer breadth of real-worldinformation-seeking questions -- with MoNaCo providing an effective resourcefor tracking such progress. The MONACO benchmark, codebase, prompts and modelspredictions are publicly available at: https://tomerwolgithub.github.io/monaco</description><author>Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty</author><pubDate>Fri, 15 Aug 2025 00:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11133v1</guid></item><item><title>Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?</title><link>http://arxiv.org/abs/2508.11011v1</link><description>Construction safety inspections typically involve a human inspectoridentifying safety concerns on-site. With the rise of powerful Vision LanguageModels (VLMs), researchers are exploring their use for tasks such as detectingsafety rule violations from on-site images. However, there is a lack of opendatasets to comprehensively evaluate and further fine-tune VLMs in constructionsafety inspection. Current applications of VLMs use small, supervised datasets,limiting their applicability in tasks they are not directly trained for. Inthis paper, we propose the ConstructionSite 10k, featuring 10,000 constructionsite images with annotations for three inter-connected tasks, including imagecaptioning, safety rule violation visual question answering (VQA), andconstruction element visual grounding. Our subsequent evaluation of currentstate-of-the-art large pre-trained VLMs shows notable generalization abilitiesin zero-shot and few-shot settings, while additional training is needed to makethem applicable to actual construction sites. This dataset allows researchersto train and evaluate their own VLMs with new architectures and techniques,providing a valuable benchmark for construction safety inspection.</description><author>Xuezheng Chen, Zhengbo Zou</author><pubDate>Thu, 14 Aug 2025 18:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11011v1</guid></item><item><title>All for law and law for all: Adaptive RAG Pipeline for Legal Research</title><link>http://arxiv.org/abs/2508.13107v1</link><description>Retrieval-Augmented Generation (RAG) mitigates hallucinations by groundinglarge language model outputs in cited sources, a capability that is especiallycritical in the legal domain. We present an end-to-end RAG pipeline thatrevisits and extends the LegalBenchRAG baseline with three targetedenhancements: (i) a context-aware query translator that disentangles documentreferences from natural-language questions and adapts retrieval depth andresponse style based on expertise and specificity, (ii) open-source retrievalstrategies using SBERT and GTE embeddings that achieve substantial performancegains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for$K&gt;4$) while remaining cost-efficient, and (iii) a comprehensive evaluation andgeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall toassess semantic alignment and faithfulness across models and prompt designs.Our results show that carefully designed open-source pipelines can rival oroutperform proprietary approaches in retrieval quality, while a customlegal-grounded prompt consistently produces more faithful and contextuallyrelevant answers than baseline prompting. Taken together, these contributionsdemonstrate the potential of task-aware, component-level tuning to deliverlegally grounded, reproducible, and cost-effective RAG systems for legalresearch assistance.</description><author>Figarri Keisha, Prince Singh, Pallavi, Dion Fernandes, Aravindh Manivannan, Ilham Wicaksono, Faisal Ahmad</author><pubDate>Mon, 18 Aug 2025 17:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13107v1</guid></item><item><title>Checkmate: interpretable and explainable RSVQA is the endgame</title><link>http://arxiv.org/abs/2508.13086v1</link><description>Remote Sensing Visual Question Answering (RSVQA) presents unique challengesin ensuring that model decisions are both understandable and grounded in visualcontent. Current models often suffer from a lack of interpretability andexplainability, as well as from biases in dataset distributions that lead toshortcut learning. In this work, we tackle these issues by introducing a novelRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253questions and a balanced answer distribution. Each answer is linked to one ormore cells within the image, enabling fine-grained visual reasoning. Building on this dataset, we develop an explainable and interpretable modelcalled Checkmate that identifies the image cells most relevant to itsdecisions. Through extensive experiments across multiple model architectures,we show that our approach improves transparency and supports more trustworthydecision-making in RSVQA systems.</description><author>Lucrezia Tosato, Christel Tartini Chappuis, Syrielle Montariol, Flora Weissgerber, Sylvain Lobry, Devis Tuia</author><pubDate>Mon, 18 Aug 2025 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13086v1</guid></item><item><title>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review</title><link>http://arxiv.org/abs/2410.21673v3</link><description>Public Code Review (PCR) is developed in the Software Question Answering(SQA) community, assisting developers in exploring high-quality and efficientreview services. Current methods on PCR mainly focus on the reviewer'sperspective, including finding a capable reviewer, predicting comment quality,and recommending/generating review comments. However, it is not well studiedthat how to satisfy the review necessity requests posted by developers whichcan increase their visibility, which in turn acts as a prerequisite for betterreview responses. To this end, we propose K nowledge-guided P rompt learningfor P ublic Code Review (KP-PCR) to achieve developer-based code review requestquality assurance (i.e., predicting request necessity and recommending tagssubtask). Specifically, we reformulate the two subtasks via 1) text prompttuning which converts both of them into a Masked Language Model (MLM) byconstructing prompt templates using hard prompt; and 2) knowledge and codeprefix tuning which introduces knowledge guidance from fine-tuned largelanguage models by soft prompt, and uses program dependence graph tocharacterize code snippets. Finally, both of the request necessity predictionand tag recommendation subtasks output predicted results through an answerengineering module. In addition, we further analysis the time complexity of ourKP-PCR that has lightweight prefix based the operation of introducing knowledgeguidance. Experimental results on the PCR dataset for the period 2011-2023demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the requestnecessity prediction and by 1.4%-6.9% in the tag recommendation. The codeimplementation is released at https://github.com/WUT-IDEA/KP-PCR.</description><author>Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang</author><pubDate>Thu, 21 Aug 2025 17:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21673v3</guid></item><item><title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title><link>http://arxiv.org/abs/2508.15717v1</link><description>Multimodal large language models (MLLMs) have made significant progress invisual-language reasoning, but their ability to efficiently handle long videosremains limited. Despite recent advances in long-context MLLMs, storing andattending to the key-value (KV) cache for long visual contexts incurssubstantial memory and computational overhead. Existing visual compressionmethods require either encoding the entire visual context before compression orhaving access to the questions in advance, which is impractical for long videounderstanding and multi-turn conversational settings. In this work, we proposeStreamMem, a query-agnostic KV cache memory mechanism for streaming videounderstanding. Specifically, StreamMem encodes new video frames in a streamingmanner, compressing the KV cache using attention scores between visual tokensand generic query tokens, while maintaining a fixed-size KV memory to enableefficient question answering (QA) in memory-constrained, long-video scenarios.Evaluation on three long video understanding and two streaming video questionanswering benchmarks shows that StreamMem achieves state-of-the-art performancein query-agnostic KV cache compression and is competitive with query-awarecompression approaches.</description><author>Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren</author><pubDate>Thu, 21 Aug 2025 16:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15717v1</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title><link>http://arxiv.org/abs/2508.15690v1</link><description>GRAFT is a structured multimodal benchmark for evaluating models oninstruction-following, visual reasoning, and visual-textual alignment tasks. Itfeatures programmatically generated charts and synthetically rendered tables,created with Python visualization libraries to ensure control over datasemantics, structure, and clarity. Each GRAFT instance pairs a chart or tableimage with a systematically generated, multi-step analytical question basedsolely on visual content. Answers are provided in structured formats such asJSON or YAML, supporting consistent evaluation of both reasoning and outputformat. The benchmark introduces a taxonomy of reasoning types includingcomparison, trend identification, ranking, aggregation, proportion estimation,and anomaly detection to enable comprehensive assessment. Reference answersfollow strict factual and formatting guidelines for precise, aspect-basedevaluation. GRAFT offers a unified, scalable framework for fine-grainedbenchmarking of multimodal models on visually grounded, structured reasoningtasks, setting a new evaluation standard in this field.</description><author>Abhigya Verma, Sriram Puttagunta, Seganrasan Subramanian, Sravan Ramachandran</author><pubDate>Thu, 21 Aug 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15690v1</guid></item><item><title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title><link>http://arxiv.org/abs/2508.15641v1</link><description>Understanding videos requires more than answering open ended questions, itdemands the ability to pinpoint when events occur and how entities interactacross time. While recent Video LLMs have achieved remarkable progress inholistic reasoning, they remain coarse in temporal perception: timestamps areencoded only implicitly, frame level features are weak in capturing continuity,and language vision alignment often drifts from the entities of interest. Inthis paper, we present Grounded VideoDiT, a Video LLM designed to overcomethese limitations by introducing three key innovations. First, a DiffusionTemporal Latent (DTL) encoder enhances boundary sensitivity and maintainstemporal consistency. Second, object grounded representations explicitly bindquery entities to localized visual evidence, strengthening alignment. Third, amixed token scheme with discrete temporal tokens provides explicit timestampmodeling, enabling fine grained temporal reasoning. Together, these designsequip Grounded VideoDiT with robust grounding capabilities, as validated bystate of the art results on Charades STA, NExT GQA, and multiple VideoQAbenchmarks.</description><author>Pengcheng Fang, Yuxia Chen, Rui Guo</author><pubDate>Thu, 21 Aug 2025 15:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15641v1</guid></item><item><title>Using a cognitive architecture to consider antiBlackness in design and development of AI systems</title><link>http://arxiv.org/abs/2207.00644v3</link><description>How might we use cognitive modeling to consider the ways in whichantiblackness, and racism more broadly, impact the design and development of AIsystems? We provide a discussion and an example towards an answer to thisquestion. We use the ACT-R/{\Phi} cognitive architecture and an existingknowledge graph system, ConceptNet, to consider this question not only from acognitive and sociocultural perspective, but also from a physiologicalperspective. In addition to using a cognitive modeling as a means to explorehow antiblackness may manifest in the design and development of AI systems(particularly from a software engineering perspective), we also introduceconnections between antiblackness, the Human, and computational cognitivemodeling. We argue that the typical eschewing of sociocultural processes andknowledge structures in cognitive architectures and cognitive modelingimplicitly furthers a colorblind approach to cognitive modeling and hidessociocultural context that is always present in human behavior and affectscognitive processes.</description><author>Christopher L. Dancy</author><pubDate>Thu, 21 Aug 2025 12:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00644v3</guid></item><item><title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title><link>http://arxiv.org/abs/2508.14052v2</link><description>Accurate information retrieval (IR) is critical in the financial domain,where investors must identify relevant information from large collections ofdocuments. Traditional IR methods-whether sparse or dense-often fall short inretrieval accuracy, as it requires not only capturing semantic similarity butalso performing fine-grained reasoning over document structure anddomain-specific knowledge. Recent advances in large language models (LLMs) haveopened up new opportunities for retrieval with multi-step reasoning, where themodel ranks passages through iterative reasoning about which information ismost relevant to a given query. However, there exists no benchmark to evaluatesuch capabilities in the financial domain. To address this gap, we introduceFinAgentBench, the first large-scale benchmark for evaluating retrieval withmulti-step reasoning in finance -- a setting we term agentic retrieval. Thebenchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firmsand assesses whether LLM agents can (1) identify the most relevant documenttype among candidates, and (2) pinpoint the key passage within the selecteddocument. Our evaluation framework explicitly separates these two reasoningsteps to address context limitations. This design enables to provide aquantitative basis for understanding retrieval-centric LLM behavior in finance.We evaluate a suite of state-of-the-art models and further demonstrated howtargeted fine-tuning can significantly improve agentic retrieval performance.Our benchmark provides a foundation for studying retrieval-centric LLM behaviorin complex, domain-specific tasks for finance. We will release the datasetpublicly upon acceptance of the paper and plan to expand and share dataset forthe full S&amp;P 500 and beyond.</description><author>Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</author><pubDate>Thu, 21 Aug 2025 09:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14052v2</guid></item><item><title>Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering</title><link>http://arxiv.org/abs/2503.14957v2</link><description>We introduce \dataset (Procedural Knowledge Reasoning Question Answering), anew benchmark for question answering over procedural tasks that requirestructured reasoning. PKR-QA is constructed semi-automatically using aprocedural knowledge graph (PKG), which encodes task-specific knowledge acrossdiverse domains. The PKG is built by curating and linking information from theCOIN instructional video dataset and the ontology, enriched with commonsenseknowledge from ConceptNet and structured outputs from Large Language Models(LLMs), followed by manual verification. To generate question-answer pairs, wedesign graph traversal templates where each template is applied systematicallyover PKG. To enable interpretable reasoning, we propose a neurosymbolicapproach called Knowledge Module Learning (KML), which learns proceduralrelations via neural modules and composes them for structured reasoning withLLMs. Experiments demonstrate that this paradigm improves reasoning performanceon our dataset and enables step-by-step reasoning traces that facilitateinterpretability. Our theoretical analysis on KML learning shows that ourtrained models satisfy near optimal conditions for learning KG relations asneural network mapping models. Code and dataset will be released soon.</description><author>Thanh-Son Nguyen, Hong Yang, Tzeh Yuan Neoh, Hao Zhang, Ee Yeo Keat, Basura Fernando</author><pubDate>Thu, 21 Aug 2025 09:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.14957v2</guid></item><item><title>Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering</title><link>http://arxiv.org/abs/2502.11491v2</link><description>Large language models (LLMs) have shown remarkable capabilities in naturallanguage processing. However, in knowledge graph question answering tasks(KGQA), there remains the issue of answering questions that require multi-hopreasoning. Existing methods rely on entity vector matching, but the purpose ofthe question is abstract and difficult to match with specific entities. As aresult, it is difficult to establish reasoning paths to the purpose, whichleads to information loss and redundancy. To address this issue, inspired byhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), anovel framework that constructs reasoning paths from purposes back toconditions. ORT operates in three key phases: (1) using LLM to extract purposelabels and condition labels, (2) constructing label reasoning paths based onthe KG ontology, and (3) using the label reasoning paths to guide knowledgeretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achievesstate-of-the-art performance and significantly enhances the capability of LLMsfor KGQA.</description><author>Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin</author><pubDate>Thu, 21 Aug 2025 07:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.11491v2</guid></item><item><title>TComQA: Extracting Temporal Commonsense from Text</title><link>http://arxiv.org/abs/2508.15274v1</link><description>Understanding events necessitates grasping their temporal context, which isoften not explicitly stated in natural language. For example, it is not atrivial task for a machine to infer that a museum tour may last for a fewhours, but can not take months. Recent studies indicate that even advancedlarge language models (LLMs) struggle in generating text that require reasoningwith temporal commonsense due to its infrequent explicit mention in text.Therefore, automatically mining temporal commonsense for events enables thecreation of robust language models. In this work, we investigate the capacityof LLMs to extract temporal commonsense from text and evaluate multipleexperimental setups to assess their effectiveness. Here, we propose a temporalcommonsense extraction pipeline that leverages LLMs to automatically minetemporal commonsense and use it to construct TComQA, a dataset derived fromSAMSum and RealNews corpora. TComQA has been validated through crowdsourcingand achieves over 80\% precision in extracting temporal commonsense. The modeltrained with TComQA also outperforms an LLM fine-tuned on existing dataset oftemporal question answering task.</description><author>Lekshmi R Nair, Arun Sankar, Koninika Pal</author><pubDate>Thu, 21 Aug 2025 06:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15274v1</guid></item><item><title>Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering</title><link>http://arxiv.org/abs/2508.15213v1</link><description>Large Language Models (LLMs) perform well in general QA but often struggle indomain-specific scenarios. Retrieval-Augmented Generation (RAG) introducesexternal knowledge but suffers from hallucinations and latency due to noisyretrievals. Continued pretraining internalizes domain knowledge but is costlyand lacks cross-domain flexibility. We attribute this challenge to thelong-tail distribution of domain knowledge, which leaves partial yet usefulinternal knowledge underutilized. We further argue that knowledge acquisitionshould be progressive, mirroring human learning: first understanding concepts,then applying them to complex reasoning. To address this, we propose Selct2Know(S2K), a cost-effective framework that internalizes domain knowledge through aninternal-external knowledge self-selection strategy and selective supervisedfine-tuning. We also introduce a structured reasoning data generation pipelineand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,and financial QA benchmarks show that S2K consistently outperforms existingmethods and matches domain-pretrained LLMs with significantly lower cost.</description><author>Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling</author><pubDate>Thu, 21 Aug 2025 03:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15213v1</guid></item><item><title>LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</title><link>http://arxiv.org/abs/2508.15192v1</link><description>While large language models (LLMs) have shown promise in healthcare, theirapplication for rare medical conditions is still hindered by scarce andunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causingexcessive sweating beyond physiological needs, is one such rare disorder,affecting 2-3% of the population and significantly impacting both physicalcomfort and psychosocial well-being. To date, no work has tailored LLMs toadvance the diagnosis or care of hyperhidrosis. To address this gap, we presentLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy andempathetic hyperhidrosis support. The system follows a three-stage pipeline. Inthe data augmentation stage, a frontier LLM generates medically plausiblesynthetic vignettes from curated open-source data to create a diverse andbalanced question-answer dataset. In the fine-tuning stage, an open-sourcefoundation model is fine-tuned on the dataset to provide diagnosis,personalized treatment recommendations, and empathetic psychological support.In the inference and expert evaluation stage, clinical and psychologicalspecialists assess accuracy, appropriateness, and empathy, with validatedresponses iteratively enriching the dataset. Experiments show that LLM4Sweatoutperforms baselines and delivers the first open-source LLM framework forhyperhidrosis, offering a generalizable approach for other rare diseases withsimilar data and trustworthiness challenges.</description><author>Wenjie Lin, Jin Wei-Kocsis</author><pubDate>Thu, 21 Aug 2025 03:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15192v1</guid></item><item><title>Identifying and Answering Questions with False Assumptions: An Interpretable Approach</title><link>http://arxiv.org/abs/2508.15139v1</link><description>People often ask questions with false assumptions, a type of question thatdoes not have regular answers. Answering such questions require firstidentifying the false assumptions. Large Language Models (LLMs) often generatemisleading answers because of hallucinations. In this paper, we focus onidentifying and answering questions with false assumptions in several domains.We first investigate to reduce the problem to fact verification. Then, wepresent an approach leveraging external evidence to mitigate hallucinations.Experiments with five LLMs demonstrate that (1) incorporating retrievedevidence is beneficial and (2) generating and validating atomic assumptionsyields more improvements and provides an interpretable answer by specifying thefalse assumptions.</description><author>Zijie Wang, Eduardo Blanco</author><pubDate>Thu, 21 Aug 2025 00:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15139v1</guid></item><item><title>LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text</title><link>http://arxiv.org/abs/2508.15085v1</link><description>LongRecall. The completeness of machine-generated text, ensuring that itcaptures all relevant information, is crucial in domains such as medicine andlaw and in tasks like list-based question answering (QA), where omissions canhave serious consequences. However, existing recall metrics often depend onlexical overlap, leading to errors with unsubstantiated entities andparaphrased answers, while LLM-as-a-Judge methods with long holistic promptscapture broader semantics but remain prone to misalignment and hallucinationswithout structured verification. We introduce LongRecall, a general three-stagerecall evaluation framework that decomposes answers into self-contained facts,successively narrows plausible candidate matches through lexical and semanticfiltering, and verifies their alignment through structured entailment checks.This design reduces false positives and false negatives while accommodatingdiverse phrasings and contextual variations, serving as a foundational buildingblock for systematic recall assessment. We evaluate LongRecall on threechallenging long-form QA benchmarks using both human annotations and LLM-basedjudges, demonstrating substantial improvements in recall accuracy over stronglexical and LLM-as-a-Judge baselines.</description><author>MohamamdJavad Ardestani, Ehsan Kamalloo, Davood Rafiei</author><pubDate>Wed, 20 Aug 2025 21:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15085v1</guid></item><item><title>Don't Think Twice! Over-Reasoning Impairs Confidence Calibration</title><link>http://arxiv.org/abs/2508.15050v1</link><description>Large Language Models deployed as question answering tools require robustcalibration to avoid overconfidence. We systematically evaluate how reasoningcapabilities and budget affect confidence assessment accuracy, using theClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetaryhealth. Our key finding challenges the "test-time scaling" paradigm: whilerecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,increasing reasoning budgets consistently impairs rather than improvescalibration. Extended reasoning leads to systematic overconfidence that worsenswith longer thinking budgets, producing diminishing and negative returns beyondmodest computational investments. Conversely, search-augmented generationdramatically outperforms pure reasoning, achieving 89.3% accuracy by retrievingrelevant evidence. Our results suggest that information access, rather thanreasoning depth or inference budget, may be the critical bottleneck forimproved confidence calibration of knowledge-intensive tasks.</description><author>Romain Lacombe, Kerrie Wu, Eddie Dilworth</author><pubDate>Wed, 20 Aug 2025 20:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15050v1</guid></item><item><title>PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data</title><link>http://arxiv.org/abs/2502.20616v2</link><description>Personalization is critical in AI assistants, particularly in the context ofprivate AI models that work with individual users. A key scenario in thisdomain involves enabling AI models to access and interpret a user's privatedata (e.g., conversation history, user-AI interactions, app usage) tounderstand personal details such as biographical information, preferences, andsocial connections. However, due to the sensitive nature of such data, thereare no publicly available datasets that allow us to assess an AI model'sability to understand users through direct access to personal information. To address this gap, we introduce a synthetic data generation pipeline thatcreates diverse, realistic user profiles and private documents simulating humanactivities. Leveraging this synthetic data, we present PersonaBench, abenchmark designed to evaluate AI models' performance in understanding personalinformation derived from simulated private user data. We evaluate Retrieval-Augmented Generation (RAG) pipelines using questionsdirectly related to a user's personal information, supported by the relevantprivate documents provided to the models. Our results reveal that currentretrieval-augmented AI models struggle to answer private questions byextracting personal information from user documents, highlighting the need forimproved methodologies to enhance personalization capabilities in AI.</description><author>Juntao Tan, Liangwei Yang, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Tulika Manoj Awalgaonkar, Jianguo Zhang, Weiran Yao, Ming Zhu, Shirley Kokane, Silvio Savarese, Huan Wang, Caiming Xiong, Shelby Heinecke</author><pubDate>Wed, 20 Aug 2025 18:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.20616v2</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v2</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</author><pubDate>Thu, 21 Aug 2025 18:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v2</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model</title><link>http://arxiv.org/abs/2312.11370v2</link><description>Large language models (LLMs) have shown remarkable proficiency in human-levelreasoning and generation capabilities, which encourages extensive research ontheir application in mathematical problem solving. However, current work hasbeen largely focused on text-based mathematical problems, with limitedinvestigation in problems involving geometric information. Addressing this gap,we aim to enable LLMs to solve geometric problems by understanding image input.We first analyze the limitations of current Multimodal Large Language Models(MLLMs) in this area: they struggle to accurately comprehending basic geometricelements and their relationships. To overcome these challenges, we takeadvantage of the unique characteristics of geometric problems (such as uniquegeometric logical form, and geometric scalability) and the capacity of thetextual LLMs to build an enriched multimodal geometry dataset based on existingdata. The augmented dataset, Geo170K, contains more than 170K geometricimage-caption and question-answer pairs. Utilizing our constructed Geo170Kdataset, we develop G-LLaVA, which demonstrates exceptional performance insolving geometric problems, significantly outperforming GPT-4-V on theMathVista benchmark with only 7B parameters.</description><author>Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong</author><pubDate>Wed, 20 Aug 2025 15:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11370v2</guid></item><item><title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title><link>http://arxiv.org/abs/2504.19254v3</link><description>Hallucinations are a persistent problem with Large Language Models (LLMs). Asthese models become increasingly used in high-stakes domains, such ashealthcare and finance, the need for effective hallucination detection iscrucial. To this end, we outline a versatile framework for zero-resourcehallucination detection that practitioners can apply to real-world use cases.To achieve this, we adapt a variety of existing uncertainty quantification (UQ)techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,transforming them as necessary into standardized response-level confidencescores ranging from 0 to 1. To enhance flexibility, we propose a tunableensemble approach that incorporates any combination of the individualconfidence scores. This approach enables practitioners to optimize the ensemblefor a specific use case for improved performance. To streamline implementation,the full suite of scorers is offered in this paper's companion Python toolkit,UQLM. To evaluate the performance of the various scorers, we conduct anextensive set of experiments using several LLM question-answering benchmarks.We find that our tunable ensemble typically surpasses its individual componentsand outperforms existing hallucination detection methods. Our resultsdemonstrate the benefits of customized hallucination detection strategies forimproving the accuracy and reliability of LLMs.</description><author>Dylan Bouchard, Mohit Singh Chauhan</author><pubDate>Wed, 20 Aug 2025 14:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19254v3</guid></item><item><title>Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems</title><link>http://arxiv.org/abs/2508.14553v1</link><description>Over time, software systems have reached a level of complexity that makes itdifficult for their developers and users to explain particular decisions madeby them. In this paper, we focus on the explainability of component-basedsystems for Question Answering (QA). These components often conduct processesdriven by AI methods, in which behavior and decisions cannot be clearlyexplained or justified, s.t., even for QA experts interpreting the executedprocess and its results is hard. To address this challenge, we present anapproach that considers the components' input and output data flows as a sourcefor representing the behavior and provide explanations for the components,enabling users to comprehend what happened. In the QA framework used here, thedata flows of the components are represented as SPARQL queries (inputs) and RDFtriples (outputs). Hence, we are also providing valuable insights onverbalization regarding these data types. In our experiments, the approachgenerates explanations while following template-based settings (baseline) orvia the use of Large Language Models (LLMs) with different configurations(automatic generation). Our evaluation shows that the explanations generatedvia LLMs achieve high quality and mostly outperform template-based approachesaccording to the users' ratings. Therefore, it enables us to automaticallyexplain the behavior and decisions of QA components to humans while using RDFand SPARQL as a context for explanations.</description><author>Dennis Schiese, Aleksandr Perevalov, Andreas Both</author><pubDate>Wed, 20 Aug 2025 09:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14553v1</guid></item><item><title>Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2508.14427v1</link><description>This paper addresses the problems of missing reasoning chains andinsufficient entity-level semantic understanding in large language models whendealing with tasks that require structured knowledge. It proposes a fine-tuningalgorithm framework based on knowledge graph injection. The method builds onpretrained language models and introduces structured graph information forauxiliary learning. A graph neural network is used to encode entities and theirrelations, constructing a graph-based semantic representation. A fusionmechanism is then designed to jointly model the knowledge graph embeddings withthe contextual representations from the language model. To enhance therobustness of knowledge integration, a gating mechanism is introduced todynamically balance the contributions of linguistic semantics and structuralknowledge. This effectively mitigates conflicts between differentrepresentational spaces. During training, a joint loss function is constructedto account for both task performance and structural alignment objectives. Thishelps improve the accuracy of entity prediction and semantic reasoning. Thestudy also includes a series of systematic sensitivity experiments. Itevaluates the effects of learning rate, graph coverage, and structuralperturbations on model performance. The results further validate theeffectiveness and stability of the proposed method across tasks such as entityrecognition, question answering, and language generation. Experimental findingsshow that the proposed structure-aware fine-tuning framework significantlyenhances the model's ability to represent complex semantic units. Itdemonstrates better semantic consistency and contextual logic modeling inscenarios involving structural reasoning and entity extraction.</description><author>Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du</author><pubDate>Wed, 20 Aug 2025 04:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14427v1</guid></item><item><title>Thyme: Think Beyond Images</title><link>http://arxiv.org/abs/2508.11630v1</link><description>Following OpenAI's introduction of the ``thinking with images'' concept,recent efforts have explored stimulating the use of visual information in thereasoning process to enhance model performance in perception and reasoningtasks. However, to the best of our knowledge, no open-source work currentlyoffers a feature set as rich as proprietary models (O3), which can performdiverse image manipulations and simultaneously enhance logical reasoningcapabilities through code. In this paper, we make a preliminary attempt in thisdirection by introducing Thyme (Think Beyond Images), a novel paradigm forenabling MLLMs to transcend existing ``think with images'' approaches byautonomously generating and executing diverse image processing andcomputational operations via executable code. This approach not onlyfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,rotation, contrast enhancement) but also allows for mathematical computations,all while maintaining high autonomy in deciding when and how to apply theseoperations. We activate this capability through a two-stage training strategy:an initial SFT on a curated dataset of 500K samples to teach code generation,followed by a RL phase to refine decision-making. For the RL stage, we manuallycollect and design high-resolution question-answer pairs to increase thelearning difficulty, and we propose GRPO-ATS (Group Relative PolicyOptimization with Adaptive Temperature Sampling), an algorithm that appliesdistinct temperatures to text and code generation to balance reasoningexploration with code execution precision. We conduct extensive experimentalanalysis and ablation studies. Comprehensive evaluations on nearly 20benchmarks show that Thyme yields significant and consistent performance gains,particularly in challenging high-resolution perception and complex reasoningtasks.</description><author>Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, Guorui Zhou</author><pubDate>Fri, 15 Aug 2025 17:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11630v1</guid></item><item><title>A Little Human Data Goes A Long Way</title><link>http://arxiv.org/abs/2410.13098v3</link><description>Faced with an expensive human annotation process, creators of NLP systemsincreasingly turn to synthetic data generation. While this method showspromise, the extent to which synthetic data can replace human annotation ispoorly understood. We investigate the use of synthetic data in FactVerification (FV) and Question Answering (QA) by studying the effects ofincrementally replacing human generated data with synthetic points on eightdiverse datasets. Strikingly, replacing up to 90% of the training data onlymarginally decreases performance, but replacing the final 10% leads to severedeclines. We find that models trained on purely synthetic data can be reliablyimproved by including as few as 125 human generated data points. We show thatmatching the performance gain of just a little additional human data (only 200points) requires an order of magnitude more synthetic data and estimate priceratios at which human annotation would be a more cost-effective solution. Ourresults suggest that even when human annotation at scale is infeasible, thereis great value to having a small proportion of the dataset being humangenerated.</description><author>Dhananjay Ashok, Jonathan May</author><pubDate>Wed, 20 Aug 2025 01:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13098v3</guid></item><item><title>Is ChatGPT-5 Ready for Mammogram VQA?</title><link>http://arxiv.org/abs/2508.11628v1</link><description>Mammogram visual question answering (VQA) integrates image interpretationwith clinical reasoning and has potential to support breast cancer screening.We systematically evaluated the GPT-5 family and GPT-4o model on four publicmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,abnormality detection, and malignancy classification tasks. GPT-5 consistentlywas the best performing model but lagged behind both human experts anddomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scoresamong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),calcification (63.5%), and malignancy (52.8%) classification. On InBreast, itattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detectionand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADSaccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Comparedwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) andspecificity (52.3%). While GPT-5 exhibits promising capabilities for screeningtasks, its performance remains insufficient for high-stakes clinical imagingapplications without targeted domain adaptation and optimization. However, thetremendous improvements in performance from GPT-4o to GPT-5 show a promisingtrend in the potential for general large language models (LLMs) to assist withmammography VQA tasks.</description><author>Qiang Li, Shansong Wang, Mingzhe Hu, Mojtaba Safari, Zachary Eidex, Xiaofeng Yang</author><pubDate>Fri, 15 Aug 2025 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11628v1</guid></item><item><title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title><link>http://arxiv.org/abs/2503.01307v2</link><description>Test-time inference has emerged as a powerful paradigm for enabling languagemodels to ``think'' longer and more carefully about complex challenges, muchlike skilled human experts. While reinforcement learning (RL) can driveself-improvement in language models on verifiable tasks, some models exhibitsubstantial gains while others quickly plateau. For instance, we find thatQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the gameof Countdown. This discrepancy raises a critical question: what intrinsicproperties enable effective self-improvement? We introduce a framework toinvestigate this question by analyzing four key cognitive behaviors --verification, backtracking, subgoal setting, and backward chaining -- that bothexpert human problem solvers and successful language models employ. Our studyreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llamainitially lacks them. In systematic experimentation with controlled behavioraldatasets, we find that priming Llama with examples containing these reasoningbehaviors enables substantial improvements during RL, matching or exceedingQwen's performance. Importantly, the presence of reasoning behaviors, ratherthan correctness of answers, proves to be the critical factor -- models primedwith incorrect solutions containing proper reasoning patterns achievecomparable performance to those trained on correct solutions. Finally,leveraging continued pretraining with OpenWebMath data, filtered to amplifyreasoning behaviors, enables the Llama model to match Qwen's self-improvementtrajectory. Our findings establish a fundamental relationship between initialreasoning behaviors and the capacity for improvement, explaining why somelanguage models effectively utilize additional computation while othersplateau.</description><author>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</author><pubDate>Fri, 15 Aug 2025 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.01307v2</guid></item><item><title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title><link>http://arxiv.org/abs/2508.09736v2</link><description>We introduce M3-Agent, a novel multimodal agent framework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual andauditory inputs to build and update its long-term memory. Beyond episodicmemory, it also develops semantic memory, enabling it to accumulate worldknowledge over time. Its memory is organized in an entity-centric, multimodalformat, allowing deeper and more consistent understanding of the environment.Given an instruction, M3-Agent autonomously performs multi-turn, iterativereasoning and retrieves relevant information from memory to accomplish thetask. To evaluate memory effectiveness and memory-based reasoning in multimodalagents, we develop M3-Bench, a new long-video question answering benchmark.M3-Bench comprises 100 newly recorded real-world videos captured from a robot'sperspective (M3-Bench-robot) and 920 web-sourced videos across diversescenarios (M3-Bench-web). We annotate question-answer pairs designed to testkey capabilities essential for agent applications, such as human understanding,general knowledge extraction, and cross-modal reasoning. Experimental resultsshow that M3-Agent, trained via reinforcement learning, outperforms thestrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-weband VideoMME-long, respectively. Our work advances the multimodal agents towardmore human-like long-term memory and provides insights into their practicaldesign. Model, code and data are available athttps://github.com/bytedance-seed/m3-agent</description><author>Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</author><pubDate>Fri, 15 Aug 2025 13:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09736v2</guid></item></channel></rss>