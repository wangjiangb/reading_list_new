<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 27 Aug 2025 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>An Empirical Study on How Video-LLMs Answer Video Questions</title><link>http://arxiv.org/abs/2508.15360v1</link><description>Taking advantage of large-scale data and pretrained language models, VideoLarge Language Models (Video-LLMs) have shown strong capabilities in answeringvideo questions. However, most existing efforts focus on improving performance,with limited attention to understanding their internal mechanisms. This paperaims to bridge this gap through a systematic empirical study. To interpretexisting VideoLLMs, we adopt attention knockouts as our primary analytical tooland design three variants: Video Temporal Knockout, Video Spatial Knockout, andLanguage-to-Video Knockout. Then, we apply these three knockouts on differentnumbers of layers (window of layers). By carefully controlling the window oflayers and types of knockouts, we provide two settings: a global setting and afine-grained setting. Our study reveals three key findings: (1) Global settingindicates Video information extraction primarily occurs in early layers,forming a clear two-stage process -- lower layers focus on perceptual encoding,while higher layers handle abstract reasoning; (2) In the fine-grained setting,certain intermediate layers exert an outsized impact on video questionanswering, acting as critical outliers, whereas most other layers contributeminimally; (3) In both settings, we observe that spatial-temporal modelingrelies more on language-guided retrieval than on intra- and inter-frameself-attention among video tokens, despite the latter's high computationalcost. Finally, we demonstrate that these insights can be leveraged to reduceattention computation in Video-LLMs. To our knowledge, this is the first workto systematically uncover how Video-LLMs internally process and understandvideo content, offering interpretability and efficiency perspectives for futureresearch.</description><author>Chenhui Gou, Ziyu Ma, Zicheng Duan, Haoyu He, Feng Chen, Akide Liu, Bohan Zhuang, Jianfei Cai, Hamid Rezatofighi</author><pubDate>Thu, 21 Aug 2025 08:42:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15360v1</guid></item><item><title>Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding</title><link>http://arxiv.org/abs/2410.01671v3</link><description>Large language models (LLMs) have shown remarkable capabilities in naturallanguage processing; however, they still face difficulties when tasked withunderstanding lengthy contexts and executing effective question answering.These challenges often arise due to the complexity and ambiguity present inlonger texts. To enhance the performance of LLMs in such scenarios, weintroduce the Long Question Coreference Adaptation (LQCA) method. Thisinnovative framework focuses on coreference resolution tailored to longcontexts, allowing the model to identify and manage references effectively. TheLQCA method encompasses four key steps: resolving coreferences withinsub-documents, computing the distances between mentions, defining arepresentative mention for coreference, and answering questions through mentionreplacement. By processing information systematically, the framework provideseasier-to-handle partitions for LLMs, promoting better understanding.Experimental evaluations on a range of LLMs and datasets have yielded positiveresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,highlighting the effectiveness of leveraging coreference resolution to bridgecontext gaps in question answering. Our code is public athttps://github.com/OceannTwT/LQCA.</description><author>Yanming Liu, Xinyue Peng, Jiannan Cao, Yanxin Shen, Tianyu Du, Sheng Cheng, Xun Wang, Jianwei Yin, Xuhong Zhang</author><pubDate>Fri, 15 Aug 2025 05:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01671v3</guid></item><item><title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title><link>http://arxiv.org/abs/2507.22752v2</link><description>We introduce CUS-QA, a benchmark for open-ended regional question answeringthat encompasses both textual and visual modalities. We also provide strongbaselines using state-of-the-art large language models (LLMs). Our datasetconsists of manually curated questions and answers grounded in Wikipedia,created by native speakers from Czechia, Slovakia, and Ukraine, withaccompanying English translations. It includes both purely textual questionsand those requiring visual understanding. We evaluate state-of-the-art LLMsthrough prompting and complement this with human judgments of answercorrectness. Using these human evaluations, we analyze the reliability ofexisting automatic evaluation metrics. Our baseline results show that even thebest open-weight LLMs achieve only around 50% accuracy on textual questions andbelow 30% on visual questions. LLM-based evaluation metrics show strongcorrelation with human judgment, while traditional string-overlap metricsperform surprisingly well due to the prevalence of named entities in answers.</description><author>Jindřich Libovický, Jindřich Helcl, Andrei Manea, Gianluca Vico</author><pubDate>Thu, 21 Aug 2025 12:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22752v2</guid></item><item><title>Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for Multimodal Medical VQA</title><link>http://arxiv.org/abs/2507.05520v3</link><description>Dermatological care via telemedicine often lacks the rich context ofin-person visits. Clinicians must make diagnoses based on a handful of imagesand brief descriptions, without the benefit of physical exams, second opinions,or reference materials. While many medical AI systems attempt to bridge thesegaps with domain-specific fine-tuning, this work hypothesized that mimickingclinical reasoning processes could offer a more effective path forward. Thisstudy tested seven vision-language models on medical visual question answeringacross six configurations: baseline models, fine-tuned variants, and bothaugmented with either reasoning layers that combine multiple modelperspectives, analogous to peer consultation, or retrieval-augmented generationthat incorporates medical literature at inference time, serving a role similarto reference-checking. While fine-tuning degraded performance in four of sevenmodels with an average 30% decrease, baseline models collapsed on test data.Clinical-inspired architectures, meanwhile, achieved up to 70% accuracy,maintaining performance on unseen data while generating explainable,literature-grounded outputs critical for clinical adoption. These findingsdemonstrate that medical AI succeeds by reconstructing the collaborative andevidence-based practices fundamental to clinical diagnosis.</description><author>Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar</author><pubDate>Tue, 26 Aug 2025 14:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05520v3</guid></item><item><title>Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs</title><link>http://arxiv.org/abs/2508.19111v1</link><description>Large vision-language models (LVLMs) demonstrate strong visual questionanswering (VQA) capabilities but are shown to hallucinate. A reliable modelshould perceive its knowledge boundaries-knowing what it knows and what it doesnot. This paper investigates LVLMs' perception of their knowledge boundaries byevaluating three types of confidence signals: probabilistic confidence, answerconsistency-based confidence, and verbalized confidence. Experiments on threeLVLMs across three VQA datasets show that, although LVLMs possess a reasonableperception level, there is substantial room for improvement. Among the threeconfidences, probabilistic and consistency-based signals are more reliableindicators, while verbalized confidence often leads to overconfidence. Toenhance LVLMs' perception, we adapt several established confidence calibrationmethods from Large Language Models (LLMs) and propose three effective methods.Additionally, we compare LVLMs with their LLM counterparts, finding thatjointly processing visual and textual inputs decreases question-answeringperformance but reduces confidence, resulting in an improved perception levelcompared to LLMs.</description><author>Zhikai Ding, Shiyu Ni, Keping Bi</author><pubDate>Tue, 26 Aug 2025 15:14:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19111v1</guid></item><item><title>Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for Multimodal Medical VQA</title><link>http://arxiv.org/abs/2507.05520v2</link><description>Dermatological care via telemedicine often lacks the rich context ofin-person visits. Clinicians must make diagnoses based on a handful of imagesand brief descriptions, without the benefit of physical exams, second opinions,or reference materials. While many medical AI systems attempt to bridge thesegaps with domain-specific fine-tuning, this work hypothesized that mimickingclinical reasoning processes could offer a more effective path forward. Thisstudy tested seven vision-language models on medical visual question answeringacross six configurations: baseline models, fine-tuned variants, and bothaugmented with either reasoning layers that combine multiple modelperspectives, analogous to peer consultation, or retrieval-augmented generationthat incorporates medical literature at inference time, serving a role similarto reference-checking. While fine-tuning degraded performance in four of sevenmodels with an average 30\% decrease, baseline models collapsed on test data.Clinical-inspired architectures, meanwhile, achieved up to 70\% accuracy,maintaining performance on unseen data while generating explainable,literature-grounded outputs critical for clinical adoption. These findingsdemonstrate that medical AI succeeds by reconstructing the collaborative andevidence-based practices fundamental to clinical diagnosis.</description><author>Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar</author><pubDate>Mon, 25 Aug 2025 14:55:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.05520v2</guid></item><item><title>Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset</title><link>http://arxiv.org/abs/2508.11058v1</link><description>The advancement of 3D vision-language (3D VL) learning is hindered by severallimitations in existing 3D VL datasets: they rarely necessitate reasoningbeyond a close range of objects in single viewpoint, and annotations often linkinstructions to single objects, missing richer contextual alignments betweenmultiple objects. This significantly curtails the development of models capableof deep, multi-view 3D scene understanding over distant objects. To addressthese challenges, we introduce MV-ScanQA, a novel 3D question answering datasetwhere 68% of questions explicitly require integrating information from multipleviews (compared to less than 7% in existing datasets), thereby rigorouslytesting multi-view compositional reasoning. To facilitate the training ofmodels for such demanding scenarios, we present TripAlign dataset, alarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M &lt;2Dview, set of 3D objects, text&gt; triplets that explicitly aligns groups ofcontextually related objects with text, providing richer, view-groundedmulti-object multimodal alignment signals than previous single-objectannotations. We further develop LEGO, a baseline method for the multi-viewreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2DLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlignachieves state-of-the-art performance not only on the proposed MV-ScanQA, butalso on existing benchmarks for 3D dense captioning and question answering.Datasets and code are available athttps://matthewdm0816.github.io/tripalign-mvscanqa.</description><author>Wentao Mo, Qingchao Chen, Yuxin Peng, Siyuan Huang, Yang Liu</author><pubDate>Thu, 14 Aug 2025 20:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11058v1</guid></item><item><title>LLM Compression: How Far Can We Go in Balancing Size and Performance?</title><link>http://arxiv.org/abs/2508.11318v1</link><description>Quantization is an essential and popular technique for improving theaccessibility of large language models (LLMs) by reducing memory usage andcomputational costs while maintaining performance. In this study, we apply4-bit Group Scaling Quantization (GSQ) and Generative Pretrained TransformerQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating theirimpact across multiple NLP tasks. We benchmark these models on MS MARCO(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K(Mathematical Reasoning) datasets, assessing both accuracy and efficiencyacross various tasks. The study measures the trade-offs between modelcompression and task performance, analyzing key evaluation metrics, namelyaccuracy, inference latency, and throughput (total output tokens generated persecond), providing insights into the suitability of low-bit quantization forreal-world deployment. Using the results, users can then make suitabledecisions based on the specifications that need to be met. We discuss the prosand cons of GSQ and GPTQ techniques on models of different sizes, which alsoserve as a benchmark for future experiments.</description><author>Sahil Sk, Debasish Dhal, Sonal Khosla, Sk Shahid, Sambit Shekhar, Akash Dhaka, Shantipriya Parida, Dilip K. Prasad, Ondřej Bojar</author><pubDate>Fri, 15 Aug 2025 08:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11318v1</guid></item><item><title>MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks</title><link>http://arxiv.org/abs/2507.18932v2</link><description>Environmental, Social, and Governance (ESG) reports are essential forevaluating sustainability practices, ensuring regulatory compliance, andpromoting financial transparency. However, these documents are often lengthy,structurally diverse, and multimodal, comprising dense text, structured tables,complex figures, and layout-dependent semantics. Existing AI systems oftenstruggle to perform reliable document-level reasoning in such settings, and nodedicated benchmark currently exists in ESG domain. To fill the gap, weintroduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targetedto evaluate multimodal understanding and complex reasoning across structurallydiverse and multi-source ESG documents. This dataset is constructed via ahuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generatescandidate question-answer (QA) pairs by jointly interpreting rich textual,tabular, and visual information from layout-aware document pages. Second, anLLM verifies the semantic accuracy, completeness, and reasoning complexity ofeach QA pair. This automated process is followed by an expert-in-the-loopvalidation, where domain specialists validate and calibrate QA pairs to ensurequality, relevance, and diversity. MMESGBench comprises 933 validated QA pairsderived from 45 ESG documents, spanning across seven distinct document typesand three major ESG source categories. Questions are categorized assingle-page, cross-page, or unanswerable, with each accompanied by fine-grainedmultimodal evidence. Initial experiments validate that multimodal andretrieval-augmented models substantially outperform text-only baselines,particularly on visually grounded and cross-page tasks. MMESGBench is publiclyavailable as an open-source dataset athttps://github.com/Zhanglei1103/MMESGBench.</description><author>Lei Zhang, Xin Zhou, Chaoyue He, Di Wang, Yi Wu, Hong Xu, Wei Liu, Chunyan Miao</author><pubDate>Fri, 15 Aug 2025 08:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.18932v2</guid></item><item><title>AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries</title><link>http://arxiv.org/abs/2508.11285v1</link><description>Depression, anxiety, and stress are widespread mental health concerns thatincreasingly drive individuals to seek information from Large Language Models(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, GeminiPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twentypragmatic questions about depression, anxiety, and stress when those questionsare framed for six user profiles (baseline, woman, man, young, old, anduniversity student). The models generated 2,880 answers, which we scored forsentiment and emotions using state-of-the-art tools. Our analysis revealed thatoptimism, fear, and sadness dominated the emotional landscape across alloutputs, with neutral sentiment maintaining consistently high values.Gratitude, joy, and trust appeared at moderate levels, while emotions such asanger, disgust, and love were rarely expressed. The choice of LLM significantlyinfluenced emotional expression patterns. Mixtral exhibited the highest levelsof negative emotions including disapproval, annoyance, and sadness, while Llamademonstrated the most optimistic and joyful responses. The type of mentalhealth condition dramatically shaped emotional responses: anxiety promptselicited extraordinarily high fear scores (0.974), depression prompts generatedelevated sadness (0.686) and the highest negative sentiment, whilestress-related queries produced the most optimistic responses (0.755) withelevated joy and trust. In contrast, demographic framing of queries producedonly marginal variations in emotional tone. Statistical analyses confirmedsignificant model-specific and condition-specific differences, whiledemographic influences remained minimal. These findings highlight the criticalimportance of model selection in mental health applications, as each LLMexhibits a distinct emotional signature that could significantly impact userexperience and outcomes.</description><author>Arya VarastehNezhad, Reza Tavasoli, Soroush Elyasi, MohammadHossein LotfiNia, Hamed Farbeh</author><pubDate>Fri, 15 Aug 2025 07:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11285v1</guid></item><item><title>Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2508.11247v1</link><description>Multi-hop question answering (MHQA) requires integrating knowledge scatteredacross multiple passages to derive the correct answer. Traditionalretrieval-augmented generation (RAG) methods primarily focus on coarse-grainedtextual semantic similarity and ignore structural associations among dispersedknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methodsaddress this by leveraging knowledge graphs (KGs) to capture structuralassociations, but they tend to overly rely on structural information andfine-grained word- or phrase-level retrieval, resulting in an underutilizationof textual semantics. In this paper, we propose a novel RAG approach calledHGRAG for MHQA that achieves cross-granularity integration of structural andsemantic information via hypergraphs. Structurally, we construct an entityhypergraph where fine-grained entities serve as nodes and coarse-grainedpassages as hyperedges, and establish knowledge association through sharedentities. Semantically, we design a hypergraph retrieval method that integratesfine-grained entity similarity and coarse-grained passage similarity viahypergraph diffusion. Finally, we employ a retrieval enhancement module, whichfurther refines the retrieved results both semantically and structurally, toobtain the most relevant passages as context for answer generation with theLLM. Experimental results on benchmark datasets demonstrate that our approachoutperforms state-of-the-art methods in QA performance, and achieves a6$\times$ speedup in retrieval efficiency.</description><author>Changjian Wang, Weihong Deng, Weili Guan, Quan Lu, Ning Jiang</author><pubDate>Fri, 15 Aug 2025 06:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11247v1</guid></item><item><title>SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation with Casual Inference</title><link>http://arxiv.org/abs/2403.07088v7</link><description>Large language models(LLMs) have shown its outperforming ability on varioustasks and question answering. However, LLMs require substantial memory storageon low-resource devices. More critically, the computational speed on thesedevices is also severely limited. In this paper, we propose SPA(Side PluginAdaption), a lightweight architecture for fast on-devices inference on theconstraints of strict on-devices computation and memory constraints. Comparedwith other on-devices seq2seq generation, SPA could make a fast and stableinference on low-resource constraints, allowing it to obtain cost effiency. Ourmethod establish an interaction between a pretrained LLMs on-cloud and additiveparameters on-devices, which could provide the knowledge on both pretrainedLLMs and featured personal feature. Further more, SPA provides a framework tokeep feature-base parameters on low computational devices while leave theparameters containing general information on the high computational devices.</description><author>Yanming Liu, Xinyue Peng, Ningjing Sang, Yafeng Yan, Xiaolan Ke, Zhiting Zheng, Shaobo Liu, Songhang Deng, Jiannan Cao, Le Dai, Xingzu Liu, Ruilin Nong, Weihao Liu</author><pubDate>Fri, 15 Aug 2025 04:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07088v7</guid></item><item><title>PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging</title><link>http://arxiv.org/abs/2505.11872v3</link><description>Recent advancements in prompt-based medical image segmentation have enabledclinicians to identify tumors using simple input like bounding boxes or textprompts. However, existing methods face challenges when doctors need tointeract through natural language or when position reasoning is required -understanding spatial relationships between anatomical structures andpathologies. We present PRS-Med, a framework that integrates vision-languagemodels with segmentation capabilities to generate both accurate segmentationmasks and corresponding spatial reasoning outputs. Additionally, we introducethe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),which provides diverse, spatially-grounded question-answer pairs to address thelack of position reasoning data in medical imaging. PRS-Med demonstratessuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,endoscopy, RGB), significantly outperforming state-of-the-art methods in bothsegmentation accuracy and position reasoning. Our approach enables intuitivedoctor-system interaction through natural language, facilitating more efficientdiagnoses. Our dataset pipeline, model, and codebase will be released to fosterfurther research in spatially-aware multimodal reasoning for medicalapplications.</description><author>Quoc-Huy Trinh, Minh-Van Nguyen, Jung Zeng, Ulas Bagci, Debesh Jha</author><pubDate>Fri, 15 Aug 2025 02:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11872v3</guid></item><item><title>MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering</title><link>http://arxiv.org/abs/2508.11163v1</link><description>This paper presents MobQA, a benchmark dataset designed to evaluate thesemantic understanding capabilities of large language models (LLMs) for humanmobility data through natural language question answering. While existing models excel at predicting human movement patterns, it remainsunobvious how much they can interpret the underlying reasons or semanticmeaning of those patterns. MobQA provides a comprehensive evaluation frameworkfor LLMs to answer questions about diverse human GPS trajectories spanningdaily to weekly granularities. It comprises 5,800 high-quality question-answerpairs across three complementary question types: factual retrieval (precisedata extraction), multiple-choice reasoning (semantic inference), and free-formexplanation (interpretive description), which all require spatial, temporal,and semantic reasoning. Our evaluation of major LLMs reveals strong performanceon factual retrieval but significant limitations in semantic reasoning andexplanation question answering, with trajectory length substantially impactingmodel effectiveness. These findings demonstrate the achievements andlimitations of state-of-the-art LLMs for semantic mobilityunderstanding.\footnote{MobQA dataset is available athttps://github.com/CyberAgentAILab/mobqa.}</description><author>Hikaru Asano, Hiroki Ouchi, Akira Kasuga, Ryo Yonetani</author><pubDate>Fri, 15 Aug 2025 02:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11163v1</guid></item><item><title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title><link>http://arxiv.org/abs/2508.11133v1</link><description>Large language models (LLMs) are emerging as a go-to tool for queryinginformation. However, current LLM benchmarks rarely feature natural questionsthat are both information-seeking as well as genuinely time-consuming forhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 naturaland complex questions that require dozens, and at times hundreds, ofintermediate steps to solve -- far more than any existing QA benchmark. Tobuild MoNaCo, we developed a decomposed annotation pipeline to elicit andmanually answer natural time-consuming questions at scale. Frontier LLMsevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall andhallucinations. Our results underscore the need for reasoning models thatbetter handle the complexity and sheer breadth of real-worldinformation-seeking questions -- with MoNaCo providing an effective resourcefor tracking such progress. The MONACO benchmark, codebase, prompts and modelspredictions are publicly available at: https://tomerwolgithub.github.io/monaco</description><author>Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty</author><pubDate>Fri, 15 Aug 2025 00:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11133v1</guid></item><item><title>Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?</title><link>http://arxiv.org/abs/2508.11011v1</link><description>Construction safety inspections typically involve a human inspectoridentifying safety concerns on-site. With the rise of powerful Vision LanguageModels (VLMs), researchers are exploring their use for tasks such as detectingsafety rule violations from on-site images. However, there is a lack of opendatasets to comprehensively evaluate and further fine-tune VLMs in constructionsafety inspection. Current applications of VLMs use small, supervised datasets,limiting their applicability in tasks they are not directly trained for. Inthis paper, we propose the ConstructionSite 10k, featuring 10,000 constructionsite images with annotations for three inter-connected tasks, including imagecaptioning, safety rule violation visual question answering (VQA), andconstruction element visual grounding. Our subsequent evaluation of currentstate-of-the-art large pre-trained VLMs shows notable generalization abilitiesin zero-shot and few-shot settings, while additional training is needed to makethem applicable to actual construction sites. This dataset allows researchersto train and evaluate their own VLMs with new architectures and techniques,providing a valuable benchmark for construction safety inspection.</description><author>Xuezheng Chen, Zhengbo Zou</author><pubDate>Thu, 14 Aug 2025 18:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11011v1</guid></item><item><title>All for law and law for all: Adaptive RAG Pipeline for Legal Research</title><link>http://arxiv.org/abs/2508.13107v1</link><description>Retrieval-Augmented Generation (RAG) mitigates hallucinations by groundinglarge language model outputs in cited sources, a capability that is especiallycritical in the legal domain. We present an end-to-end RAG pipeline thatrevisits and extends the LegalBenchRAG baseline with three targetedenhancements: (i) a context-aware query translator that disentangles documentreferences from natural-language questions and adapts retrieval depth andresponse style based on expertise and specificity, (ii) open-source retrievalstrategies using SBERT and GTE embeddings that achieve substantial performancegains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for$K&gt;4$) while remaining cost-efficient, and (iii) a comprehensive evaluation andgeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall toassess semantic alignment and faithfulness across models and prompt designs.Our results show that carefully designed open-source pipelines can rival oroutperform proprietary approaches in retrieval quality, while a customlegal-grounded prompt consistently produces more faithful and contextuallyrelevant answers than baseline prompting. Taken together, these contributionsdemonstrate the potential of task-aware, component-level tuning to deliverlegally grounded, reproducible, and cost-effective RAG systems for legalresearch assistance.</description><author>Figarri Keisha, Prince Singh, Pallavi, Dion Fernandes, Aravindh Manivannan, Ilham Wicaksono, Faisal Ahmad</author><pubDate>Mon, 18 Aug 2025 17:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13107v1</guid></item><item><title>Checkmate: interpretable and explainable RSVQA is the endgame</title><link>http://arxiv.org/abs/2508.13086v1</link><description>Remote Sensing Visual Question Answering (RSVQA) presents unique challengesin ensuring that model decisions are both understandable and grounded in visualcontent. Current models often suffer from a lack of interpretability andexplainability, as well as from biases in dataset distributions that lead toshortcut learning. In this work, we tackle these issues by introducing a novelRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253questions and a balanced answer distribution. Each answer is linked to one ormore cells within the image, enabling fine-grained visual reasoning. Building on this dataset, we develop an explainable and interpretable modelcalled Checkmate that identifies the image cells most relevant to itsdecisions. Through extensive experiments across multiple model architectures,we show that our approach improves transparency and supports more trustworthydecision-making in RSVQA systems.</description><author>Lucrezia Tosato, Christel Tartini Chappuis, Syrielle Montariol, Flora Weissgerber, Sylvain Lobry, Devis Tuia</author><pubDate>Mon, 18 Aug 2025 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13086v1</guid></item><item><title>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review</title><link>http://arxiv.org/abs/2410.21673v3</link><description>Public Code Review (PCR) is developed in the Software Question Answering(SQA) community, assisting developers in exploring high-quality and efficientreview services. Current methods on PCR mainly focus on the reviewer'sperspective, including finding a capable reviewer, predicting comment quality,and recommending/generating review comments. However, it is not well studiedthat how to satisfy the review necessity requests posted by developers whichcan increase their visibility, which in turn acts as a prerequisite for betterreview responses. To this end, we propose K nowledge-guided P rompt learningfor P ublic Code Review (KP-PCR) to achieve developer-based code review requestquality assurance (i.e., predicting request necessity and recommending tagssubtask). Specifically, we reformulate the two subtasks via 1) text prompttuning which converts both of them into a Masked Language Model (MLM) byconstructing prompt templates using hard prompt; and 2) knowledge and codeprefix tuning which introduces knowledge guidance from fine-tuned largelanguage models by soft prompt, and uses program dependence graph tocharacterize code snippets. Finally, both of the request necessity predictionand tag recommendation subtasks output predicted results through an answerengineering module. In addition, we further analysis the time complexity of ourKP-PCR that has lightweight prefix based the operation of introducing knowledgeguidance. Experimental results on the PCR dataset for the period 2011-2023demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the requestnecessity prediction and by 1.4%-6.9% in the tag recommendation. The codeimplementation is released at https://github.com/WUT-IDEA/KP-PCR.</description><author>Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang</author><pubDate>Thu, 21 Aug 2025 17:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21673v3</guid></item><item><title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title><link>http://arxiv.org/abs/2508.15717v1</link><description>Multimodal large language models (MLLMs) have made significant progress invisual-language reasoning, but their ability to efficiently handle long videosremains limited. Despite recent advances in long-context MLLMs, storing andattending to the key-value (KV) cache for long visual contexts incurssubstantial memory and computational overhead. Existing visual compressionmethods require either encoding the entire visual context before compression orhaving access to the questions in advance, which is impractical for long videounderstanding and multi-turn conversational settings. In this work, we proposeStreamMem, a query-agnostic KV cache memory mechanism for streaming videounderstanding. Specifically, StreamMem encodes new video frames in a streamingmanner, compressing the KV cache using attention scores between visual tokensand generic query tokens, while maintaining a fixed-size KV memory to enableefficient question answering (QA) in memory-constrained, long-video scenarios.Evaluation on three long video understanding and two streaming video questionanswering benchmarks shows that StreamMem achieves state-of-the-art performancein query-agnostic KV cache compression and is competitive with query-awarecompression approaches.</description><author>Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren</author><pubDate>Thu, 21 Aug 2025 16:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15717v1</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title><link>http://arxiv.org/abs/2508.15690v1</link><description>GRAFT is a structured multimodal benchmark for evaluating models oninstruction-following, visual reasoning, and visual-textual alignment tasks. Itfeatures programmatically generated charts and synthetically rendered tables,created with Python visualization libraries to ensure control over datasemantics, structure, and clarity. Each GRAFT instance pairs a chart or tableimage with a systematically generated, multi-step analytical question basedsolely on visual content. Answers are provided in structured formats such asJSON or YAML, supporting consistent evaluation of both reasoning and outputformat. The benchmark introduces a taxonomy of reasoning types includingcomparison, trend identification, ranking, aggregation, proportion estimation,and anomaly detection to enable comprehensive assessment. Reference answersfollow strict factual and formatting guidelines for precise, aspect-basedevaluation. GRAFT offers a unified, scalable framework for fine-grainedbenchmarking of multimodal models on visually grounded, structured reasoningtasks, setting a new evaluation standard in this field.</description><author>Abhigya Verma, Sriram Puttagunta, Seganrasan Subramanian, Sravan Ramachandran</author><pubDate>Thu, 21 Aug 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15690v1</guid></item><item><title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title><link>http://arxiv.org/abs/2508.15641v1</link><description>Understanding videos requires more than answering open ended questions, itdemands the ability to pinpoint when events occur and how entities interactacross time. While recent Video LLMs have achieved remarkable progress inholistic reasoning, they remain coarse in temporal perception: timestamps areencoded only implicitly, frame level features are weak in capturing continuity,and language vision alignment often drifts from the entities of interest. Inthis paper, we present Grounded VideoDiT, a Video LLM designed to overcomethese limitations by introducing three key innovations. First, a DiffusionTemporal Latent (DTL) encoder enhances boundary sensitivity and maintainstemporal consistency. Second, object grounded representations explicitly bindquery entities to localized visual evidence, strengthening alignment. Third, amixed token scheme with discrete temporal tokens provides explicit timestampmodeling, enabling fine grained temporal reasoning. Together, these designsequip Grounded VideoDiT with robust grounding capabilities, as validated bystate of the art results on Charades STA, NExT GQA, and multiple VideoQAbenchmarks.</description><author>Pengcheng Fang, Yuxia Chen, Rui Guo</author><pubDate>Thu, 21 Aug 2025 15:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15641v1</guid></item><item><title>Using a cognitive architecture to consider antiBlackness in design and development of AI systems</title><link>http://arxiv.org/abs/2207.00644v3</link><description>How might we use cognitive modeling to consider the ways in whichantiblackness, and racism more broadly, impact the design and development of AIsystems? We provide a discussion and an example towards an answer to thisquestion. We use the ACT-R/{\Phi} cognitive architecture and an existingknowledge graph system, ConceptNet, to consider this question not only from acognitive and sociocultural perspective, but also from a physiologicalperspective. In addition to using a cognitive modeling as a means to explorehow antiblackness may manifest in the design and development of AI systems(particularly from a software engineering perspective), we also introduceconnections between antiblackness, the Human, and computational cognitivemodeling. We argue that the typical eschewing of sociocultural processes andknowledge structures in cognitive architectures and cognitive modelingimplicitly furthers a colorblind approach to cognitive modeling and hidessociocultural context that is always present in human behavior and affectscognitive processes.</description><author>Christopher L. Dancy</author><pubDate>Thu, 21 Aug 2025 12:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00644v3</guid></item><item><title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title><link>http://arxiv.org/abs/2508.14052v2</link><description>Accurate information retrieval (IR) is critical in the financial domain,where investors must identify relevant information from large collections ofdocuments. Traditional IR methods-whether sparse or dense-often fall short inretrieval accuracy, as it requires not only capturing semantic similarity butalso performing fine-grained reasoning over document structure anddomain-specific knowledge. Recent advances in large language models (LLMs) haveopened up new opportunities for retrieval with multi-step reasoning, where themodel ranks passages through iterative reasoning about which information ismost relevant to a given query. However, there exists no benchmark to evaluatesuch capabilities in the financial domain. To address this gap, we introduceFinAgentBench, the first large-scale benchmark for evaluating retrieval withmulti-step reasoning in finance -- a setting we term agentic retrieval. Thebenchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firmsand assesses whether LLM agents can (1) identify the most relevant documenttype among candidates, and (2) pinpoint the key passage within the selecteddocument. Our evaluation framework explicitly separates these two reasoningsteps to address context limitations. This design enables to provide aquantitative basis for understanding retrieval-centric LLM behavior in finance.We evaluate a suite of state-of-the-art models and further demonstrated howtargeted fine-tuning can significantly improve agentic retrieval performance.Our benchmark provides a foundation for studying retrieval-centric LLM behaviorin complex, domain-specific tasks for finance. We will release the datasetpublicly upon acceptance of the paper and plan to expand and share dataset forthe full S&amp;P 500 and beyond.</description><author>Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin Kim, Yongjae Lee</author><pubDate>Thu, 21 Aug 2025 09:53:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14052v2</guid></item><item><title>Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering</title><link>http://arxiv.org/abs/2503.14957v2</link><description>We introduce \dataset (Procedural Knowledge Reasoning Question Answering), anew benchmark for question answering over procedural tasks that requirestructured reasoning. PKR-QA is constructed semi-automatically using aprocedural knowledge graph (PKG), which encodes task-specific knowledge acrossdiverse domains. The PKG is built by curating and linking information from theCOIN instructional video dataset and the ontology, enriched with commonsenseknowledge from ConceptNet and structured outputs from Large Language Models(LLMs), followed by manual verification. To generate question-answer pairs, wedesign graph traversal templates where each template is applied systematicallyover PKG. To enable interpretable reasoning, we propose a neurosymbolicapproach called Knowledge Module Learning (KML), which learns proceduralrelations via neural modules and composes them for structured reasoning withLLMs. Experiments demonstrate that this paradigm improves reasoning performanceon our dataset and enables step-by-step reasoning traces that facilitateinterpretability. Our theoretical analysis on KML learning shows that ourtrained models satisfy near optimal conditions for learning KG relations asneural network mapping models. Code and dataset will be released soon.</description><author>Thanh-Son Nguyen, Hong Yang, Tzeh Yuan Neoh, Hao Zhang, Ee Yeo Keat, Basura Fernando</author><pubDate>Thu, 21 Aug 2025 09:37:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.14957v2</guid></item><item><title>Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering</title><link>http://arxiv.org/abs/2502.11491v2</link><description>Large language models (LLMs) have shown remarkable capabilities in naturallanguage processing. However, in knowledge graph question answering tasks(KGQA), there remains the issue of answering questions that require multi-hopreasoning. Existing methods rely on entity vector matching, but the purpose ofthe question is abstract and difficult to match with specific entities. As aresult, it is difficult to establish reasoning paths to the purpose, whichleads to information loss and redundancy. To address this issue, inspired byhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), anovel framework that constructs reasoning paths from purposes back toconditions. ORT operates in three key phases: (1) using LLM to extract purposelabels and condition labels, (2) constructing label reasoning paths based onthe KG ontology, and (3) using the label reasoning paths to guide knowledgeretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achievesstate-of-the-art performance and significantly enhances the capability of LLMsfor KGQA.</description><author>Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin</author><pubDate>Thu, 21 Aug 2025 07:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.11491v2</guid></item><item><title>TComQA: Extracting Temporal Commonsense from Text</title><link>http://arxiv.org/abs/2508.15274v1</link><description>Understanding events necessitates grasping their temporal context, which isoften not explicitly stated in natural language. For example, it is not atrivial task for a machine to infer that a museum tour may last for a fewhours, but can not take months. Recent studies indicate that even advancedlarge language models (LLMs) struggle in generating text that require reasoningwith temporal commonsense due to its infrequent explicit mention in text.Therefore, automatically mining temporal commonsense for events enables thecreation of robust language models. In this work, we investigate the capacityof LLMs to extract temporal commonsense from text and evaluate multipleexperimental setups to assess their effectiveness. Here, we propose a temporalcommonsense extraction pipeline that leverages LLMs to automatically minetemporal commonsense and use it to construct TComQA, a dataset derived fromSAMSum and RealNews corpora. TComQA has been validated through crowdsourcingand achieves over 80\% precision in extracting temporal commonsense. The modeltrained with TComQA also outperforms an LLM fine-tuned on existing dataset oftemporal question answering task.</description><author>Lekshmi R Nair, Arun Sankar, Koninika Pal</author><pubDate>Thu, 21 Aug 2025 06:07:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15274v1</guid></item><item><title>Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering</title><link>http://arxiv.org/abs/2508.15213v1</link><description>Large Language Models (LLMs) perform well in general QA but often struggle indomain-specific scenarios. Retrieval-Augmented Generation (RAG) introducesexternal knowledge but suffers from hallucinations and latency due to noisyretrievals. Continued pretraining internalizes domain knowledge but is costlyand lacks cross-domain flexibility. We attribute this challenge to thelong-tail distribution of domain knowledge, which leaves partial yet usefulinternal knowledge underutilized. We further argue that knowledge acquisitionshould be progressive, mirroring human learning: first understanding concepts,then applying them to complex reasoning. To address this, we propose Selct2Know(S2K), a cost-effective framework that internalizes domain knowledge through aninternal-external knowledge self-selection strategy and selective supervisedfine-tuning. We also introduce a structured reasoning data generation pipelineand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,and financial QA benchmarks show that S2K consistently outperforms existingmethods and matches domain-pretrained LLMs with significantly lower cost.</description><author>Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling</author><pubDate>Thu, 21 Aug 2025 03:53:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15213v1</guid></item><item><title>LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</title><link>http://arxiv.org/abs/2508.15192v1</link><description>While large language models (LLMs) have shown promise in healthcare, theirapplication for rare medical conditions is still hindered by scarce andunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causingexcessive sweating beyond physiological needs, is one such rare disorder,affecting 2-3% of the population and significantly impacting both physicalcomfort and psychosocial well-being. To date, no work has tailored LLMs toadvance the diagnosis or care of hyperhidrosis. To address this gap, we presentLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy andempathetic hyperhidrosis support. The system follows a three-stage pipeline. Inthe data augmentation stage, a frontier LLM generates medically plausiblesynthetic vignettes from curated open-source data to create a diverse andbalanced question-answer dataset. In the fine-tuning stage, an open-sourcefoundation model is fine-tuned on the dataset to provide diagnosis,personalized treatment recommendations, and empathetic psychological support.In the inference and expert evaluation stage, clinical and psychologicalspecialists assess accuracy, appropriateness, and empathy, with validatedresponses iteratively enriching the dataset. Experiments show that LLM4Sweatoutperforms baselines and delivers the first open-source LLM framework forhyperhidrosis, offering a generalizable approach for other rare diseases withsimilar data and trustworthiness challenges.</description><author>Wenjie Lin, Jin Wei-Kocsis</author><pubDate>Thu, 21 Aug 2025 03:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15192v1</guid></item><item><title>Identifying and Answering Questions with False Assumptions: An Interpretable Approach</title><link>http://arxiv.org/abs/2508.15139v1</link><description>People often ask questions with false assumptions, a type of question thatdoes not have regular answers. Answering such questions require firstidentifying the false assumptions. Large Language Models (LLMs) often generatemisleading answers because of hallucinations. In this paper, we focus onidentifying and answering questions with false assumptions in several domains.We first investigate to reduce the problem to fact verification. Then, wepresent an approach leveraging external evidence to mitigate hallucinations.Experiments with five LLMs demonstrate that (1) incorporating retrievedevidence is beneficial and (2) generating and validating atomic assumptionsyields more improvements and provides an interpretable answer by specifying thefalse assumptions.</description><author>Zijie Wang, Eduardo Blanco</author><pubDate>Thu, 21 Aug 2025 00:24:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15139v1</guid></item><item><title>LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text</title><link>http://arxiv.org/abs/2508.15085v1</link><description>LongRecall. The completeness of machine-generated text, ensuring that itcaptures all relevant information, is crucial in domains such as medicine andlaw and in tasks like list-based question answering (QA), where omissions canhave serious consequences. However, existing recall metrics often depend onlexical overlap, leading to errors with unsubstantiated entities andparaphrased answers, while LLM-as-a-Judge methods with long holistic promptscapture broader semantics but remain prone to misalignment and hallucinationswithout structured verification. We introduce LongRecall, a general three-stagerecall evaluation framework that decomposes answers into self-contained facts,successively narrows plausible candidate matches through lexical and semanticfiltering, and verifies their alignment through structured entailment checks.This design reduces false positives and false negatives while accommodatingdiverse phrasings and contextual variations, serving as a foundational buildingblock for systematic recall assessment. We evaluate LongRecall on threechallenging long-form QA benchmarks using both human annotations and LLM-basedjudges, demonstrating substantial improvements in recall accuracy over stronglexical and LLM-as-a-Judge baselines.</description><author>MohamamdJavad Ardestani, Ehsan Kamalloo, Davood Rafiei</author><pubDate>Wed, 20 Aug 2025 21:41:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15085v1</guid></item><item><title>Don't Think Twice! Over-Reasoning Impairs Confidence Calibration</title><link>http://arxiv.org/abs/2508.15050v1</link><description>Large Language Models deployed as question answering tools require robustcalibration to avoid overconfidence. We systematically evaluate how reasoningcapabilities and budget affect confidence assessment accuracy, using theClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetaryhealth. Our key finding challenges the "test-time scaling" paradigm: whilerecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,increasing reasoning budgets consistently impairs rather than improvescalibration. Extended reasoning leads to systematic overconfidence that worsenswith longer thinking budgets, producing diminishing and negative returns beyondmodest computational investments. Conversely, search-augmented generationdramatically outperforms pure reasoning, achieving 89.3% accuracy by retrievingrelevant evidence. Our results suggest that information access, rather thanreasoning depth or inference budget, may be the critical bottleneck forimproved confidence calibration of knowledge-intensive tasks.</description><author>Romain Lacombe, Kerrie Wu, Eddie Dilworth</author><pubDate>Wed, 20 Aug 2025 20:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15050v1</guid></item><item><title>PersonaBench: Evaluating AI Models on Understanding Personal Information through Accessing (Synthetic) Private User Data</title><link>http://arxiv.org/abs/2502.20616v2</link><description>Personalization is critical in AI assistants, particularly in the context ofprivate AI models that work with individual users. A key scenario in thisdomain involves enabling AI models to access and interpret a user's privatedata (e.g., conversation history, user-AI interactions, app usage) tounderstand personal details such as biographical information, preferences, andsocial connections. However, due to the sensitive nature of such data, thereare no publicly available datasets that allow us to assess an AI model'sability to understand users through direct access to personal information. To address this gap, we introduce a synthetic data generation pipeline thatcreates diverse, realistic user profiles and private documents simulating humanactivities. Leveraging this synthetic data, we present PersonaBench, abenchmark designed to evaluate AI models' performance in understanding personalinformation derived from simulated private user data. We evaluate Retrieval-Augmented Generation (RAG) pipelines using questionsdirectly related to a user's personal information, supported by the relevantprivate documents provided to the models. Our results reveal that currentretrieval-augmented AI models struggle to answer private questions byextracting personal information from user documents, highlighting the need forimproved methodologies to enhance personalization capabilities in AI.</description><author>Juntao Tan, Liangwei Yang, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Tulika Manoj Awalgaonkar, Jianguo Zhang, Weiran Yao, Ming Zhu, Shirley Kokane, Silvio Savarese, Huan Wang, Caiming Xiong, Shelby Heinecke</author><pubDate>Wed, 20 Aug 2025 18:44:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.20616v2</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v2</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</author><pubDate>Thu, 21 Aug 2025 18:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v2</guid></item><item><title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title><link>http://arxiv.org/abs/2409.08239v2</link><description>Synthetic data generation has recently emerged as a promising approach forenhancing the capabilities of large language models (LLMs) without the need forexpensive human annotations. However, existing methods often generate data thatcan be low quality or contrived. In this paper, we introduce Source2Synth, ascalable approach for synthetic data generation and curation that is groundedin real-world data sources. Source2Synth takes as input a custom data sourceand produces synthetic data examples with intermediate reasoning steps. Ourmethod improves the dataset quality by discarding low-quality generations basedon their answerability. We demonstrate the generality of this approach byapplying it to two tasks that leverage two different types of data: multi-hopquestion answering (MHQA), where we test complex reasoning abilities leveragingdocuments, and tabular question answering (TQA), where we test tool usageleveraging tables. Our method improves performance by 25.51% for TQA on WikiSQLand 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.</description><author>Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli</author><pubDate>Wed, 20 Aug 2025 16:27:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.08239v2</guid></item><item><title>G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model</title><link>http://arxiv.org/abs/2312.11370v2</link><description>Large language models (LLMs) have shown remarkable proficiency in human-levelreasoning and generation capabilities, which encourages extensive research ontheir application in mathematical problem solving. However, current work hasbeen largely focused on text-based mathematical problems, with limitedinvestigation in problems involving geometric information. Addressing this gap,we aim to enable LLMs to solve geometric problems by understanding image input.We first analyze the limitations of current Multimodal Large Language Models(MLLMs) in this area: they struggle to accurately comprehending basic geometricelements and their relationships. To overcome these challenges, we takeadvantage of the unique characteristics of geometric problems (such as uniquegeometric logical form, and geometric scalability) and the capacity of thetextual LLMs to build an enriched multimodal geometry dataset based on existingdata. The augmented dataset, Geo170K, contains more than 170K geometricimage-caption and question-answer pairs. Utilizing our constructed Geo170Kdataset, we develop G-LLaVA, which demonstrates exceptional performance insolving geometric problems, significantly outperforming GPT-4-V on theMathVista benchmark with only 7B parameters.</description><author>Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong</author><pubDate>Wed, 20 Aug 2025 15:45:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.11370v2</guid></item><item><title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title><link>http://arxiv.org/abs/2504.19254v3</link><description>Hallucinations are a persistent problem with Large Language Models (LLMs). Asthese models become increasingly used in high-stakes domains, such ashealthcare and finance, the need for effective hallucination detection iscrucial. To this end, we outline a versatile framework for zero-resourcehallucination detection that practitioners can apply to real-world use cases.To achieve this, we adapt a variety of existing uncertainty quantification (UQ)techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,transforming them as necessary into standardized response-level confidencescores ranging from 0 to 1. To enhance flexibility, we propose a tunableensemble approach that incorporates any combination of the individualconfidence scores. This approach enables practitioners to optimize the ensemblefor a specific use case for improved performance. To streamline implementation,the full suite of scorers is offered in this paper's companion Python toolkit,UQLM. To evaluate the performance of the various scorers, we conduct anextensive set of experiments using several LLM question-answering benchmarks.We find that our tunable ensemble typically surpasses its individual componentsand outperforms existing hallucination detection methods. Our resultsdemonstrate the benefits of customized hallucination detection strategies forimproving the accuracy and reliability of LLMs.</description><author>Dylan Bouchard, Mohit Singh Chauhan</author><pubDate>Wed, 20 Aug 2025 14:26:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19254v3</guid></item><item><title>Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems</title><link>http://arxiv.org/abs/2508.14553v1</link><description>Over time, software systems have reached a level of complexity that makes itdifficult for their developers and users to explain particular decisions madeby them. In this paper, we focus on the explainability of component-basedsystems for Question Answering (QA). These components often conduct processesdriven by AI methods, in which behavior and decisions cannot be clearlyexplained or justified, s.t., even for QA experts interpreting the executedprocess and its results is hard. To address this challenge, we present anapproach that considers the components' input and output data flows as a sourcefor representing the behavior and provide explanations for the components,enabling users to comprehend what happened. In the QA framework used here, thedata flows of the components are represented as SPARQL queries (inputs) and RDFtriples (outputs). Hence, we are also providing valuable insights onverbalization regarding these data types. In our experiments, the approachgenerates explanations while following template-based settings (baseline) orvia the use of Large Language Models (LLMs) with different configurations(automatic generation). Our evaluation shows that the explanations generatedvia LLMs achieve high quality and mostly outperform template-based approachesaccording to the users' ratings. Therefore, it enables us to automaticallyexplain the behavior and decisions of QA components to humans while using RDFand SPARQL as a context for explanations.</description><author>Dennis Schiese, Aleksandr Perevalov, Andreas Both</author><pubDate>Wed, 20 Aug 2025 09:14:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14553v1</guid></item><item><title>Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2508.14427v1</link><description>This paper addresses the problems of missing reasoning chains andinsufficient entity-level semantic understanding in large language models whendealing with tasks that require structured knowledge. It proposes a fine-tuningalgorithm framework based on knowledge graph injection. The method builds onpretrained language models and introduces structured graph information forauxiliary learning. A graph neural network is used to encode entities and theirrelations, constructing a graph-based semantic representation. A fusionmechanism is then designed to jointly model the knowledge graph embeddings withthe contextual representations from the language model. To enhance therobustness of knowledge integration, a gating mechanism is introduced todynamically balance the contributions of linguistic semantics and structuralknowledge. This effectively mitigates conflicts between differentrepresentational spaces. During training, a joint loss function is constructedto account for both task performance and structural alignment objectives. Thishelps improve the accuracy of entity prediction and semantic reasoning. Thestudy also includes a series of systematic sensitivity experiments. Itevaluates the effects of learning rate, graph coverage, and structuralperturbations on model performance. The results further validate theeffectiveness and stability of the proposed method across tasks such as entityrecognition, question answering, and language generation. Experimental findingsshow that the proposed structure-aware fine-tuning framework significantlyenhances the model's ability to represent complex semantic units. Itdemonstrates better semantic consistency and contextual logic modeling inscenarios involving structural reasoning and entity extraction.</description><author>Wuyang Zhang, Yexin Tian, Xiandong Meng, Mengjie Wang, Junliang Du</author><pubDate>Wed, 20 Aug 2025 04:52:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14427v1</guid></item><item><title>A Little Human Data Goes A Long Way</title><link>http://arxiv.org/abs/2410.13098v3</link><description>Faced with an expensive human annotation process, creators of NLP systemsincreasingly turn to synthetic data generation. While this method showspromise, the extent to which synthetic data can replace human annotation ispoorly understood. We investigate the use of synthetic data in FactVerification (FV) and Question Answering (QA) by studying the effects ofincrementally replacing human generated data with synthetic points on eightdiverse datasets. Strikingly, replacing up to 90% of the training data onlymarginally decreases performance, but replacing the final 10% leads to severedeclines. We find that models trained on purely synthetic data can be reliablyimproved by including as few as 125 human generated data points. We show thatmatching the performance gain of just a little additional human data (only 200points) requires an order of magnitude more synthetic data and estimate priceratios at which human annotation would be a more cost-effective solution. Ourresults suggest that even when human annotation at scale is infeasible, thereis great value to having a small proportion of the dataset being humangenerated.</description><author>Dhananjay Ashok, Jonathan May</author><pubDate>Wed, 20 Aug 2025 01:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.13098v3</guid></item><item><title>PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</title><link>http://arxiv.org/abs/2508.16439v1</link><description>Large language models (LLMs) and vision-augmented LLMs (VLMs) havesignificantly advanced medical informatics, diagnostics, and decision support.However, these models exhibit systematic biases, particularly age bias,compromising their reliability and equity. This is evident in their poorerperformance on pediatric-focused text and visual question-answering tasks. Thisbias reflects a broader imbalance in medical research, where pediatric studiesreceive less funding and representation despite the significant disease burdenin children. To address these issues, a new comprehensive multi-modal pediatricquestion-answering benchmark, PediatricsMQA, has been introduced. It consistsof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatrictopics across seven developmental stages (prenatal to adolescent) and 2,067vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256anatomical regions. The dataset was developed using a hybrid manual-automaticpipeline, incorporating peer-reviewed pediatric literature, validated questionbanks, existing benchmarks, and existing QA resources. Evaluatingstate-of-the-art open models, we find dramatic performance drops in youngercohorts, highlighting the need for age-aware methods to ensure equitable AIsupport in pediatric care.</description><author>Adil Bahaj, Mounir Ghogho</author><pubDate>Fri, 22 Aug 2025 14:50:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16439v1</guid></item><item><title>Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish</title><link>http://arxiv.org/abs/2508.16431v1</link><description>We introduce Cetvel, a comprehensive benchmark designed to evaluate largelanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lackeither task diversity or culturally relevant content, or both. Cetvel addressesthese gaps by combining a broad range of both discriminative and generativetasks ensuring content that reflects the linguistic and cultural richness ofTurkish language. Cetvel covers 23 tasks grouped into seven categories,including tasks such as grammatical error correction, machine translation, andquestion answering rooted in Turkish history and idiomatic language. Weevaluate 33 open-weight LLMs (up to 70B parameters) covering different modelfamilies and instruction paradigms. Our experiments reveal that Turkish-centricinstruction-tuned models generally underperform relative to multilingual orgeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored forthe language. Moreover, we show that tasks such as grammatical error correctionand extractive question answering are particularly discriminative indifferentiating model capabilities. Cetvel offers a comprehensive andculturally grounded evaluation suite for advancing the development andassessment of LLMs in Turkish.</description><author>Yakup Abrek Er, Ilker Kesen, Gözde Gül Şahin, Aykut Erdem</author><pubDate>Fri, 22 Aug 2025 14:42:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16431v1</guid></item><item><title>RoMedQA: The First Benchmark for Romanian Medical Question Answering</title><link>http://arxiv.org/abs/2508.16390v1</link><description>Question answering (QA) is an actively studied topic, being a core naturallanguage processing (NLP) task that needs to be addressed before achievingArtificial General Intelligence (AGI). However, the lack of QA datasets inspecific domains and languages hinders the development of robust AI models ableto generalize across various domains and languages. To this end, we introduceRoMedQA, the first Romanian QA benchmark for the medical domain, alongside acomprehensive evaluation of state-of-the-art large language models (LLMs). Weconstruct a high-quality and large-scale dataset comprising 102,646 QA pairsrelated to cancer patients. The questions regard medical case summaries of1,011 patients, requiring either keyword extraction or reasoning to be answeredcorrectly. RoMedQA is the result of a time-consuming manual annotation processcarried out by seven physicians specialized in oncology or radiotherapy, whospent a total of about 2,100 work hours to generate the QA pairs. We experimentwith four LLMs from distinct families of models on RoMedQA. Each model isemployed in two scenarios, namely one based on zero-shot prompting and onebased on supervised fine-tuning. Our results show that fine-tuned modelssignificantly outperform their zero-shot counterparts, clearly indicating thatpretrained models fail to generalize on RoMedQA. Our findings demonstrate theimportance of both domain-specific and language-specific fine-tuning forreliable clinical QA in Romanian. We publicly release our dataset and code athttps://github.com/ana-rogoz/RoMedQA.</description><author>Ana-Cristina Rogoz, Radu Tudor Ionescu, Alexandra-Valentina Anghel, Ionut-Lucian Antone-Iordache, Simona Coniac, Andreea Iuliana Ionescu</author><pubDate>Fri, 22 Aug 2025 13:48:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16390v1</guid></item><item><title>MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains</title><link>http://arxiv.org/abs/2508.18260v1</link><description>Large reasoning models (LRMs) have shown significant progress in test-timescaling through chain-of-thought prompting. Current approaches like search-o1integrate retrieval augmented generation (RAG) into multi-step reasoningprocesses but rely on a single, linear reasoning chain while incorporatingunstructured textual information in a flat, context-agnostic manner. As aresult, these approaches can lead to error accumulation throughout thereasoning chain, which significantly limits its effectiveness in medicalquestion-answering (QA) tasks where both accuracy and traceability are criticalrequirements. To address these challenges, we propose MIRAGE (Multi-chainInference with Retrieval-Augmented Graph Exploration), a novel test-timescalable reasoning framework that performs dynamic multi-chain inference overstructured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complexqueries into entity-grounded sub-questions, 2) executes parallel inferencechains, 3) retrieves evidence adaptively via neighbor expansion and multi-hoptraversal, and 4) integrates answers using cross-chain verification to resolvecontradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,Tree-of-Thought variants, and other retrieval-augmented baselines in bothautomatic and human evaluations. Additionally, MIRAGE improves interpretabilityby generating explicit reasoning chains that trace each factual claim toconcrete chains within the knowledge graph, making it well-suited for complexmedical reasoning scenarios. The code will be available for further research.</description><author>Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong</author><pubDate>Mon, 25 Aug 2025 17:53:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18260v1</guid></item><item><title>Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering</title><link>http://arxiv.org/abs/2508.18093v1</link><description>We present a case study evaluating large language models (LLMs) with128K-token context windows on a technical question answering (QA) task. Ourbenchmark is built on a user manual for an agricultural machine, available inEnglish, French, and German. It simulates a cross-lingual information retrievalscenario where questions are posed in English against all three languageversions of the manual. The evaluation focuses on realistic"needle-in-a-haystack" challenges and includes unanswerable questions to testfor hallucinations. We compare nine long-context LLMs using direct promptingagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for thisspecific manual show that Hybrid RAG consistently outperforms directlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.57B achieve high accuracy (over 85%) across all languages with RAG. This papercontributes a detailed analysis of LLM performance in a specialized industrialdomain and an open framework for similar evaluations, highlighting practicaltrade-offs and challenges.</description><author>Julius Gun, Timo Oksanen</author><pubDate>Mon, 25 Aug 2025 14:54:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18093v1</guid></item><item><title>See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops</title><link>http://arxiv.org/abs/2508.17932v1</link><description>Human video comprehension demonstrates dynamic coordination between reasoningand visual attention, adaptively focusing on query-relevant details. However,current long-form video question answering systems employ rigid pipelines thatdecouple reasoning from perception, leading to either information loss throughpremature visual abstraction or computational inefficiency through exhaustiveprocessing. The core limitation lies in the inability to adapt visualextraction to specific reasoning requirements, different queries demandfundamentally different visual evidence from the same video content. In thiswork, we present CAVIA, a training-free framework that revolutionizes videounderstanding through reasoning, perception coordination. Unlike conventionalapproaches where visual processing operates independently of reasoning, CAVIAcreates a closed-loop system where reasoning continuously guides visualextraction based on identified information gaps. CAVIA introduces threeinnovations: (1) hierarchical reasoning, guided localization to precise frames;(2) cross-modal semantic bridging for targeted extraction; (3)confidence-driven iterative synthesis. CAVIA achieves state-of-the-artperformance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamicreasoning-perception coordination provides a scalable paradigm for videounderstanding.</description><author>Zixuan Dong, Baoyun Peng, Yufei Wang, Lin Liu, Xinxin Dong, Yunlong Cao, Xiaodong Wang</author><pubDate>Mon, 25 Aug 2025 12:00:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17932v1</guid></item><item><title>The Ramon Llull's Thinking Machine for Automated Ideation</title><link>http://arxiv.org/abs/2508.19200v1</link><description>This paper revisits Ramon Llull's Ars combinatoria - a medieval framework forgenerating knowledge through symbolic recombination - as a conceptualfoundation for building a modern Llull's thinking machine for researchideation. Our approach defines three compositional axes: Theme (e.g.,efficiency, adaptivity), Domain (e.g., question answering, machinetranslation), and Method (e.g., adversarial training, linear attention). Theseelements represent high-level abstractions common in scientific work -motivations, problem settings, and technical approaches - and serve as buildingblocks for LLM-driven exploration. We mine elements from human experts orconference papers and show that prompting LLMs with curated combinationsproduces research ideas that are diverse, relevant, and grounded in currentliterature. This modern thinking machine offers a lightweight, interpretabletool for augmenting scientific creativity and suggests a path towardcollaborative ideation between humans and AI.</description><author>Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu</author><pubDate>Tue, 26 Aug 2025 17:03:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19200v1</guid></item><item><title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2505.24073v2</link><description>Large Vision-Language Models (LVLMs) have made remarkable strides inmultimodal tasks such as visual question answering, visual grounding, andcomplex reasoning. However, they remain limited by static training data,susceptibility to hallucinations, and inability to verify claims againstup-to-date, external evidence, compromising their performance in dynamicreal-world applications. Retrieval-Augmented Generation (RAG) offers apractical solution to mitigate these challenges by allowing the LVLMs to accesslarge-scale knowledge databases via retrieval mechanisms, thereby groundingmodel outputs in factual, contextually relevant information. Here in thispaper, we conduct the first systematic dissection of the multimodal RAGpipeline for LVLMs, explicitly investigating (1) the retrieval phase: on themodality configurations and retrieval strategies, (2) the re-ranking stage: onstrategies to mitigate positional biases and improve the relevance of retrievedevidence, and (3) the generation phase: we further investigate how to bestintegrate retrieved candidates into the final generation process. Finally, weextend to explore a unified agentic framework that integrates re-ranking andgeneration through self-reflection, enabling LVLMs to select relevant evidenceand suppress irrelevant context dynamically. Our full-stack exploration of RAGfor LVLMs yields substantial insights, resulting in an average performanceboost of 5% without any fine-tuning.</description><author>Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Suofei Feng, Ryan Rossi, Zhengzhong Tu</author><pubDate>Tue, 26 Aug 2025 16:42:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.24073v2</guid></item><item><title>Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis</title><link>http://arxiv.org/abs/2406.12719v4</link><description>Large Language Models (LLMs), already shown to ace various unstructured textcomprehension tasks, have also remarkably been shown to tackle table(structured) comprehension tasks without specific training. Building on earlierstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),model scale, instruction tuning, and domain bias affect Tabular QA (TQA)robustness by testing LLMs, under diverse augmentations and perturbations, ondiverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$,and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newerLLMs deliver stronger, more robust TQA performance, data contamination andreliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through anin-depth attention analysis, we reveal a strong correlation betweenperturbation-induced shifts in attention dispersion and the drops inperformance, with sensitivity peaking in the model's middle layers. Wehighlight the need for improved interpretable methodologies to develop morereliable LLMs for table comprehension. Through an in-depth attention analysis,we reveal a strong correlation between perturbation-induced shifts in attentiondispersion and performance drops, with sensitivity peaking in the model'smiddle layers. Based on these findings, we argue for the development ofstructure-aware self-attention mechanisms and domain-adaptive processingtechniques to improve the transparency, generalization, and real-worldreliability of LLMs on tabular data.</description><author>Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao</author><pubDate>Tue, 26 Aug 2025 15:27:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.12719v4</guid></item><item><title>Trustworthy Agents for Electronic Health Records through Confidence Estimation</title><link>http://arxiv.org/abs/2508.19096v1</link><description>Large language models (LLMs) show promise for extracting information fromElectronic Health Records (EHR) and supporting clinical decisions. However,deployment in clinical settings faces challenges due to hallucination risks. Wepropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metricquantifying the accuracy-reliability trade-off at varying confidencethresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporatingstepwise confidence estimation for clinical question answering. Experiments onMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines understrict reliability constraints, achieving improvements of 44.23%p and 25.34%pat HCAcc@70% while baseline methods fail at these thresholds. These resultshighlight limitations of traditional accuracy metrics in evaluating healthcareAI agents. Our work contributes to developing trustworthy clinical agents thatdeliver accurate information or transparently express uncertainty whenconfidence is low.</description><author>Yongwoo Song, Minbyul Jeong, Mujeen Sung</author><pubDate>Tue, 26 Aug 2025 14:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19096v1</guid></item><item><title>Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration</title><link>http://arxiv.org/abs/2508.03337v4</link><description>The practical application of Multimodal Large Language Models (MLLMs) toVideo Question Answering (Video-QA) is severely hindered by the high token costof processing numerous video frames. While increasing the number of sampledframes is a common strategy, we observe a "less is more" phenomenon whereexcessive frames can paradoxically degrade performance due to context dilution.Concurrently, state-of-the-art keyframe selection methods, while effective,still yield significant temporal redundancy, which we term 'visual echoes'. Toaddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novelpost-processing method that intelligently prunes the selected keyframes. AFPemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 andCLIP feature space to identify and merge these echoes into singlerepresentatives. To compensate for information loss, we then introduce alightweight, text-based semantic graph that provides critical context withminimal token overhead. Conducting extensive experiments on the LongVideoBenchand VideoMME benchmarks across multiple leading MLLMs, our full approachdemonstrates a drastic reduction in required frames by up to 86.9% and totalinput tokens by up to 83.2%. Crucially, by providing a concise, high-qualityset of frames, our method not only enhances efficiency but often improvesaccuracy over baselines that use more frames. The code will be released uponpublication.</description><author>Shaoguang Wang, Ziyang Chen, Yijie Xu, Weiyu Guo, Hui Xiong</author><pubDate>Tue, 26 Aug 2025 14:41:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03337v4</guid></item><item><title>MovieCORE: COgnitive REasoning in Movies</title><link>http://arxiv.org/abs/2508.19026v1</link><description>This paper introduces MovieCORE, a novel video question answering (VQA)dataset designed to probe deeper cognitive understanding of movie content.Unlike existing datasets that focus on surface-level comprehension, MovieCOREemphasizes questions that engage System-2 thinking while remaining specific tothe video material. We present an innovative agentic brainstorming approach,utilizing multiple large language models (LLMs) as thought agents to generateand refine high-quality question-answer pairs. To evaluate dataset quality, wedevelop a set of cognitive tests assessing depth, thought-provocationpotential, and syntactic complexity. We also propose a comprehensive evaluationscheme for assessing VQA model performance on deeper cognitive tasks. Toaddress the limitations of existing video-language models (VLMs), we introducean agentic enhancement module, Agentic Choice Enhancement (ACE), which improvesmodel reasoning capabilities post-training by up to 25%. Our work contributesto advancing movie understanding in AI systems and provides valuable insightsinto the capabilities and limitations of current VQA models when faced withmore challenging, nuanced questions about cinematic content. Our project page,dataset and code can be found athttps://joslefaure.github.io/assets/html/moviecore.html.</description><author>Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu</author><pubDate>Tue, 26 Aug 2025 13:43:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19026v1</guid></item><item><title>Enhancing Document VQA Models via Retrieval-Augmented Generation</title><link>http://arxiv.org/abs/2508.18984v1</link><description>Document Visual Question Answering (Document VQA) must cope with documentsthat span dozens of pages, yet leading systems still concatenate every page orrely on very large vision-language models, both of which are memory-hungry.Retrieval-Augmented Generation (RAG) offers an attractive alternative, firstretrieving a concise set of relevant segments before generating answers fromthis selected evidence. In this paper, we systematically evaluate the impact ofincorporating RAG into Document VQA through different retrieval variants -text-based retrieval using OCR tokens and purely visual retrieval without OCR -across multiple models and benchmarks. Evaluated on the multi-page datasetsMP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the"concatenate-all-pages" baseline by up to +22.5 ANLS, while the visual variantachieves +5.0 ANLS improvement without requiring any text extraction. Anablation confirms that retrieval and reranking components drive most of thegain, whereas the layout-guided chunking strategy - proposed in several recentworks to leverage page structure - fails to help on these datasets. Ourexperiments demonstrate that careful evidence selection consistently boostsaccuracy across multiple model sizes and multi-page benchmarks, underscoringits practical value for real-world Document VQA.</description><author>Eric López, Artemis Llabrés, Ernest Valveny</author><pubDate>Tue, 26 Aug 2025 12:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18984v1</guid></item><item><title>PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations</title><link>http://arxiv.org/abs/2508.18982v1</link><description>Time series forecasting has seen considerable improvement during the lastyears, with transformer models and large language models driving advancementsof the state of the art. Modern forecasting models are generally opaque and donot provide explanations for their forecasts, while well-known post-hocexplainability methods like LIME are not suitable for the forecasting context.We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time seriesforecasting models and their forecasts. Our method is based on localized inputperturbations and results in multi-granular explanations. Further, it is ableto characterize cross-channel correlations for multivariate time seriesforecasts. We clearly outline the algorithmic procedure behind PAX-TS,demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,compare it with two other state-of-the-art explanation algorithms, and presentthe different explanation types of the method. We found that the explanationsof high-performing and low-performing algorithms differ on the same datasets,highlighting that the explanations of PAX-TS effectively capture a model'sbehavior. Based on time step correlation matrices resulting from the benchmark,we identify 6 classes of patterns that repeatedly occur across differentdatasets and algorithms. We found that the patterns are indicators ofperformance, with noticeable differences in forecasting error between theclasses. Lastly, we outline a multivariate example where PAX-TS demonstrateshow the forecasting model takes cross-channel correlations into account. WithPAX-TS, time series forecasting models' mechanisms can be illustrated indifferent levels of detail, and its explanations can be used to answerpractical questions on forecasts.</description><author>Tim Kreuzer, Jelena Zdravkovic, Panagiotis Papapetrou</author><pubDate>Tue, 26 Aug 2025 12:31:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18982v1</guid></item><item><title>Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs</title><link>http://arxiv.org/abs/2508.17334v2</link><description>We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)on cricket scorecards, designed to evaluate large vision-language models(LVLMs) on complex numerical and cross-lingual reasoning over semi-structuredtabular images. MMCRICBENCH-3K comprises 1,463 synthetically generatedscorecard images from ODI, T20, and Test formats, accompanied by 1,500 EnglishQA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring Englishscorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindiscorecards, with all questions and answers kept in English to enable controlledcross-script evaluation. The task demands reasoning over structured numericaldata, multi-image context, and implicit domain knowledge. Empirical resultsshow that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggleon the English subset despite it being their primary training language andexhibit a further drop in performance on the Hindi subset. This reveals keylimitations in structure-aware visual text understanding, numerical reasoning,and cross-lingual generalization. The dataset is publicly available via HuggingFace at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLMresearch in this direction.</description><author>Somraj Gautam, Abhirama Subramanyam Penamakuri, Abhishek Bhandari, Gaurav Harit</author><pubDate>Tue, 26 Aug 2025 12:16:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.17334v2</guid></item><item><title>From Confidence to Collapse in LLM Factual Robustness</title><link>http://arxiv.org/abs/2508.16267v2</link><description>Ensuring the robustness of factual knowledge in LLMs is critical for reliableapplications in tasks such as question answering and reasoning. However,existing evaluation methods predominantly focus on performance-based metrics,often investigating from the perspective of prompt perturbations, whichcaptures only the externally triggered side of knowledge robustness. To bridgethis gap, we introduce a principled approach to measure factual robustness fromthe perspective of the generation process by analyzing token distributionentropy in combination with temperature scaling sensitivity. These two factorsbuild the Factual Robustness Score (FRS), a novel metric which quantifies thestability of a fact against perturbations in decoding conditions, given itsinitial uncertainty. To validate our approach, we conduct extensive experimentson 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). Weshow that factual robustness varies significantly -- smaller models report anFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ underincreased uncertainty. These insights demonstrate how entropy and temperaturescaling impact factual accuracy, and lay a foundation for developing morerobust knowledge retention and retrieval in future models.</description><author>Alina Fastowski, Bardh Prenkaj, Gjergji Kasneci</author><pubDate>Tue, 26 Aug 2025 11:54:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16267v2</guid></item><item><title>ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</title><link>http://arxiv.org/abs/2508.18190v2</link><description>Semi-structured tables, widely used in real-world applications (e.g.,financial reports, medical records, transactional orders), often involveflexible and complex layouts (e.g., hierarchical headers and merged cells).These tables generally rely on human analysts to interpret table layouts andanswer relevant natural language questions, which is costly and inefficient. Toautomate the procedure, existing methods face significant challenges. First,methods like NL2SQL require converting semi-structured tables into structuredones, which often causes substantial information loss. Second, methods likeNL2Code and multi-modal LLM QA struggle to understand the complex layouts ofsemi-structured tables and cannot accurately answer corresponding questions. Tothis end, we propose ST-Raptor, a tree-based framework for semi-structuredtable question answering using large language models. First, we introduce theHierarchical Orthogonal Tree (HO-Tree), a structural model that capturescomplex semi-structured table layouts, along with an effective algorithm forconstructing the tree. Second, we define a set of basic tree operations toguide LLMs in executing common QA tasks. Given a user question, ST-Raptordecomposes it into simpler sub-questions, generates corresponding treeoperation pipelines, and conducts operation-table alignment for accuratepipeline execution. Third, we incorporate a two-stage verification mechanism:forward validation checks the correctness of execution steps, while backwardvalidation evaluates answer reliability by reconstructing queries frompredicted answers. To benchmark the performance, we present SSTQA, a dataset of764 questions over 102 real-world semi-structured tables. Experiments show thatST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The codeis available at https://github.com/weAIDB/ST-Raptor.</description><author>Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu</author><pubDate>Tue, 26 Aug 2025 08:10:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18190v2</guid></item><item><title>Thyme: Think Beyond Images</title><link>http://arxiv.org/abs/2508.11630v1</link><description>Following OpenAI's introduction of the ``thinking with images'' concept,recent efforts have explored stimulating the use of visual information in thereasoning process to enhance model performance in perception and reasoningtasks. However, to the best of our knowledge, no open-source work currentlyoffers a feature set as rich as proprietary models (O3), which can performdiverse image manipulations and simultaneously enhance logical reasoningcapabilities through code. In this paper, we make a preliminary attempt in thisdirection by introducing Thyme (Think Beyond Images), a novel paradigm forenabling MLLMs to transcend existing ``think with images'' approaches byautonomously generating and executing diverse image processing andcomputational operations via executable code. This approach not onlyfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,rotation, contrast enhancement) but also allows for mathematical computations,all while maintaining high autonomy in deciding when and how to apply theseoperations. We activate this capability through a two-stage training strategy:an initial SFT on a curated dataset of 500K samples to teach code generation,followed by a RL phase to refine decision-making. For the RL stage, we manuallycollect and design high-resolution question-answer pairs to increase thelearning difficulty, and we propose GRPO-ATS (Group Relative PolicyOptimization with Adaptive Temperature Sampling), an algorithm that appliesdistinct temperatures to text and code generation to balance reasoningexploration with code execution precision. We conduct extensive experimentalanalysis and ablation studies. Comprehensive evaluations on nearly 20benchmarks show that Thyme yields significant and consistent performance gains,particularly in challenging high-resolution perception and complex reasoningtasks.</description><author>Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, Guorui Zhou</author><pubDate>Fri, 15 Aug 2025 17:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11630v1</guid></item><item><title>Beyond the Textual: Generating Coherent Visual Options for MCQs</title><link>http://arxiv.org/abs/2508.18772v1</link><description>Multiple-choice questions (MCQs) play a crucial role in fostering deepthinking and knowledge integration in education. However, previous research hasprimarily focused on generating MCQs with textual options, but it largelyoverlooks the visual options. Moreover, generating high-quality distractorsremains a major challenge due to the high cost and limited scalability ofmanual authoring. To tackle these problems, we propose a Cross-modal OptionsSynthesis (CmOS), a novel framework for generating educational MCQs with visualoptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoningprocess and Retrieval-Augmented Generation (RAG) to produce semanticallyplausible and visually similar answer and distractors. It also includes adiscrimination module to identify content suitable for visual options.Experimental results on test tasks demonstrate the superiority of CmOS incontent discrimination, question generation and visual option generation overexisting methods across various subjects and educational levels.</description><author>Wanqiang Wang, Longzhu He, Wei Zheng</author><pubDate>Tue, 26 Aug 2025 07:55:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18772v1</guid></item><item><title>Is ChatGPT-5 Ready for Mammogram VQA?</title><link>http://arxiv.org/abs/2508.11628v1</link><description>Mammogram visual question answering (VQA) integrates image interpretationwith clinical reasoning and has potential to support breast cancer screening.We systematically evaluated the GPT-5 family and GPT-4o model on four publicmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,abnormality detection, and malignancy classification tasks. GPT-5 consistentlywas the best performing model but lagged behind both human experts anddomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scoresamong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),calcification (63.5%), and malignancy (52.8%) classification. On InBreast, itattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detectionand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADSaccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Comparedwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) andspecificity (52.3%). While GPT-5 exhibits promising capabilities for screeningtasks, its performance remains insufficient for high-stakes clinical imagingapplications without targeted domain adaptation and optimization. However, thetremendous improvements in performance from GPT-4o to GPT-5 show a promisingtrend in the potential for general large language models (LLMs) to assist withmammography VQA tasks.</description><author>Qiang Li, Shansong Wang, Mingzhe Hu, Mojtaba Safari, Zachary Eidex, Xiaofeng Yang</author><pubDate>Fri, 15 Aug 2025 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11628v1</guid></item><item><title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title><link>http://arxiv.org/abs/2503.01307v2</link><description>Test-time inference has emerged as a powerful paradigm for enabling languagemodels to ``think'' longer and more carefully about complex challenges, muchlike skilled human experts. While reinforcement learning (RL) can driveself-improvement in language models on verifiable tasks, some models exhibitsubstantial gains while others quickly plateau. For instance, we find thatQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the gameof Countdown. This discrepancy raises a critical question: what intrinsicproperties enable effective self-improvement? We introduce a framework toinvestigate this question by analyzing four key cognitive behaviors --verification, backtracking, subgoal setting, and backward chaining -- that bothexpert human problem solvers and successful language models employ. Our studyreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llamainitially lacks them. In systematic experimentation with controlled behavioraldatasets, we find that priming Llama with examples containing these reasoningbehaviors enables substantial improvements during RL, matching or exceedingQwen's performance. Importantly, the presence of reasoning behaviors, ratherthan correctness of answers, proves to be the critical factor -- models primedwith incorrect solutions containing proper reasoning patterns achievecomparable performance to those trained on correct solutions. Finally,leveraging continued pretraining with OpenWebMath data, filtered to amplifyreasoning behaviors, enables the Llama model to match Qwen's self-improvementtrajectory. Our findings establish a fundamental relationship between initialreasoning behaviors and the capacity for improvement, explaining why somelanguage models effectively utilize additional computation while othersplateau.</description><author>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</author><pubDate>Fri, 15 Aug 2025 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.01307v2</guid></item><item><title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title><link>http://arxiv.org/abs/2508.09736v2</link><description>We introduce M3-Agent, a novel multimodal agent framework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual andauditory inputs to build and update its long-term memory. Beyond episodicmemory, it also develops semantic memory, enabling it to accumulate worldknowledge over time. Its memory is organized in an entity-centric, multimodalformat, allowing deeper and more consistent understanding of the environment.Given an instruction, M3-Agent autonomously performs multi-turn, iterativereasoning and retrieves relevant information from memory to accomplish thetask. To evaluate memory effectiveness and memory-based reasoning in multimodalagents, we develop M3-Bench, a new long-video question answering benchmark.M3-Bench comprises 100 newly recorded real-world videos captured from a robot'sperspective (M3-Bench-robot) and 920 web-sourced videos across diversescenarios (M3-Bench-web). We annotate question-answer pairs designed to testkey capabilities essential for agent applications, such as human understanding,general knowledge extraction, and cross-modal reasoning. Experimental resultsshow that M3-Agent, trained via reinforcement learning, outperforms thestrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-weband VideoMME-long, respectively. Our work advances the multimodal agents towardmore human-like long-term memory and provides insights into their practicaldesign. Model, code and data are available athttps://github.com/bytedance-seed/m3-agent</description><author>Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</author><pubDate>Fri, 15 Aug 2025 13:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09736v2</guid></item><item><title>Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis</title><link>http://arxiv.org/abs/2503.20047v2</link><description>Vision-language models (VLMs) have shown promise in 2D medical imageanalysis, but extending them to 3D remains challenging due to the highcomputational demands of volumetric data and the difficulty of aligning 3Dspatial features with clinical text. We present Med3DVLM, a 3D VLM designed toaddress these challenges through three key innovations: (1) DCFormer, anefficient encoder that uses decomposed 3D convolutions to capture fine-grainedspatial features at scale; (2) SigLIP, a contrastive learning strategy withpairwise sigmoid loss that improves image-text alignment without relying onlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuseslow- and high-level image features with text embeddings for richer multi-modalrepresentations. We evaluate our model on the M3D dataset, which includesradiology reports and VQA data for 120,084 3D medical images. Results show thatMed3DVLM achieves superior performance across multiple benchmarks. Forimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantlyoutperforming the current state-of-the-art M3D model (19.10%). For reportgeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-endedvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and inclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These resultshighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,enabling scalable, multi-task reasoning across clinical applications. Our codeis publicly available at https://github.com/mirthAI/Med3DVLM.</description><author>Yu Xin, Gorkem Can Ates, Kuang Gong, Wei Shao</author><pubDate>Fri, 15 Aug 2025 13:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.20047v2</guid></item><item><title>Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions</title><link>http://arxiv.org/abs/2508.11414v1</link><description>Large language models implicitly encode preferences over human values, yetsteering them often requires large training data. In this work, we investigatea simple approach: Can we reliably modify a model's value system in downstreambehavior by training it to answer value survey questions accordingly? We firstconstruct value profiles of several open-source LLMs by asking them to rate aseries of value-related descriptions spanning 20 distinct human values, whichwe use as a baseline for subsequent experiments. We then investigate whetherthe value system of a model can be governed by fine-tuning on the valuesurveys. We evaluate the effect of finetuning on the model's behavior in twoways; first, we assess how answers change on in-domain, held-out surveyquestions. Second, we evaluate whether the model's behavior changes inout-of-domain settings (situational scenarios). To this end, we construct acontextualized moral judgment dataset based on Reddit posts and evaluatechanges in the model's behavior in text-based adventure games. We demonstratethat our simple approach can not only change the model's answers to in-domainsurvey questions, but also produces substantial shifts (value alignment) inimplicit downstream task behavior.</description><author>Shangrui Nie, Florian Mai, David Kaczér, Charles Welch, Zhixue Zhao, Lucie Flek</author><pubDate>Fri, 15 Aug 2025 11:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11414v1</guid></item><item><title>Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries</title><link>http://arxiv.org/abs/2502.16636v2</link><description>Retrieval-augmented generation (RAG) is a paradigm that augments largelanguage models (LLMs) with external knowledge to tackle knowledge-intensivequestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)under Multimodal RAG settings, they predominantly retrieve from textual corporaand do not explicitly assess how models exploit visual evidence duringgeneration. Consequently, there still lacks benchmark that isolates andmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,a question-answering benchmark that targets visually grounded,knowledge-intensive questions. Unlike prior work, Visual-RAG requirestext-to-image retrieval and the integration of retrieved clue images to extractvisual evidence for answer generation. With Visual-RAG, we evaluate 5open-source and 3 proprietary MLLMs, showcasing that images provide strongevidence in augmented generation. However, even state-of-the-art modelsstruggle to efficiently extract and utilize visual knowledge. Our resultshighlight the need for improved visual retrieval, grounding, and attribution inmultimodal RAG systems.</description><author>Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, Wenya Wang</author><pubDate>Fri, 15 Aug 2025 09:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.16636v2</guid></item></channel></rss>