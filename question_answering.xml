<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Aug 2025 17:36:27 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding</title><link>http://arxiv.org/abs/2410.01671v3</link><description>Large language models (LLMs) have shown remarkable capabilities in naturallanguage processing; however, they still face difficulties when tasked withunderstanding lengthy contexts and executing effective question answering.These challenges often arise due to the complexity and ambiguity present inlonger texts. To enhance the performance of LLMs in such scenarios, weintroduce the Long Question Coreference Adaptation (LQCA) method. Thisinnovative framework focuses on coreference resolution tailored to longcontexts, allowing the model to identify and manage references effectively. TheLQCA method encompasses four key steps: resolving coreferences withinsub-documents, computing the distances between mentions, defining arepresentative mention for coreference, and answering questions through mentionreplacement. By processing information systematically, the framework provideseasier-to-handle partitions for LLMs, promoting better understanding.Experimental evaluations on a range of LLMs and datasets have yielded positiveresults, with a notable improvements on OpenAI-o1-mini and GPT-4o models,highlighting the effectiveness of leveraging coreference resolution to bridgecontext gaps in question answering. Our code is public athttps://github.com/OceannTwT/LQCA.</description><author>Yanming Liu, Xinyue Peng, Jiannan Cao, Yanxin Shen, Tianyu Du, Sheng Cheng, Xun Wang, Jianwei Yin, Xuhong Zhang</author><pubDate>Fri, 15 Aug 2025 05:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.01671v3</guid></item><item><title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title><link>http://arxiv.org/abs/2507.22752v2</link><description>We introduce CUS-QA, a benchmark for open-ended regional question answeringthat encompasses both textual and visual modalities. We also provide strongbaselines using state-of-the-art large language models (LLMs). Our datasetconsists of manually curated questions and answers grounded in Wikipedia,created by native speakers from Czechia, Slovakia, and Ukraine, withaccompanying English translations. It includes both purely textual questionsand those requiring visual understanding. We evaluate state-of-the-art LLMsthrough prompting and complement this with human judgments of answercorrectness. Using these human evaluations, we analyze the reliability ofexisting automatic evaluation metrics. Our baseline results show that even thebest open-weight LLMs achieve only around 50% accuracy on textual questions andbelow 30% on visual questions. LLM-based evaluation metrics show strongcorrelation with human judgment, while traditional string-overlap metricsperform surprisingly well due to the prevalence of named entities in answers.</description><author>Jindřich Libovický, Jindřich Helcl, Andrei Manea, Gianluca Vico</author><pubDate>Thu, 21 Aug 2025 12:30:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.22752v2</guid></item><item><title>Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset</title><link>http://arxiv.org/abs/2508.11058v1</link><description>The advancement of 3D vision-language (3D VL) learning is hindered by severallimitations in existing 3D VL datasets: they rarely necessitate reasoningbeyond a close range of objects in single viewpoint, and annotations often linkinstructions to single objects, missing richer contextual alignments betweenmultiple objects. This significantly curtails the development of models capableof deep, multi-view 3D scene understanding over distant objects. To addressthese challenges, we introduce MV-ScanQA, a novel 3D question answering datasetwhere 68% of questions explicitly require integrating information from multipleviews (compared to less than 7% in existing datasets), thereby rigorouslytesting multi-view compositional reasoning. To facilitate the training ofmodels for such demanding scenarios, we present TripAlign dataset, alarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M &lt;2Dview, set of 3D objects, text&gt; triplets that explicitly aligns groups ofcontextually related objects with text, providing richer, view-groundedmulti-object multimodal alignment signals than previous single-objectannotations. We further develop LEGO, a baseline method for the multi-viewreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2DLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlignachieves state-of-the-art performance not only on the proposed MV-ScanQA, butalso on existing benchmarks for 3D dense captioning and question answering.Datasets and code are available athttps://matthewdm0816.github.io/tripalign-mvscanqa.</description><author>Wentao Mo, Qingchao Chen, Yuxin Peng, Siyuan Huang, Yang Liu</author><pubDate>Thu, 14 Aug 2025 20:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11058v1</guid></item><item><title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title><link>http://arxiv.org/abs/2508.09736v2</link><description>We introduce M3-Agent, a novel multimodal agent framework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual andauditory inputs to build and update its long-term memory. Beyond episodicmemory, it also develops semantic memory, enabling it to accumulate worldknowledge over time. Its memory is organized in an entity-centric, multimodalformat, allowing deeper and more consistent understanding of the environment.Given an instruction, M3-Agent autonomously performs multi-turn, iterativereasoning and retrieves relevant information from memory to accomplish thetask. To evaluate memory effectiveness and memory-based reasoning in multimodalagents, we develop M3-Bench, a new long-video question answering benchmark.M3-Bench comprises 100 newly recorded real-world videos captured from a robot'sperspective (M3-Bench-robot) and 920 web-sourced videos across diversescenarios (M3-Bench-web). We annotate question-answer pairs designed to testkey capabilities essential for agent applications, such as human understanding,general knowledge extraction, and cross-modal reasoning. Experimental resultsshow that M3-Agent, trained via reinforcement learning, outperforms thestrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-weband VideoMME-long, respectively. Our work advances the multimodal agents towardmore human-like long-term memory and provides insights into their practicaldesign. Model, code and data are available athttps://github.com/bytedance-seed/m3-agent</description><author>Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</author><pubDate>Fri, 15 Aug 2025 13:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09736v2</guid></item><item><title>Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis</title><link>http://arxiv.org/abs/2503.20047v2</link><description>Vision-language models (VLMs) have shown promise in 2D medical imageanalysis, but extending them to 3D remains challenging due to the highcomputational demands of volumetric data and the difficulty of aligning 3Dspatial features with clinical text. We present Med3DVLM, a 3D VLM designed toaddress these challenges through three key innovations: (1) DCFormer, anefficient encoder that uses decomposed 3D convolutions to capture fine-grainedspatial features at scale; (2) SigLIP, a contrastive learning strategy withpairwise sigmoid loss that improves image-text alignment without relying onlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuseslow- and high-level image features with text embeddings for richer multi-modalrepresentations. We evaluate our model on the M3D dataset, which includesradiology reports and VQA data for 120,084 3D medical images. Results show thatMed3DVLM achieves superior performance across multiple benchmarks. Forimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantlyoutperforming the current state-of-the-art M3D model (19.10%). For reportgeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-endedvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and inclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These resultshighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,enabling scalable, multi-task reasoning across clinical applications. Our codeis publicly available at https://github.com/mirthAI/Med3DVLM.</description><author>Yu Xin, Gorkem Can Ates, Kuang Gong, Wei Shao</author><pubDate>Fri, 15 Aug 2025 13:13:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.20047v2</guid></item><item><title>Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions</title><link>http://arxiv.org/abs/2508.11414v1</link><description>Large language models implicitly encode preferences over human values, yetsteering them often requires large training data. In this work, we investigatea simple approach: Can we reliably modify a model's value system in downstreambehavior by training it to answer value survey questions accordingly? We firstconstruct value profiles of several open-source LLMs by asking them to rate aseries of value-related descriptions spanning 20 distinct human values, whichwe use as a baseline for subsequent experiments. We then investigate whetherthe value system of a model can be governed by fine-tuning on the valuesurveys. We evaluate the effect of finetuning on the model's behavior in twoways; first, we assess how answers change on in-domain, held-out surveyquestions. Second, we evaluate whether the model's behavior changes inout-of-domain settings (situational scenarios). To this end, we construct acontextualized moral judgment dataset based on Reddit posts and evaluatechanges in the model's behavior in text-based adventure games. We demonstratethat our simple approach can not only change the model's answers to in-domainsurvey questions, but also produces substantial shifts (value alignment) inimplicit downstream task behavior.</description><author>Shangrui Nie, Florian Mai, David Kaczér, Charles Welch, Zhixue Zhao, Lucie Flek</author><pubDate>Fri, 15 Aug 2025 11:36:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11414v1</guid></item><item><title>Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries</title><link>http://arxiv.org/abs/2502.16636v2</link><description>Retrieval-augmented generation (RAG) is a paradigm that augments largelanguage models (LLMs) with external knowledge to tackle knowledge-intensivequestion answering. While several benchmarks evaluate Multimodal LLMs (MLLMs)under Multimodal RAG settings, they predominantly retrieve from textual corporaand do not explicitly assess how models exploit visual evidence duringgeneration. Consequently, there still lacks benchmark that isolates andmeasures the contribution of retrieved images in RAG. We introduce Visual-RAG,a question-answering benchmark that targets visually grounded,knowledge-intensive questions. Unlike prior work, Visual-RAG requirestext-to-image retrieval and the integration of retrieved clue images to extractvisual evidence for answer generation. With Visual-RAG, we evaluate 5open-source and 3 proprietary MLLMs, showcasing that images provide strongevidence in augmented generation. However, even state-of-the-art modelsstruggle to efficiently extract and utilize visual knowledge. Our resultshighlight the need for improved visual retrieval, grounding, and attribution inmultimodal RAG systems.</description><author>Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, Wenya Wang</author><pubDate>Fri, 15 Aug 2025 09:45:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.16636v2</guid></item><item><title>LLM Compression: How Far Can We Go in Balancing Size and Performance?</title><link>http://arxiv.org/abs/2508.11318v1</link><description>Quantization is an essential and popular technique for improving theaccessibility of large language models (LLMs) by reducing memory usage andcomputational costs while maintaining performance. In this study, we apply4-bit Group Scaling Quantization (GSQ) and Generative Pretrained TransformerQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating theirimpact across multiple NLP tasks. We benchmark these models on MS MARCO(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K(Mathematical Reasoning) datasets, assessing both accuracy and efficiencyacross various tasks. The study measures the trade-offs between modelcompression and task performance, analyzing key evaluation metrics, namelyaccuracy, inference latency, and throughput (total output tokens generated persecond), providing insights into the suitability of low-bit quantization forreal-world deployment. Using the results, users can then make suitabledecisions based on the specifications that need to be met. We discuss the prosand cons of GSQ and GPTQ techniques on models of different sizes, which alsoserve as a benchmark for future experiments.</description><author>Sahil Sk, Debasish Dhal, Sonal Khosla, Sk Shahid, Sambit Shekhar, Akash Dhaka, Shantipriya Parida, Dilip K. Prasad, Ondřej Bojar</author><pubDate>Fri, 15 Aug 2025 08:41:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11318v1</guid></item><item><title>MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks</title><link>http://arxiv.org/abs/2507.18932v2</link><description>Environmental, Social, and Governance (ESG) reports are essential forevaluating sustainability practices, ensuring regulatory compliance, andpromoting financial transparency. However, these documents are often lengthy,structurally diverse, and multimodal, comprising dense text, structured tables,complex figures, and layout-dependent semantics. Existing AI systems oftenstruggle to perform reliable document-level reasoning in such settings, and nodedicated benchmark currently exists in ESG domain. To fill the gap, weintroduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targetedto evaluate multimodal understanding and complex reasoning across structurallydiverse and multi-source ESG documents. This dataset is constructed via ahuman-AI collaborative, multi-stage pipeline. First, a multimodal LLM generatescandidate question-answer (QA) pairs by jointly interpreting rich textual,tabular, and visual information from layout-aware document pages. Second, anLLM verifies the semantic accuracy, completeness, and reasoning complexity ofeach QA pair. This automated process is followed by an expert-in-the-loopvalidation, where domain specialists validate and calibrate QA pairs to ensurequality, relevance, and diversity. MMESGBench comprises 933 validated QA pairsderived from 45 ESG documents, spanning across seven distinct document typesand three major ESG source categories. Questions are categorized assingle-page, cross-page, or unanswerable, with each accompanied by fine-grainedmultimodal evidence. Initial experiments validate that multimodal andretrieval-augmented models substantially outperform text-only baselines,particularly on visually grounded and cross-page tasks. MMESGBench is publiclyavailable as an open-source dataset athttps://github.com/Zhanglei1103/MMESGBench.</description><author>Lei Zhang, Xin Zhou, Chaoyue He, Di Wang, Yi Wu, Hong Xu, Wei Liu, Chunyan Miao</author><pubDate>Fri, 15 Aug 2025 08:08:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.18932v2</guid></item><item><title>AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries</title><link>http://arxiv.org/abs/2508.11285v1</link><description>Depression, anxiety, and stress are widespread mental health concerns thatincreasingly drive individuals to seek information from Large Language Models(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, GeminiPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twentypragmatic questions about depression, anxiety, and stress when those questionsare framed for six user profiles (baseline, woman, man, young, old, anduniversity student). The models generated 2,880 answers, which we scored forsentiment and emotions using state-of-the-art tools. Our analysis revealed thatoptimism, fear, and sadness dominated the emotional landscape across alloutputs, with neutral sentiment maintaining consistently high values.Gratitude, joy, and trust appeared at moderate levels, while emotions such asanger, disgust, and love were rarely expressed. The choice of LLM significantlyinfluenced emotional expression patterns. Mixtral exhibited the highest levelsof negative emotions including disapproval, annoyance, and sadness, while Llamademonstrated the most optimistic and joyful responses. The type of mentalhealth condition dramatically shaped emotional responses: anxiety promptselicited extraordinarily high fear scores (0.974), depression prompts generatedelevated sadness (0.686) and the highest negative sentiment, whilestress-related queries produced the most optimistic responses (0.755) withelevated joy and trust. In contrast, demographic framing of queries producedonly marginal variations in emotional tone. Statistical analyses confirmedsignificant model-specific and condition-specific differences, whiledemographic influences remained minimal. These findings highlight the criticalimportance of model selection in mental health applications, as each LLMexhibits a distinct emotional signature that could significantly impact userexperience and outcomes.</description><author>Arya VarastehNezhad, Reza Tavasoli, Soroush Elyasi, MohammadHossein LotfiNia, Hamed Farbeh</author><pubDate>Fri, 15 Aug 2025 07:47:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11285v1</guid></item><item><title>Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering</title><link>http://arxiv.org/abs/2508.11247v1</link><description>Multi-hop question answering (MHQA) requires integrating knowledge scatteredacross multiple passages to derive the correct answer. Traditionalretrieval-augmented generation (RAG) methods primarily focus on coarse-grainedtextual semantic similarity and ignore structural associations among dispersedknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methodsaddress this by leveraging knowledge graphs (KGs) to capture structuralassociations, but they tend to overly rely on structural information andfine-grained word- or phrase-level retrieval, resulting in an underutilizationof textual semantics. In this paper, we propose a novel RAG approach calledHGRAG for MHQA that achieves cross-granularity integration of structural andsemantic information via hypergraphs. Structurally, we construct an entityhypergraph where fine-grained entities serve as nodes and coarse-grainedpassages as hyperedges, and establish knowledge association through sharedentities. Semantically, we design a hypergraph retrieval method that integratesfine-grained entity similarity and coarse-grained passage similarity viahypergraph diffusion. Finally, we employ a retrieval enhancement module, whichfurther refines the retrieved results both semantically and structurally, toobtain the most relevant passages as context for answer generation with theLLM. Experimental results on benchmark datasets demonstrate that our approachoutperforms state-of-the-art methods in QA performance, and achieves a6$\times$ speedup in retrieval efficiency.</description><author>Changjian Wang, Weihong Deng, Weili Guan, Quan Lu, Ning Jiang</author><pubDate>Fri, 15 Aug 2025 06:36:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11247v1</guid></item><item><title>SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation with Casual Inference</title><link>http://arxiv.org/abs/2403.07088v7</link><description>Large language models(LLMs) have shown its outperforming ability on varioustasks and question answering. However, LLMs require substantial memory storageon low-resource devices. More critically, the computational speed on thesedevices is also severely limited. In this paper, we propose SPA(Side PluginAdaption), a lightweight architecture for fast on-devices inference on theconstraints of strict on-devices computation and memory constraints. Comparedwith other on-devices seq2seq generation, SPA could make a fast and stableinference on low-resource constraints, allowing it to obtain cost effiency. Ourmethod establish an interaction between a pretrained LLMs on-cloud and additiveparameters on-devices, which could provide the knowledge on both pretrainedLLMs and featured personal feature. Further more, SPA provides a framework tokeep feature-base parameters on low computational devices while leave theparameters containing general information on the high computational devices.</description><author>Yanming Liu, Xinyue Peng, Ningjing Sang, Yafeng Yan, Xiaolan Ke, Zhiting Zheng, Shaobo Liu, Songhang Deng, Jiannan Cao, Le Dai, Xingzu Liu, Ruilin Nong, Weihao Liu</author><pubDate>Fri, 15 Aug 2025 04:39:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07088v7</guid></item><item><title>PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging</title><link>http://arxiv.org/abs/2505.11872v3</link><description>Recent advancements in prompt-based medical image segmentation have enabledclinicians to identify tumors using simple input like bounding boxes or textprompts. However, existing methods face challenges when doctors need tointeract through natural language or when position reasoning is required -understanding spatial relationships between anatomical structures andpathologies. We present PRS-Med, a framework that integrates vision-languagemodels with segmentation capabilities to generate both accurate segmentationmasks and corresponding spatial reasoning outputs. Additionally, we introducethe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),which provides diverse, spatially-grounded question-answer pairs to address thelack of position reasoning data in medical imaging. PRS-Med demonstratessuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,endoscopy, RGB), significantly outperforming state-of-the-art methods in bothsegmentation accuracy and position reasoning. Our approach enables intuitivedoctor-system interaction through natural language, facilitating more efficientdiagnoses. Our dataset pipeline, model, and codebase will be released to fosterfurther research in spatially-aware multimodal reasoning for medicalapplications.</description><author>Quoc-Huy Trinh, Minh-Van Nguyen, Jung Zeng, Ulas Bagci, Debesh Jha</author><pubDate>Fri, 15 Aug 2025 02:58:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.11872v3</guid></item><item><title>MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering</title><link>http://arxiv.org/abs/2508.11163v1</link><description>This paper presents MobQA, a benchmark dataset designed to evaluate thesemantic understanding capabilities of large language models (LLMs) for humanmobility data through natural language question answering. While existing models excel at predicting human movement patterns, it remainsunobvious how much they can interpret the underlying reasons or semanticmeaning of those patterns. MobQA provides a comprehensive evaluation frameworkfor LLMs to answer questions about diverse human GPS trajectories spanningdaily to weekly granularities. It comprises 5,800 high-quality question-answerpairs across three complementary question types: factual retrieval (precisedata extraction), multiple-choice reasoning (semantic inference), and free-formexplanation (interpretive description), which all require spatial, temporal,and semantic reasoning. Our evaluation of major LLMs reveals strong performanceon factual retrieval but significant limitations in semantic reasoning andexplanation question answering, with trajectory length substantially impactingmodel effectiveness. These findings demonstrate the achievements andlimitations of state-of-the-art LLMs for semantic mobilityunderstanding.\footnote{MobQA dataset is available athttps://github.com/CyberAgentAILab/mobqa.}</description><author>Hikaru Asano, Hiroki Ouchi, Akira Kasuga, Ryo Yonetani</author><pubDate>Fri, 15 Aug 2025 02:30:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11163v1</guid></item><item><title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title><link>http://arxiv.org/abs/2508.11133v1</link><description>Large language models (LLMs) are emerging as a go-to tool for queryinginformation. However, current LLM benchmarks rarely feature natural questionsthat are both information-seeking as well as genuinely time-consuming forhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 naturaland complex questions that require dozens, and at times hundreds, ofintermediate steps to solve -- far more than any existing QA benchmark. Tobuild MoNaCo, we developed a decomposed annotation pipeline to elicit andmanually answer natural time-consuming questions at scale. Frontier LLMsevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall andhallucinations. Our results underscore the need for reasoning models thatbetter handle the complexity and sheer breadth of real-worldinformation-seeking questions -- with MoNaCo providing an effective resourcefor tracking such progress. The MONACO benchmark, codebase, prompts and modelspredictions are publicly available at: https://tomerwolgithub.github.io/monaco</description><author>Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty</author><pubDate>Fri, 15 Aug 2025 00:58:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11133v1</guid></item><item><title>Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?</title><link>http://arxiv.org/abs/2508.11011v1</link><description>Construction safety inspections typically involve a human inspectoridentifying safety concerns on-site. With the rise of powerful Vision LanguageModels (VLMs), researchers are exploring their use for tasks such as detectingsafety rule violations from on-site images. However, there is a lack of opendatasets to comprehensively evaluate and further fine-tune VLMs in constructionsafety inspection. Current applications of VLMs use small, supervised datasets,limiting their applicability in tasks they are not directly trained for. Inthis paper, we propose the ConstructionSite 10k, featuring 10,000 constructionsite images with annotations for three inter-connected tasks, including imagecaptioning, safety rule violation visual question answering (VQA), andconstruction element visual grounding. Our subsequent evaluation of currentstate-of-the-art large pre-trained VLMs shows notable generalization abilitiesin zero-shot and few-shot settings, while additional training is needed to makethem applicable to actual construction sites. This dataset allows researchersto train and evaluate their own VLMs with new architectures and techniques,providing a valuable benchmark for construction safety inspection.</description><author>Xuezheng Chen, Zhengbo Zou</author><pubDate>Thu, 14 Aug 2025 18:23:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11011v1</guid></item><item><title>All for law and law for all: Adaptive RAG Pipeline for Legal Research</title><link>http://arxiv.org/abs/2508.13107v1</link><description>Retrieval-Augmented Generation (RAG) mitigates hallucinations by groundinglarge language model outputs in cited sources, a capability that is especiallycritical in the legal domain. We present an end-to-end RAG pipeline thatrevisits and extends the LegalBenchRAG baseline with three targetedenhancements: (i) a context-aware query translator that disentangles documentreferences from natural-language questions and adapts retrieval depth andresponse style based on expertise and specificity, (ii) open-source retrievalstrategies using SBERT and GTE embeddings that achieve substantial performancegains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for$K&gt;4$) while remaining cost-efficient, and (iii) a comprehensive evaluation andgeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall toassess semantic alignment and faithfulness across models and prompt designs.Our results show that carefully designed open-source pipelines can rival oroutperform proprietary approaches in retrieval quality, while a customlegal-grounded prompt consistently produces more faithful and contextuallyrelevant answers than baseline prompting. Taken together, these contributionsdemonstrate the potential of task-aware, component-level tuning to deliverlegally grounded, reproducible, and cost-effective RAG systems for legalresearch assistance.</description><author>Figarri Keisha, Prince Singh, Pallavi, Dion Fernandes, Aravindh Manivannan, Ilham Wicaksono, Faisal Ahmad</author><pubDate>Mon, 18 Aug 2025 17:14:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13107v1</guid></item><item><title>Checkmate: interpretable and explainable RSVQA is the endgame</title><link>http://arxiv.org/abs/2508.13086v1</link><description>Remote Sensing Visual Question Answering (RSVQA) presents unique challengesin ensuring that model decisions are both understandable and grounded in visualcontent. Current models often suffer from a lack of interpretability andexplainability, as well as from biases in dataset distributions that lead toshortcut learning. In this work, we tackle these issues by introducing a novelRSVQA dataset, Chessboard, designed to minimize biases through 3'123'253questions and a balanced answer distribution. Each answer is linked to one ormore cells within the image, enabling fine-grained visual reasoning. Building on this dataset, we develop an explainable and interpretable modelcalled Checkmate that identifies the image cells most relevant to itsdecisions. Through extensive experiments across multiple model architectures,we show that our approach improves transparency and supports more trustworthydecision-making in RSVQA systems.</description><author>Lucrezia Tosato, Christel Tartini Chappuis, Syrielle Montariol, Flora Weissgerber, Sylvain Lobry, Devis Tuia</author><pubDate>Mon, 18 Aug 2025 16:59:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13086v1</guid></item><item><title>Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review</title><link>http://arxiv.org/abs/2410.21673v3</link><description>Public Code Review (PCR) is developed in the Software Question Answering(SQA) community, assisting developers in exploring high-quality and efficientreview services. Current methods on PCR mainly focus on the reviewer'sperspective, including finding a capable reviewer, predicting comment quality,and recommending/generating review comments. However, it is not well studiedthat how to satisfy the review necessity requests posted by developers whichcan increase their visibility, which in turn acts as a prerequisite for betterreview responses. To this end, we propose K nowledge-guided P rompt learningfor P ublic Code Review (KP-PCR) to achieve developer-based code review requestquality assurance (i.e., predicting request necessity and recommending tagssubtask). Specifically, we reformulate the two subtasks via 1) text prompttuning which converts both of them into a Masked Language Model (MLM) byconstructing prompt templates using hard prompt; and 2) knowledge and codeprefix tuning which introduces knowledge guidance from fine-tuned largelanguage models by soft prompt, and uses program dependence graph tocharacterize code snippets. Finally, both of the request necessity predictionand tag recommendation subtasks output predicted results through an answerengineering module. In addition, we further analysis the time complexity of ourKP-PCR that has lightweight prefix based the operation of introducing knowledgeguidance. Experimental results on the PCR dataset for the period 2011-2023demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the requestnecessity prediction and by 1.4%-6.9% in the tag recommendation. The codeimplementation is released at https://github.com/WUT-IDEA/KP-PCR.</description><author>Lin Li, Xinchun Yu, Xinyu Chen, Peng Liang</author><pubDate>Thu, 21 Aug 2025 17:33:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.21673v3</guid></item><item><title>StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding</title><link>http://arxiv.org/abs/2508.15717v1</link><description>Multimodal large language models (MLLMs) have made significant progress invisual-language reasoning, but their ability to efficiently handle long videosremains limited. Despite recent advances in long-context MLLMs, storing andattending to the key-value (KV) cache for long visual contexts incurssubstantial memory and computational overhead. Existing visual compressionmethods require either encoding the entire visual context before compression orhaving access to the questions in advance, which is impractical for long videounderstanding and multi-turn conversational settings. In this work, we proposeStreamMem, a query-agnostic KV cache memory mechanism for streaming videounderstanding. Specifically, StreamMem encodes new video frames in a streamingmanner, compressing the KV cache using attention scores between visual tokensand generic query tokens, while maintaining a fixed-size KV memory to enableefficient question answering (QA) in memory-constrained, long-video scenarios.Evaluation on three long video understanding and two streaming video questionanswering benchmarks shows that StreamMem achieves state-of-the-art performancein query-agnostic KV cache compression and is competitive with query-awarecompression approaches.</description><author>Yanlai Yang, Zhuokai Zhao, Satya Narayan Shukla, Aashu Singh, Shlok Kumar Mishra, Lizhu Zhang, Mengye Ren</author><pubDate>Thu, 21 Aug 2025 16:56:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15717v1</guid></item><item><title>AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning</title><link>http://arxiv.org/abs/2508.07470v2</link><description>Current audio-visual (AV) benchmarks focus on final answer accuracy,overlooking the underlying reasoning process. This makes it difficult todistinguish genuine comprehension from correct answers derived through flawedreasoning or hallucinations. To address this, we introduce AURA (Audio-visualUnderstanding and Reasoning Assessment), a benchmark for evaluating thecross-modal reasoning capabilities of Audio-Visual Large Language Models(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions acrosssix challenging cognitive domains, such as causality, timbre and pitch, tempoand AV synchronization, unanswerability, implicit distractions, and skillprofiling, explicitly designed to be unanswerable from a single modality. Thisforces models to construct a valid logical path grounded in both audio andvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. Toassess reasoning traces, we propose a novel metric, AuraScore, which addressesthe lack of robust tools for evaluating reasoning fidelity. It decomposesreasoning into two aspects: (i) Factual Consistency - whether reasoning isgrounded in perceptual evidence, and (ii) Core Inference - the logical validityof each reasoning step. Evaluations of SOTA models on AURA reveal a criticalreasoning gap: although models achieve high accuracy (up to 92% on some tasks),their Factual Consistency and Core Inference scores fall below 45%. Thisdiscrepancy highlights that models often arrive at correct answers throughflawed logic, underscoring the need for our benchmark and paving the way formore robust multimodal evaluation.</description><author>Siminfar Samakoush Galougah, Rishie Raj, Sanjoy Chowdhury, Sayan Nag, Ramani Duraiswami</author><pubDate>Thu, 21 Aug 2025 16:39:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.07470v2</guid></item><item><title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title><link>http://arxiv.org/abs/2508.15690v1</link><description>GRAFT is a structured multimodal benchmark for evaluating models oninstruction-following, visual reasoning, and visual-textual alignment tasks. Itfeatures programmatically generated charts and synthetically rendered tables,created with Python visualization libraries to ensure control over datasemantics, structure, and clarity. Each GRAFT instance pairs a chart or tableimage with a systematically generated, multi-step analytical question basedsolely on visual content. Answers are provided in structured formats such asJSON or YAML, supporting consistent evaluation of both reasoning and outputformat. The benchmark introduces a taxonomy of reasoning types includingcomparison, trend identification, ranking, aggregation, proportion estimation,and anomaly detection to enable comprehensive assessment. Reference answersfollow strict factual and formatting guidelines for precise, aspect-basedevaluation. GRAFT offers a unified, scalable framework for fine-grainedbenchmarking of multimodal models on visually grounded, structured reasoningtasks, setting a new evaluation standard in this field.</description><author>Abhigya Verma, Sriram Puttagunta, Seganrasan Subramanian, Sravan Ramachandran</author><pubDate>Thu, 21 Aug 2025 16:13:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15690v1</guid></item><item><title>When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</title><link>http://arxiv.org/abs/2508.15641v1</link><description>Understanding videos requires more than answering open ended questions, itdemands the ability to pinpoint when events occur and how entities interactacross time. While recent Video LLMs have achieved remarkable progress inholistic reasoning, they remain coarse in temporal perception: timestamps areencoded only implicitly, frame level features are weak in capturing continuity,and language vision alignment often drifts from the entities of interest. Inthis paper, we present Grounded VideoDiT, a Video LLM designed to overcomethese limitations by introducing three key innovations. First, a DiffusionTemporal Latent (DTL) encoder enhances boundary sensitivity and maintainstemporal consistency. Second, object grounded representations explicitly bindquery entities to localized visual evidence, strengthening alignment. Third, amixed token scheme with discrete temporal tokens provides explicit timestampmodeling, enabling fine grained temporal reasoning. Together, these designsequip Grounded VideoDiT with robust grounding capabilities, as validated bystate of the art results on Charades STA, NExT GQA, and multiple VideoQAbenchmarks.</description><author>Pengcheng Fang, Yuxia Chen, Rui Guo</author><pubDate>Thu, 21 Aug 2025 15:12:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15641v1</guid></item><item><title>Thyme: Think Beyond Images</title><link>http://arxiv.org/abs/2508.11630v1</link><description>Following OpenAI's introduction of the ``thinking with images'' concept,recent efforts have explored stimulating the use of visual information in thereasoning process to enhance model performance in perception and reasoningtasks. However, to the best of our knowledge, no open-source work currentlyoffers a feature set as rich as proprietary models (O3), which can performdiverse image manipulations and simultaneously enhance logical reasoningcapabilities through code. In this paper, we make a preliminary attempt in thisdirection by introducing Thyme (Think Beyond Images), a novel paradigm forenabling MLLMs to transcend existing ``think with images'' approaches byautonomously generating and executing diverse image processing andcomputational operations via executable code. This approach not onlyfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,rotation, contrast enhancement) but also allows for mathematical computations,all while maintaining high autonomy in deciding when and how to apply theseoperations. We activate this capability through a two-stage training strategy:an initial SFT on a curated dataset of 500K samples to teach code generation,followed by a RL phase to refine decision-making. For the RL stage, we manuallycollect and design high-resolution question-answer pairs to increase thelearning difficulty, and we propose GRPO-ATS (Group Relative PolicyOptimization with Adaptive Temperature Sampling), an algorithm that appliesdistinct temperatures to text and code generation to balance reasoningexploration with code execution precision. We conduct extensive experimentalanalysis and ablation studies. Comprehensive evaluations on nearly 20benchmarks show that Thyme yields significant and consistent performance gains,particularly in challenging high-resolution perception and complex reasoningtasks.</description><author>Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, Guorui Zhou</author><pubDate>Fri, 15 Aug 2025 17:59:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11630v1</guid></item><item><title>Using a cognitive architecture to consider antiBlackness in design and development of AI systems</title><link>http://arxiv.org/abs/2207.00644v3</link><description>How might we use cognitive modeling to consider the ways in whichantiblackness, and racism more broadly, impact the design and development of AIsystems? We provide a discussion and an example towards an answer to thisquestion. We use the ACT-R/{\Phi} cognitive architecture and an existingknowledge graph system, ConceptNet, to consider this question not only from acognitive and sociocultural perspective, but also from a physiologicalperspective. In addition to using a cognitive modeling as a means to explorehow antiblackness may manifest in the design and development of AI systems(particularly from a software engineering perspective), we also introduceconnections between antiblackness, the Human, and computational cognitivemodeling. We argue that the typical eschewing of sociocultural processes andknowledge structures in cognitive architectures and cognitive modelingimplicitly furthers a colorblind approach to cognitive modeling and hidessociocultural context that is always present in human behavior and affectscognitive processes.</description><author>Christopher L. Dancy</author><pubDate>Thu, 21 Aug 2025 12:14:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.00644v3</guid></item><item><title>Is ChatGPT-5 Ready for Mammogram VQA?</title><link>http://arxiv.org/abs/2508.11628v1</link><description>Mammogram visual question answering (VQA) integrates image interpretationwith clinical reasoning and has potential to support breast cancer screening.We systematically evaluated the GPT-5 family and GPT-4o model on four publicmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,abnormality detection, and malignancy classification tasks. GPT-5 consistentlywas the best performing model but lagged behind both human experts anddomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scoresamong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),calcification (63.5%), and malignancy (52.8%) classification. On InBreast, itattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detectionand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADSaccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Comparedwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) andspecificity (52.3%). While GPT-5 exhibits promising capabilities for screeningtasks, its performance remains insufficient for high-stakes clinical imagingapplications without targeted domain adaptation and optimization. However, thetremendous improvements in performance from GPT-4o to GPT-5 show a promisingtrend in the potential for general large language models (LLMs) to assist withmammography VQA tasks.</description><author>Qiang Li, Shansong Wang, Mingzhe Hu, Mojtaba Safari, Zachary Eidex, Xiaofeng Yang</author><pubDate>Fri, 15 Aug 2025 17:56:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11628v1</guid></item><item><title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title><link>http://arxiv.org/abs/2503.01307v2</link><description>Test-time inference has emerged as a powerful paradigm for enabling languagemodels to ``think'' longer and more carefully about complex challenges, muchlike skilled human experts. While reinforcement learning (RL) can driveself-improvement in language models on verifiable tasks, some models exhibitsubstantial gains while others quickly plateau. For instance, we find thatQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the gameof Countdown. This discrepancy raises a critical question: what intrinsicproperties enable effective self-improvement? We introduce a framework toinvestigate this question by analyzing four key cognitive behaviors --verification, backtracking, subgoal setting, and backward chaining -- that bothexpert human problem solvers and successful language models employ. Our studyreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llamainitially lacks them. In systematic experimentation with controlled behavioraldatasets, we find that priming Llama with examples containing these reasoningbehaviors enables substantial improvements during RL, matching or exceedingQwen's performance. Importantly, the presence of reasoning behaviors, ratherthan correctness of answers, proves to be the critical factor -- models primedwith incorrect solutions containing proper reasoning patterns achievecomparable performance to those trained on correct solutions. Finally,leveraging continued pretraining with OpenWebMath data, filtered to amplifyreasoning behaviors, enables the Llama model to match Qwen's self-improvementtrajectory. Our findings establish a fundamental relationship between initialreasoning behaviors and the capacity for improvement, explaining why somelanguage models effectively utilize additional computation while othersplateau.</description><author>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</author><pubDate>Fri, 15 Aug 2025 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.01307v2</guid></item></channel></rss>