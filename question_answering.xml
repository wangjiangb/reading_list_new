<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 18 Aug 2025 18:46:12 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement</title><link>http://arxiv.org/abs/2411.15115v1</link><description>Recent text-to-video (T2V) diffusion models have demonstrated impressivegeneration capabilities across various domains. However, these models oftengenerate videos that have misalignments with text prompts, especially when theprompts describe complex scenes with multiple objects and attributes. Toaddress this, we introduce VideoRepair, a novel model-agnostic, training-freevideo refinement framework that automatically identifies fine-grainedtext-video misalignments and generates explicit spatial and textual feedback,enabling a T2V diffusion model to perform targeted, localized refinements.VideoRepair consists of four stages: In (1) video evaluation, we detectmisalignments by generating fine-grained evaluation questions and answeringthose questions with MLLM. In (2) refinement planning, we identify accuratelygenerated objects and then create localized prompts to refine other areas inthe video. Next, in (3) region decomposition, we segment the correctlygenerated area using a combined grounding module. We regenerate the video byadjusting the misaligned regions while preserving the correct regions in (4)localized refinement. On two popular video generation benchmarks (EvalCrafterand T2V-CompBench), VideoRepair substantially outperforms recent baselinesacross various text-video alignment metrics. We provide a comprehensiveanalysis of VideoRepair components and qualitative examples.</description><author>Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal</author><pubDate>Fri, 22 Nov 2024 18:31:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.15115v1</guid></item><item><title>UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models</title><link>http://arxiv.org/abs/2310.10942v5</link><description>Teaching Visual Question Answering (VQA) models to refrain from answeringunanswerable questions is necessary for building a trustworthy AI system.Existing studies, though have explored various aspects of VQA but somewhatignored this particular attribute. This paper aims to bridge the research gapby contributing a comprehensive dataset, called UNK-VQA. The dataset isspecifically designed to address the challenge of questions that models do notknow. To this end, we first augment the existing data via deliberateperturbations on either the image or question. In specific, we carefully ensurethat the question-image semantics remain close to the original unperturbeddistribution. By this means, the identification of unanswerable questionsbecomes challenging, setting our dataset apart from others that involve mereimage replacement. We then extensively evaluate the zero- and few-shotperformance of several emerging multi-modal large models and discover theirsignificant limitations when applied to our dataset. Additionally, we alsopropose a straightforward method to tackle these unanswerable questions. Thisdataset, we believe, will serve as a valuable benchmark for enhancing theabstention capability of VQA models, thereby leading to increasedtrustworthiness of AI systems. We have made the dataset(https://github.com/guoyang9/UNK-VQA) available to facilitate furtherexploration in this area.</description><author>Yangyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli</author><pubDate>Sun, 11 Aug 2024 13:24:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10942v5</guid></item><item><title>UNK-VQA: A Dataset and a Probe into the Abstention Ability of Multi-modal Large Models</title><link>http://arxiv.org/abs/2310.10942v6</link><description>Teaching Visual Question Answering (VQA) models to refrain from answeringunanswerable questions is necessary for building a trustworthy AI system.Existing studies, though have explored various aspects of VQA but somewhatignored this particular attribute. This paper aims to bridge the research gapby contributing a comprehensive dataset, called UNK-VQA. The dataset isspecifically designed to address the challenge of questions that models do notknow. To this end, we first augment the existing data via deliberateperturbations on either the image or question. In specific, we carefully ensurethat the question-image semantics remain close to the original unperturbeddistribution. By this means, the identification of unanswerable questionsbecomes challenging, setting our dataset apart from others that involve mereimage replacement. We then extensively evaluate the zero- and few-shotperformance of several emerging multi-modal large models and discover theirsignificant limitations when applied to our dataset. Additionally, we alsopropose a straightforward method to tackle these unanswerable questions. Thisdataset, we believe, will serve as a valuable benchmark for enhancing theabstention capability of VQA models, thereby leading to increasedtrustworthiness of AI systems. We have made the dataset(https://github.com/guoyang9/UNK-VQA) available to facilitate furtherexploration in this area.</description><author>Yangyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli</author><pubDate>Wed, 21 Aug 2024 06:13:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10942v6</guid></item><item><title>RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real Estate Sector</title><link>http://arxiv.org/abs/2412.10104v2</link><description>The real estate market relies heavily on structured data, such as propertydetails, market trends, and price fluctuations. However, the lack ofspecialized Tabular Question Answering datasets in this domain limits thedevelopment of automated question-answering systems. To fill this gap, weintroduce RETQA, the first large-scale open-domain Chinese Tabular QuestionAnswering dataset for Real Estate. RETQA comprises 4,932 tables and 20,762question-answer pairs across 16 sub-fields within three major domains: propertyinformation, real estate company finance information and land auctioninformation. Compared with existing tabular question answering datasets, RETQAposes greater challenges due to three key factors: long-table structures,open-domain retrieval, and multi-domain queries. To tackle these challenges, wepropose the SLUTQA framework, which integrates large language models withspoken language understanding tasks to enhance retrieval and answeringaccuracy. Extensive experiments demonstrate that SLUTQA significantly improvesthe performance of large language models on RETQA by in-context learning. RETQAand SLUTQA provide essential resources for advancing tabular question answeringresearch in the real estate domain, addressing critical challenges inopen-domain and long-table question-answering. The dataset and code arepublicly available at \url{https://github.com/jensen-w/RETQA}.</description><author>Zhensheng Wang, Wenmian Yang, Kun Zhou, Yiquan Zhang, Weijia Jia</author><pubDate>Thu, 23 Jan 2025 13:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.10104v2</guid></item><item><title>Mamba Fusion: Learning Actions Through Questioning</title><link>http://arxiv.org/abs/2409.11513v1</link><description>Video Language Models (VLMs) are crucial for generalizing across diversetasks and using language cues to enhance learning. While transformer-basedarchitectures have been the de facto in vision-language training, they facechallenges like quadratic computational complexity, high GPU memory usage, anddifficulty with long-term dependencies. To address these limitations, weintroduce MambaVL, a novel model that leverages recent advancements inselective state space modality fusion to efficiently capture long-rangedependencies and learn joint representations for vision and language data.MambaVL utilizes a shared state transition matrix across both modalities,allowing the model to capture information about actions from multipleperspectives within the scene. Furthermore, we propose a question-answeringtask that helps guide the model toward relevant cues. These questions providecritical information about actions, objects, and environmental context, leadingto enhanced performance. As a result, MambaVL achieves state-of-the-artperformance in action recognition on the Epic-Kitchens-100 dataset andoutperforms baseline methods in action anticipation.</description><author>Zhikang Dong, Apoorva Beedu, Jason Sheinkopf, Irfan Essa</author><pubDate>Tue, 17 Sep 2024 19:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11513v1</guid></item><item><title>Precise Model Benchmarking with Only a Few Observations</title><link>http://arxiv.org/abs/2410.05222v1</link><description>How can we precisely estimate a large language model's (LLM) accuracy onquestions belonging to a specific topic within a larger question-answeringdataset? The standard direct estimator, which averages the model's accuracy onthe questions in each subgroup, may exhibit high variance for subgroups(topics) with small sample sizes. Synthetic regression modeling, whichleverages the model's accuracy on questions about other topics, may yieldbiased estimates that are too unreliable for large subgroups. We prescribe asimple yet effective solution: an empirical Bayes (EB) estimator that balancesdirect and regression estimates for each subgroup separately, improving theprecision of subgroup-level estimates of model performance. Our experiments onmultiple datasets show that this approach consistently provides more preciseestimates of the LLM performance compared to the direct and regressionapproaches, achieving substantial reductions in the mean squared error.Confidence intervals for EB estimates also have near-nominal coverage and arenarrower compared to those for the direct estimator. Additional experiments ontabular and vision data validate the benefits of this EB approach.</description><author>Riccardo Fogliato, Pratik Patil, Nil-Jana Akpinar, Mathew Monfort</author><pubDate>Mon, 07 Oct 2024 17:26:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05222v1</guid></item><item><title>Assessing the Robustness of Retrieval-Augmented Generation Systems in K-12 Educational Question Answering with Knowledge Discrepancies</title><link>http://arxiv.org/abs/2412.08985v1</link><description>Retrieval-Augmented Generation (RAG) systems have demonstrated remarkablepotential as question answering systems in the K-12 Education domain, whereknowledge is typically queried within the restricted scope of authoritativetextbooks. However, the discrepancy between textbooks and the parametricknowledge in Large Language Models (LLMs) could undermine the effectiveness ofRAG systems. To systematically investigate the robustness of RAG systems undersuch knowledge discrepancies, we present EduKDQA, a question answering datasetthat simulates knowledge discrepancies in real applications by applyinghypothetical knowledge updates in answers and source documents. EduKDQAincludes 3,005 questions covering five subjects, under a comprehensive questiontypology from the perspective of context utilization and knowledge integration.We conducted extensive experiments on retrieval and question answeringperformance. We find that most RAG systems suffer from a substantialperformance drop in question answering with knowledge discrepancies, whilequestions that require integration of contextual knowledge and parametricknowledge pose a challenge to LLMs.</description><author>Tianshi Zheng, Weihan Li, Jiaxin Bai, Weiqi Wang, Yangqiu Song</author><pubDate>Thu, 12 Dec 2024 06:38:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08985v1</guid></item><item><title>Which questions should I answer? Salience Prediction of Inquisitive Questions</title><link>http://arxiv.org/abs/2404.10917v2</link><description>Inquisitive questions -- open-ended, curiosity-driven questions people ask asthey read -- are an integral part of discourse processing (Kehler and Rohde,2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP hastaken advantage of question generation capabilities of LLMs to enhance a widerange of applications. But the space of inquisitive questions is vast: manyquestions can be evoked from a given context. So which of those should beprioritized to find answers? Linguistic theories, unfortunately, have not yetprovided an answer to this question. This paper presents QSALIENCE, a saliencepredictor of inquisitive questions. QSALIENCE is instruction-tuned over ourdataset of linguist-annotated salience scores of 1,766 (context, question)pairs. A question scores high on salience if answering it would greatly enhancethe understanding of the text (Van Rooy, 2003). We show that highly salientquestions are empirically more likely to be answered in the same article,bridging potential questions (Onea, 2016) with Questions Under Discussion(Roberts, 2012). We further validate our findings by showing that answeringsalient questions is an indicator of summarization quality in news.</description><author>Yating Wu, Ritika Mangla, Alexandros G. Dimakis, Greg Durrett, Junyi Jessy Li</author><pubDate>Thu, 03 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10917v2</guid></item><item><title>RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension</title><link>http://arxiv.org/abs/2407.07321v1</link><description>Large Language Models (LLMs) have been applied to many research problemsacross various domains. One of the applications of LLMs is providingquestion-answering systems that cater to users from different fields. Theeffectiveness of LLM-based question-answering systems has already beenestablished at an acceptable level for users posing questions in popular andpublic domains such as trivia and literature. However, it has not often beenestablished in niche domains that traditionally require specialized expertise.To this end, we construct the NEPAQuAD1.0 benchmark to evaluate the performanceof three frontier LLMs -- Claude Sonnet, Gemini, and GPT-4 -- when answeringquestions originating from Environmental Impact Statements prepared by U.S.federal government agencies in accordance with the National EnvironmentalEnvironmental Act (NEPA). We specifically measure the ability of LLMs tounderstand the nuances of legal, technical, and compliance-related informationpresent in NEPA documents in different contextual scenarios. For example, wetest the LLMs' internal prior NEPA knowledge by providing questions without anycontext, as well as assess how LLMs synthesize the contextual informationpresent in long NEPA documents to facilitate the question/answering task. Wecompare the performance of the long context LLMs and RAG powered models inhandling different types of questions (e.g., problem-solving, divergent). Ourresults suggest that RAG powered models significantly outperform the longcontext models in the answer accuracy regardless of the choice of the frontierLLM. Our further analysis reveals that many models perform better answeringclosed questions than divergent and problem-solving questions.</description><author>Hung Phan, Anurag Acharya, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, Sai Munikoti, Sameera Horawalavithana</author><pubDate>Wed, 10 Jul 2024 02:33:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07321v1</guid></item><item><title>INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages</title><link>http://arxiv.org/abs/2407.13522v1</link><description>Large Language Models (LLMs) have demonstrated remarkable zero-shot andfew-shot capabilities in unseen tasks, including context-grounded questionanswering (QA) in English. However, the evaluation of LLMs' capabilities innon-English languages for context-based QA is limited by the scarcity ofbenchmarks in non-English languages. To address this gap, we introduceIndic-QA, the largest publicly available context-grounded question-answeringdataset for 11 major Indian languages from two language families. The datasetcomprises both extractive and abstractive question-answering tasks and includesexisting datasets as well as English QA datasets translated into Indianlanguages. Additionally, we generate a synthetic dataset using the Gemini modelto create question-answer pairs given a passage, which is then manuallyverified for quality assurance. We evaluate various multilingual Large LanguageModels and their instruction-fine-tuned variants on the benchmark and observethat their performance is subpar, particularly for low-resource languages. Wehope that the release of this dataset will stimulate further research on thequestion-answering abilities of LLMs for low-resource languages.</description><author>Abhishek Kumar Singh, Rudra Murthy, Vishwajeet kumar, Jaydeep Sen, Ganesh Ramakrishnan</author><pubDate>Thu, 18 Jul 2024 13:57:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13522v1</guid></item><item><title>Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs</title><link>http://arxiv.org/abs/2410.11437v1</link><description>Multimodal Large Language Models (MLLMs) demonstrate a strong understandingof the real world and can even handle complex tasks. However, they still failon some straightforward visual question-answering (VQA) problems. This paperdives deeper into this issue, revealing that models tend to err when answeringeasy questions (e.g. Yes/No questions) about an image, even though they cancorrectly describe it. We refer to this model behavior discrepancy betweendifficult and simple questions as model laziness. To systematically investigatemodel laziness, we manually construct LazyBench, a benchmark that includesYes/No, multiple choice, short answer questions, and image description tasksthat are related to the same subjects in the images. Based on LazyBench, weobserve that laziness widely exists in current advanced MLLMs (e.g. GPT-4o,Gemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced onstronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and findthat about half of its failure cases are caused by model laziness, whichfurther highlights the importance of ensuring that the model fully utilizes itscapability. To this end, we conduct preliminary exploration on how to mitigatelaziness and find that chain of thought (CoT) can effectively address thisissue.</description><author>Sihang Zhao, Youliang Yuan, Xiaoying Tang, Pinjia He</author><pubDate>Tue, 15 Oct 2024 09:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.11437v1</guid></item><item><title>Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System</title><link>http://arxiv.org/abs/2411.03906v2</link><description>In this paper, we examine the impact of lexicalization on Question Answeringover Linked Data (QALD). It is well known that one of the key challenges ininterpreting natural language questions with respect to SPARQL lies in bridgingthe lexical gap, that is mapping the words in the query to the correctvocabulary elements. We argue in this paper that lexicalization, that isexplicit knowledge about the potential interpretations of a word with respectto the given vocabulary, significantly eases the task and increases theperformance of QA systems. Towards this goal, we present a compositional QAsystem that can leverage explicit lexical knowledge in a compositional mannerto infer the meaning of a question in terms of a SPARQL query. We show thatsuch a system, given lexical knowledge, has a performance well beyond currentQA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ scorecompared to the best QA system on QALD-9. This shows the importance andpotential of including explicit lexical knowledge. In contrast, we show thatLLMs have limited abilities to exploit lexical knowledge, with only marginalimprovements compared to a version without lexical knowledge. This shows thatLLMs have no ability to compositionally interpret a question on the basis ofthe meaning of its parts, a key feature of compositional approaches. Takentogether, our work shows new avenues for QALD research, emphasizing theimportance of lexicalization and compositionality.</description><author>David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano</author><pubDate>Thu, 05 Dec 2024 12:56:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03906v2</guid></item><item><title>Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System</title><link>http://arxiv.org/abs/2411.03906v1</link><description>In this paper, we examine the impact of lexicalization on Question Answeringover Linked Data (QALD). It is well known that one of the key challenges ininterpreting natural language questions with respect to SPARQL lies in bridgingthe lexical gap, that is mapping the words in the query to the correctvocabulary elements. We argue in this paper that lexicalization, that isexplicit knowledge about the potential interpretations of a word with respectto the given vocabulary, significantly eases the task and increases theperformance of QA systems. Towards this goal, we present a compositional QAsystem that can leverage explicit lexical knowledge in a compositional mannerto infer the meaning of a question in terms of a SPARQL query. We show thatsuch a system, given lexical knowledge, has a performance well beyond currentQA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ scorecompared to the best QA system on QALD-9. This shows the importance andpotential of including explicit lexical knowledge. In contrast, we show thatLLMs have limited abilities to exploit lexical knowledge, with only marginalimprovements compared to a version without lexical knowledge. This shows thatLLMs have no ability to compositionally interpret a question on the basis ofthe meaning of its parts, a key feature of compositional approaches. Takentogether, our work shows new avenues for QALD research, emphasizing theimportance of lexicalization and compositionality.</description><author>David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano</author><pubDate>Wed, 06 Nov 2024 13:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.03906v1</guid></item><item><title>FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees</title><link>http://arxiv.org/abs/2411.02603v3</link><description>The propensity of Large Language Models (LLMs) to generate hallucinations andnon-factual content undermines their reliability in high-stakes domains, whererigorous control over Type I errors (the conditional probability of incorrectlyclassifying hallucinations as truthful content) is essential. Despite itsimportance, formal verification of LLM factuality with such guarantees remainslargely unexplored. In this paper, we introduce FactTest, a novel frameworkthat statistically assesses whether a LLM can confidently provide correctanswers to given questions with high-probability correctness guarantees. Weformulate factuality testing as hypothesis testing problem to enforce an upperbound of Type I errors at user-specified significance levels. Notably, we provethat our framework also ensures strong Type II error control under mildconditions and can be extended to maintain its effectiveness when covariateshifts exist. Our approach is distribution-free and works for any number ofhuman-annotated samples. It is model-agnostic and applies to any black-box orwhite-box LM. Extensive experiments on question-answering (QA) andmultiple-choice benchmarks demonstrate that FactTest effectively detectshallucinations and improves the model's ability to abstain from answeringunknown questions, leading to an over 40% accuracy improvement.</description><author>Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang</author><pubDate>Thu, 07 Nov 2024 03:17:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02603v3</guid></item><item><title>SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation</title><link>http://arxiv.org/abs/2405.09939v2</link><description>We introduce SciQAG, a novel framework for automatically generatinghigh-quality science question-answer pairs from a large corpus of scientificliterature based on large language models (LLMs). SciQAG consists of a QAgenerator and a QA evaluator, which work together to extract diverse andresearch-level questions and answers from scientific papers. Utilizing thisframework, we construct a large-scale, high-quality, open-ended science QAdataset containing 188,042 QA pairs extracted from 22,743 scientific papersacross 24 scientific domains. We also introduce SciQAG-24D, a new benchmarktask designed to evaluate the science question-answering ability of LLMs.Extensive experiments demonstrate that fine-tuning LLMs on the SciQAG datasetsignificantly improves their performance on both open-ended question answeringand scientific tasks. To foster research and collaboration, we make thedatasets, models, and evaluation codes publicly available, contributing to theadvancement of science question answering and developing more interpretable andreasoning-capable AI systems.</description><author>Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, Ian Foster</author><pubDate>Wed, 10 Jul 2024 01:25:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09939v2</guid></item><item><title>Decomposed Prompting to Answer Questions on a Course Discussion Board</title><link>http://arxiv.org/abs/2407.21170v1</link><description>We propose and evaluate a question-answering system that uses decomposedprompting to classify and answer student questions on a course discussionboard. Our system uses a large language model (LLM) to classify questions intoone of four types: conceptual, homework, logistics, and not answerable. Thisenables us to employ a different strategy for answering questions that fallunder different types. Using a variant of GPT-3, we achieve $81\%$classification accuracy. We discuss our system's performance on answeringconceptual questions from a machine learning course and various failure modes.</description><author>Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang</author><pubDate>Tue, 30 Jul 2024 20:24:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.21170v1</guid></item><item><title>From Pixels to Words: Leveraging Explainability in Face Recognition through Interactive Natural Language Processing</title><link>http://arxiv.org/abs/2409.16089v2</link><description>Face Recognition (FR) has advanced significantly with the development of deeplearning, achieving high accuracy in several applications. However, the lack ofinterpretability of these systems raises concerns about their accountability,fairness, and reliability. In the present study, we propose an interactiveframework to enhance the explainability of FR models by combiningmodel-agnostic Explainable Artificial Intelligence (XAI) and Natural LanguageProcessing (NLP) techniques. The proposed framework is able to accuratelyanswer various questions of the user through an interactive chatbot. Inparticular, the explanations generated by our proposed method are in the formof natural language text and visual representations, which for example candescribe how different facial regions contribute to the similarity measurebetween two faces. This is achieved through the automatic analysis of theoutput's saliency heatmaps of the face images and a BERT question-answeringmodel, providing users with an interface that facilitates a comprehensiveunderstanding of the FR decisions. The proposed approach is interactive,allowing the users to ask questions to get more precise information based onthe user's background knowledge. More importantly, in contrast to previousstudies, our solution does not decrease the face recognition performance. Wedemonstrate the effectiveness of the method through different experiments,highlighting its potential to make FR systems more interpretable anduser-friendly, especially in sensitive applications where decision-makingtransparency is crucial.</description><author>Ivan DeAndres-Tame, Muhammad Faisal, Ruben Tolosana, Rouqaiah Al-Refai, Ruben Vera-Rodriguez, Philipp Terhörst</author><pubDate>Mon, 09 Dec 2024 14:41:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16089v2</guid></item><item><title>From Pixels to Words: Leveraging Explainability in Face Recognition through Interactive Natural Language Processing</title><link>http://arxiv.org/abs/2409.16089v1</link><description>Face Recognition (FR) has advanced significantly with the development of deeplearning, achieving high accuracy in several applications. However, the lack ofinterpretability of these systems raises concerns about their accountability,fairness, and reliability. In the present study, we propose an interactiveframework to enhance the explainability of FR models by combiningmodel-agnostic Explainable Artificial Intelligence (XAI) and Natural LanguageProcessing (NLP) techniques. The proposed framework is able to accuratelyanswer various questions of the user through an interactive chatbot. Inparticular, the explanations generated by our proposed method are in the formof natural language text and visual representations, which for example candescribe how different facial regions contribute to the similarity measurebetween two faces. This is achieved through the automatic analysis of theoutput's saliency heatmaps of the face images and a BERT question-answeringmodel, providing users with an interface that facilitates a comprehensiveunderstanding of the FR decisions. The proposed approach is interactive,allowing the users to ask questions to get more precise information based onthe user's background knowledge. More importantly, in contrast to previousstudies, our solution does not decrease the face recognition performance. Wedemonstrate the effectiveness of the method through different experiments,highlighting its potential to make FR systems more interpretable anduser-friendly, especially in sensitive applications where decision-makingtransparency is crucial.</description><author>Ivan DeAndres-Tame, Muhammad Faisal, Ruben Tolosana, Rouqaiah Al-Refai, Ruben Vera-Rodriguez, Philipp Terhörst</author><pubDate>Tue, 24 Sep 2024 13:40:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.16089v1</guid></item><item><title>Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering</title><link>http://arxiv.org/abs/2403.19631v2</link><description>Large Language Models (LLMs) have shown proficiency in question-answeringtasks but often struggle to integrate real-time knowledge, leading topotentially outdated or inaccurate responses. This problem becomes even morechallenging when dealing with multi-hop questions, since they require LLMs toupdate and integrate multiple knowledge pieces relevant to the questions. Totackle the problem, we propose the Retrieval-Augmented model Editing (RAE)framework for multi-hop question answering. RAE first retrieves edited factsand then refines the language model through in-context learning. Specifically,our retrieval approach, based on mutual information maximization, leverages thereasoning abilities of LLMs to identify chain facts that traditionalsimilarity-based searches might miss. In addition, our framework includes apruning strategy to eliminate redundant information from the retrieved facts,which enhances the editing accuracy and mitigates the hallucination problem.Our framework is supported by theoretical justification for its fact retrievalefficacy. Finally, comprehensive evaluation across various LLMs validates RAE'sability in providing accurate answers with updated knowledge. Our code isavailable at: https://github.com/sycny/RAE.</description><author>Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu</author><pubDate>Tue, 13 Aug 2024 19:34:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19631v2</guid></item><item><title>Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts</title><link>http://arxiv.org/abs/2404.02022v3</link><description>In the era of large language models, applying techniques such as RetrievalAugmented Generation can better address Open-Domain Question-Answeringproblems. Due to constraints including model sizes and computing resources, thelength of context is often limited, and it becomes challenging to empower themodel to cover overlong contexts while answering questions from open domains.This paper proposes a general and convenient method to covering longer contextsin Open-Domain Question-Answering tasks. It leverages a small encoder languagemodel that effectively encodes contexts, and the encoding appliescross-attention with origin inputs. With our method, the origin language modelscan cover several times longer contexts while keeping the computingrequirements close to the baseline. Our experiments demonstrate that afterfine-tuning, there is improved performance across two held-in datasets, fourheld-out datasets, and also in two In Context Learning settings.</description><author>Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu</author><pubDate>Mon, 23 Dec 2024 07:51:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02022v3</guid></item><item><title>AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning</title><link>http://arxiv.org/abs/2411.16495v1</link><description>Recent advancements in large language models (LLMs) have led to significantimprovements in various natural language processing tasks, but it is stillchallenging for LLMs to perform knowledge-intensive complex question answeringdue to LLMs' inefficacy in reasoning planning and the hallucination problem. Atypical solution is to employ retrieval-augmented generation (RAG) coupled withchain-of-thought (CoT) reasoning, which decomposes complex questions intochain-like sub-questions and applies iterative RAG at each sub-question.However, prior works exhibit sub-optimal reasoning planning and overlookdynamic knowledge retrieval from heterogeneous sources. In this paper, wepropose AtomR, a novel heterogeneous knowledge reasoning framework thatconducts multi-source reasoning at the atomic level. Drawing inspiration fromthe graph modeling of knowledge, AtomR leverages large language models (LLMs)to decompose complex questions into combinations of three atomic knowledgeoperators, significantly enhancing the reasoning process at both the planningand execution stages. We also introduce BlendQA, a novel evaluation benchmarktailored to assess complex heterogeneous knowledge reasoning. Experiments showthat AtomR significantly outperforms state-of-the-art baselines across threesingle-source and two multi-source reasoning benchmarks, with notableperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.</description><author>Amy Xin, Jinxin Liu, Zijun Yao, Zhicheng Li, Shulin Cao, Lei Hou, Juanzi Li</author><pubDate>Mon, 25 Nov 2024 15:35:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16495v1</guid></item><item><title>Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos</title><link>http://arxiv.org/abs/2312.10300v3</link><description>A short clip of video may contain progression of multiple events and aninteresting story line. A human need to capture both the event in every shotand associate them together to understand the story behind it. In this work, wepresent a new multi-shot video understanding benchmark Shot2Story with detailedshot-level captions, comprehensive video summaries and question-answeringpairs. To facilitate better semantic understanding of videos, we providecaptions for both visual signals and human narrations. We design severaldistinct tasks including single-shot video captioning, multi-shot videosummarization, and multi-shot video question answering. Preliminary experimentsshow some challenges to generate a long and comprehensive video summary formulti-shot videos. Nevertheless, the generated imperfect summaries can alreadyachieve competitive performance on existing video understanding tasks such asvideo question-answering, promoting an under-explored setting of videounderstanding with detailed summaries.</description><author>Mingfei Han, Linjie Yang, Xiaojun Chang, Lina Yao, Heng Wang</author><pubDate>Wed, 05 Feb 2025 09:57:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.10300v3</guid></item><item><title>Extracting Emotion Phrases from Tweets using BART</title><link>http://arxiv.org/abs/2403.14050v3</link><description>Sentiment analysis is a natural language processing task that aims toidentify and extract the emotional aspects of a text. However, many existingsentiment analysis methods primarily classify the overall polarity of a text,overlooking the specific phrases that convey sentiment. In this paper, weapplied an approach to sentiment analysis based on a question-answeringframework. Our approach leverages the power of Bidirectional AutoregressiveTransformer (BART), a pre-trained sequence-to-sequence model, to extract aphrase from a given text that amplifies a given sentiment polarity. We create anatural language question that identifies the specific emotion to extract andthen guide BART to pay attention to the relevant emotional cues in the text. Weuse a classifier within BART to predict the start and end positions of theanswer span within the text, which helps to identify the precise boundaries ofthe extracted emotion phrase. Our approach offers several advantages over mostsentiment analysis studies, including capturing the complete context andmeaning of the text and extracting precise token spans that highlight theintended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.</description><author>Mahdi Rezapour</author><pubDate>Sat, 27 Jul 2024 17:37:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14050v3</guid></item><item><title>Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering</title><link>http://arxiv.org/abs/2408.07888v2</link><description>Fine-tuning Large Language Models (LLMs) incurs considerable training costs,driving the need for data-efficient training with optimised data ordering.Human-inspired strategies offer a solution by organising data based on humanlearning practices. This study evaluates the fine-tuning efficiency of fivehuman-inspired strategies across four language models, three datasets, and bothhuman- and LLM-labelled data in the context of medical question answering.These strategies achieve the best accuracy gain of 1.81% and an average gain of1.02% across datasets, with interleaved strategies delivering the best averageresults. However, the best strategy varies across model-dataset combinations,limiting the generalisability of the effects of any single strategy.Additionally, LLM-defined question difficulty outperforms human-defined labelsin curriculum-based learning, showing the potential of model-generated data asa cost-effective alternative for optimising fine-tuning.</description><author>Yushi Yang, Andrew M. Bean, Robert McCraith, Adam Mahdi</author><pubDate>Tue, 05 Nov 2024 11:07:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07888v2</guid></item><item><title>Q-Bench+: A Benchmark for Multi-modal Foundation Models on Low-level Vision from Single Images to Pairs</title><link>http://arxiv.org/abs/2402.07116v2</link><description>The rapid development of Multi-modality Large Language Models (MLLMs) hasnavigated a paradigm shift in computer vision, moving towards versatilefoundational models. However, evaluating MLLMs in low-level visual perceptionand understanding remains a yet-to-explore domain. To this end, we designbenchmark settings to emulate human language responses related to low-levelvision: the low-level visual perception (A1) via visual question answeringrelated to low-level attributes (e.g. clarity, lighting); and the low-levelvisual description (A2), on evaluating MLLMs for low-level text descriptions.Furthermore, given that pairwise comparison can better avoid ambiguity ofresponses and has been adopted by many human experiments, we further extend thelow-level perception-related question-answering and description evaluations ofMLLMs from single images to image pairs. Specifically, for perception (A1), wecarry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999image pairs each accompanied by an open-ended question about its low-levelfeatures; for description (A2), we propose the LLDescribe+ dataset, evaluatingMLLMs for low-level descriptions on 499 single images and 450 pairs.Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predictingscore, by employing a softmax-based approach to enable all MLLMs to generatequantifiable quality ratings, tested against human opinions in 7 image qualityassessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate thatseveral MLLMs have decent low-level visual competencies on single images, butonly GPT-4V exhibits higher accuracy on pairwise comparisons than single imageevaluations (like humans). We hope that our benchmark will motivate furtherresearch into uncovering and enhancing these nascent capabilities of MLLMs.Datasets will be available at https://github.com/Q-Future/Q-Bench.</description><author>Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin</author><pubDate>Sat, 10 Aug 2024 04:53:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.07116v2</guid></item><item><title>PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?</title><link>http://arxiv.org/abs/2502.04192v1</link><description>Multiple works have emerged to push the boundaries on multi-modal largelanguage models (MLLMs) towards pixel-level understanding. Such approaches haveshown strong performance on benchmarks for referring expression segmentationand grounded conversation generation. The current trend in pixel-level MLLMs isto train with pixel-level grounding supervision on large-scale labelled data.However, we show that such MLLMs when evaluated on recent challenging visioncentric benchmarks, exhibit a weak ability in visual question answering.Surprisingly, some of these methods even downgrade the grounding ability ofMLLMs that were never trained with such supervision. In this work, we proposetwo novel challenging benchmarks and show that MLLMs without pixel-levelgrounding supervision can outperform the state of the art in such tasks whenevaluating both the pixel-level grounding and visual question answering. Wepropose simple baselines to extract the grounding information that can beplugged into any MLLM, which we call as PixFoundation. More importantly, westudy the research question of ``When does grounding emerge in MLLMs that arenot trained with pixel-level grounding supervision?'' We show that groundingcan coincide with object parts or location/appearance information. Coderepository is at https://github.com/MSiam/PixFoundation/.</description><author>Mennatullah Siam</author><pubDate>Thu, 06 Feb 2025 16:29:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.04192v1</guid></item><item><title>Uncertainty Estimation of Large Language Models in Medical Question Answering</title><link>http://arxiv.org/abs/2407.08662v1</link><description>Large Language Models (LLMs) show promise for natural language generation inhealthcare, but risk hallucinating factually incorrect information. DeployingLLMs for medical question answering necessitates reliable uncertaintyestimation (UE) methods to detect hallucinations. In this work, we benchmarkpopular UE methods with different model sizes on medical question-answeringdatasets. Our results show that current approaches generally perform poorly inthis domain, highlighting the challenge of UE for medical applications. We alsoobserve that larger models tend to yield better results, suggesting acorrelation between model size and the reliability of UE. To address thesechallenges, we propose Two-phase Verification, a probability-free UncertaintyEstimation approach. First, an LLM generates a step-by-step explanationalongside its initial answer, followed by formulating verification questions tocheck the factual claims in the explanation. The model then answers thesequestions twice: first independently, and then referencing the explanation.Inconsistencies between the two sets of answers measure the uncertainty in theoriginal response. We evaluate our approach on three biomedicalquestion-answering datasets using Llama 2 Chat models and compare it againstthe benchmarked baseline methods. The results show that our Two-phaseVerification method achieves the best overall accuracy and stability acrossvarious datasets and model sizes, and its performance scales as the model sizeincreases.</description><author>Jiaxin Wu, Yizhou Yu, Hong-Yu Zhou</author><pubDate>Thu, 11 Jul 2024 16:51:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08662v1</guid></item><item><title>Precise Length Control in Large Language Models</title><link>http://arxiv.org/abs/2412.11937v1</link><description>Large Language Models (LLMs) are increasingly used in production systems,powering applications such as chatbots, summarization, and question answering.Despite their success, controlling the length of their response remains asignificant challenge, particularly for tasks requiring structured outputs orspecific levels of detail. In this work, we propose a method to adaptpre-trained decoder-only LLMs for precise control of response length. Ourapproach incorporates a secondary length-difference positional encoding (LDPE)into the input embeddings, which counts down to a user-set response terminationlength. Fine-tuning with LDPE allows the model to learn to terminate responsescoherently at the desired length, achieving mean token errors of less than 3tokens. We also introduce Max New Tokens++, an extension that enables flexibleupper-bound length control, rather than an exact target. Experimental resultson tasks such as question answering and document summarization demonstrate thatour method enables precise length control without compromising responsequality.</description><author>Bradley Butcher, Michael O'Keefe, James Titchener</author><pubDate>Mon, 16 Dec 2024 16:22:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11937v1</guid></item><item><title>Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots</title><link>http://arxiv.org/abs/2412.04235v1</link><description>I combine detection and mitigation techniques to addresses hallucinations inLarge Language Models (LLMs). Mitigation is achieved in a question-answeringRetrieval-Augmented Generation (RAG) framework while detection is obtained byintroducing the Negative Missing Information Scoring System (NMISS), whichaccounts for contextual relevance in responses. While RAG mitigateshallucinations by grounding answers in external data, NMISS refines theevaluation by identifying cases where traditional metrics incorrectly flagcontextually accurate responses as hallucinations. I use Italian health newsarticles as context to evaluate LLM performance. Results show that Gemma2 andGPT-4 outperform the other models, with GPT-4 producing answers closely alignedwith reference responses. Mid-tier models, such as Llama2, Llama3, and Mistralbenefit significantly from NMISS, highlighting their ability to provide richercontextual information. This combined approach offers new insights into thereduction and more accurate assessment of hallucinations in LLMs, withapplications in real-world healthcare tasks and other domains.</description><author>Maria Paola Priola</author><pubDate>Thu, 05 Dec 2024 15:11:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.04235v1</guid></item><item><title>An Entailment Tree Generation Approach for Multimodal Multi-Hop Question Answering with Mixture-of-Experts and Iterative Feedback Mechanism</title><link>http://arxiv.org/abs/2412.05821v2</link><description>With the rise of large-scale language models (LLMs), it is currently popularand effective to convert multimodal information into text descriptions formultimodal multi-hop question answering. However, we argue that the currentmethods of multi-modal multi-hop question answering still mainly face twochallenges: 1) The retrieved evidence containing a large amount of redundantinformation, inevitably leads to a significant drop in performance due toirrelevant information misleading the prediction. 2) The reasoning processwithout interpretable reasoning steps makes the model difficult to discover thelogical errors for handling complex questions. To solve these problems, wepropose a unified LLMs-based approach but without heavily relying on them dueto the LLM's potential errors, and innovatively treat multimodal multi-hopquestion answering as a joint entailment tree generation and question answeringproblem. Specifically, we design a multi-task learning framework with a focuson facilitating common knowledge sharing across interpretability and predictiontasks while preventing task-specific errors from interfering with each othervia mixture of experts. Afterward, we design an iterative feedback mechanism tofurther enhance both tasks by feeding back the results of the joint training tothe LLM for regenerating entailment trees, aiming to iteratively refine thepotential answer. Notably, our method has won the first place in the officialleaderboard of WebQA (since April 10, 2024), and achieves competitive resultson MultimodalQA.</description><author>Qing Zhang, Haocheng Lv, Jie Liu, Zhiyun Chen, Jianyong Duan, Hao Wang, Li He, Mingying Xv</author><pubDate>Tue, 10 Dec 2024 17:42:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05821v2</guid></item><item><title>CAD-Recode: Reverse Engineering CAD Code from Point Clouds</title><link>http://arxiv.org/abs/2412.14042v1</link><description>Computer-Aided Design (CAD) models are typically constructed by sequentiallydrawing parametric sketches and applying CAD operations to obtain a 3D model.The problem of 3D CAD reverse engineering consists of reconstructing the sketchand CAD operation sequences from 3D representations such as point clouds. Inthis paper, we address this challenge through novel contributions across threelevels: CAD sequence representation, network design, and dataset. Inparticular, we represent CAD sketch-extrude sequences as Python code. Theproposed CAD-Recode translates a point cloud into Python code that, whenexecuted, reconstructs the CAD model. Taking advantage of the exposure ofpre-trained Large Language Models (LLMs) to Python code, we leverage arelatively small LLM as a decoder for CAD-Recode and combine it with alightweight point cloud projector. CAD-Recode is trained solely on a proposedsynthetic dataset of one million diverse CAD sequences. CAD-Recodesignificantly outperforms existing methods across three datasets whilerequiring fewer input points. Notably, it achieves 10 times lower mean Chamferdistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.Furthermore, we show that our CAD Python code output is interpretable byoff-the-shelf LLMs, enabling CAD editing and CAD-specific question answeringfrom point clouds.</description><author>Danila Rukhovich, Elona Dupont, Dimitrios Mallis, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</author><pubDate>Wed, 18 Dec 2024 16:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.14042v1</guid></item><item><title>To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2501.09292v1</link><description>Retrieval-Augmented Generation equips large language models with thecapability to retrieve external knowledge, thereby mitigating hallucinations byincorporating information beyond the model's intrinsic abilities. However, mostprior works have focused on invoking retrieval deterministically, which makesit unsuitable for tasks such as long-form question answering. Instead,dynamically performing retrieval by invoking it only when the underlying LLMlacks the required knowledge can be more efficient. In this context, we delvedeeper into the question, "To Retrieve or Not to Retrieve?" by exploringmultiple uncertainty detection methods. We evaluate these methods for the taskof long-form question answering, employing dynamic retrieval, and present ourcomparisons. Our findings suggest that uncertainty detection metrics, such asDegree Matrix Jaccard and Eccentricity, can reduce the number of retrievalcalls by almost half, with only a slight reduction in question-answeringaccuracy.</description><author>Kaustubh D. Dhole</author><pubDate>Thu, 16 Jan 2025 04:56:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.09292v1</guid></item><item><title>Alt-MoE:A Scalable Framework for Bidirectional Multimodal Alignment and Efficient Knowledge Integration</title><link>http://arxiv.org/abs/2409.05929v3</link><description>Multimodal learning has advanced significantly by aligning differentmodalities within shared latent spaces, enabling tasks such as cross-modalunderstanding and generation. Current alignment strategies in multimodallearning primarily include direct alignment using pre-trained or unifiedencoders and single-directional alignment via modality-specific connectors.Direct alignment struggles to fully leverage rich intra-modal knowledge, oftenrequiring extensive training data to achieve cross-modal representation.Meanwhile, single-directional alignment methods, despite leveraging pre-trainedknowledge, restrict task adaptability and hinder the model's ability to capturebidirectional relationships, leading to incomplete knowledge fusion andunderutilization of complementary modality-specific information. To addressthese limitations, we introduce Alt-MoE, a scalable multimodal alignmentframework that employs a mixture of experts (MoE) model as a multi-directionalconnector across modalities. By utilizing a sequential alternating one-wayalignment strategy, Alt-MoE iteratively refines the model to achievebidirectional alignment. Alt-MoE operates in latent space, enabling efficientvector pre-storage and real-time retrieval via MoE, optimizing large-scale dataprocessing. Extensive empirical studies demonstrate that Alt-MoE achievescompetitive performance on cross-modal retrieval and visual question answeringby integrating diverse modality-specific knowledge, generalizing to unseendata, and easily scaling to new tasks and modalities through dynamic adjustmentof MoE capacity and expert activation.</description><author>Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji</author><pubDate>Fri, 20 Dec 2024 06:37:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.05929v3</guid></item><item><title>Meta-Analysis with Untrusted Data</title><link>http://arxiv.org/abs/2407.09387v1</link><description>[See paper for full abstract] Meta-analysis is a crucial tool for answeringscientific questions. It is usually conducted on a relatively small amount of``trusted'' data -- ideally from randomized, controlled trials -- which allowcausal effects to be reliably estimated with minimal assumptions. We show howto answer causal questions much more precisely by making two changes. First, weincorporate untrusted data drawn from large observational databases, relatedscientific literature and practical experience -- without sacrificing rigor orintroducing strong assumptions. Second, we train richer models capable ofhandling heterogeneous trials, addressing a long-standing challenge inmeta-analysis. Our approach is based on conformal prediction, whichfundamentally produces rigorous prediction intervals, but doesn't handleindirect observations: in meta-analysis, we observe only noisy effects due tothe limited number of participants in each trial. To handle noise, we develop asimple, efficient version of fully-conformal kernel ridge regression, based ona novel condition called idiocentricity. We introduce noise-correcting terms inthe residuals and analyze their interaction with a ``variance shaving''technique. In multiple experiments on healthcare datasets, our algorithmsdeliver tighter, sounder intervals than traditional ones. This paper charts anew course for meta-analysis and evidence-based medicine, where heterogeneityand untrusted data are embraced for more nuanced and precise predictions.</description><author>Shiva Kaul, Geoffrey J. Gordon</author><pubDate>Fri, 12 Jul 2024 16:07:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09387v1</guid></item><item><title>Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost</title><link>http://arxiv.org/abs/2407.19825v2</link><description>Today's large language models (LLMs) can solve challenging question-answeringtasks, and prompt engineering techniques, such as chain-of-thought (CoT), havegained attention for enhancing the explanation and correctness of outputs.However, many models and techniques tend to produce excessively verbose andlengthy answers, leading to issues with both conciseness and generation time.To address this, this paper analyzes the impact of output lengths on LLMinference pipelines by introducing and proposing novel metrics to evaluate the\textit{correct conciseness} of a model and related prompting techniques. Then,we examine the impact of controlling output length through a refined promptengineering strategy, Constrained-CoT (CCoT), which encourages the model toproduce more concise outputs. To better understand the effects of such aprompt, we also introduce two additional scores for analyzing the conciseness,measured in terms of redundancy and information flow in generated answers.Experiments on pretrained LLMs and multiple datasets demonstrate the benefitsof the proposed metrics and the effectiveness of CCoT across different models.</description><author>Sania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, Fabrizio Giacomelli</author><pubDate>Thu, 23 Jan 2025 08:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19825v2</guid></item><item><title>Mitigating Unintended Memorization with LoRA in Federated Learning for LLMs</title><link>http://arxiv.org/abs/2502.05087v1</link><description>Federated learning (FL) is a popular paradigm for collaborative trainingwhich avoids direct data exposure between clients. However, data privacy issuesstill remain: FL-trained large language models are capable of memorizing andcompleting phrases and sentences contained in training data when given withtheir prefixes. Thus, it is possible for adversarial and honest-but-curiousclients to recover training data of other participants simply through targetedprompting. In this work, we demonstrate that a popular and simple fine-tuningstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to afactor of 10. We study this effect by performing a medical question-answeringfine-tuning task and injecting multiple replicas of out-of-distributionsensitive sequences drawn from an external clinical dataset. We observe areduction in memorization for a wide variety of Llama 2 and 3 models, and findthat LoRA can reduce memorization in centralized learning as well. Furthermore,we show that LoRA can be combined with other privacy-preserving techniques suchas gradient clipping and Gaussian noising, secure aggregation, and Goldfishloss to further improve record-level privacy while maintaining performance.</description><author>Thierry Bossy, Julien Vignoud, Tahseen Rabbani, Juan R. Troncoso Pastoriza, Martin Jaggi</author><pubDate>Fri, 07 Feb 2025 17:04:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.05087v1</guid></item><item><title>Object-Centric Temporal Consistency via Conditional Autoregressive Inductive Biases</title><link>http://arxiv.org/abs/2410.15728v1</link><description>Unsupervised object-centric learning from videos is a promising approachtowards learning compositional representations that can be applied to variousdownstream tasks, such as prediction and reasoning. Recently, it was shown thatpretrained Vision Transformers (ViTs) can be useful to learn object-centricrepresentations on real-world video datasets. However, while these approachessucceed at extracting objects from the scenes, the slot-based representationsfail to maintain temporal consistency across consecutive frames in a video,i.e. the mapping of objects to slots changes across the video. To address this,we introduce Conditional Autoregressive Slot Attention (CA-SA), a frameworkthat enhances the temporal consistency of extracted object-centricrepresentations in video-centric vision tasks. Leveraging an autoregressiveprior network to condition representations on previous timesteps and a novelconsistency loss function, CA-SA predicts future slot representations andimposes consistency across frames. We present qualitative and quantitativeresults showing that our proposed method outperforms the considered baselineson downstream tasks, such as video prediction and visual question-answeringtasks.</description><author>Cristian Meo, Akihiro Nakano, Mircea Lică, Aniket Didolkar, Masahiro Suzuki, Anirudh Goyal, Mengmi Zhang, Justin Dauwels, Yutaka Matsuo, Yoshua Bengio</author><pubDate>Mon, 21 Oct 2024 07:44:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15728v1</guid></item><item><title>Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens</title><link>http://arxiv.org/abs/2410.14655v2</link><description>Language models are often trained to maximize the likelihood of the nexttoken given past tokens in the training dataset. However, during inferencetime, they are utilized differently, generating text sequentially andauto-regressively by using previously generated tokens as input to predict thenext one. Marginal differences in predictions at each step can cascade oversuccessive steps, resulting in different distributions from what the modelswere trained for and potentially leading to unpredictable behavior. This paperproposes two simple approaches based on model own generation to address thisdiscrepancy between the training and inference time. Our first approach isBatch-Scheduled Sampling, where, during training, we stochastically choosebetween the ground-truth token from the dataset and the model's own generatedtoken as input to predict the next token. This is done in an offline manner,modifying the context window by interleaving ground-truth tokens with thosegenerated by the model. Our second approach is Reference-Answer-basedCorrection, where we explicitly incorporate a self-correction capability intothe model during training. This enables the model to effectively self-correctthe gaps between the generated sequences and the ground truth data withoutrelying on an external oracle model. By incorporating our proposed strategiesduring training, we have observed an overall improvement in performancecompared to baseline methods, as demonstrated by our extensive experimentsusing summarization, general question-answering, and math question-answeringtasks.</description><author>Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhari, Huzefa Rangwala, George Karypis, Rasool Fakoor</author><pubDate>Tue, 21 Jan 2025 18:14:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14655v2</guid></item><item><title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title><link>http://arxiv.org/abs/2502.09604v1</link><description>We introduce SelfCite, a novel self-supervised approach that aligns LLMs togenerate high-quality, fine-grained, sentence-level citations for thestatements in their generated responses. Instead of only relying on costly andlabor-intensive annotations, SelfCite leverages a reward signal provided by theLLM itself through context ablation: If a citation is necessary, removing thecited text from the context should prevent the same response; if sufficient,retaining the cited text alone should preserve the same response. This rewardcan guide the inference-time best-of-N sampling strategy to improve citationquality significantly, as well as be used in preference optimization todirectly fine-tune the models for generating better citations. Theeffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3points on the LongBench-Cite benchmark across five long-form question answeringtasks.</description><author>Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Wen-tau Yih</author><pubDate>Thu, 13 Feb 2025 18:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.09604v1</guid></item><item><title>ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion</title><link>http://arxiv.org/abs/2302.04865v3</link><description>The research community has shown increasing interest in designing intelligentembodied agents that can assist humans in accomplishing tasks. Although therehave been significant advancements in related vision-language benchmarks, mostprior work has focused on building agents that follow instructions rather thanendowing agents the ability to ask questions to actively resolve ambiguitiesarising naturally in embodied environments. To address this gap, we propose anEmbodied Learning-By-Asking (ELBA) model that learns when and what questions toask to dynamically acquire additional information for completing the task. Weevaluate ELBA on the TEACh vision-dialog navigation and task completiondataset. Experimental results show that the proposed method achieves improvedtask performance compared to baseline models without question-answeringcapabilities.</description><author>Ying Shen, Daniel Bis, Cynthia Lu, Ismini Lourentzou</author><pubDate>Wed, 11 Dec 2024 22:55:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04865v3</guid></item><item><title>The representation landscape of few-shot learning and fine-tuning in large language models</title><link>http://arxiv.org/abs/2409.03662v1</link><description>In-context learning (ICL) and supervised fine-tuning (SFT) are two commonstrategies for improving the performance of modern large language models (LLMs)on specific tasks. Despite their different natures, these strategies often leadto comparable performance gains. However, little is known about whether theyinduce similar representations inside LLMs. We approach this problem byanalyzing the probability landscape of their hidden representations in the twocases. More specifically, we compare how LLMs solve the same question-answeringtask, finding that ICL and SFT create very different internal structures, inboth cases undergoing a sharp transition in the middle of the network. In thefirst half of the network, ICL shapes interpretable representationshierarchically organized according to their semantic content. In contrast, theprobability landscape obtained with SFT is fuzzier and semantically mixed. Inthe second half of the model, the fine-tuned representations developprobability modes that better encode the identity of answers, while thelandscape of ICL representations is characterized by less defined peaks. Ourapproach reveals the diverse computational strategies developed inside LLMs tosolve the same task across different conditions, allowing us to make a steptowards designing optimal methods to extract information from language models.</description><author>Diego Doimo, Alessandro Serra, Alessio Ansuini, Alberto Cazzaniga</author><pubDate>Thu, 05 Sep 2024 16:15:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03662v1</guid></item><item><title>CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs</title><link>http://arxiv.org/abs/2411.12713v1</link><description>Large Vision-Language Model (LVLM) systems have demonstrated impressivevision-language reasoning capabilities but suffer from pervasive and severehallucination issues, posing significant risks in critical domains such ashealthcare and autonomous systems. Despite previous efforts to mitigatehallucinations, a persistent issue remains: visual defect from vision-languagemisalignment, creating a bottleneck in visual processing capacity. To addressthis challenge, we develop Complementary Adaptive Token-level ContrastiveDecoding to Mitigate Hallucinations in LVLMs (CATCH), based on the InformationBottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) forvisual information separation, Non-Visual Screening (NVS) for hallucinationdetection, and Adaptive Token-level Contrastive Decoding (ATCD) forhallucination mitigation. CATCH addresses issues related to visual defects thatcause diminished fine-grained feature perception and cumulative hallucinationsin open-ended scenarios. It is applicable to various visual question-answeringtasks without requiring any specific data or prior knowledge, and generalizesrobustly to new tasks without additional training, opening new possibilitiesfor advancing LVLM in various challenging applications.</description><author>Zhehan Kan, Ce Zhang, Zihan Liao, Yapeng Tian, Wenming Yang, Junyuan Xiao, Xu Li, Dongmei Jiang, Yaowei Wang, Qingmin Liao</author><pubDate>Tue, 19 Nov 2024 18:27:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.12713v1</guid></item><item><title>Enhanced Fine-Tuning of Lightweight Domain-Specific Q&amp;A Model Based on Large Language Models</title><link>http://arxiv.org/abs/2408.12247v2</link><description>Large language models (LLMs) excel at general question-answering (Q&amp;A) butoften fall short in specialized domains due to a lack of domain-specificknowledge. Commercial companies face the dual challenges of privacy protectionand resource constraints when involving LLMs for fine-tuning. This paperpropose a novel framework, Self-Evolution, designed to address these issues byleveraging lightweight open-source LLMs through multiple iterative fine-tuningrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolutionemploy a strategy that filters and reinforces the knowledge with higher valueduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chatusing 4,000 documents containing rich domain knowledge from China Mobile,achieving a performance score 174% higher on domain-specific question-answeringevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.Self-Evolution has been deployed in China Mobile's daily operation andmaintenance for 117 days, and it improves the efficiency of locating alarms,fixing problems, and finding related reports, with an average efficiencyimprovement of over 18.6%. In addition, we release Self-Evolution frameworkcode in https://github.com/Zero-Pointer/Self-Evolution.</description><author>Shenglin Zhang, Pengtian Zhu, Minghua Ma, Jiagang Wang, Yongqian Sun, Dongwen Li, Jingyu Wang, Qianying Guo, Xiaolei Hua, Lin Zhu, Dan Pei</author><pubDate>Fri, 23 Aug 2024 01:25:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12247v2</guid></item><item><title>General LLMs as Instructors for Domain-Specific LLMs: A Sequential Fusion Method to Integrate Extraction and Editing</title><link>http://arxiv.org/abs/2403.15736v2</link><description>The substantial interest in updating Large Language Models (LLMs) withoutretraining from scratch is accompanied by several challenges. This isparticularly true when updating LLMs with datasets that necessitatedomain-expert reasoning across extensive texts, despite limited samples. Wetermed the scenario as the Few-Shot Domain-Expert Reasoning for Updating LLMs(FDoR-UL). Traditional methods such as Low-Rank Adaptation (LoRA) and RetrievalAugmented Generation (RAG) are inadequate for addressing this critical issue,particularly evident in our exploration of a specific medical dataset thatepitomizes the distinct needs of FDoR-UL. To tackle this challenge, weintroduce a Sequential Fusion method to integrate knowledge from complexcontexts into LLMs. This method employs a two-stage framework: initiallyleveraging general LLMs to perform relation extraction for knowledgeacquisition from complex texts, followed by updating domain-specific LLMsthrough Knowledge Editing (KE). Employing our method, domain-specific LLMsachieved a 71.7% accuracy (an average gain of 39.1%) in question-answeringtasks. Furthermore, we expanded our evaluation to a novel economics-managementdataset we developed, where our method achieved a 75.0% accuracy (an averagegain of 45.0%). These findings underscore the effectiveness and flexibility ofour approach in FDoR-UL across various domains.</description><author>Xin Zhang, Tianjie Ju, Huijia Liang, Ying Fu, Qin Zhang</author><pubDate>Wed, 13 Nov 2024 14:05:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15736v2</guid></item><item><title>Optimal Design for Human Preference Elicitation</title><link>http://arxiv.org/abs/2404.13895v3</link><description>Learning of preference models from human feedback has been central to recentadvances in artificial intelligence. Motivated by the cost of obtaininghigh-quality human annotations, we study efficient human preference elicitationfor learning preference models. The key idea in our work is to generalizeoptimal designs, a methodology for computing optimal information-gatheringpolicies, to questions with multiple answers, represented as lists of items.The policy is a distribution over lists and we elicit preferences from the listproportionally to its probability. To show the generality of our ideas, westudy both absolute and ranking feedback models on items in the list. We designefficient algorithms for both and analyze them. Finally, we demonstrate thatour algorithms are practical by evaluating them on existing question-answeringproblems.</description><author>Subhojyoti Mukherjee, Anusha Lalitha, Kousha Kalantari, Aniket Deshmukh, Ge Liu, Yifei Ma, Branislav Kveton</author><pubDate>Mon, 04 Nov 2024 18:41:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.13895v3</guid></item><item><title>ELBA: Learning by Asking for Embodied Visual Navigation and Task Completion</title><link>http://arxiv.org/abs/2302.04865v2</link><description>The research community has shown increasing interest in designing intelligentembodied agents that can assist humans in accomplishing tasks. Although therehave been significant advancements in related vision-language benchmarks, mostprior work has focused on building agents that follow instructions rather thanendowing agents the ability to ask questions to actively resolve ambiguitiesarising naturally in embodied environments. To address this gap, we propose anEmbodied Learning-By-Asking (ELBA) model that learns when and what questions toask to dynamically acquire additional information for completing the task. Weevaluate ELBA on the TEACh vision-dialog navigation and task completiondataset. Experimental results show that the proposed method achieves improvedtask performance compared to baseline models without question-answeringcapabilities.</description><author>Ying Shen, Ismini Lourentzou</author><pubDate>Fri, 06 Dec 2024 00:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.04865v2</guid></item><item><title>Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings</title><link>http://arxiv.org/abs/2410.22685v1</link><description>Accurately quantifying uncertainty in large language models (LLMs) is crucialfor their reliable deployment, especially in high-stakes applications. Currentstate-of-the-art methods for measuring semantic uncertainty in LLMs rely onstrict bidirectional entailment criteria between multiple generated responsesand also depend on sequence likelihoods. While effective, these approachesoften overestimate uncertainty due to their sensitivity to minor wordingdifferences, additional correct information, and non-important words in thesequence. We propose a novel approach that leverages semantic embeddings toachieve smoother and more robust estimation of semantic uncertainty in LLMs. Bycapturing semantic similarities without depending on sequence likelihoods, ourmethod inherently reduces any biases introduced by irrelevant words in theanswers. Furthermore, we introduce an amortised version of our approach byexplicitly modelling semantics as latent variables in a joint probabilisticmodel. This allows for uncertainty estimation in the embedding space with asingle forward pass, significantly reducing computational overhead compared toexisting multi-pass methods. Experiments across multiple question-answeringdatasets and frontier LLMs demonstrate that our embedding-based methods providemore accurate and nuanced uncertainty quantification than traditionalapproaches.</description><author>Yashvir S. Grewal, Edwin V. Bonilla, Thang D. Bui</author><pubDate>Wed, 30 Oct 2024 04:41:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.22685v1</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v3</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao</author><pubDate>Tue, 15 Oct 2024 20:55:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v3</guid></item><item><title>Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts</title><link>http://arxiv.org/abs/2411.11479v1</link><description>The rapid advancement of Vision-Language Models (VLMs) has expandedmultimodal applications, yet evaluations often focus on basic tasks like objectrecognition, overlooking abstract aspects such as personalities and values. Toaddress this gap, we introduce Value-Spectrum, a visual question-answeringbenchmark aimed at assessing VLMs based on Schwartz's value dimensions, whichcapture core values guiding people's beliefs and actions across cultures. Weconstructed a vectorized database of over 50,000 short videos sourced fromTikTok, YouTube Shorts, and Instagram Reels, covering multiple months and awide array of topics such as family, health, hobbies, society, and technology.We also developed a VLM agent pipeline to automate video browsing and analysis.Benchmarking representative VLMs on Value-Spectrum reveals significantdifferences in their responses to value-oriented content, with most modelsexhibiting a preference for hedonistic topics. Beyond identifying naturalpreferences, we explored the ability of VLM agents to adopt specific personaswhen explicitly prompted, revealing insights into the models' adaptability inrole-playing scenarios. These findings highlight the potential ofValue-Spectrum as a comprehensive evaluation set for tracking VLM advancementsin value-based tasks and for developing more sophisticated role-playing AIagents.</description><author>Jingxuan Li, Yuning Yang, Shengqi Yang, Yizhou Zhao, Ying Nian Wu</author><pubDate>Mon, 18 Nov 2024 11:31:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.11479v1</guid></item><item><title>V-RoAst: A New Dataset for Visual Road Assessment</title><link>http://arxiv.org/abs/2408.10872v1</link><description>Road traffic crashes cause millions of deaths annually and have a significanteconomic impact, particularly in low- and middle-income countries (LMICs). Thispaper presents an approach using Vision Language Models (VLMs) for road safetyassessment, overcoming the limitations of traditional Convolutional NeuralNetworks (CNNs). We introduce a new task ,V-RoAst (Visual question answeringfor Road Assessment), with a real-world dataset. Our approach optimizes promptengineering and evaluates advanced VLMs, including Gemini-1.5-flash andGPT-4o-mini. The models effectively examine attributes for road assessment.Using crowdsourced imagery from Mapillary, our scalable solution influentiallyestimates road safety levels. In addition, this approach is designed for localstakeholders who lack resources, as it does not require training data. Itoffers a cost-effective and automated methods for global road safetyassessments, potentially saving lives and reducing economic burdens.</description><author>Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Srirrongvikrai, Meihui Wang, James Haworth</author><pubDate>Tue, 20 Aug 2024 14:03:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10872v1</guid></item><item><title>BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering</title><link>http://arxiv.org/abs/2402.11129v2</link><description>Retrieval-augmented Large Language Models (LLMs) offer substantial benefitsin enhancing performance across knowledge-intensive scenarios. However, thesemethods often face challenges with complex inputs and encounter difficultiesdue to noisy knowledge retrieval, notably hindering model effectiveness. Toaddress this issue, we introduce BlendFilter, a novel approach that elevatesretrieval-augmented LLMs by integrating query generation blending withknowledge filtering. BlendFilter proposes the blending process through itsquery generation method, which integrates both external and internal knowledgeaugmentation with the original query, ensuring comprehensive informationgathering. Additionally, our distinctive knowledge filtering module capitalizeson the intrinsic capabilities of the LLM, effectively eliminating extraneousdata. We conduct extensive experiments on three open-domain question answeringbenchmarks, and the findings clearly indicate that our innovative BlendFiltersurpasses state-of-the-art baselines significantly.</description><author>Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao</author><pubDate>Thu, 11 Jul 2024 19:26:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.11129v2</guid></item><item><title>Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks</title><link>http://arxiv.org/abs/2409.07353v1</link><description>Large Vision-Language Models (LVLMs), trained on multimodal big datasets,have significantly advanced AI by excelling in vision-language tasks. However,these models remain vulnerable to adversarial attacks, particularly jailbreakattacks, which bypass safety protocols and cause the model to generatemisleading or harmful responses. This vulnerability stems from both theinherent susceptibilities of LLMs and the expanded attack surface introduced bythe visual modality. We propose Sim-CLIP+, a novel defense mechanism thatadversarially fine-tunes the CLIP vision encoder by leveraging a Siamesearchitecture. This approach maximizes cosine similarity between perturbed andclean samples, facilitating resilience against adversarial manipulations.Sim-CLIP+ offers a plug-and-play solution, allowing seamless integration intoexisting LVLM architectures as a robust vision encoder. Unlike previousdefenses, our method requires no structural modifications to the LVLM andincurs minimal computational overhead. Sim-CLIP+ demonstrates effectivenessagainst both gradient-based adversarial attacks and various jailbreaktechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attackstrategies and perform clean evaluations using standard downstream datasets,including COCO for image captioning and OKVQA for visual question answering.Extensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracywhile substantially improving robustness against both gradient-basedadversarial attacks and jailbreak techniques. Our code and robust visionencoders are available athttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.</description><author>Md Zarif Hossain, Ahmed Imteaj</author><pubDate>Wed, 11 Sep 2024 15:39:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.07353v1</guid></item><item><title>On Bits and Bandits: Quantifying the Regret-Information Trade-off</title><link>http://arxiv.org/abs/2405.16581v3</link><description>In many sequential decision problems, an agent performs a repeated task. Hethen suffers regret and obtains information that he may use in the followingrounds. However, sometimes the agent may also obtain information and avoidsuffering regret by querying external sources. We study the trade-off betweenthe information an agent accumulates and the regret it suffers. We invokeinformation-theoretic methods for obtaining regret lower bounds, that alsoallow us to easily re-derive several known lower bounds. We introduce the firstBayesian regret lower bounds that depend on the information an agentaccumulates. We also prove regret upper bounds using the amount of informationthe agent accumulates. These bounds show that information measured in bits, canbe traded off for regret, measured in reward. Finally, we demonstrate theutility of these bounds in improving the performance of a question-answeringtask with large language models, allowing us to obtain valuable insights.</description><author>Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor</author><pubDate>Mon, 07 Oct 2024 13:12:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.16581v3</guid></item><item><title>RAG based Question-Answering for Contextual Response Prediction System</title><link>http://arxiv.org/abs/2409.03708v2</link><description>Large Language Models (LLMs) have shown versatility in various NaturalLanguage Processing (NLP) tasks, including their potential as effectivequestion-answering systems. However, to provide precise and relevantinformation in response to specific customer queries in industry settings, LLMsrequire access to a comprehensive knowledge base to avoid hallucinations.Retrieval Augmented Generation (RAG) emerges as a promising technique toaddress this challenge. Yet, developing an accurate question-answeringframework for real-world applications using RAG entails several challenges: 1)data availability issues, 2) evaluating the quality of generated content, and3) the costly nature of human evaluation. In this paper, we introduce anend-to-end framework that employs LLMs with RAG capabilities for industry usecases. Given a customer query, the proposed system retrieves relevant knowledgedocuments and leverages them, along with previous chat history, to generateresponse suggestions for customer service agents in the contact centers of amajor retail company. Through comprehensive automated and human evaluations, weshow that this solution outperforms the current BERT-based algorithms inaccuracy and relevance. Our findings suggest that RAG-based LLMs can be anexcellent support to human customer service representatives by lightening theirworkload.</description><author>Sriram Veturi, Saurabh Vaichal, Reshma Lal Jagadheesh, Nafis Irtiza Tripto, Nian Yan</author><pubDate>Fri, 06 Sep 2024 14:18:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03708v2</guid></item><item><title>Internal and External Knowledge Interactive Refinement Framework for Knowledge-Intensive Question Answering</title><link>http://arxiv.org/abs/2408.12979v1</link><description>Recent works have attempted to integrate external knowledge into LLMs toaddress the limitations and potential factual errors in LLM-generated content.However, how to retrieve the correct knowledge from the large amount ofexternal knowledge imposes a challenge. To this end, we empirically observethat LLMs have already encoded rich knowledge in their pretrained parametersand utilizing these internal knowledge improves the retrieval of externalknowledge when applying them to knowledge-intensive tasks. In this paper, wepropose a new internal and external knowledge interactive refinement paradigmdubbed IEKR to utilize internal knowledge in LLM to help retrieve relevantknowledge from the external knowledge base, as well as exploit the externalknowledge to refine the hallucination of generated internal knowledge. Bysimply adding a prompt like 'Tell me something about' to the LLMs, we try toreview related explicit knowledge and insert them with the query into theretriever for external retrieval. The external knowledge is utilized tocomplement the internal knowledge into input of LLM for answers. We conductexperiments on 3 benchmark datasets in knowledge-intensive question answeringtask with different LLMs and domains, achieving the new state-of-the-art.Further analysis shows the effectiveness of different modules in our approach.</description><author>Haowei Du, Dongyan Zhao</author><pubDate>Fri, 23 Aug 2024 10:52:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12979v1</guid></item><item><title>Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision</title><link>http://arxiv.org/abs/2410.08209v1</link><description>Current large multimodal models (LMMs) face challenges in grounding, whichrequires the model to relate language components to visual entities. Contraryto the common practice that fine-tunes LMMs with additional groundingsupervision, we find that the grounding ability can in fact emerge in LMMstrained without explicit grounding supervision. To reveal this emerginggrounding, we introduce an "attend-and-segment" method which leveragesattention maps from standard LMMs to perform pixel-level segmentation.Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMMutilizing a diffusion-based visual encoder, as opposed to the standard CLIPvisual encoder, and trained with the same weak supervision. Without beingconstrained by the biases and limited scale of grounding-specific supervisiondata, our approach is more generalizable and scalable. We achieve competitiveperformance on both grounding-specific and general visual question answeringbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.Notably, we achieve a 44.2 grounding mask recall on grounded conversationgeneration without any grounding supervision, outperforming the extensivelysupervised model GLaMM. Project page: https://groundLMM.github.io.</description><author>Shengcao Cao, Liang-Yan Gui, Yu-Xiong Wang</author><pubDate>Thu, 10 Oct 2024 17:59:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08209v1</guid></item><item><title>RAG based Question-Answering for Contextual Response Prediction System</title><link>http://arxiv.org/abs/2409.03708v1</link><description>Large Language Models (LLMs) have shown versatility in various NaturalLanguage Processing (NLP) tasks, including their potential as effectivequestion-answering systems. However, to provide precise and relevantinformation in response to specific customer queries in industry settings, LLMsrequire access to a comprehensive knowledge base to avoid hallucinations.Retrieval Augmented Generation (RAG) emerges as a promising technique toaddress this challenge. Yet, developing an accurate question-answeringframework for real-world applications using RAG entails several challenges: 1)data availability issues, 2) evaluating the quality of generated content, and3) the costly nature of human evaluation. In this paper, we introduce anend-to-end framework that employs LLMs with RAG capabilities for industry usecases. Given a customer query, the proposed system retrieves relevant knowledgedocuments and leverages them, along with previous chat history, to generateresponse suggestions for customer service agents in the contact centers of amajor retail company. Through comprehensive automated and human evaluations, weshow that this solution outperforms the current BERT-based algorithms inaccuracy and relevance. Our findings suggest that RAG-based LLMs can be anexcellent support to human customer service representatives by lightening theirworkload.</description><author>Sriram Veturi, Saurabh Vaichal, Nafis Irtiza Tripto, Reshma Lal Jagadheesh, Nian Yan</author><pubDate>Thu, 05 Sep 2024 17:14:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03708v1</guid></item><item><title>V-RoAst: A New Dataset for Visual Road Assessment</title><link>http://arxiv.org/abs/2408.10872v2</link><description>Road traffic crashes cause millions of deaths annually and have a significanteconomic impact, particularly in low- and middle-income countries (LMICs). Thispaper presents an approach using Vision Language Models (VLMs) for road safetyassessment, overcoming the limitations of traditional Convolutional NeuralNetworks (CNNs). We introduce a new task ,V-RoAst (Visual question answeringfor Road Assessment), with a real-world dataset. Our approach optimizes promptengineering and evaluates advanced VLMs, including Gemini-1.5-flash andGPT-4o-mini. The models effectively examine attributes for road assessment.Using crowdsourced imagery from Mapillary, our scalable solution influentiallyestimates road safety levels. In addition, this approach is designed for localstakeholders who lack resources, as it does not require training data. Itoffers a cost-effective and automated methods for global road safetyassessments, potentially saving lives and reducing economic burdens.</description><author>Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Srirrongvikrai, Meihui Wang, James Haworth</author><pubDate>Wed, 21 Aug 2024 11:40:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.10872v2</guid></item><item><title>Efficient Multivariate Time Series Anomaly Detection Through Transfer Learning for Large-Scale Web services</title><link>http://arxiv.org/abs/2408.12247v1</link><description>Large language models (LLMs) excel at general question-answering (Q&amp;A) butoften fall short in specialized domains due to a lack of domain-specificknowledge. Commercial companies face the dual challenges of privacy protectionand resource constraints when involving LLMs for fine-tuning. This paperpropose a novel framework, Self-Evolution, designed to address these issues byleveraging lightweight open-source LLMs through multiple iterative fine-tuningrounds. To enhance the efficiency of iterative fine-tuning, Self-Evolutionemploy a strategy that filters and reinforces the knowledge with higher valueduring the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chatusing 4,000 documents containing rich domain knowledge from China Mobile,achieving a performance score 174% higher on domain-specific question-answeringevaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat.Self-Evolution has been deployed in China Mobile's daily operation andmaintenance for 117 days, and it improves the efficiency of locating alarms,fixing problems, and finding related reports, with an average efficiencyimprovement of over 18.6%. In addition, we release Self-Evolution frameworkcode in https://github.com/Zero-Pointer/Self-Evolution.</description><author>Shenglin Zhang, Pengtian Zhu, Minghua Ma, Jiagang Wang, Yongqian Sun, Dongwen Li, Jingyu Wang, Qianying Guo, Xiaolei Hua, Lin Zhu, Dan Pei</author><pubDate>Thu, 22 Aug 2024 09:36:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.12247v1</guid></item><item><title>Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens</title><link>http://arxiv.org/abs/2410.14655v1</link><description>Language models are often trained to maximize the likelihood of the nexttoken given past tokens in the training dataset. However, during inferencetime, they are utilized differently, generating text sequentially andauto-regressively by using previously generated tokens as input to predict thenext one. Marginal differences in predictions at each step can cascade oversuccessive steps, resulting in different distributions from what the modelswere trained for and potentially leading to unpredictable behavior. This paperproposes two simple approaches based on model own generation to address thisdiscrepancy between the training and inference time. Our first approach isBatch-Scheduled Sampling, where, during training, we stochastically choosebetween the ground-truth token from the dataset and the model's own generatedtoken as input to predict the next token. This is done in an offline manner,modifying the context window by interleaving ground-truth tokens with thosegenerated by the model. Our second approach is Reference-Answer-basedCorrection, where we explicitly incorporate a self-correction capability intothe model during training. This enables the model to effectively self-correctthe gaps between the generated sequences and the ground truth data withoutrelying on an external oracle model. By incorporating our proposed strategiesduring training, we have observed an overall improvement in performancecompared to baseline methods, as demonstrated by our extensive experimentsusing summarization, general question-answering, and math question-answeringtasks.</description><author>Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhar, Huzefa Rangwala, George Karypis, Rasool Fakoor</author><pubDate>Fri, 18 Oct 2024 17:48:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.14655v1</guid></item><item><title>Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue</title><link>http://arxiv.org/abs/2406.06399v3</link><description>We study the limitations of Large Language Models (LLMs) for the task ofresponse generation in human-machine dialogue. Several techniques have beenproposed in the literature for different dialogue types (e.g., Open-Domain).However, the evaluations of these techniques have been limited in terms of baseLLMs, dialogue types and evaluation metrics. In this work, we extensivelyanalyze different LLM adaptation techniques when applied to different dialoguetypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialoguetypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.We evaluate the performance of in-context learning and fine-tuning techniquesacross datasets selected for each dialogue type. We assess the impact ofincorporating external knowledge to ground the generation in both scenarios ofRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistentevaluation and explainability criteria for automatic metrics and humanevaluation protocols. Our analysis shows that there is no universalbest-technique for adapting large language models as the efficacy of eachtechnique depends on both the base LLM and the specific type of dialogue. Lastbut not least, the assessment of the best adaptation technique should includehuman evaluation to avoid false expectations and outcomes derived fromautomatic metrics.</description><author>Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi</author><pubDate>Sat, 03 Aug 2024 15:12:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.06399v3</guid></item><item><title>Contextual Object Detection with Multimodal Large Language Models</title><link>http://arxiv.org/abs/2305.18279v2</link><description>Recent Multimodal Large Language Models (MLLMs) are remarkable invision-language tasks, such as image captioning and question answering, butlack the essential perception ability, i.e., object detection. In this work, weaddress this limitation by introducing a novel research problem of contextualobject detection -- understanding visible objects within different human-AIinteractive contexts. Three representative scenarios are investigated,including the language cloze test, visual captioning, and question answering.Moreover, we present ContextDET, a unified multimodal model that is capable ofend-to-end differentiable modeling of visual-language contexts, so as tolocate, identify, and associate visual objects with language inputs forhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visualencoder for extracting visual representations, (ii) a pre-trained LLM formultimodal context decoding, and (iii) a visual decoder for predicting boundingboxes given contextual object words. The new generate-then-detect frameworkenables us to detect object words within human vocabulary. Extensiveexperiments show the advantages of ContextDET on our proposed CODE benchmark,open-vocabulary detection, and referring image segmentation. Github:https://github.com/yuhangzang/ContextDET.</description><author>Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy</author><pubDate>Mon, 12 Aug 2024 07:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18279v2</guid></item><item><title>Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost</title><link>http://arxiv.org/abs/2407.19825v1</link><description>Today's large language models (LLMs) can solve challenging question-answeringtasks, and prompt engineering techniques, such as chain-of-thought (CoT), havegained attention for enhancing the explanation and correctness of outputs.Nevertheless, models require significant time to generate answers augmentedwith lengthy reasoning details. To address this issue, this paper analyzes theimpact of output lengths on LLM inference pipelines and proposes novel metricsto evaluate them in terms of \textit{correct conciseness}. It also examines theimpact of controlling output length through a refined prompt engineeringstrategy, Constrained-CoT (CCoT), which encourages the model to limit outputlength. Experiments on pre-trained LLMs demonstrated the benefit of theproposed metrics and the effectiveness of CCoT across different models. Forinstance, constraining the reasoning of LLaMA2-70b to 100 words improves theaccuracy from 36.01\% (CoT) to 41.07\% (CCoT) on the GSM8K dataset, whilereducing the average output length by 28 words.</description><author>Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, Fabrizio Giacomelli</author><pubDate>Mon, 29 Jul 2024 09:21:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19825v1</guid></item><item><title>UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models</title><link>http://arxiv.org/abs/2407.18391v1</link><description>Smaller-scale Vision-Langauge Models (VLMs) often claim to perform on parwith larger models in general-domain visual grounding and question-answeringbenchmarks while offering advantages in computational efficiency and storage.However, their ability to handle rare objects, which fall into the long tail ofdata distributions, is less understood. To rigorously evaluate this aspect, weintroduce the "Uncontextualized Uncommon Objects" (UOUO) benchmark. Thisbenchmark focuses on systematically testing VLMs with both large and smallparameter counts on rare and specialized objects. Our comprehensive analysisreveals that while smaller VLMs maintain competitive performance on commondatasets, they significantly underperform on tasks involving uncommon objects.We also propose an advanced, scalable pipeline for data collection andcleaning, ensuring the UOUO benchmark provides high-quality, challenginginstances. These findings highlight the need to consider long-taildistributions when assessing the true capabilities of VLMs.</description><author>Xinyu Pi, Mingyuan Wu, Jize Jiang, Haozhen Zheng, Beitong Tian, Chengxiang Zhai, Klara Nahrstedt, Zhiting Hu</author><pubDate>Thu, 25 Jul 2024 20:49:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.18391v1</guid></item><item><title>VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images</title><link>http://arxiv.org/abs/2408.16176v1</link><description>Images are increasingly becoming the currency for documenting biodiversity onthe planet, providing novel opportunities for accelerating scientificdiscoveries in the field of organismal biology, especially with the advent oflarge vision-language models (VLMs). We ask if pre-trained VLMs can aidscientists in answering a range of biologically relevant questions without anyadditional fine-tuning. In this paper, we evaluate the effectiveness of 12state-of-the-art (SOTA) VLMs in the field of organismal biology using a noveldataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K imagesfrom three groups of organisms: fishes, birds, and butterflies, covering fivebiologically relevant tasks. We also explore the effects of applying promptingtechniques and tests for reasoning hallucination on the performance of VLMs,shedding new light on the capabilities of current SOTA VLMs in answeringbiologically relevant questions using images. The code and datasets for runningall the analyses reported in this paper can be found athttps://github.com/sammarfy/VLM4Bio.</description><author>M. Maruf, Arka Daw, Kazi Sajeed Mehrab, Harish Babu Manogaran, Abhilash Neog, Medha Sawhney, Mridul Khurana, James P. Balhoff, Yasin Bakis, Bahadir Altintas, Matthew J. Thompson, Elizabeth G. Campolongo, Josef C. Uyeda, Hilmar Lapp, Henry L. Bart, Paula M. Mabee, Yu Su, Wei-Lun Chao, Charles Stewart, Tanya Berger-Wolf, Wasila Dahdul, Anuj Karpatne</author><pubDate>Wed, 28 Aug 2024 23:53:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.16176v1</guid></item><item><title>The Two-Hop Curse: LLMs trained on A$\rightarrow$B, B$\rightarrow$C fail to learn A$\rightarrow$C</title><link>http://arxiv.org/abs/2411.16353v2</link><description>[Notice: This version is outdated. Recent research contradicts some keyclaims; we are working on a major revision with more nuanced analysis. Pleasewait for the updated version.] While LLMs excel at multi-hop questions (e.g. "Who is the spouse of theperformer of Imagine?") when using chain-of-thought reasoning (CoT), theystruggle when forced to reason internally (without CoT). Previous work on thesize and nature of this gap produced mixed evidence with inconclusive results.In this paper, we introduce a controlled setting for investigating two-hopreasoning in LLMs, where the above-chance performance constitutes undeniableevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instructand GPT-4o) on fictional facts and confirm that they generalize to answeringtwo-hop questions about them using CoT. We find that models can perform latentreasoning when facts appear together during training or in the prompt. However,to our surprise, models completely fail at two-hop reasoning without CoT whenlearned facts only appear in different documents, achieving chance-levelaccuracy and chance-level test loss. We call this complete failure to composeseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontierLLMs on real-world facts, finding that models completely fail at two-hop no-CoTreasoning for over half of question categories while maintaining partialsuccess with CoT across most categories. These results suggest that LLMs lack ageneral capability for latent multi-hop reasoning independent of the questiontype.</description><author>Mikita Balesni, Tomek Korbak, Owain Evans</author><pubDate>Mon, 06 Jan 2025 17:37:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16353v2</guid></item><item><title>The Two-Hop Curse: LLMs trained on A-&gt;B, B-&gt;C fail to learn A--&gt;C</title><link>http://arxiv.org/abs/2411.16353v1</link><description>While LLMs excel at multi-hop questions (e.g. "Who is the spouse of theperformer of Imagine?") when using chain-of-thought reasoning (CoT), theystruggle when forced to reason internally (without CoT). Previous work on thesize and nature of this gap produced mixed evidence with inconclusive results.In this paper, we introduce a controlled setting for investigating two-hopreasoning in LLMs, where the above-chance performance constitutes undeniableevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instructand GPT-4o) on fictional facts and confirm that they generalize to answeringtwo-hop questions about them using CoT. We find that models can perform latentreasoning when facts appear together during training or in the prompt. However,to our surprise, models completely fail at two-hop reasoning without CoT whenlearned facts only appear in different documents, achieving chance-levelaccuracy and chance-level test loss. We call this complete failure to composeseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontierLLMs on real-world facts, finding that models completely fail at two-hop no-CoTreasoning for over half of question categories while maintaining partialsuccess with CoT across most categories. These results suggest that LLMs lack ageneral capability for latent multi-hop reasoning independent of the questiontype.</description><author>Mikita Balesni, Tomek Korbak, Owain Evans</author><pubDate>Mon, 25 Nov 2024 13:04:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16353v1</guid></item><item><title>Context Awareness Gate For Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2411.16133v2</link><description>Retrieval Augmented Generation (RAG) has emerged as a widely adopted approachto mitigate the limitations of large language models (LLMs) in answeringdomain-specific questions. Previous research has predominantly focused onimproving the accuracy and quality of retrieved data chunks to enhance theoverall performance of the generation pipeline. However, despite ongoingadvancements, the critical issue of retrieving irrelevant information -- whichcan impair the ability of the model to utilize its internal knowledgeeffectively -- has received minimal attention. In this work, we investigate theimpact of retrieving irrelevant information in open-domain question answering,highlighting its significant detrimental effect on the quality of LLM outputs.To address this challenge, we propose the Context Awareness Gate (CAG)architecture, a novel mechanism that dynamically adjusts the LLMs' input promptbased on whether the user query necessitates external context retrieval.Additionally, we introduce the Vector Candidates method, a core mathematicalcomponent of CAG that is statistical, LLM-independent, and highly scalable. Wefurther examine the distributions of relationships between contexts andquestions, presenting a statistical analysis of these distributions. Thisanalysis can be leveraged to enhance the context retrieval process in RetrievalAugmented Generation (RAG) systems.</description><author>Mohammad Hassan Heydari, Arshia Hemmat, Erfan Naman, Afsaneh Fatemi</author><pubDate>Mon, 06 Jan 2025 18:23:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16133v2</guid></item><item><title>BudgetFusion: Perceptually-Guided Adaptive Diffusion Models</title><link>http://arxiv.org/abs/2412.05780v3</link><description>Diffusion models have shown unprecedented success in the task oftext-to-image generation. While these models are capable of generatinghigh-quality and realistic images, the complexity of sequential denoising hasraised societal concerns regarding high computational demands and energyconsumption. In response, various efforts have been made to improve inferenceefficiency. However, most of the existing efforts have taken a fixed approachwith neural network simplification or text prompt optimization. Are the qualityimprovements from all denoising computations equally perceivable to humans? Weobserved that images from different text prompts may require differentcomputational efforts given the desired content. The observation motivates usto present BudgetFusion, a novel model that suggests the most perceptuallyefficient number of diffusion steps before a diffusion model starts to generatean image. This is achieved by predicting multi-level perceptual metricsrelative to diffusion steps. With the popular Stable Diffusion as an example,we conduct both numerical analyses and user studies. Our experiments show thatBudgetFusion saves up to five seconds per prompt without compromisingperceptual similarity. We hope this work can initiate efforts toward answeringa core question: how much do humans perceptually gain from images created by agenerative model, per watt of energy?</description><author>Qinchan Li, Kenneth Chen, Changyue Su, Qi Sun</author><pubDate>Mon, 23 Dec 2024 11:42:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05780v3</guid></item><item><title>DataVisT5: A Pre-trained Language Model for Jointly Understanding Text and Data Visualization</title><link>http://arxiv.org/abs/2408.07401v2</link><description>Data visualization (DV) is the fundamental and premise tool to improve theefficiency in conveying the insights behind the big data, which has been widelyaccepted in existing data-driven world. Task automation in DV, such asconverting natural language queries to visualizations (i.e., text-to-vis),generating explanations from visualizations (i.e., vis-to-text), answeringDV-related questions in free form (i.e. FeVisQA), and explicating tabular data(i.e., table-to-text), is vital for advancing the field. Despite theirpotential, the application of pre-trained language models (PLMs) like T5 andBERT in DV has been limited by high costs and challenges in handlingcross-modal information, leading to few studies on PLMs for DV. We introduceDataVisT5, a novel PLM tailored for DV that enhances the T5 architecturethrough a hybrid objective pre-training and multi-task fine-tuning strategy,integrating text and DV datasets to effectively interpret cross-modalsemantics. Extensive evaluations on public datasets show that DataVisT5consistently outperforms current state-of-the-art models on various DV-relatedtasks. We anticipate that DataVisT5 will not only inspire further research onvertical PLMs but also expand the range of applications for PLMs.</description><author>Zhuoyue Wan, Yuanfeng Song, Shuaimin Li, Chen Jason Zhang, Raymond Chi-Wing Wong</author><pubDate>Wed, 27 Nov 2024 17:42:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07401v2</guid></item><item><title>BudgetFusion: Perceptually-Guided Adaptive Diffusion Models</title><link>http://arxiv.org/abs/2412.05780v2</link><description>Diffusion models have shown unprecedented success in the task oftext-to-image generation. While these models are capable of generatinghigh-quality and realistic images, the complexity of sequential denoising hasraised societal concerns regarding high computational demands and energyconsumption. In response, various efforts have been made to improve inferenceefficiency. However, most of the existing efforts have taken a fixed approachwith neural network simplification or text prompt optimization. Are the qualityimprovements from all denoising computations equally perceivable to humans? Weobserved that images from different text prompts may require differentcomputational efforts given the desired content. The observation motivates usto present BudgetFusion, a novel model that suggests the most perceptuallyefficient number of diffusion steps before a diffusion model starts to generatean image. This is achieved by predicting multi-level perceptual metricsrelative to diffusion steps. With the popular Stable Diffusion as an example,we conduct both numerical analyses and user studies. Our experiments show thatBudgetFusion saves up to five seconds per prompt without compromisingperceptual similarity. We hope this work can initiate efforts toward answeringa core question: how much do humans perceptually gain from images created by agenerative model, per watt of energy?</description><author>Qinchan Li, Kenneth Chen, Changyue Su, Qi Sun</author><pubDate>Tue, 10 Dec 2024 15:18:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.05780v2</guid></item><item><title>Context Awareness Gate For Retrieval Augmented Generation</title><link>http://arxiv.org/abs/2411.16133v1</link><description>Retrieval Augmented Generation (RAG) has emerged as a widely adopted approachto mitigate the limitations of large language models (LLMs) in answeringdomain-specific questions. Previous research has predominantly focused onimproving the accuracy and quality of retrieved data chunks to enhance theoverall performance of the generation pipeline. However, despite ongoingadvancements, the critical issue of retrieving irrelevant information -- whichcan impair the ability of the model to utilize its internal knowledgeeffectively -- has received minimal attention. In this work, we investigate theimpact of retrieving irrelevant information in open-domain question answering,highlighting its significant detrimental effect on the quality of LLM outputs.To address this challenge, we propose the Context Awareness Gate (CAG)architecture, a novel mechanism that dynamically adjusts the LLMs' input promptbased on whether the user query necessitates external context retrieval.Additionally, we introduce the Vector Candidates method, a core mathematicalcomponent of CAG that is statistical, LLM-independent, and highly scalable. Wefurther examine the distributions of relationships between contexts andquestions, presenting a statistical analysis of these distributions. Thisanalysis can be leveraged to enhance the context retrieval process in RetrievalAugmented Generation (RAG) systems.</description><author>Mohammad Hassan Heydari, Arshia Hemmat, Erfan Naman, Afsaneh Fatemi</author><pubDate>Mon, 25 Nov 2024 06:48:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16133v1</guid></item><item><title>Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant</title><link>http://arxiv.org/abs/2409.02864v1</link><description>We present a prototype for a Bioinformatics Retrieval Augmentation Data(BRAD) digital assistant. BRAD integrates a suite of tools to handle a widerange of bioinformatics tasks, from code execution to online search. Wedemonstrate BRAD's capabilities through (1) improved question-and-answeringwith retrieval augmented generation (RAG), (2) BRAD's ability to run and writecomplex software pipelines, and (3) BRAD's ability to organize and distributetasks across individual and teams of agents. We use BRAD for automation ofbioinformatics workflows, performing tasks ranging from gene enrichment andsearching the archive to automatic code generation and running biomarkeridentification pipelines. BRAD is a step toward the ultimate goal to develop adigital twin of laboratories driven by self-contained loops for hypothesisgeneration and testing of digital biology experiments.</description><author>Joshua Pickard, Marc Andrew Choi, Natalie Oliven, Cooper Stansbury, Jillian Cwycyshyn, Nicholas Galioto, Alex Gorodetsky, Alvaro Velasquez, Indika Rajapakse</author><pubDate>Wed, 04 Sep 2024 16:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.02864v1</guid></item><item><title>DataVisT5: A Pre-trained Language Model for Jointly Understanding Text and Data Visualization</title><link>http://arxiv.org/abs/2408.07401v1</link><description>Data visualization (DV) is the fundamental and premise tool to improve theefficiency in conveying the insights behind the big data, which has been widelyaccepted in existing data-driven world. Task automation in DV, such asconverting natural language queries to visualizations (i.e., text-to-vis),generating explanations from visualizations (i.e., vis-to-text), answeringDV-related questions in free form (i.e. FeVisQA), and explicating tabular data(i.e., table-to-text), is vital for advancing the field. Despite theirpotential, the application of pre-trained language models (PLMs) like T5 andBERT in DV has been limited by high costs and challenges in handlingcross-modal information, leading to few studies on PLMs for DV. We introduce\textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5architecture through a hybrid objective pre-training and multi-task fine-tuningstrategy, integrating text and DV datasets to effectively interpret cross-modalsemantics. Extensive evaluations on public datasets show that DataVisT5consistently outperforms current state-of-the-art models on various DV-relatedtasks. We anticipate that DataVisT5 will not only inspire further research onvertical PLMs but also expand the range of applications for PLMs.</description><author>Zhuoyue Wan, Yuanfeng Song, Shuaimin Li, Chen Jason Zhang, Raymond Chi-Wing Wong</author><pubDate>Wed, 14 Aug 2024 09:20:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.07401v1</guid></item><item><title>On the robustness of ChatGPT in teaching Korean Mathematics</title><link>http://arxiv.org/abs/2502.11915v1</link><description>ChatGPT, an Artificial Intelligence model, has the potential to revolutionizeeducation. However, its effectiveness in solving non-English questions remainsuncertain. This study evaluates ChatGPT's robustness using 586 Koreanmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering391 out of 586 questions. We also assess its ability to rate mathematicsquestions based on eleven criteria and perform a topic analysis. Our findingsshow that ChatGPT's ratings align with educational theory and test-takerperspectives. While ChatGPT performs well in question classification, itstruggles with non-English contexts, highlighting areas for improvement. Futureresearch should address linguistic biases and enhance accuracy across diverselanguages. Domain-specific optimizations and multilingual training couldimprove ChatGPT's role in personalized education.</description><author>Phuong-Nam Nguyen, Quang Nguyen-The, An Vu-Minh, Diep-Anh Nguyen, Xuan-Lam Pham</author><pubDate>Mon, 17 Feb 2025 15:31:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.11915v1</guid></item><item><title>InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models</title><link>http://arxiv.org/abs/2404.07940v3</link><description>Large Language Models for code (code LLMs) have witnessed tremendous progressin recent years. With the rapid development of code LLMs, many popularevaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged tomeasure the performance of code LLMs with a particular focus on code generationtasks. However, they are insufficient to cover the full range of expectedcapabilities of code LLMs, which span beyond code generation to answeringdiverse coding-related questions. To fill this gap, we propose InfiBench, thefirst large-scale freeform question-answering (QA) benchmark for code to ourknowledge, comprising 234 carefully selected high-quality Stack Overflowquestions that span across 15 programming languages. InfiBench uses four typesof model-free automatic metrics to evaluate response correctness where domainexperts carefully concretize the criterion for each question. We conduct asystematic evaluation for over 100 latest code LLMs on InfiBench, leading to aseries of novel and insightful findings. Our detailed analyses showcasepotential directions for further advancement of code LLMs. InfiBench is fullyopen source at https://infi-coder.github.io/infibench and continuouslyexpanding to foster more scientific and systematic practices for code LLMevaluation.</description><author>Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, Hongxia Yang</author><pubDate>Thu, 14 Nov 2024 11:51:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.07940v3</guid></item><item><title>Thinking LLMs: General Instruction Following with Thought Generation</title><link>http://arxiv.org/abs/2410.10630v1</link><description>LLMs are typically trained to answer user questions or follow instructionssimilarly to how human experts respond. However, in the standard alignmentframework they lack the basic ability of explicit thinking before answering.Thinking is important for complex questions that require reasoning and planning-- but can be applied to any task. We propose a training method for equippingexisting LLMs with such thinking abilities for general instruction followingwithout use of additional human data. We achieve this by an iterative searchand optimization procedure that explores the space of possible thoughtgenerations, allowing the model to learn how to think without directsupervision. For each instruction, the thought candidates are scored using ajudge model to evaluate their responses only, and then optimized via preferenceoptimization. We show that this procedure leads to superior performance onAlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoningcategories such as marketing, health and general knowledge, in addition to moretraditional reasoning &amp; problem-solving tasks.</description><author>Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar</author><pubDate>Mon, 14 Oct 2024 15:38:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.10630v1</guid></item><item><title>EchoSight: Advancing Visual-Language Models with Wiki Knowledge</title><link>http://arxiv.org/abs/2407.12735v3</link><description>Knowledge-based Visual Question Answering (KVQA) tasks require answeringquestions about images using extensive background knowledge. Despitesignificant advancements, generative models often struggle with these tasks dueto the limited integration of external knowledge. In this paper, we introduceEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) frameworkthat enables large language models (LLMs) to answer visual questions requiringfine-grained encyclopedic knowledge. To strive for high-performing retrieval,EchoSight first searches wiki articles by using visual-only information,subsequently, these candidate articles are further reranked according to theirrelevance to the combined text-image query. This approach significantlyimproves the integration of multimodal knowledge, leading to enhanced retrievaloutcomes and more accurate VQA responses. Our experimental results on theEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishesnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of41.8% on Encyclopedic VQA and 31.3% on InfoSeek.</description><author>Yibin Yan, Weidi Xie</author><pubDate>Tue, 05 Nov 2024 07:24:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12735v3</guid></item><item><title>EchoSight: Advancing Visual-Language Models with Wiki Knowledge</title><link>http://arxiv.org/abs/2407.12735v1</link><description>Knowledge-based Visual Question Answering (KVQA) tasks require answeringquestions about images using extensive background knowledge. Despitesignificant advancements, generative models often struggle with these tasks dueto the limited integration of external knowledge. In this paper, we introduceEchoSight, a novel multimodal Retrieval-Augmented Generation (RAG) frameworkthat enables large language models (LLMs) to answer visual questions requiringfine-grained encyclopedic knowledge. To strive for high-performing retrieval,EchoSight first searches wiki articles by using visual-only information,subsequently, these candidate articles are further reranked according to theirrelevance to the combined text-image query. This approach significantlyimproves the integration of multimodal knowledge, leading to enhanced retrievaloutcomes and more accurate VQA responses. Our experimental results on theEncyclopedic VQA and InfoSeek datasets demonstrate that EchoSight establishesnew state-of-the-art results in knowledge-based VQA, achieving an accuracy of41.8% on Encyclopedic VQA and 31.3% on InfoSeek.</description><author>Yibin Yan, Weidi Xie</author><pubDate>Wed, 17 Jul 2024 16:55:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12735v1</guid></item><item><title>SCITAT: A Question Answering Benchmark for Scientific Tables and Text Covering Diverse Reasoning Types</title><link>http://arxiv.org/abs/2412.11757v1</link><description>Scientific question answering (SQA) is an important task aimed at answeringquestions based on papers. However, current SQA datasets have limited reasoningtypes and neglect the relevance between tables and text, creating a significantgap with real scenarios. To address these challenges, we propose a QA benchmarkfor scientific tables and text with diverse reasoning types (SciTaT). To covermore reasoning types, we summarize various reasoning types from real-worldquestions. To involve both tables and text, we require the questions toincorporate tables and text as much as possible. Based on SciTaT, we propose astrong baseline (CaR), which combines various reasoning methods to addressdifferent reasoning types and process tables and text at the same time. CaRbrings average improvements of 12.9% over other baselines on SciTaT, validatingits effectiveness. Error analysis reveals the challenges of SciTaT, such ascomplex numerical calculations and domain knowledge.</description><author>Xuanliang Zhang, Dingzirui Wang, Baoxin Wang, Longxu Dou, Xinyuan Lu, Keyan Xu, Dayong Wu, Qingfu Zhu, Wanxiang Che</author><pubDate>Mon, 16 Dec 2024 13:21:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.11757v1</guid></item><item><title>Goal-Oriented Semantic Communication for Wireless Visual Question Answering</title><link>http://arxiv.org/abs/2411.02452v2</link><description>The rapid progress of artificial intelligence (AI) and computer vision (CV)has facilitated the development of computation-intensive applications likeVisual Question Answering (VQA), which integrates visual perception and naturallanguage processing to generate answers. To overcome the limitations oftraditional VQA constrained by local computation resources, edge computing hasbeen incorporated to provide extra computation capability at the edge side.Meanwhile, this brings new communication challenges between the local and edge,including limited bandwidth, channel noise, and multipath effects, whichdegrade VQA performance and user quality of experience (QoE), particularlyduring the transmission of large high-resolution images. To overcome thesebottlenecks, we propose a goal-oriented semantic communication (GSC) frameworkthat focuses on effectively extracting and transmitting semantic informationmost relevant to the VQA goals, improving the answering accuracy and enhancingthe effectiveness and efficiency. The objective is to maximize the answeringaccuracy, and we propose a bounding box (BBox)-based image semantic extractionand ranking approach to prioritize the semantic information based on the goalof questions. We then extend it by incorporating a scene graphs (SG)-basedapproach to handle questions with complex relationships. Experimental resultsdemonstrate that our GSC framework improves answering accuracy by up to 49%under AWGN channels and 59% under Rayleigh channels while reducing totallatency by up to 65% compared to traditional bit-oriented transmission.</description><author>Sige Liu, Nan Li, Yansha Deng, Tony Q. S. Quek</author><pubDate>Wed, 27 Nov 2024 11:52:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02452v2</guid></item><item><title>ErgoChat: a Visual Query System for the Ergonomic Risk Assessment of Construction Workers</title><link>http://arxiv.org/abs/2412.19954v1</link><description>In the construction sector, workers often endure prolonged periods ofhigh-intensity physical work and prolonged use of tools, resulting in injuriesand illnesses primarily linked to postural ergonomic risks, a longstandingpredominant health concern. To mitigate these risks, researchers have appliedvarious technological methods to identify the ergonomic risks that constructionworkers face. However, traditional ergonomic risk assessment (ERA) techniquesdo not offer interactive feedback. The rapidly developing vision-languagemodels (VLMs), capable of generating textual descriptions or answeringquestions about ergonomic risks based on image inputs, have not yet receivedwidespread attention. This research introduces an interactive visual querysystem tailored to assess the postural ergonomic risks of construction workers.The system's capabilities include visual question answering (VQA), whichresponds to visual queries regarding workers' exposure to postural ergonomicrisks, and image captioning (IC), which generates textual descriptions of theserisks from images. Additionally, this study proposes a dataset designed fortraining and testing such methodologies. Systematic testing indicates that theVQA functionality delivers an accuracy of 96.5%. Moreover, evaluations usingnine metrics for IC and assessments from human experts indicate that theproposed approach surpasses the performance of a method using the samearchitecture trained solely on generic datasets. This study sets a newdirection for future developments in interactive ERA using generativeartificial intelligence (AI) technologies.</description><author>Chao Fan, Qipei Mei, Xiaonan Wang, Xinming Li</author><pubDate>Fri, 27 Dec 2024 23:25:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19954v1</guid></item><item><title>Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method</title><link>http://arxiv.org/abs/2410.16788v1</link><description>Multi-Span Question Answering (MSQA) requires models to extract one ormultiple answer spans from a given context to answer a question. Prior workmainly focuses on designing specific methods or applying heuristic strategiesto encourage models to predict more correct predictions. However, these modelsare trained on gold answers and fail to consider the incorrect predictions.Through a statistical analysis, we observe that models with stronger abilitiesdo not predict less incorrect predictions compared with other models. In thiswork, we propose Answering-Classifying-Correcting (ACC) framework, whichemploys a post-processing strategy to handle incorrect predictions.Specifically, the ACC framework first introduces a classifier to classify thepredictions into three types and exclude "wrong predictions", then introduces acorrector to modify "partially correct predictions". Experiments on severalMSQA datasets show that ACC framework significantly improves the Exact Match(EM) scores, and further analysis demostrates that ACC framework efficientlyreduces the number of incorrect predictions, improving the quality ofpredictions.</description><author>Jiayi Lin, Chenyang Zhang, Haibo Tong, Dongyu Zhang, Qingqing Hong, Bingxuan Hou, Junli Wang</author><pubDate>Tue, 22 Oct 2024 08:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16788v1</guid></item><item><title>KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations</title><link>http://arxiv.org/abs/2403.01469v3</link><description>We present KorMedMCQA, the first Korean Medical Multiple-Choice QuestionAnswering benchmark, derived from professional healthcare licensingexaminations conducted in Korea between 2012 and 2024. The dataset contains7,469 questions from examinations for doctor, nurse, pharmacist, and dentist,covering a wide range of medical disciplines. We evaluate the performance of 59large language models, spanning proprietary and open-source models,multilingual and Korean-specialized models, and those fine-tuned for clinicalapplications. Our results show that applying Chain of Thought (CoT) reasoningcan enhance the model performance by up to 4.5% compared to direct answeringapproaches. We also investigate whether MedQA, one of the most widely usedmedical benchmarks derived from the U.S. Medical Licensing Examination, canserve as a reliable proxy for evaluating model performance in other regions-inthis case, Korea. Our correlation analysis between model scores on KorMedMCQAand MedQA reveals that these two benchmarks align no better than benchmarksfrom entirely different domains (e.g., MedQA and MMLU-Pro). This findingunderscores the substantial linguistic and clinical differences between Koreanand U.S. medical contexts, reinforcing the need for region-specific medical QAbenchmarks. To support ongoing research in Korean healthcare AI, we publiclyrelease the KorMedMCQA via Huggingface.</description><author>Sunjun Kweon, Byungjin Choi, Gyouk Chu, Junyeong Song, Daeun Hyeon, Sujin Gan, Jueon Kim, Minkyu Kim, Rae Woong Park, Edward Choi</author><pubDate>Mon, 09 Dec 2024 06:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01469v3</guid></item><item><title>A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting</title><link>http://arxiv.org/abs/2407.11638v1</link><description>Recently, Large Language Models (LLMs) have demonstrated great potential invarious data mining tasks, such as knowledge question answering, mathematicalreasoning, and commonsense reasoning. However, the reasoning capability of LLMson temporal event forecasting has been under-explored. To systematicallyinvestigate their abilities in temporal event forecasting, we conduct acomprehensive evaluation of LLM-based methods for temporal event forecasting.Due to the lack of a high-quality dataset that involves both graph and textualdata, we first construct a benchmark dataset, named MidEast-TE-mini. Based onthis dataset, we design a series of baseline methods, characterized by variousinput formats and retrieval augmented generation(RAG) modules. From extensiveexperiments, we find that directly integrating raw texts into the input of LLMsdoes not enhance zero-shot extrapolation performance. In contrast,incorporating raw texts in specific complex events and fine-tuning LLMssignificantly improves performance. Moreover, enhanced with retrieval modules,LLM can effectively capture temporal relational patterns hidden in historicalevents. Meanwhile, issues such as popularity bias and the long-tail problemstill persist in LLMs, particularly in the RAG-based method. These findings notonly deepen our understanding of LLM-based event forecasting methods but alsohighlight several promising research directions.We consider that thiscomprehensive evaluation, along with the identified research opportunities,will significantly contribute to future research on temporal event forecastingthrough LLMs.</description><author>He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua</author><pubDate>Tue, 16 Jul 2024 11:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11638v1</guid></item><item><title>Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector</title><link>http://arxiv.org/abs/2402.03094v3</link><description>This paper studies the challenging cross-domain few-shot object detection(CD-FSOD), aiming to develop an accurate object detector for novel domains withminimal labeled examples. While transformer-based open-set detectors, such asDE-ViT, show promise in traditional few-shot object detection, theirgeneralization to CD-FSOD remains unclear: 1) can such open-set detectionmethods easily generalize to CD-FSOD? 2) If not, how can models be enhancedwhen facing huge domain gaps? To answer the first question, we employ measuresincluding style, inter-class variance (ICV), and indefinable boundaries (IB) tounderstand the domain gap. Based on these measures, we establish a newbenchmark named CD-FSOD to evaluate object detection methods, revealing thatmost of the current approaches fail to generalize across domains. Technically,we observe that the performance decline is associated with our proposedmeasures: style, ICV, and IB. Consequently, we propose several novel modules toaddress these issues. First, the learnable instance features align initialfixed instances with target categories, enhancing feature distinctiveness.Second, the instance reweighting module assigns higher importance tohigh-quality instances with slight IB. Third, the domain prompter encouragesfeatures resilient to different styles by synthesizing imaginary domainswithout altering semantic contents. These techniques collectively contribute tothe development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),significantly improving upon the base DE-ViT. Experimental results validate theefficacy of our model.</description><author>Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang</author><pubDate>Tue, 16 Jul 2024 12:44:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03094v3</guid></item><item><title>SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading</title><link>http://arxiv.org/abs/2406.10421v2</link><description>With the rapid development of Large Language Models (LLMs), it is crucial tohave benchmarks which can evaluate the ability of LLMs on different domains.One common use of LLMs is performing tasks on scientific topics, such aswriting algorithms, querying databases or giving mathematical proofs. Inspiredby the way university students are evaluated on such tasks, in this paper, wepropose SciEx - a benchmark consisting of university computer science examquestions, to evaluate LLMs ability on solving scientific tasks. SciEx is (1)multilingual, containing both English and German exams, and (2) multi-modal,containing questions that involve images, and (3) contains various types offreeform questions with different difficulty levels, due to the nature ofuniversity exams. We evaluate the performance of various state-of-the-art LLMson our new benchmark. Since SciEx questions are freeform, it is notstraightforward to evaluate LLM performance. Therefore, we provide human expertgrading of the LLM outputs on SciEx. We show that the free-form exams in SciExremain challenging for the current LLMs, where the best LLM only achieves59.4\% exam grade on average. We also provide detailed comparisons between LLMperformance and student performance on SciEx. To enable future evaluation ofnew LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx.Our experiments show that, although they do not perform perfectly on solvingthe exams, LLMs are decent as graders, achieving 0.948 Pearson correlation withexpert grading.</description><author>Tu Anh Dinh, Carlos Mullov, Leonard Bärmann, Zhaolin Li, Danni Liu, Simon Reiß, Jueun Lee, Nathan Lerzer, Fabian Ternava, Jianfeng Gao, Tobias Röddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Böhm, Jan Niehues</author><pubDate>Fri, 12 Jul 2024 10:17:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.10421v2</guid></item><item><title>Video-Language Alignment Pre-training via Spatio-Temporal Graph Transformer</title><link>http://arxiv.org/abs/2407.11677v1</link><description>Video-language alignment is a crucial multi-modal task that benefits variousdownstream applications, e.g., video-text retrieval and video questionanswering. Existing methods either utilize multi-modal information invideo-text pairs or apply global and local alignment techniques to promotealignment precision. However, these methods often fail to fully explore thespatio-temporal relationships among vision tokens within video and acrossdifferent video-text pairs. In this paper, we propose a novel Spatio-TemporalGraph Transformer module to uniformly learn spatial and temporal contexts forvideo-language alignment pre-training (dubbed STGT). Specifically, our STGTcombines spatio-temporal graph structure information with attention intransformer block, effectively utilizing the spatio-temporal contexts. In thisway, we can model the relationships between vision tokens, promoting video-textalignment precision for benefiting downstream tasks. In addition, we propose aself-similarity alignment loss to explore the inherent self-similarity in thevideo and text. With the initial optimization achieved by contrastive learning,it can further promote the alignment accuracy between video and text.Experimental results on challenging downstream tasks, including video-textretrieval and video question answering, verify the superior performance of ourmethod.</description><author>Shi-Xue Zhang, Hongfa Wang, Xiaobin Zhu, Weibo Gu, Tianjin Zhang, Chun Yang, Wei Liu, Xu-Cheng Yin</author><pubDate>Tue, 16 Jul 2024 12:52:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11677v1</guid></item><item><title>Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</title><link>http://arxiv.org/abs/2407.10151v2</link><description>Multi-object tracking (MOT) endeavors to precisely estimate the positions andidentities of multiple objects over time. The prevailing approach,tracking-by-detection (TbD), first detects objects and then links detections,resulting in a simple yet effective method. However, contemporary detectors mayoccasionally miss some objects in certain frames, causing trackers to ceasetracking prematurely. To tackle this issue, we propose BUSCA, meaning `tosearch', a versatile framework compatible with any online TbD system, enhancingits ability to persistently track those objects missed by the detector,primarily due to occlusions. Remarkably, this is accomplished without modifyingpast tracking results or accessing future frames, i.e., in a fully onlinemanner. BUSCA generates proposals based on neighboring tracks, motion, andlearned tokens. Utilizing a decision Transformer that integrates multimodalvisual and spatiotemporal information, it addresses the object-proposalassociation as a multi-choice question-answering task. BUSCA is trainedindependently of the underlying tracker, solely on synthetic data, withoutrequiring fine-tuning. Through BUSCA, we showcase consistent performanceenhancements across five different trackers and establish a newstate-of-the-art baseline across three different benchmarks. Code available at:https://github.com/lorenzovaquero/BUSCA.</description><author>Lorenzo Vaquero, Yihong Xu, Xavier Alameda-Pineda, Victor M. Brea, Manuel Mucientes</author><pubDate>Tue, 16 Jul 2024 14:19:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10151v2</guid></item><item><title>Inference Optimization of Foundation Models on AI Accelerators</title><link>http://arxiv.org/abs/2407.09111v1</link><description>Powerful foundation models, including large language models (LLMs), withTransformer architectures have ushered in a new era of Generative AI acrossvarious industries. Industry and research community have witnessed a largenumber of new applications, based on those foundation models. Such applicationsinclude question and answer, customer services, image and video generation, andcode completions, among others. However, as the number of model parametersreaches to hundreds of billions, their deployment incurs prohibitive inferencecosts and high latency in real-world scenarios. As a result, the demand forcost-effective and fast inference using AI accelerators is ever more higher. Tothis end, our tutorial offers a comprehensive discussion on complementaryinference optimization techniques using AI accelerators. Beginning with anoverview of basic Transformer architectures and deep learning systemframeworks, we deep dive into system optimization techniques for fast andmemory-efficient attention computations and discuss how they can be implementedefficiently on AI accelerators. Next, we describe architectural elements thatare key for fast transformer inference. Finally, we examine various modelcompression and fast decoding strategies in the same context.</description><author>Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis</author><pubDate>Fri, 12 Jul 2024 09:24:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09111v1</guid></item><item><title>ConvNLP: Image-based AI Text Detection</title><link>http://arxiv.org/abs/2407.07225v1</link><description>The potentials of Generative-AI technologies like Large Language models(LLMs) to revolutionize education are undermined by ethical considerationsaround their misuse which worsens the problem of academic dishonesty. LLMs likeGPT-4 and Llama 2 are becoming increasingly powerful in generatingsophisticated content and answering questions, from writing academic essays tosolving complex math problems. Students are relying on these LLMs to completetheir assignments and thus compromising academic integrity. Solutions to detectLLM-generated text are compute-intensive and often lack generalization. Thispaper presents a novel approach for detecting LLM-generated AI-text using avisual representation of word embedding. We have formulated a novelConvolutional Neural Network called ZigZag ResNet, as well as a scheduler forimproving generalization, named ZigZag Scheduler. Through extensive evaluationusing datasets of text generated by six different state-of-the-art LLMs, ourmodel demonstrates strong intra-domain and inter-domain generalizationcapabilities. Our best model detects AI-generated text with an impressiveaverage detection rate (over inter- and intra-domain test data) of 88.35%.Through an exhaustive ablation study, our ZigZag ResNet and ZigZag Schedulerprovide a performance improvement of nearly 4% over the vanilla ResNet. Theend-to-end inference latency of our model is below 2.5ms per sentence. Oursolution offers a lightweight, computationally efficient, and fasteralternative to existing tools for AI-generated text detection, with bettergeneralization performance. It can help academic institutions in their fightagainst the misuse of LLMs in academic settings. Through this work, we aim tocontribute to safeguarding the principles of academic integrity and ensuringthe trustworthiness of student work in the era of advanced LLMs.</description><author>Suriya Prakash Jambunathan, Ashwath Shankarnarayan, Parijat Dube</author><pubDate>Tue, 09 Jul 2024 20:44:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07225v1</guid></item><item><title>Towards a Benchmark for Causal Business Process Reasoning with LLMs</title><link>http://arxiv.org/abs/2406.05506v2</link><description>Large Language Models (LLMs) are increasingly used for boostingorganizational efficiency and automating tasks. While not originally designedfor complex cognitive processes, recent efforts have further extended to employLLMs in activities such as reasoning, planning, and decision-making. Inbusiness processes, such abilities could be invaluable for leveraging on themassive corpora LLMs have been trained on for gaining deep understanding ofsuch processes. In this work, we plant the seeds for the development of abenchmark to assess the ability of LLMs to reason about causal and processperspectives of business operations. We refer to this view asCausally-augmented Business Processes (BP^C). The core of the benchmarkcomprises a set of BP^C related situations, a set of questions about thesesituations, and a set of deductive rules employed to systematically resolve theground truth answers to these questions. Also with the power of LLMs, the seedis then instantiated into a larger-scale set of domain-specific situations andquestions. Reasoning on BP^C is of crucial importance for process interventionsand process improvement. Our benchmark, accessible athttps://huggingface.co/datasets/ibm/BPC, can be used in one of two possiblemodalities: testing the performance of any target LLM and training an LLM toadvance its capability to reason about BP^C.</description><author>Fabiana Fournier, Lior Limonad, Inna Skarbovsky</author><pubDate>Tue, 16 Jul 2024 15:48:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.05506v2</guid></item><item><title>Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering</title><link>http://arxiv.org/abs/2407.11930v1</link><description>Long-form question answering (LFQA) aims to provide thorough and in-depthanswers to complex questions, enhancing comprehension. However, such detailedresponses are prone to hallucinations and factual inconsistencies, challengingtheir faithful evaluation. This work introduces HaluQuestQA, the firsthallucination dataset with localized error annotations for human-written andmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7kspan-level error annotations for five different error types by expertannotators, along with preference judgments. Using our collected data, wethoroughly analyze the shortcomings of long-form answers and find that theylack comprehensiveness and provide unhelpful references. We train an automaticfeedback model on this dataset that predicts error spans with incompleteinformation and provides associated explanations. Finally, we propose aprompt-based approach, Error-informed refinement, that uses signals from thelearned feedback model to refine generated answers, which we show reduceshallucination and improves answer quality. Furthermore, humans find answersgenerated by our approach comprehensive and highly prefer them (84%) over thebaseline answers.</description><author>Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych</author><pubDate>Tue, 16 Jul 2024 17:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11930v1</guid></item><item><title>Semi-Supervised Learning for Deep Causal Generative Models</title><link>http://arxiv.org/abs/2403.18717v2</link><description>Developing models that are capable of answering questions of the form "Howwould x change if y had been z?'" is fundamental to advancing medical imageanalysis. Training causal generative models that address such counterfactualquestions, though, currently requires that all relevant variables have beenobserved and that the corresponding labels are available in the training data.However, clinical data may not have complete records for all patients and stateof the art causal generative models are unable to take full advantage of this.We thus develop, for the first time, a semi-supervised deep causal generativemodel that exploits the causal relationships between variables to maximise theuse of all available data. We explore this in the setting where each sample iseither fully labelled or fully unlabelled, as well as the more clinicallyrealistic case of having different labels missing for each sample. We leveragetechniques from causal inference to infer missing values and subsequentlygenerate realistic counterfactuals, even for samples with incomplete labels.</description><author>Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas</author><pubDate>Fri, 12 Jul 2024 14:13:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.18717v2</guid></item><item><title>sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting</title><link>http://arxiv.org/abs/2407.09879v2</link><description>Despite the remarkable success of LLMs in English, there is a significant gapin performance in non-English languages. In order to address this, we introducea novel recipe for creating a multilingual synthetic instruction tuningdataset, sPhinX, which is created by selectively translating instructionresponse pairs from English into 50 languages. We test the effectiveness ofsPhinX by using it to fine-tune two state-of-the-art models, Phi-3-small andMistral-7B and then evaluating them across a comprehensive suite ofmultilingual benchmarks that test reasoning, question answering, and readingcomprehension. Our results show that Phi-3-small and Mistral-7B fine-tuned withsPhinX perform better on an average by 4.2%pt and 5%pt respectively as comparedto the baselines. We also devise a strategy to incorporate N-shot examples ineach fine-tuning sample which further boosts the performance of these models by3%pt and 10%pt respectively. Additionally, sPhinX also outperforms othermultilingual instruction tuning datasets on the same benchmarks along withbeing sample efficient and diverse, thereby reducing dataset creation costs.Additionally, instruction tuning with sPhinX does not lead to regression onmost standard LLM benchmarks.</description><author>Sanchit Ahuja, Kumar Tanmay, Hardik Hansrajbhai Chauhan, Barun Patra, Kriti Aggarwal, Luciano Del Corro, Arindam Mitra, Tejas Indulal Dhamecha, Ahmed Awadallah, Monojit Choudhary, Vishrav Chaudhary, Sunayana Sitaram</author><pubDate>Tue, 16 Jul 2024 17:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09879v2</guid></item><item><title>Backpropagation through space, time, and the brain</title><link>http://arxiv.org/abs/2403.16933v2</link><description>How physical networks of neurons, bound by spatio-temporal localityconstraints, can perform efficient credit assignment, remains, to a largeextent, an open question. In machine learning, the answer is almost universallygiven by the error backpropagation algorithm, through both space and time.However, this algorithm is well-known to rely on biologically implausibleassumptions, in particular with respect to spatio-temporal (non-)locality.Alternative forward-propagation models such as real-time recurrent learningonly partially solve the locality problem, but only at the cost of scaling, dueto prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational frameworkfor fully local spatio-temporal credit assignment in physical, dynamicalnetworks of neurons. We start by defining an energy based on neuron-localmismatches, from which we derive both neuronal dynamics via stationarity andparameter dynamics via gradient descent. The resulting dynamics can beinterpreted as a real-time, biologically plausible approximation ofbackpropagation through space and time in deep cortical networks withcontinuous-time neuronal dynamics and continuously active, local synapticplasticity. In particular, GLE exploits the morphology of dendritic trees toenable more complex information storage and processing in single neurons, aswell as the ability of biological neurons to phase-shift their output rate withrespect to their membrane potential, which is essential in both directions ofinformation propagation. For the forward computation, it enables the mapping oftime-continuous inputs to neuronal space, effectively performing aspatio-temporal convolution. For the backward computation, it permits thetemporal inversion of feedback signals, which consequently approximate theadjoint variables necessary for useful parameter updates.</description><author>Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici</author><pubDate>Tue, 16 Jul 2024 17:37:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.16933v2</guid></item><item><title>DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection</title><link>http://arxiv.org/abs/2407.09283v1</link><description>Semantic role labeling (SRL) enriches many downstream applications, e.g.,machine translation, question answering, summarization, and stance/beliefdetection. However, building multilingual SRL models is challenging due to thescarcity of semantically annotated corpora for multiple languages. Moreover,state-of-the-art SRL projection (XSRL) based on large language models (LLMs)yields output that is riddled with spurious role labels. Remediation of suchhallucinations is not straightforward due to the lack of explainability ofLLMs. We show that hallucinated role labels are related to naturally occurringdivergence types that interfere with initial alignments. We implementDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraginglinguistically-informed alignment remediation followed by greedy First-ComeFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRLprojection without additional transformer-based machinery, beating XSRL in bothhuman and automatic comparisons, and advancing beyond headwords to accommodatephrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as ourground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt ourapproach to other language pairs (e.g., English-Tagalog).</description><author>Sangpil Youm, Brodie Mather, Chathuri Jayaweera, Juliana Prada, Bonnie Dorr</author><pubDate>Fri, 12 Jul 2024 14:13:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09283v1</guid></item><item><title>FlowLearn: Evaluating Large Vision-Language Models on Flowchart Understanding</title><link>http://arxiv.org/abs/2407.05183v2</link><description>Flowcharts are graphical tools for representing complex concepts in concisevisual representations. This paper introduces the FlowLearn dataset, a resourcetailored to enhance the understanding of flowcharts. FlowLearn contains complexscientific flowcharts and simulated flowcharts. The scientific subset contains3,858 flowcharts sourced from scientific literature and the simulated subsetcontains 10,000 flowcharts created using a customizable script. The dataset isenriched with annotations for visual components, OCR, Mermaid coderepresentation, and VQA question-answer pairs. Despite the proven capabilitiesof Large Vision-Language Models (LVLMs) in various visual understanding tasks,their effectiveness in decoding flowcharts - a crucial element of scientificcommunication - has yet to be thoroughly investigated. The FlowLearn test setis crafted to assess the performance of LVLMs in flowchart comprehension. Ourstudy thoroughly evaluates state-of-the-art LVLMs, identifying existinglimitations and establishing a foundation for future enhancements in thisrelatively underexplored domain. For instance, in tasks involving simulatedflowcharts, GPT-4V achieved the highest accuracy (58%) in counting the numberof nodes, while Claude recorded the highest accuracy (83%) in OCR tasks.Notably, no single model excels in all tasks within the FlowLearn framework,highlighting significant opportunities for further development.</description><author>Huitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, Longin Jan Latecki</author><pubDate>Tue, 09 Jul 2024 21:16:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.05183v2</guid></item><item><title>Iris: An AI-Driven Virtual Tutor For Computer Science Education</title><link>http://arxiv.org/abs/2405.08008v2</link><description>Integrating AI-driven tools in higher education is an emerging area withtransformative potential. This paper introduces Iris, a chat-based virtualtutor integrated into the interactive learning platform Artemis that offerspersonalized, context-aware assistance in large-scale educational settings.Iris supports computer science students by guiding them through programmingexercises and is designed to act as a tutor in a didactically meaningful way.Its calibrated assistance avoids revealing complete solutions, offering subtlehints or counter-questions to foster independent problem-solving skills. Foreach question, it issues multiple prompts in a Chain-of-Thought toGPT-3.5-Turbo. The prompts include a tutor role description and examples ofmeaningful answers through few-shot learning. Iris employs contextual awarenessby accessing the problem statement, student code, and automated feedback toprovide tailored advice. An empirical evaluation shows that students perceive Iris as effectivebecause it understands their questions, provides relevant support, andcontributes to the learning process. While students consider Iris a valuabletool for programming exercises and homework, they also feel confident solvingprogramming tasks in computer-based exams without Iris. The findings underscorestudents' appreciation for Iris' immediate and personalized support, thoughstudents predominantly view it as a complement to, rather than a replacementfor, human tutors. Nevertheless, Iris creates a space for students to askquestions without being judged by others.</description><author>Patrick Bassner, Eduard Frankford, Stephan Krusche</author><pubDate>Wed, 10 Jul 2024 07:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.08008v2</guid></item><item><title>Language models are better than humans at next-token prediction</title><link>http://arxiv.org/abs/2212.11281v2</link><description>Current language models are considered to have sub-human capabilities atnatural language tasks like question-answering or writing code. However,language models are not trained to perform well at these tasks, they aretrained to accurately predict the next token given previous tokes in tokenizedtext. It is not clear whether language models are better or worse than humansat next token prediction. To try to answer this question, we performed twodistinct experiments to directly compare humans and language models on thisfront: one measuring top-1 accuracy and the other measuring perplexity. In bothexperiments, we find humans to be consistently \emph{worse} than evenrelatively small language models like GPT3-Ada at next-token prediction.</description><author>Buck Shlegeris, Fabien Roger, Lawrence Chan, Euan McLean</author><pubDate>Mon, 15 Jul 2024 15:04:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.11281v2</guid></item></channel></rss>