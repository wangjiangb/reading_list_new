<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivquestion answering</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Fri, 27 Oct 2023 06:00:19 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Adaptive loose optimization for robust question answering</title><link>http://arxiv.org/abs/2305.03971v2</link><description>Question answering methods are well-known for leveraging data bias, such asthe language prior in visual question answering and the position bias inmachine reading comprehension (extractive question answering). Currentdebiasing methods often come at the cost of significant in-distributionperformance to achieve favorable out-of-distribution generalizability, whilenon-debiasing methods sacrifice a considerable amount of out-of-distributionperformance in order to obtain high in-distribution performance. Therefore, itis challenging for them to deal with the complicated changing real-worldsituations. In this paper, we propose a simple yet effective novel lossfunction with adaptive loose optimization, which seeks to make the best of bothworlds for question answering. Our main technical contribution is to reduce theloss adaptively according to the ratio between the previous and currentoptimization state on mini-batch training data. This loose optimization can beused to prevent non-debiasing methods from overlearning data bias whileenabling debiasing methods to maintain slight bias learning. Experiments on thevisual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,GQA-OOD, and the extractive question answering dataset SQuAD demonstrate thatour approach enables QA methods to obtain state-of-the-art in- andout-of-distribution performance in most cases. The source code has beenreleased publicly in \url{https://github.com/reml-group/ALO}.</description><author>Jie Ma, Pinghui Wang, Zewei Wang, Dechen Kong, Min Hu, Ting Han, Jun Liu</author><pubDate>Tue, 16 May 2023 08:02:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03971v2</guid></item><item><title>Adaptive loose optimization for robust question answering</title><link>http://arxiv.org/abs/2305.03971v1</link><description>Question answering methods are well-known for leveraging data bias, such asthe language prior in visual question answering and the position bias inmachine reading comprehension (extractive question answering). Currentdebiasing methods often come at the cost of significant in-distributionperformance to achieve favorable out-of-distribution generalizability, whilenon-debiasing methods sacrifice a considerable amount of out-of-distributionperformance in order to obtain high in-distribution performance. Therefore, itis challenging for them to deal with the complicated changing real-worldsituations. In this paper, we propose a simple yet effective novel lossfunction with adaptive loose optimization, which seeks to make the best of bothworlds for question answering. Our main technical contribution is to reduce theloss adaptively according to the ratio between the previous and currentoptimization state on mini-batch training data. This loose optimization can beused to prevent non-debiasing methods from overlearning data bias whileenabling debiasing methods to maintain slight bias learning. Experiments on thevisual question answering datasets, including VQA v2, VQA-CP v1, VQA-CP v2,GQA-OOD, and the extractive question answering dataset SQuAD demonstrate thatour approach enables QA methods to obtain state-of-the-art in- andout-of-distribution performance in most cases. The source code has beenreleased publicly in \url{https://github.com/reml-group/ALO}.</description><author>Jie Ma, Pinghui Wang, Zewei Wang, Dechen Kong, Min Hu, Ting Han, Jun Liu</author><pubDate>Sat, 06 May 2023 09:09:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03971v1</guid></item><item><title>QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs</title><link>http://arxiv.org/abs/2205.12665v3</link><description>Existing benchmarks for open-domain question answering (ODQA) typically focuson questions whose answers can be extracted from a single paragraph. Bycontrast, many natural questions, such as "What players were drafted by theBrooklyn Nets?" have a list of answers. Answering such questions requiresretrieving and reading from many passages, in a large corpus. We introduceQAMPARI, an ODQA benchmark, where question answers are lists of entities,spread across many paragraphs. We created QAMPARI by (a) generating questionswith multiple answers from Wikipedia's knowledge graph and tables, (b)automatically pairing answers with supporting evidence in Wikipedia paragraphs,and (c) manually paraphrasing questions and validating each answer. We trainODQA models from the retrieve-and-read family and find that QAMPARI ischallenging in terms of both passage retrieval and answer generation, reachingan F1 score of 32.8 at best. Our results highlight the need for developing ODQAmodels that handle a broad range of question types, including single andmulti-answer questions.</description><author>Samuel Joseph Amouyal, Tomer Wolfson, Ohad Rubin, Ori Yoran, Jonathan Herzig, Jonathan Berant</author><pubDate>Wed, 10 May 2023 09:23:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.12665v3</guid></item><item><title>QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs</title><link>http://arxiv.org/abs/2205.12665v4</link><description>Existing benchmarks for open-domain question answering (ODQA) typically focuson questions whose answers can be extracted from a single paragraph. Bycontrast, many natural questions, such as "What players were drafted by theBrooklyn Nets?" have a list of answers. Answering such questions requiresretrieving and reading from many passages, in a large corpus. We introduceQAMPARI, an ODQA benchmark, where question answers are lists of entities,spread across many paragraphs. We created QAMPARI by (a) generating questionswith multiple answers from Wikipedia's knowledge graph and tables, (b)automatically pairing answers with supporting evidence in Wikipedia paragraphs,and (c) manually paraphrasing questions and validating each answer. We trainODQA models from the retrieve-and-read family and find that QAMPARI ischallenging in terms of both passage retrieval and answer generation, reachingan F1 score of 32.8 at best. Our results highlight the need for developing ODQAmodels that handle a broad range of question types, including single andmulti-answer questions.</description><author>Samuel Joseph Amouyal, Tomer Wolfson, Ohad Rubin, Ori Yoran, Jonathan Herzig, Jonathan Berant</author><pubDate>Mon, 29 May 2023 07:16:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.12665v4</guid></item><item><title>Top K Relevant Passage Retrieval for Biomedical Question Answering</title><link>http://arxiv.org/abs/2308.04028v1</link><description>Question answering is a task that answers factoid questions using a largecollection of documents. It aims to provide precise answers in response to theuser's questions in natural language. Question answering relies on efficientpassage retrieval to select candidate contexts, where traditional sparse vectorspace models, such as TF-IDF or BM25, are the de facto method. On the web,there is no single article that could provide all the possible answersavailable on the internet to the question of the problem asked by the user. Theexisting Dense Passage Retrieval model has been trained on Wikipedia dump fromDec. 20, 2018, as the source documents for answering questions. Questionanswering (QA) has made big strides with several open-domain and machinecomprehension systems built using large-scale annotated datasets. However, inthe clinical domain, this problem remains relatively unexplored. According tomultiple surveys, Biomedical Questions cannot be answered correctly fromWikipedia Articles. In this work, we work on the existing DPR framework for thebiomedical domain and retrieve answers from the Pubmed articles which is areliable source to answer medical questions. When evaluated on a BioASQ QAdataset, our fine-tuned dense retriever results in a 0.81 F1 score.</description><author>Shashank Gupta</author><pubDate>Tue, 08 Aug 2023 05:06:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04028v1</guid></item><item><title>Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models</title><link>http://arxiv.org/abs/2308.09363v1</link><description>Video Question Answering (VideoQA) is a challenging task that entails complexmulti-modal reasoning. In contrast to multiple-choice VideoQA which aims topredict the answer given several options, the goal of open-ended VideoQA is toanswer questions without restricting candidate answers. However, the majorityof previous VideoQA models formulate open-ended VideoQA as a classificationtask to classify the video-question pairs into a fixed answer set, i.e.,closed-vocabulary, which contains only frequent answers (e.g., top-1000answers). This leads the model to be biased toward only frequent answers andfail to generalize on out-of-vocabulary answers. We hence propose a newbenchmark, Open-vocabulary Video Question Answering (OVQA), to measure thegeneralizability of VideoQA models by considering rare and unseen answers. Inaddition, in order to improve the model's generalization power, we introduce anovel GNN-based soft verbalizer that enhances the prediction on rare and unseenanswers by aggregating the information from their similar words. Forevaluation, we introduce new baselines by modifying the existing(closed-vocabulary) open-ended VideoQA models and improve their performances byfurther taking into account rare and unseen answers. Our ablation studies andqualitative analyses demonstrate that our GNN-based soft verbalizer furtherimproves the model performance, especially on rare and unseen answers. We hopethat our benchmark OVQA can serve as a guide for evaluating thegeneralizability of VideoQA models and inspire future research. Code isavailable at https://github.com/mlvlab/OVQA.</description><author>Dohwan Ko, Ji Soo Lee, Miso Choi, Jaewon Chu, Jihwan Park, Hyunwoo J. Kim</author><pubDate>Fri, 18 Aug 2023 08:45:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09363v1</guid></item><item><title>Weakly Supervised Visual Question Answer Generation</title><link>http://arxiv.org/abs/2306.06622v2</link><description>Growing interest in conversational agents promote twoway human-computercommunications involving asking and answering visual questions have become anactive area of research in AI. Thus, generation of visual questionanswerpair(s) becomes an important and challenging task. To address this issue, wepropose a weakly-supervised visual question answer generation method thatgenerates a relevant question-answer pairs for a given input image andassociated caption. Most of the prior works are supervised and depend on theannotated question-answer datasets. In our work, we present a weakly supervisedmethod that synthetically generates question-answer pairs procedurally fromvisual information and captions. The proposed method initially extracts list ofanswer words, then does nearest question generation that uses the caption andanswer word to generate synthetic question. Next, the relevant questiongenerator converts the nearest question to relevant language question bydependency parsing and in-order tree traversal, finally, fine-tune a ViLBERTmodel with the question-answer pair(s) generated at end. We perform anexhaustive experimental analysis on VQA dataset and see that our modelsignificantly outperform SOTA methods on BLEU scores. We also show the resultswrt baseline models and ablation study.</description><author>Charani Alampalle, Shamanthak Hegde, Soumya Jahagirdar, Shankar Gangisetty</author><pubDate>Mon, 11 Sep 2023 08:11:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06622v2</guid></item><item><title>Weakly Supervised Visual Question Answer Generation</title><link>http://arxiv.org/abs/2306.06622v1</link><description>Growing interest in conversational agents promote twoway human-computercommunications involving asking and answering visual questions have become anactive area of research in AI. Thus, generation of visual questionanswerpair(s) becomes an important and challenging task. To address this issue, wepropose a weakly-supervised visual question answer generation method thatgenerates a relevant question-answer pairs for a given input image andassociated caption. Most of the prior works are supervised and depend on theannotated question-answer datasets. In our work, we present a weakly supervisedmethod that synthetically generates question-answer pairs procedurally fromvisual information and captions. The proposed method initially extracts list ofanswer words, then does nearest question generation that uses the caption andanswer word to generate synthetic question. Next, the relevant questiongenerator converts the nearest question to relevant language question bydependency parsing and in-order tree traversal, finally, fine-tune a ViLBERTmodel with the question-answer pair(s) generated at end. We perform anexhaustive experimental analysis on VQA dataset and see that our modelsignificantly outperform SOTA methods on BLEU scores. We also show the resultswrt baseline models and ablation study.</description><author>Charani Alampalle, Shamanthak Hegde, Soumya Jahagirdar, Shankar Gangisetty</author><pubDate>Sun, 11 Jun 2023 09:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.06622v1</guid></item><item><title>Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions</title><link>http://arxiv.org/abs/2306.09922v1</link><description>When robots perform long action sequences, users will want to easily andreliably find out what they have done. We therefore demonstrate the task oflearning to summarize and answer questions about a robot agent's past actionsusing natural language alone. A single system with a large language model atits core is trained to both summarize and answer questions about actionsequences given ego-centric video frames of a virtual robot and a questionprompt. To enable training of question answering, we develop a method toautomatically generate English-language questions and answers about objects,actions, and the temporal order in which actions occurred during episodes ofrobot action in the virtual environment. Training one model to both summarizeand answer questions enables zero-shot transfer of representations of objectslearned through question answering to improved action summarization. %involving objects not seen in training to summarize.</description><author>Chad DeChant, Iretiayo Akinola, Daniel Bauer</author><pubDate>Fri, 16 Jun 2023 16:47:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09922v1</guid></item><item><title>ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs</title><link>http://arxiv.org/abs/2208.06501v2</link><description>Question answering over temporal knowledge graphs (TKGQA) has recently foundincreasing interest. TKGQA requires temporal reasoning techniques to extractthe relevant information from temporal knowledge bases. The only existing TKGQAdataset, i.e., CronQuestions, consists of temporal questions based on the factsfrom a fixed time period, where a temporal knowledge graph (TKG) spanning thesame period can be fully used for answer inference, allowing the TKGQA modelsto use even the future knowledge to answer the questions based on the pastfacts. In real-world scenarios, however, it is also common that given theknowledge until now, we wish the TKGQA systems to answer the questions askingabout the future. As humans constantly seek plans for the future, buildingTKGQA systems for answering such forecasting questions is important.Nevertheless, this has still been unexplored in previous research. In thispaper, we propose a novel task: forecasting question answering over temporalknowledge graphs. We also propose a large-scale TKGQA benchmark dataset, i.e.,ForecastTKGQuestions, for this task. It includes three types of questions,i.e., entity prediction, yes-no, and fact reasoning questions. For everyforecasting question in our dataset, QA models can only have access to the TKGinformation before the timestamp annotated in the given question for answerinference. We find that the state-of-the-art TKGQA methods perform poorly onforecasting questions, and they are unable to answer yes-no questions and factreasoning questions. To this end, we propose ForecastTKGQA, a TKGQA model thatemploys a TKG forecasting module for future inference, to answer all threetypes of questions. Experimental results show that ForecastTKGQA outperformsrecent TKGQA methods on the entity prediction questions, and it also showsgreat effectiveness in answering the other two types of questions.</description><author>Zifeng Ding, Zongyue Li, Ruoxia Qi, Jingpei Wu, Bailan He, Yunpu Ma, Zhao Meng, Shuo Chen, Ruotong Liao, Zhen Han, Volker Tresp</author><pubDate>Tue, 18 Jul 2023 16:05:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2208.06501v2</guid></item><item><title>MultiTabQA: Generating Tabular Answers\\ for Multi-Table Question Answering</title><link>http://arxiv.org/abs/2305.12820v1</link><description>Recent advances in tabular question answering (QA) with large language modelsare constrained in their coverage and only answer questions over a singletable. However, real-world queries are complex in nature, often over multipletables in a relational database or web page. Single table questions do notinvolve common table operations such as set operations, Cartesian products(joins), or nested queries. Furthermore, multi-table operations often result ina tabular output, which necessitates table generation capabilities of tabularQA models. To fill this gap, we propose a new task of answering questions overmultiple tables. Our model, MultiTabQA, not only answers questions overmultiple tables, but also generalizes to generate tabular answers. To enableeffective training, we build a pre-training dataset comprising of 132,645 SQLqueries and tabular answers. Further, we evaluate the generated tables byintroducing table-specific metrics of varying strictness assessing variouslevels of granularity of the table structure. MultiTabQA outperformsstate-of-the-art single table QA models adapted to a multi-table QA setting byfinetuning on three datasets: Spider, Atis and GeoQuery.</description><author>Vaishali Pal, Andrew Yates, Evangelos Kanoulas, Maarten de Rijke</author><pubDate>Mon, 22 May 2023 09:25:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12820v1</guid></item><item><title>MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering</title><link>http://arxiv.org/abs/2305.12820v2</link><description>Recent advances in tabular question answering (QA) with large language modelsare constrained in their coverage and only answer questions over a singletable. However, real-world queries are complex in nature, often over multipletables in a relational database or web page. Single table questions do notinvolve common table operations such as set operations, Cartesian products(joins), or nested queries. Furthermore, multi-table operations often result ina tabular output, which necessitates table generation capabilities of tabularQA models. To fill this gap, we propose a new task of answering questions overmultiple tables. Our model, MultiTabQA, not only answers questions overmultiple tables, but also generalizes to generate tabular answers. To enableeffective training, we build a pre-training dataset comprising of 132,645 SQLqueries and tabular answers. Further, we evaluate the generated tables byintroducing table-specific metrics of varying strictness assessing variouslevels of granularity of the table structure. MultiTabQA outperformsstate-of-the-art single table QA models adapted to a multi-table QA setting byfinetuning on three datasets: Spider, Atis and GeoQuery.</description><author>Vaishali Pal, Andrew Yates, Evangelos Kanoulas, Maarten de Rijke</author><pubDate>Wed, 24 May 2023 18:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12820v2</guid></item><item><title>OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual Question Answering in Vietnamese</title><link>http://arxiv.org/abs/2305.04183v1</link><description>In recent years, visual question answering (VQA) has attracted attention fromthe research community because of its highly potential applications (such asvirtual assistance on intelligent cars, assistant devices for blind people, orinformation retrieval from document images using natural language as queries)and challenge. The VQA task requires methods that have the ability to fuse theinformation from questions and images to produce appropriate answers. Neuralvisual question answering models have achieved tremendous growth on large-scaledatasets which are mostly for resource-rich languages such as English. However,available datasets narrow the VQA task as the answers selection task or answerclassification task. We argue that this form of VQA is far from human abilityand eliminates the challenge of the answering aspect in the VQA task by justselecting answers rather than generating them. In this paper, we introduce theOpenViVQA (Open-domain Vietnamese Visual Question Answering) dataset, the firstlarge-scale dataset for VQA with open-ended answers in Vietnamese, consists of11,000+ images associated with 37,000+ question-answer pairs (QAs). Moreover,we proposed FST, QuMLAG, and MLPAG which fuse information from images andanswers, then use these fused features to construct answers as humansiteratively. Our proposed methods achieve results that are competitive withSOTA models such as SAAA, MCAN, LORA, and M4C. The dataset is available toencourage the research community to develop more generalized algorithmsincluding transformers for low-resource languages such as Vietnamese.</description><author>Nghia Hieu Nguyen, Duong T. D. Vo, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen</author><pubDate>Sun, 07 May 2023 04:59:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04183v1</guid></item><item><title>Knowledge Detection by Relevant Question and Image Attributes in Visual Question Answering</title><link>http://arxiv.org/abs/2306.04938v1</link><description>Visual question answering (VQA) is a Multidisciplinary research problem thatpursued through practices of natural language processing and computer vision.Visual question answering automatically answers natural language questionsaccording to the content of an image. Some testing questions require externalknowledge to derive a solution. Such knowledge-based VQA uses various methodsto retrieve features of image and text, and combine them to generate theanswer. To generate knowledgebased answers either question dependent or imagedependent knowledge retrieval methods are used. If knowledge about all theobjects in the image is derived, then not all knowledge is relevant to thequestion. On other side only question related knowledge may lead to incorrectanswers and over trained model that answers question that is irrelevant toimage. Our proposed method takes image attributes and question features asinput for knowledge derivation module and retrieves only question relevantknowledge about image objects which can provide accurate answers.</description><author>Param Ahir, Dr. Hiteishi Diwanji</author><pubDate>Thu, 08 Jun 2023 06:08:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04938v1</guid></item><item><title>Relation-Aware Language-Graph Transformer for Question Answering</title><link>http://arxiv.org/abs/2212.00975v2</link><description>Question Answering (QA) is a task that entails reasoning over naturallanguage contexts, and many relevant works augment language models (LMs) withgraph neural networks (GNNs) to encode the Knowledge Graph (KG) information.However, most existing GNN-based modules for QA do not take advantage of richrelational information of KGs and depend on limited information interactionbetween the LM and the KG. To address these issues, we propose QuestionAnswering Transformer (QAT), which is designed to jointly reason over languageand graphs with respect to entity relations in a unified manner. Specifically,QAT constructs Meta-Path tokens, which learn relation-centric embeddings basedon diverse structural and semantic relations. Then, our Relation-AwareSelf-Attention module comprehensively integrates different modalities via theCross-Modal Relative Position Bias, which guides information exchange betweenrelevant entites of different modalities. We validate the effectiveness of QATon commonsense question answering datasets like CommonsenseQA and OpenBookQA,and on a medical question answering dataset, MedQA-USMLE. On all the datasets,our method achieves state-of-the-art performance. Our code is available athttp://github.com/mlvlab/QAT.</description><author>Jinyoung Park, Hyeong Kyu Choi, Juyeon Ko, Hyeonjin Park, Ji-Hoon Kim, Jisu Jeong, Kyungmin Kim, Hyunwoo J. Kim</author><pubDate>Tue, 25 Apr 2023 10:02:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.00975v2</guid></item><item><title>Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering</title><link>http://arxiv.org/abs/2306.16713v1</link><description>We study visual question answering in a setting where the answer has to bemined from a pool of relevant and irrelevant images given as a context. Forsuch a setting, a model must first retrieve relevant images from the pool andanswer the question from these retrieved images. We refer to this problem asretrieval-based visual question answering (or RETVQA in short). The RETVQA isdistinctively different and more challenging than the traditionally-studiedVisual Question Answering (VQA), where a given question has to be answered witha single relevant image in context. Towards solving the RETVQA task, we proposea unified Multi Image BART (MI-BART) that takes a question and retrieved imagesusing our relevance encoder for free-form fluent answer generation. Further, weintroduce the largest dataset in this space, namely RETVQA, which has thefollowing salient features: multi-image and retrieval requirement for VQA,metadata-independent questions over a pool of heterogeneous images, expecting amix of classification-oriented and open-ended generative answers. Our proposedframework achieves an accuracy of 76.5% and a fluency of 79.3% on the proposeddataset, namely RETVQA and also outperforms state-of-the-art methods by 4.9%and 11.8% on the image segment of the publicly available WebQA dataset on theaccuracy and fluency metrics, respectively.</description><author>Abhirama Subramanyam Penamakuri, Manish Gupta, Mithun Das Gupta, Anand Mishra</author><pubDate>Thu, 29 Jun 2023 07:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16713v1</guid></item><item><title>Exploring the State of the Art in Legal QA Systems</title><link>http://arxiv.org/abs/2304.06623v3</link><description>Answering questions related to the legal domain is a complex task, primarilydue to the intricate nature and diverse range of legal document systems.Providing an accurate answer to a legal query typically necessitatesspecialized knowledge in the relevant domain, which makes this task all themore challenging, even for human experts. Question answering (QA) systems aredesigned to generate answers to questions asked in human languages. QA usesnatural language processing to understand questions and search throughinformation to find relevant answers. QA has various practical applications,including customer service, education, research, and cross-lingualcommunication. However, QA faces challenges such as improving natural languageunderstanding and handling complex and ambiguous questions. Answering questionsrelated to the legal domain is a complex task, primarily due to the intricatenature and diverse range of legal document systems. Providing an accurateanswer to a legal query typically necessitates specialized knowledge in therelevant domain, which makes this task all the more challenging, even for humanexperts. At this time, there is a lack of surveys that discuss legal questionanswering. To address this problem, we provide a comprehensive survey thatreviews 14 benchmark datasets for question-answering in the legal field as wellas presents a comprehensive review of the state-of-the-art Legal QuestionAnswering deep learning models. We cover the different architectures andtechniques used in these studies and the performance and limitations of thesemodels. Moreover, we have established a public GitHub repository where weregularly upload the most recent articles, open data, and source code. Therepository is available at:\url{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review}.</description><author>Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt</author><pubDate>Fri, 15 Sep 2023 11:19:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.06623v3</guid></item><item><title>SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs</title><link>http://arxiv.org/abs/2308.03349v1</link><description>In this work, we present SciGraphQA, a synthetic multi-turn question-answerdataset related to academic graphs. SciGraphQA is 13 times larger thanChartVQA, the previously largest chart-visual question-answering dataset. It isalso the largest open-sourced chart VQA dataset with non-synthetic charts. Tobuild our dataset, we selected 290,000 Computer Science or Machine LearningArXiv papers published between 2010 and 2020, and then used Palm-2 to generate295K samples of open-vocabulary multi-turn question-answering dialogues aboutthe graphs. As context, we provided the text-only Palm-2 with paper title,abstract, paragraph mentioning the graph, and rich text contextual data fromthe graph itself, obtaining dialogues with an average 2.23 question-answerturns for each graph. We asked GPT-4 to assess the matching quality of ourquestion-answer turns given the paper's context, obtaining an average rating of8.7/10 on our 3K test set. We evaluated the 0-shot capability of the mostpopular MLLM models such as LLaVa, mPLUGowl, BLIP-2, and openFlamingo's on ourdataset, finding LLaVA-13B being the most performant with a CIDEr score of0.08. We further enriched the question prompts for LLAVA by including theserialized data tables extracted from the graphs using the DePlot model,boosting LLaVA's 0-shot CIDEr to 0.15. To verify the validity of our dataset,we also fine-tuned LLaVa using our dataset, reaching a substantially higherCIDEr score of 0.26. We anticipate further accuracy improvement by includingsegmentation mask tokens and leveraging larger LLM backbones coupled withemergent prompting techniques. Our code and data are open-sourced.</description><author>Shengzhi Li, Nima Tajbakhsh</author><pubDate>Mon, 07 Aug 2023 08:03:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03349v1</guid></item><item><title>TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering</title><link>http://arxiv.org/abs/2303.11897v3</link><description>Despite thousands of researchers, engineers, and artists actively working onimproving text-to-image generation models, systems often fail to produce imagesthat accurately align with the text inputs. We introduce TIFA (Text-to-ImageFaithfulness evaluation with question Answering), an automatic evaluationmetric that measures the faithfulness of a generated image to its text inputvia visual question answering (VQA). Specifically, given a text input, weautomatically generate several question-answer pairs using a language model. Wecalculate image faithfulness by checking whether existing VQA models can answerthese questions using the generated image. TIFA is a reference-free metric thatallows for fine-grained and interpretable evaluations of generated images. TIFAalso has better correlations with human judgments than existing metrics. Basedon this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diversetext inputs and 25K questions across 12 categories (object, counting, etc.). Wepresent a comprehensive evaluation of existing text-to-image models using TIFAv1.0 and highlight the limitations and challenges of current models. Forinstance, we find that current text-to-image models, despite doing well oncolor and material, still struggle in counting, spatial relations, andcomposing multiple objects. We hope our benchmark will help carefully measurethe research progress in text-to-image synthesis and provide valuable insightsfor further research.</description><author>Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, Noah A Smith</author><pubDate>Thu, 17 Aug 2023 22:45:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11897v3</guid></item><item><title>Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions</title><link>http://arxiv.org/abs/2308.08661v1</link><description>Many open-domain questions are under-specified and thus have multiplepossible answers, each of which is correct under a different interpretation ofthe question. Answering such ambiguous questions is challenging, as it requiresretrieving and then reasoning about diverse information from multiple passages.We present a new state-of-the-art for answering ambiguous questions thatexploits a database of unambiguous questions generated from Wikipedia. On thechallenging ASQA benchmark, which requires generating long-form answers thatsummarize the multiple answers to an ambiguous question, our method improvesperformance by 15% (relative improvement) on recall measures and 10% onmeasures which evaluate disambiguating questions from predicted outputs.Retrieving from the database of generated questions also gives largeimprovements in diverse passage retrieval (by matching user questions q topassages p indirectly, via questions q' generated from p).</description><author>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov</author><pubDate>Wed, 16 Aug 2023 21:23:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08661v1</guid></item><item><title>Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering</title><link>http://arxiv.org/abs/2305.14901v1</link><description>We train a language model (LM) to robustly answer multistep questions bygenerating and answering sub-questions. We propose Chain-of-Questions, aframework that trains a model to generate sub-questions and sub-answers one ata time by leveraging human annotated question decomposition meaningrepresentation (QDMR). The key technical challenge is that QDMR only containssub-questions but not answers to those sub-questions, so we treat sub-answersas latent variables and optimize them using a novel dynamic mixture of Hard-EMand MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methodsby 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQAadversarial set, thus demonstrating the effectiveness and robustness of ourframework.</description><author>Wang Zhu, Jesse Thomason, Robin Jia</author><pubDate>Wed, 24 May 2023 09:55:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14901v1</guid></item><item><title>Localized Questions in Medical Visual Question Answering</title><link>http://arxiv.org/abs/2307.01067v1</link><description>Visual Question Answering (VQA) models aim to answer natural languagequestions about given images. Due to its ability to ask questions that differfrom those used when training the model, medical VQA has received substantialattention in recent years. However, existing medical VQA models typically focuson answering questions that refer to an entire image rather than where therelevant content may be located in the image. Consequently, VQA models arelimited in their interpretability power and the possibility to probe the modelabout specific image regions. This paper proposes a novel approach for medicalVQA that addresses this limitation by developing a model that can answerquestions about image regions while considering the context necessary to answerthe questions. Our experimental results demonstrate the effectiveness of ourproposed model, outperforming existing methods on three datasets. Our code anddata are available at https://github.com/sergiotasconmorales/locvqa.</description><author>Sergio Tascon-Morales, Pablo MÃ¡rquez-Neila, Raphael Sznitman</author><pubDate>Mon, 03 Jul 2023 15:47:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.01067v1</guid></item><item><title>Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval Augmented Generation Models for Open Book Question-Answering</title><link>http://arxiv.org/abs/2307.05915v2</link><description>We propose a framework - Prompt, Generate, Train (PGT) - to efficientlydevelop a generative question-answering model for open-book question-answeringover a proprietary collection of text documents. The framework adapts aretriever augmented generation (RAG) model to the target domain usingsupervised fine-tuning and reinforcement learning with synthetic feedback in afew-shot setting. This, we hypothesize, will yield an aligned, uncertaintycalibrated model that is competitive with GPT-4 based in-context retrievalaugmented generation in generating relevant answers at lower serving costs. Theframework's synthetic generation pipeline will generate synthetic training datacomprising &lt;passage, question, answer&gt; tuples using an open-source LLM and anovel consistency filtering scheme. The pipeline will be designed to generateboth abstractive and extractive questions that span the entire corpus. Theframework proposes to fine-tune a smaller RAG model comprising a denseretriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. Inparallel, the framework will train a Reward model to score domain groundedanswers higher than hallucinated answers using an a priori relevance orderingof synthetically assembled samples. In the next phase, the framework will alignthe RAG model with the target domain using reinforcement learning (ProximalPolicy Optimization). This step may improve the RAG model's ability to generategrounded answers and ignore out of domain questions. In the final phase, theframework will calibrate the model's uncertainty for extractivequestion-answers.</description><author>C. S. Krishna</author><pubDate>Wed, 26 Jul 2023 04:32:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05915v2</guid></item><item><title>NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario</title><link>http://arxiv.org/abs/2305.14836v1</link><description>We introduce a novel visual question answering (VQA) task in the context ofautonomous driving, aiming to answer natural language questions based onstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous drivingscenario presents more challenges. Firstly, the raw visual data aremulti-modal, including images and point clouds captured by camera and LiDAR,respectively. Secondly, the data are multi-frame due to the continuous,real-time acquisition. Thirdly, the outdoor scenes exhibit both movingforeground and static background. Existing VQA benchmarks fail to adequatelyaddress these complexities. To bridge this gap, we propose NuScenes-QA, thefirst benchmark for VQA in the autonomous driving scenario, encompassing 34Kvisual scenes and 460K question-answer pairs. Specifically, we leverageexisting 3D detection annotations to generate scene graphs and design questiontemplates manually. Subsequently, the question-answer pairs are generatedprogrammatically based on these templates. Comprehensive statistics prove thatour NuScenes-QA is a balanced large-scale benchmark with diverse questionformats. Built upon it, we develop a series of baselines that employ advanced3D detection and VQA techniques. Our extensive experiments highlight thechallenges posed by this new task. Codes and dataset are available athttps://github.com/qiantianwen/NuScenes-QA.</description><author>Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang</author><pubDate>Wed, 24 May 2023 08:40:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14836v1</guid></item><item><title>Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery</title><link>http://arxiv.org/abs/2305.11692v1</link><description>Despite the availability of computer-aided simulators and recorded videos ofsurgical procedures, junior residents still heavily rely on experts to answertheir queries. However, expert surgeons are often overloaded with clinical andacademic workloads and limit their time in answering. For this purpose, wedevelop a surgical question-answering system to facilitate robot-assistedsurgical scene and activity understanding from recorded videos. Most of theexisting VQA methods require an object detector and regions based featureextractor to extract visual features and fuse them with the embedded text ofthe question for answer generation. However, (1) surgical object detectionmodel is scarce due to smaller datasets and lack of bounding box annotation;(2) current fusion strategy of heterogeneous modalities like text and image isnaive; (3) the localized answering is missing, which is crucial in complexsurgical scenarios. In this paper, we propose Visual QuestionLocalized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specificsurgical area during the answer prediction. To deal with the fusion of theheterogeneous modalities, we design gated vision-language embedding (GVLE) tobuild input patches for the Language Vision Transformer (LViT) to predict theanswer. To get localization, we add the detection head in parallel with theprediction head of the LViT. We also integrate GIoU loss to boost localizationperformance by preserving the accuracy of the question-answering model. Weannotate two datasets of VQLA by utilizing publicly available surgical videosfrom MICCAI challenges EndoVis-17 and 18. Our validation results suggest thatSurgical-VQLA can better understand the surgical scene and localize thespecific area related to the question-answering. GVLE presents an efficientlanguage-vision embedding technique by showing superior performance over theexisting benchmarks.</description><author>Long Bai, Mobarakol Islam, Lalithkumar Seenivasan, Hongliang Ren</author><pubDate>Fri, 19 May 2023 15:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.11692v1</guid></item><item><title>Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering</title><link>http://arxiv.org/abs/2307.05915v1</link><description>We present a framework - Prompt, Generate, Train (PGT) - to efficientlydevelop a generative question-answering model for open-book question-answeringover a proprietary collection of text documents. The framework adapts aretriever augmented generation model to the target domain using supervisedfinetuning and reinforcement learning with synthetic feedback in a few-shotsetting. This yields an aligned, uncertainty calibrated model that iscompetitive with GPT-4 based in-context retrieval augmented generation ingenerating relevant answers at lower serving costs. The synthetic generationpipeline generates high quality synthetic training data musing a medium sizedLLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline isdesigned to generate both abstractive and extractive questions that span theentire corpus. Using samples from this dataset, the framework fine-tunes asmaller RAG model comprising a dense retriever and a smaller sized LLM onsamples from the dataset. In parallel, the framework trains a Reward model toscore domain grounded answers higher than hallucinated answers. In the nextphase, the framework aligns to the RAG model with the target domain usingreinforcement learning. This step improves the RAG model's ability to generategrounded answers and ignore out of domain questions. In the final phase, theframework calibrates the model uncertainty for extractive question-answers.This is a desirable feature since the model can be integrated into a cascadingsystem where the RAG model's answer is surfaced only when the model isconfident of its answer.</description><author>C. S. Krishna</author><pubDate>Wed, 12 Jul 2023 05:44:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05915v1</guid></item><item><title>Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering</title><link>http://arxiv.org/abs/2309.17133v1</link><description>Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems toutilize knowledge from existing knowledge bases to answer visually-groundedquestions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strongframework to tackle KB-VQA, first retrieves related documents with DensePassage Retrieval (DPR) and then uses them to answer questions. This paperproposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) whichsignificantly improves knowledge retrieval in RA-VQA. FLMR addresses two majorlimitations in RA-VQA's retriever: (1) the image representations obtained viaimage-to-text transforms can be incomplete and inaccurate and (2) relevancescores between queries and documents are computed with one-dimensionalembeddings, which can be insensitive to finer-grained relevance. FLMR overcomesthese limitations by obtaining image representations that complement those fromthe image-to-text transforms using a vision model aligned with an existingtext-based retriever through a simple alignment network. FLMR also encodesimages and questions using multi-dimensional embeddings to capturefiner-grained relevance between queries and documents. FLMR significantlyimproves the original RA-VQA retriever's PRRecall@5 by approximately 8\%.Finally, we equipped RA-VQA with two state-of-the-art largemulti-modal/language models to achieve $\sim61\%$ VQA score in the OK-VQAdataset.</description><author>Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, Bill Byrne</author><pubDate>Fri, 29 Sep 2023 11:54:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.17133v1</guid></item><item><title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title><link>http://arxiv.org/abs/2305.06849v1</link><description>Long-form question answering (LFQA) aims at answering complex, open-endedquestions with detailed, paragraph-length responses. The de facto paradigm ofLFQA necessitates two procedures: information retrieval, which searches forrelevant supporting facts, and information synthesis, which integrates thesefacts into a coherent answer. In this paper, we introduce WebCPM, the firstChinese LFQA dataset. One unique feature of WebCPM is that its informationretrieval is based on interactive web search, which engages with a searchengine in real time. Following WebGPT, we develop a web search interface. Werecruit annotators to search for relevant information using our interface andthen answer questions. Meanwhile, the web search behaviors of our annotatorswould be recorded. In total, we collect 5,500 high-quality question-answerpairs, together with 14,315 supporting facts and 121,330 web search actions. Wefine-tune pre-trained language models to imitate human behaviors for web searchand to generate answers based on the collected facts. Our LFQA pipeline, builton these fine-tuned models, generates answers that are no worse thanhuman-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,respectively.</description><author>Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou</author><pubDate>Thu, 11 May 2023 15:47:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06849v1</guid></item><item><title>WebCPM: Interactive Web Search for Chinese Long-form Question Answering</title><link>http://arxiv.org/abs/2305.06849v2</link><description>Long-form question answering (LFQA) aims at answering complex, open-endedquestions with detailed, paragraph-length responses. The de facto paradigm ofLFQA necessitates two procedures: information retrieval, which searches forrelevant supporting facts, and information synthesis, which integrates thesefacts into a coherent answer. In this paper, we introduce WebCPM, the firstChinese LFQA dataset. One unique feature of WebCPM is that its informationretrieval is based on interactive web search, which engages with a searchengine in real time. Following WebGPT, we develop a web search interface. Werecruit annotators to search for relevant information using our interface andthen answer questions. Meanwhile, the web search behaviors of our annotatorswould be recorded. In total, we collect 5,500 high-quality question-answerpairs, together with 14,315 supporting facts and 121,330 web search actions. Wefine-tune pre-trained language models to imitate human behaviors for web searchand to generate answers based on the collected facts. Our LFQA pipeline, builton these fine-tuned models, generates answers that are no worse thanhuman-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader,respectively.</description><author>Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou</author><pubDate>Tue, 23 May 2023 14:15:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.06849v2</guid></item><item><title>Few-shot In-context Learning for Knowledge Base Question Answering</title><link>http://arxiv.org/abs/2305.01750v1</link><description>Question answering over knowledge bases is considered a difficult problem dueto the challenge of generalizing to a wide variety of possible natural languagequestions. Additionally, the heterogeneity of knowledge base schema itemsbetween different knowledge bases often necessitates specialized training fordifferent knowledge base question-answering (KBQA) datasets. To handlequestions over diverse KBQA datasets with a unified training-free framework, wepropose KB-BINDER, which for the first time enables few-shot in-contextlearning over KBQA tasks. Firstly, KB-BINDER leverages large language modelslike Codex to generate logical forms as the draft for a specific question byimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledgebase to bind the generated draft to an executable one with BM25 score matching.The experimental results on four public heterogeneous KBQA datasets show thatKB-BINDER can achieve a strong performance with only a few in-contextdemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can evenoutperform the state-of-the-art trained models. On GrailQA and WebQSP, ourmodel is also on par with other fully-trained models. We believe KB-BINDER canserve as an important baseline for future research. We plan to release all thecode and data.</description><author>Tianle LI, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, Wenhu Chen</author><pubDate>Tue, 02 May 2023 20:31:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01750v1</guid></item><item><title>Few-shot In-context Learning for Knowledge Base Question Answering</title><link>http://arxiv.org/abs/2305.01750v2</link><description>Question answering over knowledge bases is considered a difficult problem dueto the challenge of generalizing to a wide variety of possible natural languagequestions. Additionally, the heterogeneity of knowledge base schema itemsbetween different knowledge bases often necessitates specialized training fordifferent knowledge base question-answering (KBQA) datasets. To handlequestions over diverse KBQA datasets with a unified training-free framework, wepropose KB-BINDER, which for the first time enables few-shot in-contextlearning over KBQA tasks. Firstly, KB-BINDER leverages large language modelslike Codex to generate logical forms as the draft for a specific question byimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledgebase to bind the generated draft to an executable one with BM25 score matching.The experimental results on four public heterogeneous KBQA datasets show thatKB-BINDER can achieve a strong performance with only a few in-contextdemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can evenoutperform the state-of-the-art trained models. On GrailQA and WebQSP, ourmodel is also on par with other fully-trained models. We believe KB-BINDER canserve as an important baseline for future research. Our code is available athttps://github.com/ltl3A87/KB-BINDER.</description><author>Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, Wenhu Chen</author><pubDate>Thu, 04 May 2023 15:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01750v2</guid></item><item><title>Implications of Deep Circuits in Improving Quality of Quantum Question Answering</title><link>http://arxiv.org/abs/2305.07374v1</link><description>Question Answering (QA) has proved to be an arduous challenge in the area ofnatural language processing (NLP) and artificial intelligence (AI). Manyattempts have been made to develop complete solutions for QA as well asimproving significant sub-modules of the QA systems to improve the overallperformance through the course of time. Questions are the most important pieceof QA, because knowing the question is equivalent to knowing what counts as ananswer (Harrah in Philos Sci, 1961 [1]). In this work, we have attempted tounderstand questions in a better way by using Quantum Machine Learning (QML).The properties of Quantum Computing (QC) have enabled classically intractabledata processing. So, in this paper, we have performed question classificationon questions from two classes of SelQA (Selection-based Question Answering)dataset using quantum-based classifier algorithms-quantum support vectormachine (QSVM) and variational quantum classifier (VQC) from Qiskit (QuantumInformation Science toolKIT) for Python. We perform classification with bothclassifiers in almost similar environments and study the effects of circuitdepths while comparing the results of both classifiers. We also use theseclassification results with our own rule-based QA system and observesignificant performance improvement. Hence, this experiment has helped inimproving the quality of QA in general.</description><author>Pragya Katyayan, Nisheeth Joshi</author><pubDate>Fri, 12 May 2023 11:52:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07374v1</guid></item><item><title>PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph Question-Answering Grounding</title><link>http://arxiv.org/abs/2310.12638v1</link><description>The Scholarly Question Answering over Linked Data (Scholarly QALD) at TheInternational Semantic Web Conference (ISWC) 2023 challenge presents twosub-tasks to tackle question answering (QA) over knowledge graphs (KGs). Weanswer the KGQA over DBLP (DBLP-QUAD) task by proposing a neuro-symbolic (NS)framework based on PSYCHIC, an extractive QA model capable of identifying thequery and entities related to a KG question. Our system achieved a F1 score of00.18% on question answering and came in third place for entity linking (EL)with a score of 71.00%.</description><author>Hanna Abi Akl</author><pubDate>Thu, 19 Oct 2023 11:53:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.12638v1</guid></item><item><title>Learning to Select the Relevant History Turns in Conversational Question Answering</title><link>http://arxiv.org/abs/2308.02294v1</link><description>The increasing demand for the web-based digital assistants has given a rapidrise in the interest of the Information Retrieval (IR) community towards thefield of conversational question answering (ConvQA). However, one of thecritical aspects of ConvQA is the effective selection of conversational historyturns to answer the question at hand. The dependency between relevant historyselection and correct answer prediction is an intriguing but under-exploredarea. The selected relevant context can better guide the system so as to whereexactly in the passage to look for an answer. Irrelevant context, on the otherhand, brings noise to the system, thereby resulting in a decline in the model'sperformance. In this paper, we propose a framework, DHS-ConvQA (Dynamic HistorySelection in Conversational Question Answering), that first generates thecontext and question entities for all the history turns, which are then prunedon the basis of similarity they share in common with the question at hand. Wealso propose an attention-based mechanism to re-rank the pruned terms based ontheir calculated weights of how useful they are in answering the question. Inthe end, we further aid the model by highlighting the terms in the re-rankedconversational history using a binary classification task and keeping theuseful terms (predicted as 1) and ignoring the irrelevant terms (predicted as0). We demonstrate the efficacy of our proposed framework with extensiveexperimental results on CANARD and QuAC -- the two popularly utilized datasetsin ConvQA. We demonstrate that selecting relevant turns works better thanrewriting the original question. We also investigate how adding the irrelevanthistory turns negatively impacts the model's performance and discuss theresearch challenges that demand more attention from the IR community.</description><author>Munazza Zaib, Wei Emma Zhang, Quan Z. Sheng, Subhash Sagar, Adnan Mahmood, Yang Zhang</author><pubDate>Fri, 04 Aug 2023 13:59:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.02294v1</guid></item><item><title>Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering</title><link>http://arxiv.org/abs/2306.00526v1</link><description>The pre-training-fine-tuning paradigm based on layout-aware multimodalpre-trained models has achieved significant progress on document image questionanswering. However, domain pre-training and task fine-tuning for additionalvisual, layout, and task modules prevent them from directly utilizingoff-the-shelf instruction-tuning language foundation models, which haverecently shown promising potential in zero-shot learning. Contrary to aligninglanguage models to the domain of document image question answering, we aligndocument image question answering to off-the-shell instruction-tuning languagefoundation models to utilize their zero-shot capability. Specifically, wepropose layout and task aware instruction prompt called LATIN-Prompt, whichconsists of layout-aware document content and task-aware descriptions. Theformer recovers the layout information among text segments from OCR tools byappropriate spaces and line breaks. The latter ensures that the model generatesanswers that meet the requirements, especially format requirements, through adetailed description of task. Experimental results on three benchmarks showthat LATIN-Prompt can improve the zero-shot performance of instruction-tuninglanguage foundation models on document image question answering and help themachieve comparable levels to SOTAs based on the pre-training-fine-tuningparadigm. Quantitative analysis and qualitative analysis demonstrate theeffectiveness of LATIN-Prompt. We provide the code in supplementary and willrelease the code to facilitate future research.</description><author>Wenjin Wang, Yunhao Li, Yixin Ou, Yin Zhang</author><pubDate>Thu, 01 Jun 2023 11:28:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00526v1</guid></item><item><title>Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering</title><link>http://arxiv.org/abs/2306.00526v2</link><description>The pre-training-fine-tuning paradigm based on layout-aware multimodalpre-trained models has achieved significant progress on document image questionanswering. However, domain pre-training and task fine-tuning for additionalvisual, layout, and task modules prevent them from directly utilizingoff-the-shelf instruction-tuning language foundation models, which haverecently shown promising potential in zero-shot learning. Contrary to aligninglanguage models to the domain of document image question answering, we aligndocument image question answering to off-the-shell instruction-tuning languagefoundation models to utilize their zero-shot capability. Specifically, wepropose layout and task aware instruction prompt called LATIN-Prompt, whichconsists of layout-aware document content and task-aware descriptions. Theformer recovers the layout information among text segments from OCR tools byappropriate spaces and line breaks. The latter ensures that the model generatesanswers that meet the requirements, especially format requirements, through adetailed description of task. Experimental results on three benchmarks showthat LATIN-Prompt can improve the zero-shot performance of instruction-tuninglanguage foundation models on document image question answering and help themachieve comparable levels to SOTAs based on the pre-training-fine-tuningparadigm. Quantitative analysis and qualitative analysis demonstrate theeffectiveness of LATIN-Prompt. We provide the code in supplementary and willrelease the code to facilitate future research.</description><author>Wenjin Wang, Yunhao Li, Yixin Ou, Yin Zhang</author><pubDate>Fri, 30 Jun 2023 13:03:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00526v2</guid></item><item><title>Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering</title><link>http://arxiv.org/abs/2306.00526v3</link><description>The pre-training-fine-tuning paradigm based on layout-aware multimodalpre-trained models has achieved significant progress on document image questionanswering. However, domain pre-training and task fine-tuning for additionalvisual, layout, and task modules prevent them from directly utilizingoff-the-shelf instruction-tuning language foundation models, which haverecently shown promising potential in zero-shot learning. Contrary to aligninglanguage models to the domain of document image question answering, we aligndocument image question answering to off-the-shell instruction-tuning languagefoundation models to utilize their zero-shot capability. Specifically, wepropose layout and task aware instruction prompt called LATIN-Prompt, whichconsists of layout-aware document content and task-aware descriptions. Theformer recovers the layout information among text segments from OCR tools byappropriate spaces and line breaks. The latter ensures that the model generatesanswers that meet the requirements, especially format requirements, through adetailed description of task. Experimental results on three benchmarks showthat LATIN-Prompt can improve the zero-shot performance of instruction-tuninglanguage foundation models on document image question answering and help themachieve comparable levels to SOTAs based on the pre-training-fine-tuningparadigm. Quantitative analysis and qualitative analysis demonstrate theeffectiveness of LATIN-Prompt. We provide the code in supplementary and willrelease the code to facilitate future research.</description><author>Wenjin Wang, Yunhao Li, Yixin Ou, Yin Zhang</author><pubDate>Wed, 06 Sep 2023 04:30:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.00526v3</guid></item><item><title>Is GPT-3 all you need for Visual Question Answering in Cultural Heritage?</title><link>http://arxiv.org/abs/2207.12101v2</link><description>The use of Deep Learning and Computer Vision in the Cultural Heritage domainis becoming highly relevant in the last few years with lots of applicationsabout audio smart guides, interactive museums and augmented reality. All thesetechnologies require lots of data to work effectively and be useful for theuser. In the context of artworks, such data is annotated by experts in anexpensive and time consuming process. In particular, for each artwork, an imageof the artwork and a description sheet have to be collected in order to performcommon tasks like Visual Question Answering. In this paper we propose a methodfor Visual Question Answering that allows to generate at runtime a descriptionsheet that can be used for answering both visual and contextual questions aboutthe artwork, avoiding completely the image and the annotation process. For thispurpose, we investigate on the use of GPT-3 for generating descriptions forartworks analyzing the quality of generated descriptions through captioningmetrics. Finally we evaluate the performance for Visual Question Answering andcaptioning tasks.</description><author>Pietro Bongini, Federico Becattini, Alberto Del Bimbo</author><pubDate>Fri, 19 May 2023 10:56:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2207.12101v2</guid></item><item><title>VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering</title><link>http://arxiv.org/abs/2205.11501v2</link><description>Visual question answering (VQA) requires systems to perform concept-levelreasoning by unifying unstructured (e.g., the context in question and answer;"QA context") and structured (e.g., knowledge graph for the QA context andscene; "concept graph") multimodal knowledge. Existing works typically combinea scene graph and a concept graph of the scene by connecting correspondingvisual nodes and concept nodes, then incorporate the QA context representationto perform question answering. However, these methods only perform aunidirectional fusion from unstructured knowledge to structured knowledge,limiting their potential to capture joint reasoning over the heterogeneousmodalities of knowledge. To perform more expressive reasoning, we proposeVQA-GNN, a new VQA model that performs bidirectional fusion betweenunstructured and structured multimodal knowledge to obtain unified knowledgerepresentations. Specifically, we inter-connect the scene graph and the conceptgraph through a super node that represents the QA context, and introduce a newmultimodal GNN technique to perform inter-modal message passing for reasoningthat mitigates representational gaps between modalities. On two challenging VQAtasks (VCR and GQA), our method outperforms strong baseline VQA methods by 3.2%on VCR (Q-AR) and 4.6% on GQA, suggesting its strength in performingconcept-level reasoning. Ablation studies further demonstrate the efficacy ofthe bidirectional fusion and multimodal GNN method in unifying unstructured andstructured multimodal knowledge.</description><author>Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, Jure Leskovec</author><pubDate>Fri, 15 Sep 2023 09:16:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.11501v2</guid></item><item><title>MaXM: Towards Multilingual Visual Question Answering</title><link>http://arxiv.org/abs/2209.05401v3</link><description>Visual Question Answering (VQA) has been primarily studied through the lensof the English language. Yet, tackling VQA in other languages in the samemanner would require a considerable amount of resources. In this paper, wepropose scalable solutions to multilingual visual question answering (mVQA), onboth data and modeling fronts. We first propose a translation-based frameworkto mVQA data generation that requires much less human annotation efforts thanthe conventional approach of directly collection questions and answers. Then,we apply our framework to the multilingual captions in the Crossmodal-3600dataset and develop an efficient annotation protocol to create MaXM, atest-only VQA benchmark in 7 diverse languages. Finally, we develop a simple,lightweight, and effective approach as well as benchmark state-of-the-artEnglish and multilingual VQA models. We hope that our benchmark encouragesfurther research on mVQA.</description><author>Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish V. Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, Radu Soricut</author><pubDate>Tue, 24 Oct 2023 06:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.05401v3</guid></item><item><title>DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering</title><link>http://arxiv.org/abs/2308.10959v2</link><description>In this paper, we propose Docprompt for document question answering taskswith powerful zero-shot and few-shot performance. We proposed a novel weaklysupervised data generation method, a novel multl-stage training method and anovel understanding model \&amp; generation model ensemble method. We achievedstate-of-the-art performance on 4 document question answering tasks. Thismethod greatly improves the delivery efficiency and model performance ofdocument question answering customer projects, reducing annotation costs andlabor costs. Our demo can be found athttps://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.</description><author>Sijin Wu, Dan Zhang, Teng Hu, Shikun Feng</author><pubDate>Thu, 31 Aug 2023 10:14:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10959v2</guid></item><item><title>Understanding Video Scenes through Text: Insights from Text-based Video Question Answering</title><link>http://arxiv.org/abs/2309.01380v2</link><description>Researchers have extensively studied the field of vision and language,discovering that both visual and textual content is crucial for understandingscenes effectively. Particularly, comprehending text in videos holds greatsignificance, requiring both scene text understanding and temporal reasoning.This paper focuses on exploring two recently introduced datasets, NewsVideoQAand M4-ViteVQA, which aim to address video question answering based on textualcontent. The NewsVideoQA dataset contains question-answer pairs related to thetext in news videos, while M4-ViteVQA comprises question-answer pairs fromdiverse categories like vlogging, traveling, and shopping. We provide ananalysis of the formulation of these datasets on various levels, exploring thedegree of visual understanding and multi-frame comprehension required foranswering the questions. Additionally, the study includes experimentation withBERT-QA, a text-only model, which demonstrates comparable performance to theoriginal methods on both datasets, indicating the shortcomings in theformulation of these datasets. Furthermore, we also look into the domainadaptation aspect by examining the effectiveness of training on M4-ViteVQA andevaluating on NewsVideoQA and vice-versa, thereby shedding light on thechallenges and potential benefits of out-of-domain training.</description><author>Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar</author><pubDate>Mon, 11 Sep 2023 08:01:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01380v2</guid></item><item><title>Understanding Video Scenes through Text: Insights from Text-based Video Question Answering</title><link>http://arxiv.org/abs/2309.01380v1</link><description>Researchers have extensively studied the field of vision and language,discovering that both visual and textual content is crucial for understandingscenes effectively. Particularly, comprehending text in videos holds greatsignificance, requiring both scene text understanding and temporal reasoning.This paper focuses on exploring two recently introduced datasets, NewsVideoQAand M4-ViteVQA, which aim to address video question answering based on textualcontent. The NewsVideoQA dataset contains question-answer pairs related to thetext in news videos, while M4-ViteVQA comprises question-answer pairs fromdiverse categories like vlogging, traveling, and shopping. We provide ananalysis of the formulation of these datasets on various levels, exploring thedegree of visual understanding and multi-frame comprehension required foranswering the questions. Additionally, the study includes experimentation withBERT-QA, a text-only model, which demonstrates comparable performance to theoriginal methods on both datasets, indicating the shortcomings in theformulation of these datasets. Furthermore, we also look into the domainadaptation aspect by examining the effectiveness of training on M4-ViteVQA andevaluating on NewsVideoQA and vice-versa, thereby shedding light on thechallenges and potential benefits of out-of-domain training.</description><author>Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar</author><pubDate>Mon, 04 Sep 2023 07:11:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.01380v1</guid></item><item><title>Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of GPT family LLMs' Question Answering Performance</title><link>http://arxiv.org/abs/2303.07992v2</link><description>ChatGPT is a powerful large language model (LLM) that covers knowledgeresources such as Wikipedia and supports natural language question answeringusing its own knowledge. Therefore, there is growing interest in exploringwhether ChatGPT can replace traditional knowledge-based question answering(KBQA) models. Although there have been some works analyzing the questionanswering performance of ChatGPT, there is still a lack of large-scale,comprehensive testing of various types of complex questions to analyze thelimitations of the model. In this paper, we present a framework that followsthe black-box testing specifications of CheckList proposed by Ribeiro et. al.We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complexquestion answering datasets, which include six English datasets and twomultilingual datasets. The total number of test cases is approximately 190,000.In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5to identify commonalities between the GPT family and other LLMs. The datasetand code are available athttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git</description><author>Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi</author><pubDate>Fri, 04 Aug 2023 11:25:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07992v2</guid></item><item><title>DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering</title><link>http://arxiv.org/abs/2308.10959v1</link><description>In this paper, we propose Docprompt for document question answering taskswith powerful zero-shot and few-shot performance. We proposed a novel weaklysupervised data generation method, a novel multl-stage training method and anovel understanding model &amp; generation model ensemble method. Experimentresults show that the Docprompt model after continue pretrain significantlyoutperforms the existing strong baseline models on document question answeringtasks. This method greatly improves the delivery efficiency and modelperformance of document question answering customer projects, reducingannotation costs and labor costs. Our demo can be found athttps://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.</description><author>Sijin Wu, Dan Zhang, Teng Hu, Shikun Feng</author><pubDate>Mon, 21 Aug 2023 19:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.10959v1</guid></item><item><title>Learning Situation Hyper-Graphs for Video Question Answering</title><link>http://arxiv.org/abs/2304.08682v2</link><description>Answering questions about complex situations in videos requires not onlycapturing the presence of actors, objects, and their relations but also theevolution of these relationships over time. A situation hyper-graph is arepresentation that describes situations as scene sub-graphs for video framesand hyper-edges for connected sub-graphs and has been proposed to capture allsuch information in a compact structured form. In this work, we propose anarchitecture for Video Question Answering (VQA) that enables answeringquestions related to video content by predicting situation hyper-graphs, coinedSituation Hyper-Graph based Video Question Answering (SHG-VQA). To this end, wetrain a situation hyper-graph decoder to implicitly identify graphrepresentations with actions and object/human-object relationships from theinput video clip. and to use cross-attention between the predicted situationhyper-graphs and the question embedding to predict the correct answer. Theproposed method is trained in an end-to-end manner and optimized by a VQA losswith the cross-entropy function and a Hungarian matching loss for the situationgraph prediction. The effectiveness of the proposed architecture is extensivelyevaluated on two challenging benchmarks: AGQA and STAR. Our results show thatlearning the underlying situation hyper-graphs helps the system tosignificantly improve its performance for novel challenges of videoquestion-answering tasks.</description><author>Aisha Urooj Khan, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bousselham, Chuang Gan, Niels Lobo, Mubarak Shah</author><pubDate>Sat, 06 May 2023 07:44:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.08682v2</guid></item><item><title>ADMUS: A Progressive Question Answering Framework Adaptable to Multiple Knowledge Sources</title><link>http://arxiv.org/abs/2308.04800v1</link><description>With the introduction of deep learning models, semantic parsingbasedknowledge base question answering (KBQA) systems have achieved high performancein handling complex questions. However, most existing approaches primarilyfocus on enhancing the model's effectiveness on individual benchmark datasets,disregarding the high costs of adapting the system to disparate datasets inreal-world scenarios (e.g., multi-tenant platform). Therefore, we presentADMUS, a progressive knowledge base question answering framework designed toaccommodate a wide variety of datasets, including multiple languages, diversebackbone knowledge bases, and disparate question answering datasets. Toaccomplish the purpose, we decouple the architecture of conventional KBQAsystems and propose this dataset-independent framework. Our framework supportsthe seamless integration of new datasets with minimal effort, only requiringcreating a dataset-related micro-service at a negligible cost. To enhance theusability of ADUMS, we design a progressive framework consisting of threestages, ranges from executing exact queries, generating approximate queries andretrieving open-domain knowledge referring from large language models. Anonline demonstration of ADUMS is available at:https://answer.gstore.cn/pc/index.html</description><author>Yirui Zhan, Yanzeng Li, Minhao Zhang, Lei Zou</author><pubDate>Wed, 09 Aug 2023 09:46:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04800v1</guid></item><item><title>Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family</title><link>http://arxiv.org/abs/2303.07992v3</link><description>ChatGPT is a powerful large language model (LLM) that covers knowledgeresources such as Wikipedia and supports natural language question answeringusing its own knowledge. Therefore, there is growing interest in exploringwhether ChatGPT can replace traditional knowledge-based question answering(KBQA) models. Although there have been some works analyzing the questionanswering performance of ChatGPT, there is still a lack of large-scale,comprehensive testing of various types of complex questions to analyze thelimitations of the model. In this paper, we present a framework that followsthe black-box testing specifications of CheckList proposed by Ribeiro et. al.We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complexquestion answering datasets, which include six English datasets and twomultilingual datasets. The total number of test cases is approximately 190,000.In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5to identify commonalities between the GPT family and other LLMs. The datasetand code are available athttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git</description><author>Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi</author><pubDate>Wed, 20 Sep 2023 06:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.07992v3</guid></item><item><title>Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes</title><link>http://arxiv.org/abs/2306.02329v1</link><description>Training models to apply common-sense linguistic knowledge and visualconcepts from 2D images to 3D scene understanding is a promising direction thatresearchers have only recently started to explore. However, it still remainsunderstudied whether 2D distilled knowledge can provide useful representationsfor downstream 3D vision-language tasks such as 3D question answering. In thispaper, we propose a novel 3D pre-training Vision-Language method, namelyMulti-CLIP, that enables a model to learn language-grounded and transferable 3Dscene point cloud representations. We leverage the representational power ofthe CLIP model by maximizing the agreement between the encoded 3D scenefeatures and the corresponding 2D multi-view image and text embeddings in theCLIP space via a contrastive objective. To validate our approach, we considerthe challenging downstream tasks of 3D Visual Question Answering (3D-VQA) and3D Situated Question Answering (3D-SQA). To this end, we develop novelmulti-modal transformer-based architectures and we demonstrate how ourpre-training method can benefit their performance. Quantitative and qualitativeexperimental results show that Multi-CLIP outperforms state-of-the-art worksacross the downstream tasks of 3D-VQA and 3D-SQA and leads to a well-structured3D scene feature space.</description><author>Alexandros Delitzas, Maria Parelli, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, Thomas Hofmann</author><pubDate>Sun, 04 Jun 2023 12:08:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02329v1</guid></item><item><title>Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion</title><link>http://arxiv.org/abs/2211.07098v4</link><description>Over the past few years, large knowledge bases have been constructed to storemassive amounts of knowledge. However, these knowledge bases are highlyincomplete. To solve this problem, we propose a web-based question answeringsystem system with multimodal fusion of unstructured and structuredinformation, to fill in missing information for knowledge bases. To utilizeunstructured information from the Web for knowledge base completion, we designa web-based question answering system using multimodal features and questiontemplates to extract missing facts, which can achieve good performance withvery few questions. To help improve extraction quality, the question answeringsystem employs structured information from knowledge bases, such as entitytypes and entity-to-entity relatedness.</description><author>Yang Peng, Daisy Zhe Wang</author><pubDate>Sun, 07 May 2023 05:49:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.07098v4</guid></item><item><title>Unifying Image Processing as Visual Prompting Question Answering</title><link>http://arxiv.org/abs/2310.10513v1</link><description>Image processing is a fundamental task in computer vision, which aims atenhancing image quality and extracting essential features for subsequent visionapplications. Traditionally, task-specific models are developed for individualtasks and designing such models requires distinct expertise. Building upon thesuccess of large language models (LLMs) in natural language processing (NLP),there is a similar trend in computer vision, which focuses on developinglarge-scale models through pretraining and in-context learning. This paradigmshift reduces the reliance on task-specific models, yielding a powerful unifiedmodel to deal with various tasks. However, these advances have predominantlyconcentrated on high-level vision tasks, with less attention paid to low-levelvision tasks. To address this issue, we propose a universal model for generalimage processing that covers image restoration, image enhancement, imagefeature extraction tasks, \textit{etc}. Our proposed framework, namedPromptGIP, unifies these diverse image processing tasks within a universalframework. Inspired by NLP question answering (QA) techniques, we employ avisual prompting question answering paradigm. Specifically, we treat theinput-output image pair as a structured question-answer sentence, therebyreprogramming the image processing task as a prompting QA problem. PromptGIPcan undertake diverse \textbf{cross-domain} tasks using provided visualprompts, eliminating the need for task-specific finetuning. Our methodologyoffers a universal and adaptive solution to general image processing. WhilePromptGIP has demonstrated a certain degree of out-of-domain taskgeneralization capability, further research is expected to fully explore itsmore powerful emergent generalization.</description><author>Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</author><pubDate>Mon, 16 Oct 2023 16:32:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10513v1</guid></item><item><title>AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking</title><link>http://arxiv.org/abs/2305.02235v1</link><description>Annotating long-document question answering (long-document QA) pairs istime-consuming and expensive. To alleviate the problem, it might be possible togenerate long-document QA pairs via unsupervised question answering (UQA)methods. However, existing UQA tasks are based on short documents, and canhardly incorporate long-range information. To tackle the problem, we propose anew task, named unsupervised long-document question answering (ULQA), aiming togenerate high-quality long-document QA instances in an unsupervised manner.Besides, we propose AttenWalker, a novel unsupervised method to aggregate andgenerate answers with long-range dependency so as to construct long-document QApairs. Specifically, AttenWalker is composed of three modules, i.e., spancollector, span linker and answer aggregator. Firstly, the span collector takesadvantage of constituent parsing and reconstruction loss to select informativecandidate spans for constructing answers. Secondly, by going through theattention graph of a pre-trained long-document model, potentially interrelatedtext spans (that might be far apart) could be linked together via anattention-walking algorithm. Thirdly, in the answer aggregator, linked spansare aggregated into the final answer via the mask-filling ability of apre-trained model. Extensive experiments show that AttenWalker outperformsprevious methods on Qasper and NarrativeQA. In addition, AttenWalker also showsstrong performance in the few-shot learning setting.</description><author>Yuxiang Nie, Heyan Huang, Wei Wei, Xian-Ling Mao</author><pubDate>Wed, 03 May 2023 17:16:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.02235v1</guid></item><item><title>CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering</title><link>http://arxiv.org/abs/2305.14869v1</link><description>The task of zero-shot commonsense question answering evaluates models ontheir capacity to reason about general scenarios beyond those presented inspecific datasets. Existing approaches for tackling this task leverage externalknowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model onsynthetic QA pairs constructed from CSKBs. In these approaches, negativeexamples (distractors) are formulated by randomly sampling from CSKBs usingfairly primitive keyword constraints. However, two bottlenecks limit theseapproaches: the inherent incompleteness of CSKBs limits the semantic coverageof synthetic QA pairs, and the lack of human annotations makes the samplednegative examples potentially uninformative and contradictory. To tackle theselimitations above, we propose Conceptualization-Augmented Reasoner (CAR), azero-shot commonsense question-answering framework that fully leverages thepower of conceptualization. Specifically, CAR abstracts a commonsense knowledgetriple to many higher-level instances, which increases the coverage of CSKB andexpands the ground-truth answer space, reducing the likelihood of selectingfalse-negative distractors. Extensive experiments demonstrate that CAR morerobustly generalizes to answering questions about zero-shot commonsensescenarios than existing methods, including large language models, such asGPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available athttps://github.com/HKUST-KnowComp/CAR.</description><author>Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu Song, Antoine Bosselut</author><pubDate>Wed, 24 May 2023 09:21:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14869v1</guid></item><item><title>CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering</title><link>http://arxiv.org/abs/2305.14869v2</link><description>The task of zero-shot commonsense question answering evaluates models ontheir capacity to reason about general scenarios beyond those presented inspecific datasets. Existing approaches for tackling this task leverage externalknowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model onsynthetic QA pairs constructed from CSKBs. In these approaches, negativeexamples (distractors) are formulated by randomly sampling from CSKBs usingfairly primitive keyword constraints. However, two bottlenecks limit theseapproaches: the inherent incompleteness of CSKBs limits the semantic coverageof synthetic QA pairs, and the lack of human annotations makes the samplednegative examples potentially uninformative and contradictory. To tackle theselimitations above, we propose Conceptualization-Augmented Reasoner (CAR), azero-shot commonsense question-answering framework that fully leverages thepower of conceptualization. Specifically, CAR abstracts a commonsense knowledgetriple to many higher-level instances, which increases the coverage of CSKB andexpands the ground-truth answer space, reducing the likelihood of selectingfalse-negative distractors. Extensive experiments demonstrate that CAR morerobustly generalizes to answering questions about zero-shot commonsensescenarios than existing methods, including large language models, such asGPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available athttps://github.com/HKUST-KnowComp/CAR.</description><author>Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu Song, Antoine Bosselut</author><pubDate>Fri, 20 Oct 2023 05:49:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14869v2</guid></item><item><title>KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource Language</title><link>http://arxiv.org/abs/2205.02364v3</link><description>The need for Question Answering datasets in low resource languages is themotivation of this research, leading to the development of Kencorpus SwahiliQuestion Answering Dataset, KenSwQuAD. This dataset is annotated from raw storytexts of Swahili low resource language, which is a predominantly spoken inEastern African and in other parts of the world. Question Answering (QA)datasets are important for machine comprehension of natural language for taskssuch as internet search and dialog systems. Machine learning systems needtraining data such as the gold standard Question Answering set developed inthis research. The research engaged annotators to formulate QA pairs fromSwahili texts collected by the Kencorpus project, a Kenyan languages corpus.The project annotated 1,445 texts from the total 2,585 texts with at least 5 QApairs each, resulting into a final dataset of 7,526 QA pairs. A qualityassurance set of 12.5% of the annotated texts confirmed that the QA pairs wereall correctly annotated. A proof of concept on applying the set to the QA taskconfirmed that the dataset can be usable for such tasks. KenSwQuAD has alsocontributed to resourcing of the Swahili language.</description><author>Barack W. Wanjawa, Lilian D. A. Wanzare, Florence Indede, Owen McOnyango, Lawrence Muchemi, Edward Ombui</author><pubDate>Sun, 09 Jul 2023 15:06:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2205.02364v3</guid></item><item><title>Explaining Autonomous Driving Actions with Visual Question Answering</title><link>http://arxiv.org/abs/2307.10408v1</link><description>The end-to-end learning ability of self-driving vehicles has achievedsignificant milestones over the last decade owing to rapid advances in deeplearning and computer vision algorithms. However, as autonomous drivingtechnology is a safety-critical application of artificial intelligence (AI),road accidents and established regulatory principles necessitate the need forthe explainability of intelligent action choices for self-driving vehicles. Tofacilitate interpretability of decision-making in autonomous driving, wepresent a Visual Question Answering (VQA) framework, which explains drivingactions with question-answering-based causal reasoning. To do so, we firstcollect driving videos in a simulation environment using reinforcement learning(RL) and extract consecutive frames from this log data uniformly for fiveselected action categories. Further, we manually annotate the extracted framesusing question-answer pairs as justifications for the actions chosen in eachscenario. Finally, we evaluate the correctness of the VQA-predicted answers foractions on unseen driving scenes. The empirical results suggest that the VQAmechanism can provide support to interpret real-time decisions of autonomousvehicles and help enhance overall driving safety.</description><author>Shahin Atakishiyev, Mohammad Salameh, Housam Babiker, Randy Goebel</author><pubDate>Wed, 19 Jul 2023 19:37:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.10408v1</guid></item><item><title>A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information</title><link>http://arxiv.org/abs/2305.07565v1</link><description>Existing question answering methods often assume that the input content(e.g., documents or videos) is always accessible to solve the task.Alternatively, memory networks were introduced to mimic the human process ofincremental comprehension and compression of the information in afixed-capacity memory. However, these models only learn how to maintain memoryby backpropagating errors in the answers through the entire network. Instead,it has been suggested that humans have effective mechanisms to boost theirmemorization capacities, such as rehearsal and anticipation. Drawinginspiration from these, we propose a memory model that performs rehearsal andanticipation while processing inputs to memorize important information forsolving question answering tasks from streaming data. The proposed mechanismsare applied self-supervised during training through masked modeling tasksfocused on coreference information. We validate our model on a short-sequence(bAbI) dataset as well as large-sequence textual (NarrativeQA) and video(ActivityNet-QA) question answering datasets, where it achieves substantialimprovements over previous memory network approaches. Furthermore, our ablationstudy confirms the proposed mechanisms' importance for memory models.</description><author>Vladimir Araujo, Alvaro Soto, Marie-Francine Moens</author><pubDate>Fri, 12 May 2023 16:46:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.07565v1</guid></item><item><title>Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification</title><link>http://arxiv.org/abs/2203.11155v3</link><description>Quantum density matrix represents all the information of the entire quantumsystem, and novel models of meaning employing density matrices naturally modellinguistic phenomena such as hyponymy and linguistic ambiguity, among others inquantum question answering tasks. Naturally, we argue that applying the quantumdensity matrix into classical Question Answering (QA) tasks can show moreeffective performance. Specifically, we (i) design a new mechanism based onLong Short-Term Memory (LSTM) to accommodate the case when the inputs arematrixes; (ii) apply the new mechanism to QA problems with Convolutional NeuralNetwork (CNN) and gain the LSTM-based QA model with the quantum density matrix.Experiments of our new model on TREC-QA and WIKI-QA data sets show encouragingresults. Similarly, we argue that the quantum density matrix can also enhancethe image feature information and the relationship between the features for theclassical image classification. Thus, we (i) combine density matrices and CNNto design a new mechanism; (ii) apply the new mechanism to some representativeclassical image classification tasks. A series of experiments show that theapplication of quantum density matrix in image classification has thegeneralization and high efficiency on different datasets. The application ofquantum density matrix both in classical question answering tasks and classicalimage classification tasks show more effective performance.</description><author>X. Q. Zhao, H. Wan</author><pubDate>Fri, 15 Sep 2023 08:55:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11155v3</guid></item><item><title>Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification</title><link>http://arxiv.org/abs/2203.11155v2</link><description>Quantum density matrix represents all the information of the entire quantumsystem, and novel models of meaning employing density matrices naturally modellinguistic phenomena such as hyponymy and linguistic ambiguity, among others inquantum question answering tasks. Naturally, we argue that applying the quantumdensity matrix into classical Question Answering (QA) tasks can show moreeffective performance. Specifically, we (i) design a new mechanism based onLong Short-Term Memory (LSTM) to accommodate the case when the inputs arematrixes; (ii) apply the new mechanism to QA problems with Convolutional NeuralNetwork (CNN) and gain the LSTM-based QA model with the quantum density matrix.Experiments of our new model on TREC-QA and WIKI-QA data sets show encouragingresults. Similarly, we argue that the quantum density matrix can also enhancethe image feature information and the relationship between the features for theclassical image classification. Thus, we (i) combine density matrices and CNNto design a new mechanism; (ii) apply the new mechanism to some representativeclassical image classification tasks. A series of experiments show that theapplication of quantum density matrix in image classification has thegeneralization and high efficiency on different datasets. The application ofquantum density matrix both in classical question answering tasks and classicalimage classification tasks show more effective performance.</description><author>X. Q. Zhao, H. Wan</author><pubDate>Thu, 11 May 2023 10:47:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2203.11155v2</guid></item><item><title>Generative Pre-trained Transformer for Vietnamese Community-based COVID-19 Question Answering</title><link>http://arxiv.org/abs/2310.14602v1</link><description>Recent studies have provided empirical evidence of the wide-ranging potentialof Generative Pre-trained Transformer (GPT), a pretrained language model, inthe field of natural language processing. GPT has been effectively employed asa decoder within state-of-the-art (SOTA) question answering systems, yieldingexceptional performance across various tasks. However, the current researchlandscape concerning GPT's application in Vietnamese remains limited. Thispaper aims to address this gap by presenting an implementation of GPT-2 forcommunity-based question answering specifically focused on COVID-19 relatedqueries in Vietnamese. We introduce a novel approach by conducting acomparative analysis of different Transformers vs SOTA models in thecommunity-based COVID-19 question answering dataset. The experimental findingsdemonstrate that the GPT-2 models exhibit highly promising outcomes,outperforming other SOTA models as well as previous community-based COVID-19question answering models developed for Vietnamese.</description><author>Tam Minh Vo, Khiem Vinh Tran</author><pubDate>Mon, 23 Oct 2023 07:14:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.14602v1</guid></item><item><title>Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi</title><link>http://arxiv.org/abs/2308.09862v2</link><description>The recent advances in deep-learning have led to the development of highlysophisticated systems with an unquenchable appetite for data. On the otherhand, building good deep-learning models for low-resource languages remains achallenging task. This paper focuses on developing a Question Answering datasetfor two such languages- Hindi and Marathi. Despite Hindi being the 3rd mostspoken language worldwide, with 345 million speakers, and Marathi being the11th most spoken language globally, with 83.2 million speakers, both languagesface limited resources for building efficient Question Answering systems. Totackle the challenge of data scarcity, we have developed a novel approach fortranslating the SQuAD 2.0 dataset into Hindi and Marathi. We release thelargest Question-Answering dataset available for these languages, with eachdataset containing 28,000 samples. We evaluate the dataset on variousarchitectures and release the best-performing models for both Hindi andMarathi, which will facilitate further research in these languages. Leveragingsimilarity tools, our method holds the potential to create datasets in diverselanguages, thereby enhancing the understanding of natural language acrossvaried linguistic contexts. Our fine-tuned models, code, and dataset will bemade publicly available.</description><author>Maithili Sabane, Onkar Litake, Aman Chadha</author><pubDate>Fri, 01 Sep 2023 01:37:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.09862v2</guid></item><item><title>T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering</title><link>http://arxiv.org/abs/2305.03453v3</link><description>Large Language Models (LLMs) have recently demonstrated exceptionalperformance in various Natural Language Processing (NLP) tasks. They have alsoshown the ability to perform chain-of-thought (CoT) reasoning to solve complexproblems. Recent studies have explored CoT reasoning in complex multimodalscenarios, such as the science question answering task, by fine-tuningmultimodal models with high-quality human-annotated CoT rationales. However,collecting high-quality COT rationales is usually time-consuming and costly.Besides, the annotated rationales are hardly accurate due to the externalessential information missed. To address these issues, we propose a novelmethod termed \emph{T-SciQ} that aims at teaching science question answeringwith LLM signals. The T-SciQ approach generates high-quality CoT rationales asteaching signals and is advanced to train much smaller models to perform CoTreasoning in complex modalities. Additionally, we introduce a novel data mixingstrategy to produce more effective teaching data samples by policy for simpleand complex science question answer problems. Extensive experimental resultsshow that our T-SciQ method achieves a new state-of-the-art performance on theScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approachoutperforms the most powerful fine-tuned baseline by 4.5\%.</description><author>Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen</author><pubDate>Wed, 16 Aug 2023 13:50:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03453v3</guid></item><item><title>T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering</title><link>http://arxiv.org/abs/2305.03453v1</link><description>Large Language Models (LLMs) have recently demonstrated exceptionalperformance in various Natural Language Processing (NLP) tasks. They have alsoshown the ability to perform chain-of-thought (CoT) reasoning to solve complexproblems. Recent studies have explored CoT reasoning in complex multimodalscenarios, such as the science question answering task, by fine-tuningmultimodal models with high-quality human-annotated CoT rationales. However,collecting high-quality COT rationales is usually time-consuming and costly.Besides, the annotated rationales are hardly accurate due to the redundantinformation involved or the essential information missed. To address theseissues, we propose a novel method termed \emph{T-SciQ} that aims at teachingscience question answering with LLM signals. The T-SciQ approach generateshigh-quality CoT rationales as teaching signals and is advanced to train muchsmaller models to perform CoT reasoning in complex modalities. Additionally, weintroduce a novel data mixing strategy to produce more effective teaching datasamples for simple and complex science question answer problems. Extensiveexperimental results show that our T-SciQ method achieves a newstate-of-the-art performance on the ScienceQA benchmark, with an accuracy of96.18%. Moreover, our approach outperforms the most powerful fine-tunedbaseline by 4.5%.</description><author>Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen</author><pubDate>Fri, 05 May 2023 12:56:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03453v1</guid></item><item><title>T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering</title><link>http://arxiv.org/abs/2305.03453v2</link><description>Large Language Models (LLMs) have recently demonstrated exceptionalperformance in various Natural Language Processing (NLP) tasks. They have alsoshown the ability to perform chain-of-thought (CoT) reasoning to solve complexproblems. Recent studies have explored CoT reasoning in complex multimodalscenarios, such as the science question answering task, by fine-tuningmultimodal models with high-quality human-annotated CoT rationales. However,collecting high-quality COT rationales is usually time-consuming and costly.Besides, the annotated rationales are hardly accurate due to the redundantinformation involved or the essential information missed. To address theseissues, we propose a novel method termed \emph{T-SciQ} that aims at teachingscience question answering with LLM signals. The T-SciQ approach generateshigh-quality CoT rationales as teaching signals and is advanced to train muchsmaller models to perform CoT reasoning in complex modalities. Additionally, weintroduce a novel data mixing strategy to produce more effective teaching datasamples for simple and complex science question answer problems. Extensiveexperimental results show that our T-SciQ method achieves a newstate-of-the-art performance on the ScienceQA benchmark, with an accuracy of96.18%. Moreover, our approach outperforms the most powerful fine-tunedbaseline by 4.5%.</description><author>Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen</author><pubDate>Tue, 09 May 2023 12:31:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03453v2</guid></item><item><title>Attention-Based Methods For Audio Question Answering</title><link>http://arxiv.org/abs/2305.19769v1</link><description>Audio question answering (AQA) is the task of producing natural languageanswers when a system is provided with audio and natural language questions. Inthis paper, we propose neural network architectures based on self-attention andcross-attention for the AQA task. The self-attention layers extract powerfulaudio and textual representations. The cross-attention maps audio features thatare relevant to the textual features to produce answers. All our models aretrained on the recently proposed Clotho-AQA dataset for both binary yes/noquestions and single-word answer questions. Our results clearly showimprovement over the reference method reported in the original paper. On theyes/no binary classification task, our proposed model achieves an accuracy of68.3% compared to 62.7% in the reference model. For the single-word answersmulticlass classifier, our model produces a top-1 and top-5 accuracy of 57.9%and 99.8% compared to 54.2% and 93.7% in the reference model respectively. Wefurther discuss some of the challenges in the Clotho-AQA dataset such as thepresence of the same answer word in multiple tenses, singular and plural forms,and the presence of specific and generic answers to the same question. Weaddress these issues and present a revised version of the dataset.</description><author>Parthasaarathy Sudarsanam, Tuomas Virtanen</author><pubDate>Wed, 31 May 2023 13:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.19769v1</guid></item><item><title>Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs</title><link>http://arxiv.org/abs/2307.16214v1</link><description>With the rising popularity of user-generated genealogical family trees, newgenealogical information systems have been developed. State-of-the-art naturalquestion answering algorithms use deep neural network (DNN) architecture basedon self-attention networks. However, some of these models use sequence-basedinputs and are not suitable to work with graph-based structure, whilegraph-based DNN models rely on high levels of comprehensiveness of knowledgegraphs that is nonexistent in the genealogical domain. Moreover, thesesupervised DNN models require training datasets that are absent in thegenealogical domain. This study proposes an end-to-end approach for questionanswering using genealogical family trees by: 1) representing genealogical dataas knowledge graphs, 2) converting them to texts, 3) combining them withunstructured texts, and 4) training a trans-former-based question answeringmodel. To evaluate the need for a dedicated approach, a comparison between thefine-tuned model (Uncle-BERT) trained on the auto-generated genealogicaldataset and state-of-the-art question-answering models was per-formed. Thefindings indicate that there are significant differences between answeringgenealogical questions and open-domain questions. Moreover, the proposedmethodology reduces complexity while increasing accuracy and may have practicalimplications for genealogical research and real-world projects, makinggenealogical data accessible to experts as well as the general public.</description><author>Omri Suissa, Maayan Zhitomirsky-Geffet, Avshalom Elmalech</author><pubDate>Sun, 30 Jul 2023 13:49:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16214v1</guid></item><item><title>Question Answering as Programming for Solving Time-Sensitive Questions</title><link>http://arxiv.org/abs/2305.14221v3</link><description>Question answering plays a pivotal role in human daily life because itinvolves our acquisition of knowledge about the world. However, due to thedynamic and ever-changing nature of real-world facts, the answer can becompletely different when the time constraint in the question changes.Recently, Large Language Models (LLMs) have shown remarkable intelligence inquestion answering, while our experiments reveal that the aforementionedproblems still pose a significant challenge to existing LLMs. This can beattributed to the LLMs' inability to perform rigorous reasoning based onsurface-level text semantics. To overcome this limitation, rather thanrequiring LLMs to directly answer the question, we propose a novel approachwhere we reframe the $\textbf{Q}$uestion $\textbf{A}$nswering task$\textbf{a}$s $\textbf{P}$rogramming ($\textbf{QAaP}$). Concretely, byleveraging modern LLMs' superior capability in understanding both naturallanguage and programming language, we endeavor to harness LLMs to representdiversely expressed text as well-structured code and select the best matchinganswer from multiple candidates through programming. We evaluate our QAaPframework on several time-sensitive question answering datasets and achievedecent improvement, up to $14.5$% over strong baselines. Our codes and data areavailable at https://github.com/TianHongZXY/qaap</description><author>Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, Yujiu Yang</author><pubDate>Fri, 20 Oct 2023 14:21:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14221v3</guid></item><item><title>Question Answering as Programming for Solving Time-Sensitive Questions</title><link>http://arxiv.org/abs/2305.14221v2</link><description>Question answering plays a pivotal role in human daily life because itinvolves our acquisition of knowledge about the world. However, due to thedynamic and ever-changing nature of real-world facts, the answer can becompletely different when the time constraint in the question changes.Recently, Large Language Models (LLMs) have shown remarkable intelligence inquestion answering, while our experiments reveal that the aforementionedproblems still pose a significant challenge to existing LLMs. This can beattributed to the LLMs' inability to perform rigorous reasoning based onsurface-level text semantics. To overcome this limitation, rather thanrequiring LLMs to directly answer the question, we propose a novel approachwhere we reframe the $\textbf{Q}$uestion $\textbf{A}$nswering task$\textbf{a}$s $\textbf{P}$rogramming ($\textbf{QAaP}$). Concretely, byleveraging modern LLMs' superior capability in understanding both naturallanguage and programming language, we endeavor to harness LLMs to representdiversely expressed text as well-structured code and select the best matchinganswer from multiple candidates through programming. We evaluate our QAaPframework on several time-sensitive question answering datasets and achievedecent improvement, up to $14.5$% over strong baselines. Our codes and data areavailable at https://github.com/TianHongZXY/qaap</description><author>Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, Yujiu Yang</author><pubDate>Wed, 18 Oct 2023 13:44:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14221v2</guid></item><item><title>Feature Engineering in Learning-to-Rank for Community Question Answering Task</title><link>http://arxiv.org/abs/2309.07610v1</link><description>Community question answering (CQA) forums are Internet-based platforms whereusers ask questions about a topic and other expert users try to providesolutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer,StackExchange exist with a lot of user-generated data. These data are leveragedin automated CQA ranking systems where similar questions (and answers) arepresented in response to the query of the user. In this work, we empiricallyinvestigate a few aspects of this domain. Firstly, in addition to traditionalfeatures like TF-IDF, BM25 etc., we introduce a BERT-based feature thatcaptures the semantic similarity between the question and answer. Secondly,most of the existing research works have focused on features extracted onlyfrom the question part; features extracted from answers have not been exploredextensively. We combine both types of features in a linear fashion. Thirdly,using our proposed concepts, we conduct an empirical investigation withdifferent rank-learning algorithms, some of which have not been used so far inCQA domain. On three standard CQA datasets, our proposed framework achievesstate-of-the-art performance. We also analyze importance of the features we usein our investigation. This work is expected to guide the practitioners toselect a better set of features for the CQA retrieval task.</description><author>Nafis Sajid, Md Rashidul Hasan, Muhammad Ibrahim</author><pubDate>Thu, 14 Sep 2023 12:18:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.07610v1</guid></item><item><title>Integrating Image Features with Convolutional Sequence-to-sequence Network for Multilingual Visual Question Answering</title><link>http://arxiv.org/abs/2303.12671v2</link><description>Visual Question Answering (VQA) is a task that requires computers to givecorrect answers for the input questions based on the images. This task can besolved by humans with ease but is a challenge for computers. TheVLSP2022-EVJVQA shared task carries the Visual Question Answering task in themultilingual domain on a newly released dataset: UIT-EVJVQA, in which thequestions and answers are written in three different languages: English,Vietnamese and Japanese. We approached the challenge as a sequence-to-sequencelearning task, in which we integrated hints from pre-trained state-of-the-artVQA models and image features with Convolutional Sequence-to-Sequence networkto generate the desired answers. Our results obtained up to 0.3442 by F1 scoreon the public test set, 0.4210 on the private test set, and placed 3rd in thecompetition.</description><author>Triet Minh Thai, Son T. Luu</author><pubDate>Sun, 03 Sep 2023 15:50:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12671v2</guid></item><item><title>Question answering using deep learning in low resource Indian language Marathi</title><link>http://arxiv.org/abs/2309.15779v1</link><description>Precise answers are extracted from a text for a given input question in aquestion answering system. Marathi question answering system is created inrecent studies by using ontology, rule base and machine learning basedapproaches. Recently transformer models and transfer learning approaches areused to solve question answering challenges. In this paper we investigatedifferent transformer models for creating a reading comprehension-based Marathiquestion answering system. We have experimented on different pretrained Marathilanguage multilingual and monolingual models like Multilingual Representationsfor Indian Languages (MuRIL), MahaBERT, Indic Bidirectional EncoderRepresentations from Transformers (IndicBERT) and fine-tuned it on a Marathireading comprehension-based data set. We got the best accuracy in a MuRILmultilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuningthe model on the Marathi dataset.</description><author>Dhiraj Amin, Sharvari Govilkar, Sagar Kulkarni</author><pubDate>Wed, 27 Sep 2023 17:53:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15779v1</guid></item><item><title>Prompt Guided Copy Mechanism for Conversational Question Answering</title><link>http://arxiv.org/abs/2308.03422v1</link><description>Conversational Question Answering (CQA) is a challenging task that aims togenerate natural answers for conversational flow questions. In this paper, wepropose a pluggable approach for extractive methods that introduces a novelprompt-guided copy mechanism to improve the fluency and appropriateness of theextracted answers. Our approach uses prompts to link questions to answers andemploys attention to guide the copy mechanism to verify the naturalness ofextracted answers, making necessary edits to ensure that the answers are fluentand appropriate. The three prompts, including a question-rationale relationshipprompt, a question description prompt, and a conversation history prompt,enhance the copy mechanism's performance. Our experiments demonstrate that thisapproach effectively promotes the generation of natural answers and achievesgood results in the CoQA challenge.</description><author>Yong Zhang, Zhitao Li, Jianzong Wang, Yiming Gao, Ning Cheng, Fengying Yu, Jing Xiao</author><pubDate>Mon, 07 Aug 2023 10:15:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.03422v1</guid></item><item><title>Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering</title><link>http://arxiv.org/abs/2305.05936v1</link><description>Commonsense question answering (QA) research requires machines to answerquestions based on commonsense knowledge. However, this research requiresexpensive labor costs to annotate data as the basis of research, and modelsthat rely on fine-tuning paradigms only apply to specific tasks, rather thanlearn a general commonsense reasoning ability. As a more robust method,zero-shot commonsense question answering shows a good prospect. The currentzero-shot framework tries to convert triples in commonsense knowledge graphs(KGs) into QA-form samples as the pre-trained data source to incorporatecommonsense knowledge into the model. However, this method ignores themulti-hop relationship in the KG, which is also an important central problem incommonsense reasoning. In this paper, we propose a novel multi-hop commonsenseknowledge injection framework. Specifically, it explores multi-hop reasoningparadigm in KGs that conform to linguistic logic, and we further propose twomulti-hop QA generation methods based on KGs. Then, we utilize contrastivelearning to pre-train the model with the synthetic QA dataset to injectmulti-hop commonsense knowledge. Extensive experiments on five commonsensequestion answering benchmarks demonstrate that our framework achievesstate-of-art performance.</description><author>Xin Guan, Biwei Cao, Qingqing Gao, Zheng Yin, Bo Liu, Jiuxin Cao</author><pubDate>Wed, 10 May 2023 08:13:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.05936v1</guid></item><item><title>Progressive Spatio-temporal Perception for Audio-Visual Question Answering</title><link>http://arxiv.org/abs/2308.05421v1</link><description>Audio-Visual Question Answering (AVQA) task aims to answer questions aboutdifferent visual objects, sounds, and their associations in videos. Suchnaturally multi-modal videos are composed of rich and complex dynamicaudio-visual components, where most of which could be unrelated to the givenquestions, or even play as interference in answering the content of interest.Oppositely, only focusing on the question-aware audio-visual content could getrid of influence, meanwhile enabling the model to answer more efficiently. Inthis paper, we propose a Progressive Spatio-Temporal Perception Network(PSTP-Net), which contains three modules that progressively identify keyspatio-temporal regions w.r.t. questions. Specifically, a temporal segmentselection module is first introduced to select the most relevant audio-visualsegments related to the given question. Then, a spatial region selection moduleis utilized to choose the most relevant regions associated with the questionfrom the selected temporal segments. To further refine the selection offeatures, an audio-guided visual attention module is employed to perceive theassociation between auido and selected spatial regions. Finally, thespatio-temporal features from these modules are integrated for answering thequestion. Extensive experimental results on the public MUSIC-AVQA and AVQAdatasets provide compelling evidence of the effectiveness and efficiency ofPSTP-Net. Code is available at:\href{https://github.com/GeWu-Lab/PSTP-Net}{https://github.com/GeWu-Lab/PSTP-Net}</description><author>Guangyao Li, Wenxuan Hou, Di Hu</author><pubDate>Thu, 10 Aug 2023 09:29:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.05421v1</guid></item><item><title>TG-VQA: Ternary Game of Video Question Answering</title><link>http://arxiv.org/abs/2305.10049v2</link><description>Video question answering aims at answering a question about the video contentby reasoning the alignment semantics within them. However, since relyingheavily on human instructions, i.e., annotations or priors, current contrastivelearning-based VideoQA methods remains challenging to perform fine-grainedvisual-linguistic alignments. In this work, we innovatively resort to gametheory, which can simulate complicated relationships among multiple playerswith specific interaction strategies, e.g., video, question, and answer asternary players, to achieve fine-grained alignment for VideoQA task.Specifically, we carefully design a VideoQA-specific interaction strategy totailor the characteristics of VideoQA, which can mathematically generate thefine-grained visual-linguistic alignment label without label-intensive efforts.Our TG-VQA outperforms existing state-of-the-art by a large margin (more than5%) on long-term and short-term VideoQA datasets, verifying its effectivenessand generalization ability. Thanks to the guidance of game-theoreticinteraction, our model impressively convergences well on limited data (${10}^4~videos$), surpassing most of those pre-trained on large-scale data($10^7~videos$).</description><author>Hao Li, Peng Jin, Zesen Cheng, Songyang Zhang, Kai Chen, Zhennan Wang, Chang Liu, Jie Chen</author><pubDate>Thu, 18 May 2023 07:27:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10049v2</guid></item><item><title>TG-VQA: Ternary Game of Video Question Answering</title><link>http://arxiv.org/abs/2305.10049v1</link><description>Video question answering aims at answering a question about the video contentby reasoning the alignment semantics within them. However, since relyingheavily on human instructions, i.e., annotations or priors, current contrastivelearning-based VideoQA methods remains challenging to perform fine-grainedvisual-linguistic alignments. In this work, we innovatively resort to gametheory, which can simulate complicated relationships among multiple playerswith specific interaction strategies, e.g., video, question, and answer asternary players, to achieve fine-grained alignment for VideoQA task.Specifically, we carefully design a VideoQA-specific interaction strategy totailor the characteristics of VideoQA, which can mathematically generate thefine-grained visual-linguistic alignment label without label-intensive efforts.Our TG-VQA outperforms existing state-of-the-art by a large margin (more than5%) on long-term and short-term VideoQA datasets, verifying its effectivenessand generalization ability. Thanks to the guidance of game-theoreticinteraction, our model impressively convergences well on limited data (${10}^4~videos$), surpassing most of those pre-trained on large-scale data($10^7~videos$).</description><author>Hao Li, Peng Jin, Zesen Cheng, Songyang Zhang, Kai Chen, Zhennan Wang, Chang Liu, Jie Chen</author><pubDate>Wed, 17 May 2023 09:42:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.10049v1</guid></item><item><title>Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering</title><link>http://arxiv.org/abs/2305.15056v1</link><description>Explainable question answering (XQA) aims to answer a given question andprovide an explanation why the answer is selected. Existing XQA methods focuson reasoning on a single knowledge source, e.g., structured knowledge bases,unstructured corpora, etc. However, integrating information from heterogeneousknowledge sources is essential to answer complex questions. In this paper, wepropose to leverage question decomposing for heterogeneous knowledgeintegration, by breaking down a complex question into simpler ones, andselecting the appropriate knowledge source for each sub-question. To facilitatereasoning, we propose a novel two-stage XQA framework, Reasoning overHierarchical Question Decomposition Tree (RoHT). First, we build theHierarchical Question Decomposition Tree (HQDT) to understand the semantics ofa complex question; then, we conduct probabilistic reasoning over HQDT fromroot to leaves recursively, to aggregate heterogeneous knowledge at differenttree levels and search for a best solution considering the decomposing andanswering probabilities. The experiments on complex QA datasets KQA Pro andMusique show that our framework outperforms SOTA methods significantly,demonstrating the effectiveness of leveraging question decomposing forknowledge integration and our RoHT framework.</description><author>Jiajie Zhang, Shulin Cao, Tingjia Zhang, Xin Lv, Jiaxin Shi, Qi Tian, Juanzi Li, Lei Hou</author><pubDate>Wed, 24 May 2023 12:45:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.15056v1</guid></item><item><title>Pre-Training Multi-Modal Dense Retrievers for Outside-Knowledge Visual Question Answering</title><link>http://arxiv.org/abs/2306.16478v1</link><description>This paper studies a category of visual question answering tasks, in whichaccessing external knowledge is necessary for answering the questions. Thiscategory is called outside-knowledge visual question answering (OK-VQA). Amajor step in developing OK-VQA systems is to retrieve relevant documents forthe given multi-modal query. Current state-of-the-art asymmetric denseretrieval model for this task uses an architecture with a multi-modal queryencoder and a uni-modal document encoder. Such an architecture requires a largeamount of training data for effective performance. We propose an automatic datageneration pipeline for pre-training passage retrieval models for OK-VQA tasks.The proposed approach leads to 26.9% Precision@5 improvements compared to thecurrent state-of-the-art asymmetric architecture. Additionally, the proposedpre-training approach exhibits a good ability in zero-shot retrieval scenarios.</description><author>Alireza Salemi, Mahta Rafiee, Hamed Zamani</author><pubDate>Wed, 28 Jun 2023 19:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16478v1</guid></item><item><title>Comparative Analysis of Artificial Intelligence for Indian Legal Question Answering (AILQA) Using Different Retrieval and QA Models</title><link>http://arxiv.org/abs/2309.14735v1</link><description>Legal question-answering (QA) systems have the potential to revolutionize theway legal professionals interact with case law documents. This paper conducts acomparative analysis of existing artificial intelligence models for theirutility in answering legal questions within the Indian legal system,specifically focusing on Indian Legal Question Answering (AILQA) and our studyinvestigates the efficacy of different retrieval and QA algorithms currentlyavailable. Utilizing the OpenAI GPT model as a benchmark, along with queryprompts, our investigation shows that existing AILQA systems can automaticallyinterpret natural language queries from users and generate highly accurateresponses. This research is particularly focused on applications within theIndian criminal justice domain, which has its own set of challenges due to itscomplexity and resource constraints. In order to rigorously assess theperformance of these models, empirical evaluations are complemented by feedbackfrom practicing legal professionals, thereby offering a multifaceted view onthe capabilities and limitations of AI in the context of Indian legalquestion-answering.</description><author>Shubham Kumar Nigam, Shubham Kumar Mishra, Ayush Kumar Mishra, Noel Shallum, Arnab Bhattacharya</author><pubDate>Tue, 26 Sep 2023 08:56:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14735v1</guid></item><item><title>Improving Selective Visual Question Answering by Learning from Your Peers</title><link>http://arxiv.org/abs/2306.08751v1</link><description>Despite advances in Visual Question Answering (VQA), the ability of models toassess their own correctness remains underexplored. Recent work has shown thatVQA models, out-of-the-box, can have difficulties abstaining from answeringwhen they are wrong. The option to abstain, also called Selective Prediction,is highly relevant when deploying systems to users who must trust the system'soutput (e.g., VQA assistants for users with visual impairments). For suchscenarios, abstention can be especially important as users may provideout-of-distribution (OOD) or adversarial inputs that make incorrect answersmore likely. In this work, we explore Selective VQA in both in-distribution(ID) and OOD scenarios, where models are presented with mixtures of ID and OODdata. The goal is to maximize the number of questions answered while minimizingthe risk of error on those questions. We propose a simple yet effectiveLearning from Your Peers (LYP) approach for training multimodal selectionfunctions for making abstention decisions. Our approach uses predictions frommodels trained on distinct subsets of the training data as targets foroptimizing a Selective VQA model. It does not require additional manual labelsor held-out data and provides a signal for identifying examples that areeasy/difficult to generalize to. In our extensive evaluations, we show thisbenefits a number of models across different architectures and scales. Overall,for ID, we reach 32.92% in the selective prediction metric coverage at 1% riskof error (C@1%) which doubles the previous best coverage of 15.79% on thistask. For mixed ID/OOD, using models' softmax confidences for abstentiondecisions performs very poorly, answering &lt;5% of questions at 1% risk of erroreven when faced with only 10% OOD examples, but a learned selection functionwith LYP can increase that to 25.38% C@1%.</description><author>Corentin Dancette, Spencer Whitehead, Rishabh Maheshwary, Ramakrishna Vedantam, Stefan Scherer, Xinlei Chen, Matthieu Cord, Marcus Rohrbach</author><pubDate>Wed, 14 Jun 2023 22:22:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.08751v1</guid></item><item><title>Mixture of Prompt Experts for Generalizable and Interpretable Question Answering</title><link>http://arxiv.org/abs/2305.14628v1</link><description>One of the ultimate quests of question answering (QA) is to deploy a systemthat can answer any type of question from the users, and refrain from answeringwhen it does not know the answer. While recent advancements in scaling largelanguage models (LLMs) brought significant improvements on various QA datasets,it remains difficult for a single model to generalize across question typesthat require distinct reasoning abilities. In this paper, we first provideempirical evidence that state-of-the-art LLMs such as Codex suffer from poorgeneralizability on question types beyond those seen in the prompt. To addressthis, we propose a Mixture-of-Prompt-Experts (MOPE) system that ensemblesmultiple specialized LLMs. We first implement each specialized model based onthe same backbone model (Codex) but with prompts optimized for differentreasoning categories including factual, multihop, mathematical, and commonsensereasoning. By strategically selecting the best specialized model for each givenquestion, our MOPE system significantly outperforms any single specializedmodel on a collection of 12 QA datasets from four reasoning types. Moreover,the attribution and agreement among specialized expert models offer greaterinterpretability, allowing for better selective question answering. Our humanstudy further confirms that presenting the expert predictions and answerselection process helps annotators more accurately decide when to trust thesystem's output. We release all code and data to facilitate future work.</description><author>Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan Boyd-Graber</author><pubDate>Wed, 24 May 2023 03:00:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14628v1</guid></item><item><title>KEPR: Knowledge Enhancement and Plausibility Ranking for Generative Commonsense Question Answering</title><link>http://arxiv.org/abs/2305.08347v1</link><description>Generative commonsense question answering (GenCQA) is a task of automaticallygenerating a list of answers given a question. The answer list is required tocover all reasonable answers. This presents the considerable challenges ofproducing diverse answers and ranking them properly. Incorporating a variety ofclosely-related background knowledge into the encoding of questions enables thegeneration of different answers. Meanwhile, learning to distinguish positiveanswers from negative ones potentially enhances the probabilistic estimation ofplausibility, and accordingly, the plausibility-based ranking. Therefore, wepropose a Knowledge Enhancement and Plausibility Ranking (KEPR) approachgrounded on the Generate-Then-Rank pipeline architecture. Specifically, weexpand questions in terms of Wiktionary commonsense knowledge of keywords, andreformulate them with normalized patterns. Dense passage retrieval is utilizedfor capturing relevant knowledge, and different PLM-based (BART, GPT2 and T5)networks are used for generating answers. On the other hand, we develop anELECTRA-based answer ranking model, where logistic regression is conductedduring training, with the aim of approximating different levels of plausibilityin a polar classification scenario. Extensive experiments on the benchmarkProtoQA show that KEPR obtains substantial improvements, compared to the strongbaselines. Within the experimental models, the T5-based GenCQA with KEPRobtains the best performance, which is up to 60.91% at the primary canonicalmetric Inc@3. It outperforms the existing GenCQA models on the currentleaderboard of ProtoQA.</description><author>Zhifeng Li, Bowei Zou, Yifan Fan, Yu Hong</author><pubDate>Mon, 15 May 2023 05:58:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08347v1</guid></item><item><title>Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering</title><link>http://arxiv.org/abs/2309.14389v1</link><description>Recent document question answering models consist of two key components: thevision encoder, which captures layout and visual elements in images, and aLarge Language Model (LLM) that helps contextualize questions to the image andsupplements them with external world knowledge to generate accurate answers.However, the relative contributions of the vision encoder and the languagemodel in these tasks remain unclear. This is especially interesting given theeffectiveness of instruction-tuned LLMs, which exhibit remarkable adaptabilityto new tasks. To this end, we explore the following aspects in this work: (1)The efficacy of an LLM-only approach on document question answering tasks (2)strategies for serializing textual information within document images andfeeding it directly to an instruction-tuned LLM, thus bypassing the need for anexplicit vision encoder (3) thorough quantitative analysis on the feasibilityof such an approach. Our comprehensive analysis encompasses six diversebenchmark datasets, utilizing LLMs of varying scales. Our findings reveal thata strategy exclusively reliant on the LLM yields results that are on par withor closely approach state-of-the-art performance across a range of datasets. Weposit that this evaluation framework will serve as a guiding resource forselecting appropriate datasets for future research endeavors that emphasize thefundamental importance of layout and image content information.</description><author>Nidhi Hegde, Sujoy Paul, Gagan Madan, Gaurav Aggarwal</author><pubDate>Mon, 25 Sep 2023 08:01:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.14389v1</guid></item><item><title>MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering</title><link>http://arxiv.org/abs/2310.05007v1</link><description>Few-shot question answering (QA) aims at achieving satisfactory results onmachine question answering when only a few training samples are available.Recent advances mostly rely on the power of pre-trained large language models(LLMs) and fine-tuning in specific settings. Although the pre-training stagehas already equipped LLMs with powerful reasoning capabilities, LLMs still needto be fine-tuned to adapt to specific domains to achieve the best results. Inthis paper, we propose to select the most informative data for fine-tuning,thereby improving the efficiency of the fine-tuning process with comparative oreven better accuracy on the open-domain QA task. We present MinPrompt, aminimal data augmentation framework for open-domain QA based on an approximategraph algorithm and unsupervised question generation. We transform the raw textinto a graph structure to build connections between different factualsentences, then apply graph algorithms to identify the minimal set of sentencesneeded to cover the most information in the raw text. We then generate QA pairsbased on the identified sentence subset and train the model on the selectedsentences to obtain the final model. Empirical results on several benchmarkdatasets and theoretical analysis show that MinPrompt is able to achievecomparable or better results than baselines with a high degree of efficiency,bringing improvements in F-1 scores by up to 27.5%.</description><author>Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Wei Wang</author><pubDate>Sun, 08 Oct 2023 05:44:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05007v1</guid></item><item><title>Towards Graph-hop Retrieval and Reasoning in Complex Question Answering over Textual Database</title><link>http://arxiv.org/abs/2305.14211v1</link><description>In Textual question answering (TQA) systems, complex questions often requireretrieving multiple textual fact chains with multiple reasoning steps. Whileexisting benchmarks are limited to single-chain or single-hop retrievalscenarios. In this paper, we propose to conduct Graph-Hop -- a novelmulti-chains and multi-hops retrieval and reasoning paradigm in complexquestion answering. We construct a new benchmark called ReasonGraphQA, whichprovides explicit and fine-grained evidence graphs for complex questions tosupport interpretable reasoning, comprehensive and detailed reasoning. AndReasonGraphQA also shows an advantage in reasoning diversity and scale.Moreover, We propose a strong graph-hop baseline called Bidirectional GraphRetrieval (BGR) method for generating an explanation graph of textual evidencein knowledge reasoning and question answering. We have thoroughly evaluatedexisting evidence retrieval and reasoning models on the ReasonGraphQA.Experiments highlight Graph-Hop is a promising direction for answering complexquestions, but it still has certain limitations. We have further studiedmitigation strategies to meet these challenges and discuss future directions.</description><author>Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao</author><pubDate>Tue, 23 May 2023 17:28:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.14211v1</guid></item><item><title>Contributions to the Improvement of Question Answering Systems in the Biomedical Domain</title><link>http://arxiv.org/abs/2307.13631v1</link><description>This thesis work falls within the framework of question answering (QA) in thebiomedical domain where several specific challenges are addressed, such asspecialized lexicons and terminologies, the types of treated questions, and thecharacteristics of targeted documents. We are particularly interested instudying and improving methods that aim at finding accurate and short answersto biomedical natural language questions from a large scale of biomedicaltextual documents in English. QA aims at providing inquirers with direct, shortand precise answers to their natural language questions. In this Ph.D. thesis,we propose four contributions to improve the performance of QA in thebiomedical domain. In our first contribution, we propose a machinelearning-based method for question type classification to determine the typesof given questions which enable to a biomedical QA system to use theappropriate answer extraction method. We also propose an another machinelearning-based method to assign one or more topics (e.g., pharmacological,test, treatment, etc.) to given questions in order to determine the semantictypes of the expected answers which are very useful in generating specificanswer retrieval strategies. In the second contribution, we first propose adocument retrieval method to retrieve a set of relevant documents that arelikely to contain the answers to biomedical questions from the MEDLINEdatabase. We then present a passage retrieval method to retrieve a set ofrelevant passages to questions. In the third contribution, we propose specificanswer extraction methods to generate both exact and ideal answers. Finally, inthe fourth contribution, we develop a fully automated semantic biomedical QAsystem called SemBioNLQA which is able to deal with a variety of naturallanguage questions and to generate appropriate answers by providing both exactand ideal answers.</description><author>Mourad Sarrouti</author><pubDate>Tue, 25 Jul 2023 17:31:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.13631v1</guid></item><item><title>BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph</title><link>http://arxiv.org/abs/2212.05798v2</link><description>Answering complex questions over textual resources remains a challenge,particularly when dealing with nuanced relationships between multiple entitiesexpressed within natural-language sentences. To this end, curated knowledgebases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely usedand gained great acceptance for question-answering (QA) applications in thepast decade. While these KBs offer a structured knowledge representation, theylack the contextual diversity found in natural-language sources. To addressthis limitation, BigText-QA introduces an integrated QA approach, which is ableto answer questions based on a more redundant form of a knowledge graph (KG)that organizes both structured and unstructured (i.e., "hybrid") knowledge in aunified graphical representation. Thereby, BigText-QA is able to combine thebest of both worlds$\unicode{x2013}$a canonical set of named entities, mappedto a structured background KB (such as YAGO or Wikidata), as well as an openset of textual clauses providing highly diversified relational paraphrases withrich context information. Our experimental results demonstrate that BigText-QAoutperforms DrQA, a neural-network-based QA system, and achieves competitiveresults to QUEST, a graph-based unsupervised QA system.</description><author>Jingjing Xu, Maria Biryukov, Martin Theobald, Vinu Ellampallil Venugopal</author><pubDate>Thu, 07 Sep 2023 13:22:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.05798v2</guid></item><item><title>UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering</title><link>http://arxiv.org/abs/2307.02783v1</link><description>In recent years, artificial intelligence has played an important role inmedicine and disease diagnosis, with many applications to be mentioned, one ofwhich is Medical Visual Question Answering (MedVQA). By combining computervision and natural language processing, MedVQA systems can assist experts inextracting relevant information from medical image based on a given questionand providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023challenge carried out visual question answering task in the gastrointestinaldomain, which includes gastroscopy and colonoscopy images. Our team approachedTask 1 of the challenge by proposing a multimodal learning method with imageenhancement to improve the VQA performance on gastrointestinal images. Themultimodal architecture is set up with BERT encoder and different pre-trainedvision models based on convolutional neural network (CNN) and Transformerarchitecture for features extraction from question and endoscopy image. Theresult of this study highlights the dominance of Transformer-based visionmodels over the CNNs and demonstrates the effectiveness of the imageenhancement process, with six out of the eight vision models achieving betterF1-Score. Our best method, which takes advantages of BERT+BEiT fusion and imageenhancement, achieves up to 87.25% accuracy and 91.85% F1-Score on thedevelopment test set, while also producing good result on the private test setwith accuracy of 82.01%.</description><author>Triet M. Thai, Anh T. Vo, Hao K. Tieu, Linh N. P. Bui, Thien T. B. Nguyen</author><pubDate>Thu, 06 Jul 2023 06:22:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02783v1</guid></item><item><title>A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering</title><link>http://arxiv.org/abs/2304.13649v1</link><description>Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering aquestion about an image whose answer does not lie in the image. This paperpresents a new pipeline for KI-VQA tasks, consisting of a retriever and areader. First, we introduce DEDR, a symmetric dual encoding dense retrievalframework in which documents and queries are encoded into a shared embeddingspace using uni-modal (textual) and multi-modal encoders. We introduce aniterative knowledge distillation approach that bridges the gap between therepresentation spaces in these two encoders. Extensive evaluation on twowell-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDRoutperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA,respectively. Utilizing the passages retrieved by DEDR, we further introduceMM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generatinga textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, andeach retrieved passage separately and uses all passages jointly in its decoder.Compared to competitive baselines in the literature, this approach leads to5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQAand FVQA, respectively.</description><author>Alireza Salemi, Juan Altmayer Pizzorno, Hamed Zamani</author><pubDate>Wed, 26 Apr 2023 17:14:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13649v1</guid></item><item><title>Medical Visual Question Answering: A Survey</title><link>http://arxiv.org/abs/2111.10056v3</link><description>Medical Visual Question Answering~(VQA) is a combination of medicalartificial intelligence and popular VQA challenges. Given a medical image and aclinically relevant question in natural language, the medical VQA system isexpected to predict a plausible and convincing answer. Although thegeneral-domain VQA has been extensively studied, the medical VQA still needsspecific investigation and exploration due to its task features. In the firstpart of this survey, we collect and discuss the publicly available medical VQAdatasets up-to-date about the data source, data quantity, and task feature. Inthe second part, we review the approaches used in medical VQA tasks. Wesummarize and discuss their techniques, innovations, and potentialimprovements. In the last part, we analyze some medical-specific challenges forthe field and discuss future research directions. Our goal is to providecomprehensive and helpful information for researchers interested in the medicalvisual question answering field and encourage them to conduct further researchin this field.</description><author>Zhihong Lin, Donghao Zhang, Qingyi Tao, Danli Shi, Gholamreza Haffari, Qi Wu, Mingguang He, Zongyuan Ge</author><pubDate>Wed, 07 Jun 2023 01:37:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2111.10056v3</guid></item><item><title>Context Generation Improves Open Domain Question Answering</title><link>http://arxiv.org/abs/2210.06349v2</link><description>Closed-book question answering (QA) requires a model to directly answer anopen-domain question without access to any external knowledge. Prior work onclosed-book QA either directly finetunes or prompts a pretrained language model(LM) to leverage the stored knowledge. However, they do not fully exploit theparameterized knowledge. To address this issue, we propose a two-stage,closed-book QA framework which employs a coarse-to-fine approach to extractrelevant knowledge and answer a question. Our approach first generates arelated context for a given question by prompting a pretrained LM. We thenprompt the same LM for answer prediction using the generated context and thequestion. Additionally, to eliminate failure caused by context uncertainty, wemarginalize over generated contexts. Experimental results on three QAbenchmarks show that our method significantly outperforms previous closed-bookQA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-bookmethods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Ourmethod is able to better exploit the stored knowledge in pretrained LMs withoutadding extra learnable parameters or needing finetuning, and paves the way forhybrid models that integrate pretrained LMs with external knowledge.</description><author>Dan Su, Mostofa Patwary, Shrimai Prabhumoye, Peng Xu, Ryan Prenger, Mohammad Shoeybi, Pascale Fung, Anima Anandkumar, Bryan Catanzaro</author><pubDate>Thu, 27 Apr 2023 06:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.06349v2</guid></item><item><title>Elaboration-Generating Commonsense Question Answering at Scale</title><link>http://arxiv.org/abs/2209.01232v2</link><description>In question answering requiring common sense, language models (e.g., GPT-3)have been used to generate text expressing background knowledge that helpsimprove performance. Yet the cost of working with such models is very high; inthis work, we finetune smaller language models to generate useful intermediatecontext, referred to here as elaborations. Our framework alternates betweenupdating two language models -- an elaboration generator and an answerpredictor -- allowing each to influence the other. Using less than 0.5% of theparameters of GPT-3, our model outperforms alternatives with similar sizes andcloses the gap on GPT-3 on four commonsense question answering benchmarks.Human evaluations show that the quality of the generated elaborations is high.</description><author>Wenya Wang, Vivek Srikumar, Hanna Hajishirzi, Noah A. Smith</author><pubDate>Fri, 14 Jul 2023 22:43:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01232v2</guid></item><item><title>HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge</title><link>http://arxiv.org/abs/2308.11257v1</link><description>The semantic parsing-based method is an important research branch forknowledge-based question answering. It usually generates executable programslean upon the question and then conduct them to reason answers over a knowledgebase. Benefit from this inherent mechanism, it has advantages in theperformance and the interpretability. However,traditional semantic parsingmethods usually generate a complete program before executing it, whichstruggles with multi-hop question answering over heterogeneous knowledge.Firstly,a complete multi-hop program relies on multiple heterogeneoussupporting facts, and it is difficult for models to receive these factssimultaneously. Secondly,these methods ignore the interaction informationbetween the previous-hop execution result and the current-hop programgeneration. To alleviate these challenges, we propose a self-iterativeframework for multi-hop program generation (HopPG) over heterogeneousknowledge, which leverages the previous-hop execution results to retrievesupporting facts and generate subsequent programs iteratively. We evaluate ourmodel on MMQA-T^2. The experimental results show that HopPG outperformsexisting semantic-parsing-based baselines, especially on the multi-hopquestions.</description><author>Yingyao Wang, Yongwei Zhou, Chaoqun Duan, Junwei Bao, Tiejun Zhao</author><pubDate>Tue, 22 Aug 2023 09:00:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11257v1</guid></item><item><title>Visual Causal Scene Refinement for Video Question Answering</title><link>http://arxiv.org/abs/2305.04224v2</link><description>Existing methods for video question answering (VideoQA) often suffer fromspurious correlations between different modalities, leading to a failure inidentifying the dominant visual evidence and the intended question. Moreover,these methods function as black boxes, making it difficult to interpret thevisual scene during the QA process. In this paper, to discover critical videosegments and frames that serve as the visual causal scene for generatingreliable answers, we present a causal analysis of VideoQA and propose aframework for cross-modal causal relational reasoning, named Visual CausalScene Refinement (VCSR). Particularly, a set of causal front-door interventionoperations is introduced to explicitly find the visual causal scenes at bothsegment and frame levels. Our VCSR involves two essential modules: i) theQuestion-Guided Refiner (QGR) module, which refines consecutive video framesguided by the question semantics to obtain more representative segment featuresfor causal front-door intervention; ii) the Causal Scene Separator (CSS)module, which discovers a collection of visual causal and non-causal scenesbased on the visual-linguistic causal relevance and estimates the causal effectof the scene-separating intervention in a contrastive learning manner.Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasetsdemonstrate the superiority of our VCSR in discovering visual causal scene andachieving robust video question answering. The code is available athttps://github.com/YangLiu9208/VCSR.</description><author>Yushen Wei, Yang Liu, Hong Yan, Guanbin Li, Liang Lin</author><pubDate>Tue, 01 Aug 2023 03:46:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04224v2</guid></item><item><title>Visual Causal Scene Refinement for Video Question Answering</title><link>http://arxiv.org/abs/2305.04224v1</link><description>Existing methods for video question answering (VideoQA) often suffer fromspurious correlations between different modalities, leading to a failure inidentifying the dominant visual evidence and the intended question. Moreover,these methods function as black boxes, making it difficult to interpret thevisual scene during the QA process. In this paper, to discover critical videosegments and frames that serve as the visual causal scene for generatingreliable answers, we present a causal analysis of VideoQA and propose aframework for cross-modal causal relational reasoning, named Visual CausalScene Refinement (VCSR). Particularly, a set of causal front-door interventionoperations is introduced to explicitly find the visual causal scenes at bothsegment and frame levels. Our VCSR involves two essential modules: i) theQuestion-Guided Refiner (QGR) module, which refines consecutive video framesguided by the question semantics to obtain more representative segment featuresfor causal front-door intervention; ii) the Causal Scene Separator (CSS)module, which discovers a collection of visual causal and non-causal scenesbased on the visual-linguistic causal relevance and estimates the causal effectof the scene-separating intervention in a contrastive learning manner.Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasetsdemonstrate the superiority of our VCSR in discovering visual causal scene andachieving robust video question answering.</description><author>Yushen Wei, Yang Liu, Hong Yan, Guanbin Li, Liang Lin</author><pubDate>Sun, 07 May 2023 10:05:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.04224v1</guid></item><item><title>Separate and Locate: Rethink the Text in Text-based Visual Question Answering</title><link>http://arxiv.org/abs/2308.16383v1</link><description>Text-based Visual Question Answering (TextVQA) aims at answering questionsabout the text in images. Most works in this field focus on designing networkstructures or pre-training tasks. All these methods list the OCR texts inreading order (from left to right and top to bottom) to form a sequence, whichis treated as a natural language ``sentence''. However, they ignore the factthat most OCR words in the TextVQA task do not have a semantical contextualrelationship. In addition, these approaches use 1-D position embedding toconstruct the spatial relation between OCR tokens sequentially, which is notreasonable. The 1-D position embedding can only represent the left-rightsequence relationship between words in a sentence, but not the complex spatialposition relationship. To tackle these problems, we propose a novel methodnamed Separate and Locate (SaL) that explores text contextual cues and designsspatial position embedding to construct spatial relations between OCR texts.Specifically, we propose a Text Semantic Separate (TSS) module that helps themodel recognize whether words have semantic contextual relations. Then, weintroduce a Spatial Circle Position (SCP) module that helps the model betterconstruct and reason the spatial position relationships between OCR texts. OurSaL model outperforms the baseline model by 4.44% and 3.96% accuracy on TextVQAand ST-VQA datasets. Compared with the pre-training state-of-the-art methodpre-trained on 64 million pre-training samples, our method, without anypre-training tasks, still achieves 2.68% and 2.52% accuracy improvement onTextVQA and ST-VQA. Our code and models will be released athttps://github.com/fangbufang/SaL.</description><author>Chengyang Fang, Jiangnan Li, Liang Li, Can Ma, Dayong Hu</author><pubDate>Thu, 31 Aug 2023 02:00:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.16383v1</guid></item><item><title>Question-Answering Approach to Evaluate Legal Summaries</title><link>http://arxiv.org/abs/2309.15016v1</link><description>Traditional evaluation metrics like ROUGE compare lexical overlap between thereference and generated summaries without taking argumentative structure intoaccount, which is important for legal summaries. In this paper, we propose anovel legal summarization evaluation framework that utilizes GPT-4 to generatea set of question-answer pairs that cover main points and information in thereference summary. GPT-4 is then used to generate answers based on thegenerated summary for the questions from the reference summary. Finally, GPT-4grades the answers from the reference summary and the generated summary. Weexamined the correlation between GPT-4 grading with human grading. The resultssuggest that this question-answering approach with GPT-4 can be a useful toolfor gauging the quality of the summary.</description><author>Huihui Xu, Kevin Ashley</author><pubDate>Tue, 26 Sep 2023 16:36:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.15016v1</guid></item><item><title>Toloka Visual Question Answering Benchmark</title><link>http://arxiv.org/abs/2309.16511v1</link><description>In this paper, we present Toloka Visual Question Answering, a newcrowdsourced dataset allowing comparing performance of machine learning systemsagainst human level of expertise in the grounding visual question answeringtask. In this task, given an image and a textual question, one has to draw thebounding box around the object correctly responding to that question. Everyimage-question pair contains the response, with only one correct response perimage. Our dataset contains 45,199 pairs of images and questions in English,provided with ground truth bounding boxes, split into train and two testsubsets. Besides describing the dataset and releasing it under a CC BY license,we conducted a series of experiments on open source zero-shot baseline modelsand organized a multi-phase competition at WSDM Cup that attracted 48participants worldwide. However, by the time of paper submission, no machinelearning model outperformed the non-expert crowdsourcing baseline according tothe intersection over union evaluation score.</description><author>Dmitry Ustalov, Nikita Pavlichenko, Sergey Koshelev, Daniil Likhobaba, Alisa Smirnova</author><pubDate>Thu, 28 Sep 2023 16:18:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.16511v1</guid></item><item><title>Using contradictions improves question answering systems</title><link>http://arxiv.org/abs/2211.05598v2</link><description>This work examines the use of contradiction in natural language inference(NLI) for question answering (QA). Typically, NLI systems help answer questionsby determining if a potential answer is \emph{entailed} (supported) by somebackground context. But is it useful to also determine if an answer contradictsthe context? We test this in two settings, multiple choice and extractive QA,and find that systems that incorporate contradiction can do slightly betterthan entailment-only systems on certain datasets. However, the bestperformances come from using contradiction, entailment, and QA model confidencescores together. This has implications for the deployment of QA systems indomains such as medicine and science where safety is an issue.</description><author>Ãtienne Fortier-Dubois, Domenic Rosati</author><pubDate>Mon, 29 May 2023 14:14:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.05598v2</guid></item><item><title>Structured Knowledge Grounding for Question Answering</title><link>http://arxiv.org/abs/2209.08284v3</link><description>Can language models (LM) ground question-answering (QA) tasks in theknowledge base via inherent relational reasoning ability? While previous modelsthat use only LMs have seen some success on many QA tasks, more recent methodsinclude knowledge graphs (KG) to complement LMs with their more logic-drivenimplicit knowledge. However, effectively extracting information from structureddata, like KGs, empowers LMs to remain an open question, and current modelsrely on graph techniques to extract knowledge. In this paper, we propose tosolely leverage the LMs to combine the language and knowledge for knowledgebased question-answering with flexibility, breadth of coverage and structuredreasoning. Specifically, we devise a knowledge construction method thatretrieves the relevant context with a dynamic hop, which expresses morecomprehensivenes than traditional GNN-based techniques. And we devise a deepfusion mechanism to further bridge the information exchanging bottleneckbetween the language and the knowledge. Extensive experiments show that ourmodel consistently demonstrates its state-of-the-art performance overCommensenseQA benchmark, showcasing the possibility to leverage LMs solely torobustly ground QA into the knowledge base.</description><author>Yujie Lu, Siqi Ouyang, Kairui Zhou</author><pubDate>Mon, 05 Jun 2023 21:41:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.08284v3</guid></item></channel></rss>