<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivreinforcement learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 24 Aug 2025 17:37:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</title><link>http://arxiv.org/abs/2508.11503v1</link><description>Reliable autonomous navigation across the unstructured terrains of distantplanetary surfaces is a critical enabler for future space exploration. However,the deployment of learning-based controllers is hindered by the inherentsim-to-real gap, particularly for the complex dynamics of wheel interactionswith granular media. This work presents a complete sim-to-real framework fordeveloping and validating robust control policies for dynamic waypoint trackingon such challenging surfaces. We leverage massively parallel simulation totrain reinforcement learning agents across a vast distribution of procedurallygenerated environments with randomized physics. These policies are thentransferred zero-shot to a physical wheeled rover operating in a lunar-analoguefacility. Our experiments systematically compare multiple reinforcementlearning algorithms and action smoothing filters to identify the most effectivecombinations for real-world deployment. Crucially, we provide strong empiricalevidence that agents trained with procedural diversity achieve superiorzero-shot performance compared to those trained on static scenarios. We alsoanalyze the trade-offs of fine-tuning with high-fidelity particle physics,which offers minor gains in low-speed precision at a significant computationalcost. Together, these contributions establish a validated workflow for creatingreliable learning-based navigation systems, marking a critical step towardsdeploying autonomous robots in the final frontier.</description><author>Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez</author><pubDate>Fri, 15 Aug 2025 14:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11503v1</guid></item><item><title>SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</title><link>http://arxiv.org/abs/2508.11553v1</link><description>We introduce SeamlessFlow, a server based reinforcement learning (RL)framework that addresses two core challenges in industrial scale RL: (1)decoupling RL training from the complex execution flow of agents; (2)maximizing GPU utilization with minimal idle time while preserving thestability and scalability required for large-scale deployments. First,SeamlessFlow introduces a data plane that decouples the RL trainer fromdiverse, complex agent implementations while sustaining high throughput. Acentral trajectory manager maintains complete interaction histories andsupports partial rollout, allowing rollout to pause for weight updates andresume seamlessly, keeping agents unaware of service interruptions. Second, wepropose a tag driven scheduling paradigm that abstracts hardware intocapability tagged resources, unifying colocated and disaggregatedarchitectures. Based on this, SeamlessFlow introduces a spatiotemporalmultiplexing pipeline that dynamically reassigns idle training nodes to rolloutin a train rollout separated setup, eliminating pipeline bubbles and fullyexploiting heterogeneous cluster resources. By combining these innovations,SeamlessFlow delivers both stability and high performance, making it wellsuited for multi agent, long horizon, and other complex RL tasks.</description><author>Jinghui Wang, Shaojie Wang, Yinghan Cui, Xuxing Chen, Chao Wang, Xiaojiang Zhang, Minglei Zhang, Jiarong Zhang, Wenhao Zhuang, Yuchen Cao, Wankang Bao, Haimo Li, Zheng Lin, Huiming Wang, Haoyang Huang, Zongxian Feng, Zizheng Zhan, Ken Deng, Wen Xiang, Huaixi Tang, Kun Wu, Mengtong Li, Mengfei Xie, Junyi Peng, Haotian Zhang, Bin Chen, Bing Yu</author><pubDate>Fri, 15 Aug 2025 15:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11553v1</guid></item><item><title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title><link>http://arxiv.org/abs/2508.11538v1</link><description>Video reasoning segmentation (VRS) endeavors to delineate referred objects invideos guided by implicit instructions that encapsulate human intent andtemporal logic. Previous approaches leverage large vision language models(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.However, this paradigm suffers from limited interpretability during inferenceand suboptimal performance due to inadequate spatiotemporal reasoning. Drawinginspiration from seminal breakthroughs in reinforcement learning, we introduceVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning insegmentation. Veason-R1 is trained through Group Relative Policy Optimization(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, wecurate high-quality CoT training data to instill structured reasoningtrajectories, bridging video-level semantics and frame-level spatial grounding,yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPOfine-tuning encourages efficient exploration of the reasoning space byoptimizing reasoning chains. To this end, we incorporate a holistic rewardmechanism that synergistically enhances spatial alignment and temporalconsistency, bolstering keyframe localization and fine-grained grounding.Comprehensive empirical evaluations demonstrate that Veason-R1 achievesstate-of-the-art performance on multiple benchmarks, surpassing prior art bysignificant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),while exhibiting robustness to hallucinations (+8.8 R). Our code and modelweights will be available at Veason-R1.</description><author>Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</author><pubDate>Fri, 15 Aug 2025 15:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11538v1</guid></item><item><title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title><link>http://arxiv.org/abs/2503.01307v2</link><description>Test-time inference has emerged as a powerful paradigm for enabling languagemodels to ``think'' longer and more carefully about complex challenges, muchlike skilled human experts. While reinforcement learning (RL) can driveself-improvement in language models on verifiable tasks, some models exhibitsubstantial gains while others quickly plateau. For instance, we find thatQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the gameof Countdown. This discrepancy raises a critical question: what intrinsicproperties enable effective self-improvement? We introduce a framework toinvestigate this question by analyzing four key cognitive behaviors --verification, backtracking, subgoal setting, and backward chaining -- that bothexpert human problem solvers and successful language models employ. Our studyreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llamainitially lacks them. In systematic experimentation with controlled behavioraldatasets, we find that priming Llama with examples containing these reasoningbehaviors enables substantial improvements during RL, matching or exceedingQwen's performance. Importantly, the presence of reasoning behaviors, ratherthan correctness of answers, proves to be the critical factor -- models primedwith incorrect solutions containing proper reasoning patterns achievecomparable performance to those trained on correct solutions. Finally,leveraging continued pretraining with OpenWebMath data, filtered to amplifyreasoning behaviors, enables the Llama model to match Qwen's self-improvementtrajectory. Our findings establish a fundamental relationship between initialreasoning behaviors and the capacity for improvement, explaining why somelanguage models effectively utilize additional computation while othersplateau.</description><author>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</author><pubDate>Fri, 15 Aug 2025 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.01307v2</guid></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2507.01006v5</link><description>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models(VLMs) designed to advance general-purpose multimodal understanding andreasoning. In this report, we share our key findings in the development of thereasoning-centric training framework. We first develop a capable visionfoundation model with significant potential through large-scale pre-training,which arguably sets the upper bound for the final performance. We then proposeReinforcement Learning with Curriculum Sampling (RLCS) to unlock the fullpotential of the model, leading to comprehensive capability enhancement acrossa diverse range of tasks, including STEM problem solving, video understanding,content recognition, coding, grounding, GUI-based agents, and long documentinterpretation. In a comprehensive evaluation across 42 public benchmarks,GLM-4.5V achieves state-of-the-art performance on nearly all tasks amongopen-source models of similar size, and demonstrates competitive or evensuperior results compared to closed-source models such as Gemini-2.5-Flash onchallenging tasks including Coding and GUI Agents. Meanwhile, the smallerGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results tothe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source bothGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information arereleased at https://github.com/zai-org/GLM-V.</description><author>GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan</author><pubDate>Fri, 15 Aug 2025 13:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01006v5</guid></item><item><title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title><link>http://arxiv.org/abs/2508.09736v2</link><description>We introduce M3-Agent, a novel multimodal agent framework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual andauditory inputs to build and update its long-term memory. Beyond episodicmemory, it also develops semantic memory, enabling it to accumulate worldknowledge over time. Its memory is organized in an entity-centric, multimodalformat, allowing deeper and more consistent understanding of the environment.Given an instruction, M3-Agent autonomously performs multi-turn, iterativereasoning and retrieves relevant information from memory to accomplish thetask. To evaluate memory effectiveness and memory-based reasoning in multimodalagents, we develop M3-Bench, a new long-video question answering benchmark.M3-Bench comprises 100 newly recorded real-world videos captured from a robot'sperspective (M3-Bench-robot) and 920 web-sourced videos across diversescenarios (M3-Bench-web). We annotate question-answer pairs designed to testkey capabilities essential for agent applications, such as human understanding,general knowledge extraction, and cross-modal reasoning. Experimental resultsshow that M3-Agent, trained via reinforcement learning, outperforms thestrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-weband VideoMME-long, respectively. Our work advances the multimodal agents towardmore human-like long-term memory and provides insights into their practicaldesign. Model, code and data are available athttps://github.com/bytedance-seed/m3-agent</description><author>Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</author><pubDate>Fri, 15 Aug 2025 13:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09736v2</guid></item><item><title>Embedding Safety into RL: A New Take on Trust Region Methods</title><link>http://arxiv.org/abs/2411.02957v4</link><description>Reinforcement Learning (RL) agents can solve diverse tasks but often exhibitunsafe behavior. Constrained Markov Decision Processes (CMDPs) address this byenforcing safety constraints, yet existing methods either sacrifice rewardmaximization or allow unsafe training. We introduce Constrained Trust RegionPolicy Optimization (C-TRPO), which reshapes the policy space geometry toensure trust regions contain only safe policies, guaranteeing constraintsatisfaction throughout training. We analyze its theoretical properties andconnections to TRPO, Natural Policy Gradient (NPG), and Constrained PolicyOptimization (CPO). Experiments show that C-TRPO reduces constraint violationswhile maintaining competitive returns.</description><author>Nikola Milosevic, Johannes Müller, Nico Scherf</author><pubDate>Fri, 15 Aug 2025 12:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02957v4</guid></item><item><title>Exploring Superior Function Calls via Reinforcement Learning</title><link>http://arxiv.org/abs/2508.05118v3</link><description>Function calling capabilities are crucial for deploying Large Language Modelsin real-world applications, yet current training approaches fail to developrobust reasoning strategies. Supervised fine-tuning produces models that relyon superficial pattern matching, while standard reinforcement learning methodsstruggle with the complex action space of structured function calls. We presenta novel reinforcement learning framework designed to enhance group relativepolicy optimization through strategic entropy based exploration specificallytailored for function calling tasks. Our approach addresses three criticalchallenges in function calling: insufficient exploration during policylearning, lack of structured reasoning in chain-of-thought generation, andinadequate verification of parameter extraction. Our two-stage data preparationpipeline ensures high-quality training samples through iterative LLM evaluationand abstract syntax tree validation. Extensive experiments on the BerkeleyFunction Calling Leaderboard demonstrate that this framework achievesstate-of-the-art performance among open-source models with 86.02\% overallaccuracy, outperforming standard GRPO by up to 6\% on complex multi-functionscenarios. Notably, our method shows particularly strong improvements oncode-pretrained models, suggesting that structured language generationcapabilities provide an advantageous starting point for reinforcement learningin function calling tasks. We will release all the code, models and dataset tobenefit the community.</description><author>Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</author><pubDate>Fri, 15 Aug 2025 12:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05118v3</guid></item><item><title>On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting</title><link>http://arxiv.org/abs/2508.11408v1</link><description>Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are twoprominent post-training paradigms for refining the capabilities and aligningthe behavior of Large Language Models (LLMs). Existing approaches thatintegrate SFT and RL often face the risk of disrupting established modelpatterns and inducing overfitting to expert data. To address this, we present anovel investigation into the unified view of SFT and RL through an off-policyversus on-policy lens. We propose CHORD, a framework for the ControllableHarmonization of On- and Off-Policy Reinforcement Learning via DynamicWeighting, which reframes SFT not as a separate stage but as a dynamicallyweighted auxiliary objective within the on-policy RL process. Based on ananalysis of off-policy expert data's influence at both holistic and granularlevels, we incorporate a dual-control mechanism in CHORD. Specifically, theframework first employs a global coefficient to holistically guide thetransition from off-policy imitation to on-policy exploration, and then appliesa token-wise weighting function that enables granular learning from experttokens, which preserves on-policy exploration and mitigates disruption fromoff-policy data. We conduct extensive experiments on widely used benchmarks,providing empirical evidence that CHORD achieves a stable and efficientlearning process. By effectively harmonizing off-policy expert data withon-policy exploration, CHORD demonstrates significant improvements overbaselines. We release the implementation athttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord toinspire further research.</description><author>Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou</author><pubDate>Fri, 15 Aug 2025 11:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11408v1</guid></item><item><title>Sketch Decompositions for Classical Planning via Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2412.08574v2</link><description>In planning and reinforcement learning, the identification of common subgoalstructures across problems is important when goals are to be achieved over longhorizons. Recently, it has been shown that such structures can be expressed asfeature-based rules, called sketches, over a number of classical planningdomains. These sketches split problems into subproblems which then becomesolvable in low polynomial time by a greedy sequence of IW$(k)$ searches.Methods for learning sketches using feature pools and min-SAT solvers have beendeveloped, yet they face two key limitations: scalability and expressivity. Inthis work, we address these limitations by formulating the problem of learningsketch decompositions as a deep reinforcement learning (DRL) task, wheregeneral policies are sought in a modified planning problem where the successorstates of a state s are defined as those reachable from s through an IW$(k)$search. The sketch decompositions obtained through this method areexperimentally evaluated across various domains, and problems are regarded assolved by the decomposition when the goal is reached through a greedy sequenceof IW$(k)$ searches. While our DRL approach for learning sketch decompositionsdoes not yield interpretable sketches in the form of rules, we demonstrate thatthe resulting decompositions can often be understood in a crisp manner.</description><author>Michael Aichmüller, Hector Geffner</author><pubDate>Fri, 15 Aug 2025 10:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08574v2</guid></item><item><title>Fusing Rewards and Preferences in Reinforcement Learning</title><link>http://arxiv.org/abs/2508.11363v1</link><description>We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm thatfuses both individual rewards and pairwise preferences (if available) into asingle update rule. DFA uses the policy's log-probabilities directly to modelthe preference probability, avoiding a separate reward-modeling step.Preferences can be provided by human-annotators (at state-level ortrajectory-level) or be synthesized online from Q-values stored in anoff-policy replay buffer. Under a Bradley-Terry model, we prove that minimizingDFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)policy. Our simulation results show that DFA trained on generated preferencesmatches or exceeds SAC on six control environments and demonstrates a morestable training process. With only a semi-synthetic preference dataset underBradley-Terry model, our algorithm outperforms reward-modeling reinforcementlearning from human feedback (RLHF) baselines in a stochastic GridWorld andapproaches the performance of an oracle with true rewards.</description><author>Sadegh Khorasani, Saber Salehkaleybar, Negar Kiyavash, Matthias Grossglauser</author><pubDate>Fri, 15 Aug 2025 09:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11363v1</guid></item><item><title>CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</title><link>http://arxiv.org/abs/2508.11360v1</link><description>As autonomous agents become adept at understanding and interacting withgraphical user interface (GUI) environments, a new era of automated taskexecution is emerging. Recent studies have demonstrated that ReinforcementLearning (RL) can effectively enhance agents' performance in dynamicinteractive GUI environments. However, these methods face two key limitations:(1) they overlook the significant variation in difficulty across different GUItasks by treating the entire training data as a uniform set, which hampers theagent's ability to adapt its learning process; and (2) most approaches collapsetask-specific nuances into a single, coarse reward, leaving the agent with auniform signal that yields inefficient policy updates. To address theselimitations, we propose CRAFT-GUI, a curriculum learning framework based onGroup Relative Policy Optimization (GRPO) that explicitly accounts for thevarying difficulty across trajectories. To enable more fine-grained policyoptimization, we design a reward function that combines simple rule-basedsignals with model-judged evaluation, providing richer and more nuancedfeedback during training. Experimental results demonstrate that our methodachieves significant improvements over previous state-of-the-art approaches,outperforming them by 5.6% on public benchmarks Android Control and 10.3% onour internal online benchmarks, respectively. These findings empiricallyvalidate the effectiveness of integrating reinforcement learning withcurriculum learning in GUI interaction tasks.</description><author>Songqin Nong, Jingxuan Xu, Sheng Zhou, Jianfeng Chen, Xiaoxuan Tang, Tao Jiang, Wenhao Xu</author><pubDate>Fri, 15 Aug 2025 09:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11360v1</guid></item><item><title>ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism</title><link>http://arxiv.org/abs/2508.11356v1</link><description>Recent advancements in Large Language Models have yielded significantimprovements in complex reasoning tasks such as mathematics and programming.However, these models remain heavily dependent on annotated data and exhibitlimited adaptability in unsupervised scenarios. To address these limitations,test-time reinforcement learning (TTRL) has been proposed, which enablesself-optimization by leveraging model-generated pseudo-labels. Despite itspromise, TTRL faces several key challenges, including high inference costs dueto parallel rollouts and early-stage estimation bias that fostersoverconfidence, reducing output diversity and causing performance plateaus. Toaddress these challenges, we introduce an entropy-based mechanism to enhancethe exploration-exploitation balance in test-time reinforcement learningthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) andEntropy-based Advantage Reshaping (EAR). Compared with the baseline, ourapproach enables Llama3.1-8B to achieve a 68 percent relative improvement inPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent ofthe rollout tokens budget. This highlights our method's ability to effectivelyoptimize the trade-off between inference efficiency, diversity, and estimationrobustness, thereby advancing unsupervised reinforcement learning foropen-domain reasoning tasks.</description><author>Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, ShaoGuo Liu, TingTing Gao</author><pubDate>Fri, 15 Aug 2025 09:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11356v1</guid></item><item><title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title><link>http://arxiv.org/abs/2506.07468v2</link><description>Conventional language model (LM) safety alignment relies on a reactive,disjoint procedure: attackers exploit a static model, followed by defensivefine-tuning to patch exposed vulnerabilities. This sequential approach createsa mismatch -- attackers overfit to obsolete defenses, while defendersperpetually lag behind emerging threats. To address this, we proposeSelf-RedTeam, an online self-play reinforcement learning algorithm where anattacker and defender agent co-evolve through continuous interaction. We castsafety alignment as a two-player zero-sum game, where a single model alternatesbetween attacker and defender roles -- generating adversarial prompts andsafeguarding against them -- while a reward LM adjudicates outcomes. Thisenables dynamic co-adaptation. Grounded in the game-theoretic framework ofzero-sum games, we establish a theoretical safety guarantee which motivates thedesign of our method: if self-play converges to a Nash Equilibrium, thedefender will reliably produce safe responses to any adversarial input.Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) comparedto attackers trained against static defenders and achieves higher robustness onsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trainedagainst static attackers. We further propose hidden Chain-of-Thought, allowingagents to plan privately, which boosts adversarial diversity and reducesover-refusals. Our results motivate a shift from reactive patching to proactiveco-evolution in LM safety training, enabling scalable, autonomous, and robustself-improvement of LMs via multi-agent reinforcement learning (MARL).</description><author>Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</author><pubDate>Fri, 15 Aug 2025 09:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07468v2</guid></item><item><title>HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</title><link>http://arxiv.org/abs/2508.11350v1</link><description>Understanding and recognizing human-object interaction (HOI) is a pivotalapplication in AR/VR and robotics. Recent open-vocabulary HOI detectionapproaches depend exclusively on large language models for richer textualprompts, neglecting their inherent 3D spatial understanding capabilities. Toaddress this shortcoming, we introduce HOID-R1, the first HOI detectionframework that integrates chain-of-thought (CoT) guided supervised fine-tuning(SFT) with group relative policy optimization (GRPO) within a reinforcementlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the modelwith essential reasoning capabilities, forcing the model to articulate itsthought process in the output. Subsequently, we integrate GRPO to leveragemulti-reward signals for policy optimization, thereby enhancing alignmentacross diverse modalities. To mitigate hallucinations in the CoT reasoning, weintroduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,further improving generalization. Extensive experiments show that HOID-R1achieves state-of-the-art performance on HOI detection benchmarks andoutperforms existing methods in open-world generalization to novel scenarios.</description><author>Zhenhao Zhang, Hanqing Wang, Xiangyu Zeng, Ziyu Cheng, Jiaxin Liu, Haoyu Yan, Zhirui Liu, Kaiyang Ji, Tianxiang Gui, Ke Hu, Kangyi Chen, Yahao Fan, Mokai Pan</author><pubDate>Fri, 15 Aug 2025 09:28:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11350v1</guid></item><item><title>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model</title><link>http://arxiv.org/abs/2508.06571v3</link><description>Vision-Language-Action (VLA) models have demonstrated potential in autonomousdriving. However, two critical challenges hinder their development: (1)Existing VLA architectures are typically based on imitation learning inopen-loop setup which tends to capture the recorded behaviors in the dataset,leading to suboptimal and constrained performance, (2) Close-loop trainingrelies heavily on high-fidelity sensor simulation, where domain gaps andcomputational inefficiencies pose significant barriers. In this paper, weintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world modelwith a self-built VLA approach. Our framework proceeds in a three-stageparadigm: In the first stage, we propose a VLA architecture and pretrain theVLA policy via imitation learning. In the second stage, we construct alightweight reward world model via inverse reinforcement learning to enableefficient close-loop reward computation. To further enhance planningperformance, finally, we design specialized reward world model guidencereinforcement learning via PPO(Proximal Policy Optimization) to effectivelybalance the safety incidents, comfortable driving, and traffic efficiency. Ourapproach achieves state-of-the-art performance in NAVSIM v2 end-to-end drivingbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope thatour framework will accelerate VLA research in close-loop autonomous driving.</description><author>Anqing Jiang, Yu Gao, Yiru Wang, Zhigang Sun, Shuo Wang, Yuwen Heng, Hao Sun, Shichen Tang, Lijuan Zhu, Jinhao Chai, Jijun Wang, Zichong Gu, Hao Jiang, Li Sun</author><pubDate>Fri, 15 Aug 2025 05:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06571v3</guid></item><item><title>Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</title><link>http://arxiv.org/abs/2508.11204v1</link><description>Sampling efficiency is critical for deploying visuomotor learning inreal-world robotic manipulation. While task symmetry has emerged as a promisinginductive bias to improve efficiency, most prior work is limited to isometricsymmetries -- applying the same group transformation to all task objects acrossall timesteps. In this work, we explore non-isometric symmetries, applyingmultiple independent group transformations across spatial and temporaldimensions to relax these constraints. We introduce a novel formulation of thepartially observable Markov decision process (POMDP) that incorporates thenon-isometric symmetry structures, and propose a simple yet effective dataaugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrateMEA with offline reinforcement learning to enhance sampling efficiency, andintroduce a voxel-based visual representation that preserves translationalequivariance. Extensive simulation and real-robot experiments across twomanipulation domains demonstrate the effectiveness of our approach.</description><author>Hongbin Lin, Juan Rojas, Kwok Wai Samuel Au</author><pubDate>Fri, 15 Aug 2025 04:30:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11204v1</guid></item><item><title>RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System</title><link>http://arxiv.org/abs/2508.09186v2</link><description>The proliferation of AI-powered cameras in Intelligent Transportation Systems(ITS) creates a severe conflict between the need for rich visual data and theright to privacy. Existing privacy-preserving methods, such as blurring orencryption, are often insufficient due to creating an undesirable trade-offwhere either privacy is compromised against advanced reconstruction attacks ordata utility is critically degraded. To resolve this challenge, we proposeRL-MoE, a novel framework that transforms sensitive visual data intoprivacy-preserving textual descriptions, eliminating the need for direct imagetransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecturefor nuanced, multi-aspect scene decomposition with a Reinforcement Learning(RL) agent that optimizes the generated text for a dual objective of semanticaccuracy and privacy preservation. Extensive experiments demonstrate thatRL-MoE provides superior privacy protection, reducing the success rate ofreplay attacks to just 9.4\% on the CFP-FP dataset, while simultaneouslygenerating richer textual content than baseline methods. Our work provides apractical and scalable solution for building trustworthy AI systems inprivacy-sensitive domains, paving the way for more secure smart city andautonomous vehicle networks.</description><author>Abdolazim Rezaei, Mehdi Sookhak, Mahboobeh Haghparast</author><pubDate>Fri, 15 Aug 2025 04:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09186v2</guid></item><item><title>UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</title><link>http://arxiv.org/abs/2508.11196v1</link><description>Recent advances in vision-language models (VLMs) have demonstrated stronggeneralization in natural image tasks. However, their performance oftendegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which featureshigh resolution, complex spatial semantics, and strict real-time constraints.These challenges limit the applicability of general-purpose VLMs to structuredaerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, alightweight VLM explicitly designed for aerial visual reasoning. It is trainedusing a hybrid method that combines supervised fine-tuning (SFT) andmulti-stage reinforcement learning (RL). We leverage the group relative policyoptimization (GRPO) algorithm to promote structured and interpretable reasoningthrough rule-guided rewards and intra-group policy alignment. To support modeltraining and evaluation, we introduce a high-resolution visual questionanswering dataset named HRVQA-VL, which consists of 50,019 annotated samplescovering eight UAV-relevant reasoning tasks, including object counting,transportation recognition, and spatial scene inference. Experimental resultsshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than theQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, whichis 36x larger, on multiple tasks. Ablation studies reveal that while SFTimproves semantic alignment, it may reduce reasoning diversity in mathematicaltasks. GRPO-based RL compensates for this limitation by enhancing logicalflexibility and the robustness of inference. Additionally, UAV-VL-R1 requiresonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB withINT8, supporting real-time deployment on resource-constrained UAV platforms.</description><author>Jiajin Guan, Haibo Mei, Bonan Zhang, Dan Liu, Yuanshuang Fu, Yue Zhang</author><pubDate>Fri, 15 Aug 2025 04:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11196v1</guid></item><item><title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title><link>http://arxiv.org/abs/2508.00344v2</link><description>Large Language Models (LLMs) have shown remarkable advancements in tacklingagent-oriented tasks. Despite their potential, existing work faces challengeswhen deploying LLMs in agent-based environments. The widely adopted agentparadigm ReAct centers on integrating single-step reasoning with immediateaction execution, which limits its effectiveness in complex tasks requiringlong-term strategic planning. Furthermore, the coordination between the plannerand executor during problem-solving is also a critical factor to consider inagent design. Additionally, current approaches predominantly rely on supervisedfine-tuning, which often leads models to memorize established task completiontrajectories, thereby restricting their generalization ability when confrontedwith novel problem contexts. To address these challenges, we introduce anadaptive global plan-based agent paradigm AdaPlan, aiming to synergizehigh-level explicit guidance with execution to support effective long-horizondecision-making. Based on the proposed paradigm, we further put forwardPilotRL, a global planning-guided training framework for LLM agents driven byprogressive reinforcement learning. We first develop the model's ability tofollow explicit guidance from global plans when addressing agent tasks.Subsequently, based on this foundation, we focus on optimizing the quality ofgenerated plans. Finally, we conduct joint optimization of the model's planningand execution coordination. Experiments indicate that PilotRL could achievestate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassingclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%comparing to GPT-4o-mini at a comparable parameter scale.</description><author>Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang</author><pubDate>Fri, 15 Aug 2025 03:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.00344v2</guid></item><item><title>Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward</title><link>http://arxiv.org/abs/2508.11143v1</link><description>Existing reinforcement learning (RL) methods struggle with long-horizonrobotic manipulation tasks, particularly those involving sparse rewards. Whileaction chunking is a promising paradigm for robotic manipulation, using RL todirectly learn continuous action chunks in a stable and data-efficient mannerremains a critical challenge. This paper introduces AC3 (Actor-Critic forContinuous Chunks), a novel RL framework that learns to generatehigh-dimensional, continuous action sequences. To make this learning processstable and data-efficient, AC3 incorporates targeted stabilization mechanismsfor both the actor and the critic. First, to ensure reliable policyimprovement, the actor is trained with an asymmetric update rule, learningexclusively from successful trajectories. Second, to enable effective valuelearning despite sparse rewards, the critic's update is stabilized usingintra-chunk $n$-step returns and further enriched by a self-supervised moduleproviding intrinsic rewards at anchor points aligned with each action chunk. Weconducted extensive experiments on 25 tasks from the BiGym and RLBenchbenchmarks. Results show that by using only a few demonstrations and a simplemodel architecture, AC3 achieves superior success rates on most tasks,validating its effective design.</description><author>Jiarui Yang, Bin Zhu, Jingjing Chen, Yu-Gang Jiang</author><pubDate>Fri, 15 Aug 2025 01:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11143v1</guid></item><item><title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title><link>http://arxiv.org/abs/2508.10501v2</link><description>Existing tool-augmented agentic systems are limited in the real world by (i)black-box reasoning steps that undermine trust of decision-making and posesafety risks, (ii) poor multimodal integration, which is inherently criticalfor healthcare tasks, and (iii) rigid and computationally inefficient agenticpipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), thefirst multimodal framework to address these challenges in the context of ChestX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over amulti-tool graph, yielding decision paths annotated with interpretableprobabilities. Given the complex CXR reasoning task with multimodal medicaldata, PASS leverages its learned task-conditioned distribution over the agenticsupernet. Thus, it adaptively selects the most suitable tool at each supernetlayer, offering probability-annotated trajectories for post-hoc audits anddirectly enhancing medical AI safety. PASS also continuously compresses salientfindings into an evolving personalized memory, while dynamically decidingwhether to deepen its reasoning path or invoke an early exit for efficiency. Tooptimize a Pareto frontier balancing performance and cost, we design a novelthree-stage training procedure, including expert knowledge warm-up, contrastivepath-ranking, and cost-aware reinforcement learning. To facilitate rigorousevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,safety-critical, free-form CXR reasoning. Experiments across various benchmarksvalidate that PASS significantly outperforms strong baselines in multiplemetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,pushing a new paradigm shift towards interpretable, adaptive, and multimodalmedical agentic systems.</description><author>Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu</author><pubDate>Fri, 15 Aug 2025 01:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10501v2</guid></item><item><title>GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning</title><link>http://arxiv.org/abs/2508.11049v1</link><description>Recent advances have shown that video generation models can enhance robotlearning by deriving effective robot actions through inverse dynamics. However,these methods heavily depend on the quality of generated data and struggle withfine-grained manipulation due to the lack of environment feedback. Whilevideo-based reinforcement learning improves policy robustness, it remainsconstrained by the uncertainty of video generation and the challenges ofcollecting large-scale robot datasets for training diffusion models. To addressthese limitations, we propose GenFlowRL, which derives shaped rewards fromgenerated flow trained from diverse cross-embodiment datasets. This enableslearning generalizable and robust policies from diverse demonstrations usinglow-dimensional, object-centric features. Experiments on 10 manipulation tasks,both in simulation and real-world cross-embodiment evaluations, demonstratethat GenFlowRL effectively leverages manipulation features extracted fromgenerated object-centric flow, consistently achieving superior performanceacross diverse and challenging scenarios. Our Project Page:https://colinyu1.github.io/genflowrl</description><author>Kelin Yu, Sheng Zhang, Harshit Soora, Furong Huang, Heng Huang, Pratap Tokekar, Ruohan Gao</author><pubDate>Thu, 14 Aug 2025 20:19:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11049v1</guid></item><item><title>Sophisticated Learning: A novel algorithm for active learning during model-based planning</title><link>http://arxiv.org/abs/2308.08029v2</link><description>We introduce Sophisticated Learning (SL), a planning-to-learn algorithm thatembeds active parameter learning inside the Sophisticated Inference (SI)tree-search framework of Active Inference. Unlike SI -- which optimizes beliefsabout hidden states -- SL also updates beliefs about model parameters withineach simulated branch, enabling counterfactual reasoning about how futureobservations would improve subsequent planning. We compared SL with Bayes-adaptive Reinforcement Learning (BARL) agents aswell as with its parent algorithm, SI. Using a biologically inspired seasonalforaging task in which resources shift probabilistically over a 10x10 grid, wedesigned experiments that forced agents to balance probabilistic rewardharvesting against information gathering. In early trials, where rapid learning is vital, SL agents survive, onaverage, 8.2% longer than SI and 35% longer than Bayes-adaptive ReinforcementLearning. While both SL and SI showed equal convergence performance, SL reachedthis convergence 40% faster than SI. Additionally, SL showed robustout-performance of other algorithms in altered environment configurations. Our results show that incorporating active learning into multi-step planningmaterially improves decision making under radical uncertainty, and reinforcesthe broader utility of Active Inference for modeling biologically relevantbehavior.</description><author>Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith</author><pubDate>Thu, 14 Aug 2025 20:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08029v2</guid></item><item><title>CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention</title><link>http://arxiv.org/abs/2508.11016v1</link><description>Recent advances in Reinforcement Learning with Verified Reward (RLVR) havedriven the emergence of more sophisticated cognitive behaviors in largelanguage models (LLMs), thereby enhancing their reasoning capabilities.However, in prior RLVR pipelines, the repeated use of static initial-statesampling drawn exactly from the dataset distribution during each sampling phaseproduced overly deterministic, low diversity model behavior, which manifestedas rapid entropy collapse and hindered sustained performance gains duringprolonged training. To address this issue, we introduce CURE(Critical-token-gUided Re concatenation for Entropy-collapse prevention), atwo-stage framework that balances exploration and exploitation. Specifically,in the first stage, to deliberately steer the model toward novel yet coherentcontexts, we re-generate at high-entropy critical tokens and jointly optimizethe original and the branched trajectories. The further comparison with vanillaDAPO shows that the regeneration process achieves a better performance on mathreasoning tasks while sustaining a high-level entropy degree for exploration.In the second stage, we continue training with static initial-state sampling byDAPO, intentionally placing the model in a familiar state to graduallystrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,compared to other RLVR methods, CURE achieves a 5% performance gain across sixmath benchmarks, establishing state-of-the-art performance in both entropy andaccuracy. A series of experiments further validate the effectiveness of ourapproach. Code is available at https://github.com/CURE-Project/CURE.</description><author>Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang</author><pubDate>Thu, 14 Aug 2025 18:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11016v1</guid></item><item><title>MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models</title><link>http://arxiv.org/abs/2508.13148v1</link><description>Diffusion language models, as a promising alternative to traditionalautoregressive (AR) models, enable faster generation and richer conditioning onbidirectional context. However, they suffer from a key discrepancy betweentraining and inference: during inference, MDLMs progressively reveal thestructure of the generated sequence by producing fewer and fewer masked tokens,whereas this structure is ignored in training as tokens are masked at random.Although this discrepancy between training and inference can lead to suboptimalperformance, it has been largely overlooked by previous works, leaving closingthis gap between the two stages an open problem. To address this, we frame theproblem of learning effective denoising trajectories as a sequentialdecision-making problem and use the resulting framework to apply reinforcementlearning. We propose a novel Masked Diffusion Policy Optimization (MDPO) toexploit the Markov property diffusion possesses and explicitly train the modelunder the same progressive refining schedule used at inference. MDPO matchesthe performance of the previous state-of-the-art (SOTA) method with 60x fewergradient updates, while achieving average improvements of 9.6% on MATH500 and54.2% on Countdown over SOTA when trained within the same number of weightupdates. Additionally, we improve the remasking strategy of MDLMs as a plug-ininference replacement to overcome the limitation that the model cannot refinetokens flexibly. This simple yet effective training-free strategy, what werefer to as RCR, consistently improves performance and yields additional gainswhen combined with MDPO. Our findings establish great potential forinvestigating the discrepancy between pre-training and inference of MDLMs.Code: https://github.com/autonomousvision/mdpo. Project Page:https://cli212.github.io/MDPO/.</description><author>Haoyu He, Katrin Renz, Yong Cao, Andreas Geiger</author><pubDate>Mon, 18 Aug 2025 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13148v1</guid></item><item><title>Improving Detection of Watermarked Language Models</title><link>http://arxiv.org/abs/2508.13131v1</link><description>Watermarking has recently emerged as an effective strategy for detecting thegenerations of large language models (LLMs). The strength of a watermarktypically depends strongly on the entropy afforded by the language model andthe set of input prompts. However, entropy can be quite limited in practice,especially for models that are post-trained, for example via instruction tuningor reinforcement learning from human feedback (RLHF), which makes detectionbased on watermarking alone challenging. In this work, we investigate whetherdetection can be improved by combining watermark detectors with non-watermarkones. We explore a number of hybrid schemes that combine the two, observingperformance gains over either class of detector under a wide range ofexperimental conditions.</description><author>Dara Bahri, John Wieting</author><pubDate>Mon, 18 Aug 2025 17:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13131v1</guid></item><item><title>LLMs Are In-Context Bandit Reinforcement Learners</title><link>http://arxiv.org/abs/2410.05362v3</link><description>Large Language Models (LLMs) excel at in-context learning (ICL), a supervisedlearning technique that relies on adding annotated examples to the modelcontext. We investigate a contextual bandit version of in-context reinforcementlearning (ICRL), where models learn in-context, online, from external reward,instead of supervised data. We show that LLMs effectively demonstrate suchlearning, and provide a detailed study of the phenomena, experimenting withchallenging classification tasks and models of sizes from 500M to 70Bparameters. This includes identifying and addressing the instability of theprocess, demonstrating learning with both semantic and abstract labels, andshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, whilealso underscoring fundamental limitations in their implicit reasoning abouterrors.</description><author>Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi</author><pubDate>Mon, 18 Aug 2025 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05362v3</guid></item><item><title>CaRL: Learning Scalable Planning Policies with Simple Rewards</title><link>http://arxiv.org/abs/2504.17838v2</link><description>We investigate reinforcement learning (RL) for privileged planning inautonomous driving. State-of-the-art approaches for this task are rule-based,but these methods do not scale to the long tail. RL, on the other hand, isscalable and does not suffer from compounding errors like imitation learning.Contemporary RL approaches for driving use complex shaped rewards that summultiple individual rewards, \eg~progress, position, or orientation rewards. Weshow that PPO fails to optimize a popular version of these rewards when themini-batch size is increased, which limits the scalability of these approaches.Instead, we propose a new reward design based primarily on optimizing a singleintuitive reward term: route completion. Infractions are penalized byterminating the episode or multiplicatively reducing route completion. We findthat PPO scales well with higher mini-batch sizes when trained with our simplereward, even improving performance. Training with large mini-batch sizesenables efficient scaling via distributed data parallelism. We scale PPO to300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. Theresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,outperforming other RL methods with more complex rewards by a large margin.Requiring only minimal adaptations from its use in CARLA, the same method isthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and90.6 in reactive traffic on the Val14 benchmark while being an order ofmagnitude faster than prior work.</description><author>Bernhard Jaeger, Daniel Dauner, Jens Beißwenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger</author><pubDate>Mon, 18 Aug 2025 16:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17838v2</guid></item><item><title>Reinforced Context Order Recovery for Adaptive Reasoning and Planning</title><link>http://arxiv.org/abs/2508.13070v1</link><description>Modern causal language models, followed by rapid developments in discretediffusion models, can now produce a wide variety of interesting and usefulcontent. However, these families of models are predominantly trained to outputtokens with a fixed (left-to-right) or random order, which may deviate from thelogical order in which tokens are generated originally. In this paper, weobserve that current causal and diffusion models encounter difficulties inproblems that require adaptive token generation orders to solve tractably,which we characterize with the $\mathcal{V}$-information framework. Motivatedby this, we propose Reinforced Context Order Recovery (ReCOR), areinforcement-learning-based framework to extract adaptive, data-dependenttoken generation orders from text data without annotations. Self-supervised bytoken prediction statistics, ReCOR estimates the hardness of predicting everyunfilled token and adaptively selects the next token during both training andinference. Experiments on challenging reasoning and planning datasetsdemonstrate the superior performance of ReCOR compared with baselines,sometimes outperforming oracle models supervised with the ground-truth order.</description><author>Long Ma, Fangwei Zhong, Yizhou Wang</author><pubDate>Mon, 18 Aug 2025 16:42:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13070v1</guid></item><item><title>Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</title><link>http://arxiv.org/abs/2508.13037v1</link><description>Recent studies have demonstrated that Large Language Models (LLMs) havestrong mathematical reasoning abilities but rely on hundreds of billions ofparameters. To tackle the challenge of poor reasoning in Small Language Models(SLMs), existing methods typically leverage LLMs to generate massive amounts ofdata for cramming training. In psychology, they are akin to System 1 thinking,which resolves reasoning problems rapidly based on experience and intuition.However, human learning also requires System 2 thinking, where knowledge isfirst acquired and then reinforced through practice. Inspired by such twodistinct modes of thinking, we propose a novel method based on the multi-LoRAInteraction for mathematical reasoning Distillation (LoRID). First, we inputthe question and reasoning of each sample into an LLM to createknowledge-enhanced datasets. Subsequently, we train a LoRA block on the studentmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughtsfor problem-solving. Then, to imitate System 2 thinking, we train the KnowledgeGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs onlyknowledge after receiving problems, while the latter uses that knowledge toperform reasoning. Finally, to address the randomness in the generation of IRand DR, we evaluate whether their outputs are consistent, and the inferenceprocess needs to be iterated if not. This step can enhance the mathematicalreasoning ability of SLMs through mutual feedback. Experimental results showthat LoRID achieves state-of-the-art performance, especially on the GSM8Kdataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,12.3%, and 1.8% accuracy across the five base models, respectively.</description><author>Xinhe Li, Jiajun Liu, Peng Wang</author><pubDate>Mon, 18 Aug 2025 15:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13037v1</guid></item><item><title>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</title><link>http://arxiv.org/abs/2508.13023v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhancedthe reasoning abilities of large language models (LLMs). Its success, however,largely depends on strong base models with rich world knowledge, yielding onlymodest improvements for small-size language models (SLMs). To address thislimitation, we investigate Guided GRPO, which injects ground-truth reasoningsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.Through a comprehensive study of various guidance configurations, we find thatnaively adding guidance delivers limited gains. These insights motivateG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strengthin response to the model's evolving training dynamics. Experiments onmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-Asubstantially outperforms vanilla GRPO. Our code and models are available athttps://github.com/T-Lab-CUHKSZ/G2RPO-A.</description><author>Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</author><pubDate>Mon, 18 Aug 2025 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13023v1</guid></item><item><title>An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models</title><link>http://arxiv.org/abs/2404.15518v4</link><description>In traditional statistical learning, data points are usually assumed to beindependently and identically distributed (i.i.d.) following an unknownprobability distribution. This paper presents a contrasting viewpoint,perceiving data points as interconnected and employing a Markov reward process(MRP) for data modeling. We reformulate the typical supervised learning as anon-policy policy evaluation problem within reinforcement learning (RL),introducing a generalized temporal difference (TD) learning algorithm as aresolution. Theoretically, our analysis establishes connections between thesolutions of linear TD learning and ordinary least squares (OLS). Underspecific conditions -- particularly when the noise is correlated -- the TDsolution serves as a more effective estimator than OLS. Furthermore, we showthat when our algorithm is applied with many commonly used loss functions --such as those found in generalized linear models -- it corresponds to theapplication of a novel and generalized Bellman operator. We prove that thisoperator admits a unique fixed point, and based on this, we establishconvergence guarantees for our generalized TD algorithm under linear functionapproximation. Empirical studies verify our theoretical results, examine thevital design of our TD algorithm and show practical utility across variousdatasets, encompassing tasks such as regression and image classification withdeep learning.</description><author>Yangchen Pan, Junfeng Wen, Chenjun Xiao, Philip Torr</author><pubDate>Mon, 18 Aug 2025 15:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15518v4</guid></item><item><title>Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</title><link>http://arxiv.org/abs/2508.15766v1</link><description>Recent efforts have extended the capabilities of transformers in logicalreasoning and symbolic computations. In this work, we investigate theircapacity for non-linear latent pattern discovery in the context of functionaldecomposition, focusing on the challenging algebraic task of multivariatepolynomial decomposition. This problem, with widespread applications in scienceand engineering, is proved to be NP-hard, and demands both precision andinsight. Our contributions are threefold: First, we develop a synthetic datageneration pipeline providing fine-grained control over problem complexity.Second, we train transformer models via supervised learning and evaluate themacross four key dimensions involving scaling behavior and generalizability.Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), arank-aware reinforcement learning method suitable for hard algebraic problems.Finetuning with BGRPO improves accuracy while reducing beam width by up tohalf, resulting in approximately 75% lower inference compute. Additionally, ourmodel demonstrates competitive performance in polynomial simplification,outperforming Mathematica in various cases.</description><author>Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU</author><pubDate>Thu, 21 Aug 2025 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15766v1</guid></item><item><title>Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</title><link>http://arxiv.org/abs/2508.15764v1</link><description>We address the problem of detecting adversarial attacks against cooperativemulti-agent reinforcement learning with continuous action space. We propose adecentralized detector that relies solely on the local observations of theagents and makes use of a statistical characterization of the normal behaviorof observable agents. The proposed detector utilizes deep neural networks toapproximate the normal behavior of agents as parametric multivariate Gaussiandistributions. Based on the predicted density functions, we define a normalityscore and provide a characterization of its mean and variance. Thischaracterization allows us to employ a two-sided CUSUM procedure for detectingdeviations of the normality score from its mean, serving as a detector ofanomalous behavior in real-time. We evaluate our scheme on various multi-agentPettingZoo benchmarks against different state-of-the-art attack methods, andour results demonstrate the effectiveness of our method in detecting impactfuladversarial attacks. Particularly, it outperforms the discrete counterpart byachieving AUC-ROC scores of over 0.95 against the most impactful attacks in allevaluated environments.</description><author>Kiarash Kazari, Ezzeldin Shereen, György Dán</author><pubDate>Thu, 21 Aug 2025 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15764v1</guid></item><item><title>Intern-S1: A Scientific Multimodal Foundation Model</title><link>http://arxiv.org/abs/2508.15763v1</link><description>In recent years, a plethora of open-source foundation models have emerged,achieving remarkable progress in some widely attended fields, with performancebeing quite close to that of closed-source models. However, in high-value butmore challenging scientific professional fields, either the fields still relyon expert models, or the progress of general foundation models lagssignificantly compared to those in popular areas, far from sufficient fortransforming scientific research and leaving substantial gap betweenopen-source models and closed-source models in these scientific domains. Tomitigate this gap and explore a step further toward Artificial GeneralIntelligence (AGI), we introduce Intern-S1, a specialized generalist equippedwith general understanding and reasoning capabilities with expertise to analyzemultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)model with 28 billion activated parameters and 241 billion total parameters,continually pre-trained on 5T tokens, including over 2.5T tokens fromscientific domains. In the post-training stage, Intern-S1 undergoes offline andthen online reinforcement learning (RL) in InternBootCamp, where we proposeMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 taskssimultaneously. Through integrated innovations in algorithms, data, andtraining systems, Intern-S1 achieved top-tier performance in online RLtraining.On comprehensive evaluation benchmarks, Intern-S1 demonstratescompetitive performance on general reasoning tasks among open-source models andsignificantly outperforms open-source models in scientific domains, surpassingclosed-source state-of-the-art models in professional tasks, such as molecularsynthesis planning, reaction condition prediction, predicting thermodynamicstabilities for crystals. Our models are available athttps://huggingface.co/internlm/Intern-S1.</description><author>Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Ju</author><pubDate>Thu, 21 Aug 2025 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15763v1</guid></item><item><title>NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</title><link>http://arxiv.org/abs/2508.15693v1</link><description>We present NiceWebRL, a research tool that enables researchers to use machinereinforcement learning (RL) environments for online human subject experiments.NiceWebRL is a Python library that allows any Jax-based environment to betransformed into an online interface, supporting both single-agent andmulti-agent environments. As such, NiceWebRL enables AI researchers to comparetheir algorithms to human performance, cognitive scientists to test MLalgorithms as theories for human cognition, and multi-agent researchers todevelop algorithms for human-AI collaboration. We showcase NiceWebRL with 3case studies that demonstrate its potential to help develop Human-like AI,Human-compatible AI, and Human-assistive AI. In the first case study(Human-like AI), NiceWebRL enables the development of a novel RL model ofcognition. Here, NiceWebRL facilitates testing this model against humanparticipants in both a grid world and Craftax, a 2D Minecraft domain. In oursecond case study (Human-compatible AI), NiceWebRL enables the development of anovel multi-agent RL algorithm that can generalize to human partners in theOvercooked domain. Finally, in our third case study (Human-assistive AI), weshow how NiceWebRL can allow researchers to study how an LLM can assist humanson complex tasks in XLand-Minigrid, an environment with millions ofhierarchical tasks. The library is available athttps://github.com/KempnerInstitute/nicewebrl.</description><author>Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha</author><pubDate>Thu, 21 Aug 2025 16:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15693v1</guid></item><item><title>Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</title><link>http://arxiv.org/abs/2508.15680v1</link><description>This paper argues that a techno-philosophical reading of the EU AI Actprovides insight into the long-term dynamics of data in AI systems,specifically, how the lifecycle from ingestion to deployment generatesrecursive value chains that challenge existing frameworks for Responsible AI.We introduce a conceptual tool to frame the AI pipeline, spanning data,training regimes, architectures, feature stores, and transfer learning. Usingcross-disciplinary methods, we develop a technically grounded andphilosophically coherent analysis of regulatory blind spots. Our central claimis that what remains absent from policymaking is an account of the dynamic ofbecoming that underpins both the technical operation and economic logic of AI.To address this, we advance a formal reading of AI inspired by Simondonianphilosophy of technology, reworking his concept of individuation to model theAI lifecycle, including the pre-individual milieu, individuation, andindividuated AI. To translate these ideas, we introduce futurity: theself-reinforcing lifecycle of AI, where more data enhances performance, deepenspersonalisation, and expands application domains. Futurity highlights therecursively generative, non-rivalrous nature of data, underpinned byinfrastructures like feature stores that enable feedback, adaptation, andtemporal recursion. Our intervention foregrounds escalating power asymmetries,particularly the tech oligarchy whose infrastructures of capture, training, anddeployment concentrate value and decision-making. We argue that effectiveregulation must address these infrastructural and temporal dynamics, andpropose measures including lifecycle audits, temporal traceability, feedbackaccountability, recursion transparency, and a right to contest recursive reuse.</description><author>Mark Cote, Susana Aires</author><pubDate>Thu, 21 Aug 2025 16:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15680v1</guid></item><item><title>Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation</title><link>http://arxiv.org/abs/2504.14716v2</link><description>Large Language Models (LLMs) are widely used as proxies for human labelers inboth training (Reinforcement Learning from AI Feedback) and large-scaleresponse evaluation (LLM-as-a-judge). Alignment and evaluation are criticalcomponents in the development of reliable LLMs, and the choice of feedbackprotocol plays a central role in both but remains understudied. In this work,we show that the choice of feedback protocol for evaluation (absolute scoresversus relative preferences) can significantly affect evaluation reliabilityand induce systematic biases. In the context of LLM-as-a-judge evaluation, weshow that pairwise protocols are more vulnerable to distracted evaluation.Generator models can exploit spurious attributes (or distractor features)favored by the LLM judge, resulting in inflated scores for lower-qualityoutputs. We find that absolute scoring is more robust to such manipulation,producing judgments that better reflect response quality and are lessinfluenced by distractor features. Our results demonstrate that generatormodels can flip preferences by embedding distractor features, skewingLLM-as-a-judge comparisons and leading to inaccurate conclusions about modelquality in benchmark evaluations. Pairwise preferences flip in about 35% of thecases, compared to only 9% for absolute scores. We offer recommendations forchoosing feedback protocols based on dataset characteristics and evaluationobjectives.</description><author>Tuhina Tripathi, Manya Wadhwa, Greg Durrett, Scott Niekum</author><pubDate>Thu, 21 Aug 2025 15:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.14716v2</guid></item><item><title>Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2508.15652v1</link><description>To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it iscrucial to understand individual agent behaviors within a team. While priorwork typically evaluates overall team performance based on explicit rewardsignals or learned value functions, it is unclear how to infer agentcontributions in the absence of any value feedback. In this work, weinvestigate whether meaningful insights into agent behaviors can be extractedthat are consistent with the underlying value functions, solely by analyzingthe policy distribution. Inspired by the phenomenon that intelligent agentstend to pursue convergent instrumental values, which generally increase thelikelihood of task success, we introduce Intended Cooperation Values (ICVs), amethod based on information-theoretic Shapley values for quantifying eachagent's causal influence on their co-players' instrumental empowerment.Specifically, ICVs measure an agent's action effect on its teammates' policiesby assessing their decision uncertainty and preference alignment. The analysisacross cooperative and competitive MARL environments reveals the extent towhich agents adopt similar or diverse strategies. By comparing action effectsbetween policies and value functions, our method identifies which agentbehaviors are beneficial to team success, either by fostering deterministicdecisions or by preserving flexibility for future action choices. Our proposedmethod offers novel insights into cooperation dynamics and enhancesexplainability in MARL systems.</description><author>Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci</author><pubDate>Thu, 21 Aug 2025 15:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15652v1</guid></item><item><title>A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification</title><link>http://arxiv.org/abs/2508.15588v1</link><description>The application of reinforcement learning to safety-critical systems islimited by the lack of formal methods for verifying the robustness and safetyof learned policies. This paper introduces a novel framework that addressesthis gap by analyzing the combination of an RL agent and its environment as adiscrete-time autonomous dynamical system. By leveraging tools from dynamicalsystems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), weidentify and visualize Lagrangian Coherent Structures (LCS) that act as thehidden "skeleton" governing the system's behavior. We demonstrate thatrepelling LCS function as safety barriers around unsafe regions, whileattracting LCS reveal the system's convergence properties and potential failuremodes, such as unintended "trap" states. To move beyond qualitativevisualization, we introduce a suite of quantitative metrics, Mean BoundaryRepulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), andTemporally-Aware Spurious Attractor Strength (TASAS), to formally measure apolicy's safety margin and robustness. We further provide a method for derivinglocal stability guarantees and extend the analysis to handle model uncertainty.Through experiments in both discrete and continuous control environments, weshow that this framework provides a comprehensive and interpretable assessmentof policy behavior, successfully identifying critical flaws in policies thatappear successful based on reward alone.</description><author>Ahmed Nasir, Abdelhafid Zenati</author><pubDate>Thu, 21 Aug 2025 14:00:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15588v1</guid></item><item><title>Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning</title><link>http://arxiv.org/abs/2508.15507v1</link><description>Large Language Models (LLMs) with chains-of-thought have demonstrated strongperformance on an increasing range of tasks, particularly those involvingcomplex logical reasoning. However, excessively long chains can lead tooverthinking, causing computational waste and slower responses. This raises aquestion: can LLMs dynamically adjust the length of their reasoning processesbased on task complexity? To address this, we propose the Think in Blocksframework, which enables adaptive reasoning-from zero to deep reasoning-bypartitioning the reasoning process into a tunable number of blocks. Our maincontributions are: (1) Establishing an explicit block-structured paradigm inwhich the model first predicts an integer reasoning budget-the number ofblocks-and then partitions its reasoning accordingly; (2) Training an adaptivemodel through a three-stage pipeline-Supervised Fine-Tuning, reward-guidedDirect Preference Optimization, and Reinforcement Learning-that adjusts itsreasoning depth to problem difficulty; (3) Exploiting the explicit block countto dynamically control reasoning depth at inference time, allowing flexibleadjustment of chain-of-thought length during deployment.</description><author>Yekun Zhu, Guang Chen, Chengjun Mao</author><pubDate>Thu, 21 Aug 2025 12:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15507v1</guid></item><item><title>Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</title><link>http://arxiv.org/abs/2508.11618v1</link><description>Carbon capture and storage (CCS) projects typically involve a diverse arrayof stakeholders or players from public, private, and regulatory sectors, eachwith different objectives and responsibilities. Given the complexity, scale,and long-term nature of CCS operations, determining whether individualstakeholders can independently maximize their interests or whethercollaborative coalition agreements are needed remains a central question foreffective CCS project planning and management. CCS projects are oftenimplemented in geologically connected sites, where shared geological featuressuch as pressure space and reservoir pore capacity can lead to competitivebehavior among stakeholders. Furthermore, CO2 storage sites are often locatedin geologically mature basins that previously served as sites for hydrocarbonextraction or wastewater disposal in order to leverage existinginfrastructures, which makes unilateral optimization even more complicated andunrealistic. In this work, we propose a paradigm based on Markov games to quantitativelyinvestigate how different coalition structures affect the goals ofstakeholders. We frame this multi-stakeholder multi-site problem as amulti-agent reinforcement learning problem with safety constraints. Ourapproach enables agents to learn optimal strategies while compliant with safetyregulations. We present an example where multiple operators are injecting CO2into their respective project areas in a geologically connected basin. Toaddress the high computational cost of repeated simulations of high-fidelitymodels, a previously developed surrogate model based on the Embed-to-Control(E2C) framework is employed. Our results demonstrate the effectiveness of theproposed framework in addressing optimal management of CO2 storage whenmultiple stakeholders with various objectives and goals are involved.</description><author>Jungang Chen, Seyyed A. Hosseini</author><pubDate>Fri, 15 Aug 2025 17:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11618v1</guid></item><item><title>ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</title><link>http://arxiv.org/abs/2508.08170v2</link><description>Reinforcement learning for training end-to-end autonomous driving models inclosed-loop simulations is gaining growing attention. However, most simulationenvironments differ significantly from real-world conditions, creating asubstantial simulation-to-reality (sim2real) gap. To bridge this gap, someapproaches utilize scene reconstruction techniques to create photorealisticenvironments as a simulator. While this improves realistic sensor simulation,these methods are inherently constrained by the distribution of the trainingdata, making it difficult to render high-quality sensor data for noveltrajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, aframework designed to integrate video diffusion priors into scenereconstruction to aid reinforcement learning, thereby enhancing end-to-endautonomous driving training. Specifically, in ReconDreamer-RL, we introduceReconSimulator, which combines the video diffusion prior for appearancemodeling and incorporates a kinematic model for physical modeling, therebyreconstructing driving scenarios from real-world data. This narrows thesim2real gap for closed-loop evaluation and reinforcement learning. To covermore corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),which adjusts the trajectories of surrounding vehicles relative to the egovehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issueof training data distribution, which is often biased toward simplestraight-line movements. Experiments show that ReconDreamer-RL improvesend-to-end autonomous driving training, outperforming imitation learningmethods with a 5x reduction in the Collision Ratio.</description><author>Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Xinze Chen, Guanghong Jia, Guan Huang, Wenjun Mei</author><pubDate>Thu, 21 Aug 2025 11:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.08170v2</guid></item></channel></rss>