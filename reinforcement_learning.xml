<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivreinforcement learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 18 Aug 2025 19:32:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Secure Control Systems for Autonomous Quadrotors against Cyber-Attacks</title><link>http://arxiv.org/abs/2409.11897v1</link><description>The problem of safety for robotic systems has been extensively studied.However, little attention has been given to security issues forthree-dimensional systems, such as quadrotors. Malicious adversaries cancompromise robot sensors and communication networks, causing incidents,achieving illegal objectives, or even injuring people. This study first designsan intelligent control system for autonomous quadrotors. Then, it investigatesthe problems of optimal false data injection attack scheduling andcountermeasure design for unmanned aerial vehicles. Using a state-of-the-artdeep learning-based approach, an optimal false data injection attack scheme isproposed to deteriorate a quadrotor's tracking performance with limited attackenergy. Subsequently, an optimal tracking control strategy is learned tomitigate attacks and recover the quadrotor's tracking performance. We base ourwork on Agilicious, a state-of-the-art quadrotor recently deployed forautonomous settings. This paper is the first in the United Kingdom to deploythis quadrotor and implement reinforcement learning on its platform. Therefore,to promote easy reproducibility with minimal engineering overhead, we furtherprovide (1) a comprehensive breakdown of this quadrotor, including softwarestacks and hardware alternatives; (2) a detailed reinforcement-learningframework to train autonomous controllers on Agilicious agents; and (3) a newopen-source environment that builds upon PyFlyt for future reinforcementlearning research on Agilicious platforms. Both simulated and real-worldexperiments are conducted to show the effectiveness of the proposed frameworksin section 5.2.</description><author>Samuel Belkadi</author><pubDate>Wed, 18 Sep 2024 11:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11897v1</guid></item><item><title>How to Choose a Reinforcement-Learning Algorithm</title><link>http://arxiv.org/abs/2407.20917v1</link><description>The field of reinforcement learning offers a large variety of concepts andmethods to tackle sequential decision-making problems. This variety has becomeso large that choosing an algorithm for a task at hand can be challenging. Inthis work, we streamline the process of choosing reinforcement-learningalgorithms and action-distribution families. We provide a structured overviewof existing methods and their properties, as well as guidelines for when tochoose which methods. An interactive version of these guidelines is availableonline at https://rl-picker.github.io/.</description><author>Fabian Bongratz, Vladimir Golkov, Lukas Mautner, Luca Della Libera, Frederik Heetmeyer, Felix Czaja, Julian Rodemann, Daniel Cremers</author><pubDate>Tue, 30 Jul 2024 15:54:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20917v1</guid></item><item><title>KANQAS: Kolmogorov-Arnold Network for Quantum Architecture Search</title><link>http://arxiv.org/abs/2406.17630v2</link><description>Quantum architecture search (QAS) is a promising direction for optimizationand automated design of quantum circuits towards quantum advantage. Recenttechniques in QAS focus on machine learning-based approaches from reinforcementlearning, like deep Q-network. While multi-layer perceptron-based deepQ-networks have been applied for QAS, their interpretability remainschallenging due to the high number of parameters. In this work, we evaluate thepracticality of Kolmogorov-Arnold Networks (KANs) in QAS problems, analyzingtheir efficiency in the task of quantum state preparation and quantumchemistry. In quantum state preparation, our results show that in a noiselessscenario, the probability of success and the number of optimal quantum circuitconfigurations to generate the multi-qubit maximally entangled states are$2\times$ to $5\times$ higher than Multi-Layer perceptions (MLPs). Moreover, innoisy scenarios, KAN can achieve a better fidelity in approximating maximallyentangled state than MLPs, where the performance of the MLP significantlydepends on the choice of activation function. In tackling quantum chemistryproblems, we enhance the recently proposed QAS algorithm by integratingCurriculum Reinforcement Learning (CRL) with a KAN structure instead of thetraditional MLP. This modification allows us to design a parameterized quantumcircuit that contains fewer 2-qubit gates and has a shallower depth, therebyimproving the efficiency of finding the ground state of a chemical Hamiltonian.Further investigation reveals that KAN requires a significantly smaller numberof learnable parameters compared to MLPs; however, the average time ofexecuting each episode for KAN is higher.</description><author>Akash Kundu, Aritra Sarkar, Abhishek Sadhu</author><pubDate>Mon, 22 Jul 2024 12:00:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17630v2</guid></item><item><title>Large Language Models for Human-like Autonomous Driving: A Survey</title><link>http://arxiv.org/abs/2407.19280v1</link><description>Large Language Models (LLMs), AI models trained on massive text corpora withremarkable language understanding and generation capabilities, are transformingthe field of Autonomous Driving (AD). As AD systems evolve from rule-based andoptimization-based methods to learning-based techniques like deep reinforcementlearning, they are now poised to embrace a third and more advanced category:knowledge-based AD empowered by LLMs. This shift promises to bring AD closer tohuman-like AD. However, integrating LLMs into AD systems poses challenges inreal-time inference, safety assurance, and deployment costs. This surveyprovides a comprehensive and critical review of recent progress in leveragingLLMs for AD, focusing on their applications in modular AD pipelines andend-to-end AD systems. We highlight key advancements, identify pressingchallenges, and propose promising research directions to bridge the gap betweenLLMs and AD, thereby facilitating the development of more human-like ADsystems. The survey first introduces LLMs' key features and common trainingschemes, then delves into their applications in modular AD pipelines andend-to-end AD, respectively, followed by discussions on open challenges andfuture directions. Through this in-depth analysis, we aim to provide insightsand inspiration for researchers and practitioners working at the intersectionof AI and autonomous vehicles, ultimately contributing to safer, smarter, andmore human-centric AD technologies.</description><author>Yun Li, Kai Katsumata, Ehsan Javanmardi, Manabu Tsukada</author><pubDate>Sat, 27 Jul 2024 15:24:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.19280v1</guid></item><item><title>A Survey on Reinforcement Learning in Aviation Applications</title><link>http://arxiv.org/abs/2211.02147v3</link><description>Compared with model-based control and optimization methods, reinforcementlearning (RL) provides a data-driven, learning-based framework to formulate andsolve sequential decision-making problems. The RL framework has becomepromising due to largely improved data availability and computing power in theaviation industry. Many aviation-based applications can be formulated ortreated as sequential decision-making problems. Some of them are offlineplanning problems, while others need to be solved online and aresafety-critical. In this survey paper, we first describe standard RLformulations and solutions. Then we survey the landscape of existing RL-basedapplications in aviation. Finally, we summarize the paper, identify thetechnical gaps, and suggest future directions of RL research in aviation.</description><author>Pouria Razzaghi, Amin Tabrizian, Wei Guo, Shulu Chen, Abenezer Taye, Ellis Thompson, Alexis Bregeon, Ali Baheri, Peng Wei</author><pubDate>Thu, 25 Jul 2024 20:19:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02147v3</guid></item><item><title>Two-Step Offline Preference-Based Reinforcement Learning with Constrained Actions</title><link>http://arxiv.org/abs/2401.00330v3</link><description>Preference-based reinforcement learning (PBRL) in the offline setting hassucceeded greatly in industrial applications such as chatbots. A two-steplearning framework where one applies a reinforcement learning step after areward modeling step has been widely adopted for the problem. However, such amethod faces challenges from the risk of reward hacking and the complexity ofreinforcement learning. To overcome the challenge, our insight is that bothchallenges come from the state-actions not supported in the dataset. Suchstate-actions are unreliable and increase the complexity of the reinforcementlearning problem at the second step. Based on the insight, we develop a noveltwo-step learning method called PRC: preference-based reinforcement learningwith constrained actions. The high-level idea is to limit the reinforcementlearning agent to optimize over a constrained action space that excludes theout-of-distribution state-actions. We empirically verify that our method hashigh learning efficiency on various datasets in robotic control environments.</description><author>Yinglun Xu, Tarun Suresh, Rohan Gumaste, David Zhu, Ruirui Li, Zhengyang Wang, Haoming Jiang, Xianfeng Tang, Qingyu Yin, Monica Xiao Cheng, Qi Zeng, Chao Zhang, Gagandeep Singh</author><pubDate>Fri, 25 Oct 2024 17:31:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00330v3</guid></item><item><title>Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot Task Planning</title><link>http://arxiv.org/abs/2412.19538v1</link><description>To improve the efficiency of warehousing system and meet huge customerorders, we aim to solve the challenges of dimension disaster and dynamicproperties in hyper scale multi-robot task planning (MRTP) for robotic mobilefulfillment system (RMFS). Existing research indicates that hierarchicalreinforcement learning (HRL) is an effective method to reduce these challenges.Based on that, we construct an efficient multi-stage HRL-based multi-robot taskplanner for hyper scale MRTP in RMFS, and the planning process is representedwith a special temporal graph topology. To ensure optimality, the planner isdesigned with a centralized architecture, but it also brings the challenges ofscaling up and generalization that require policies to maintain performance forvarious unlearned scales and maps. To tackle these difficulties, we firstconstruct a hierarchical temporal attention network (HTAN) to ensure basicability of handling inputs with unfixed lengths, and then design multi-stagecurricula for hierarchical policy learning to further improve the scaling upand generalization ability while avoiding catastrophic forgetting.Additionally, we notice that policies with hierarchical structure suffer fromunfair credit assignment that is similar to that in multi-agent reinforcementlearning, inspired of which, we propose a hierarchical reinforcement learningalgorithm with counterfactual rollout baseline to improve learning performance.Experimental results demonstrate that our planner outperform otherstate-of-the-art methods on various MRTP instances in both simulated andreal-world RMFS. Also, our planner can successfully scale up to hyper scaleMRTP instances in RMFS with up to 200 robots and 1000 retrieval racks onunlearned maps while keeping superior performance over other methods.</description><author>Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen</author><pubDate>Fri, 27 Dec 2024 09:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.19538v1</guid></item><item><title>WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control</title><link>http://arxiv.org/abs/2501.13592v1</link><description>The wind farm control problem is challenging, since conventional model-basedcontrol strategies require tractable models of complex aerodynamicalinteractions between the turbines and suffer from the curse of dimension whenthe number of turbines increases. Recently, model-free and multi-agentreinforcement learning approaches have been used to address this challenge. Inthis article, we introduce WFCRL (Wind Farm Control with ReinforcementLearning), the first open suite of multi-agent reinforcement learningenvironments for the wind farm control problem. WFCRL frames a cooperativeMulti-Agent Reinforcement Learning (MARL) problem: each turbine is an agent andcan learn to adjust its yaw, pitch or torque to maximize the common objective(e.g. the total power production of the farm). WFCRL also offers turbine loadobservations that will allow to optimize the farm performance while limitingturbine structural damages. Interfaces with two state-of-the-art farmsimulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamicsimulator (FAST.Farm). For each simulator, $10$ wind layouts are provided,including $5$ real wind farms. Two state-of-the-art online MARL algorithms areimplemented to illustrate the scaling challenges. As learning online onFAST.Farm is highly time-consuming, WFCRL offers the possibility of designingtransfer learning strategies from FLORIS to FAST.Farm.</description><author>Claire Bizon Monroc, Ana Bušić, Donatien Dubuc, Jiamin Zhu</author><pubDate>Thu, 23 Jan 2025 12:01:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.13592v1</guid></item><item><title>MoveLight: Enhancing Traffic Signal Control through Movement-Centric Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2407.17303v1</link><description>This paper introduces MoveLight, a novel traffic signal control system thatenhances urban traffic management through movement-centric deep reinforcementlearning. By leveraging detailed real-time data and advanced machine learningtechniques, MoveLight overcomes the limitations of traditional traffic signalcontrol methods. It employs a lane-level control approach using the FRAPalgorithm to achieve dynamic and adaptive traffic signal control, optimizingtraffic flow, reducing congestion, and improving overall efficiency. Ourresearch demonstrates the scalability and effectiveness of MoveLight acrosssingle intersections, arterial roads, and network levels. Experimental resultsusing real-world datasets from Cologne and Hangzhou show significantimprovements in metrics such as queue length, delay, and throughput compared toexisting methods. This study highlights the transformative potential of deepreinforcement learning in intelligent traffic signal control, setting a newstandard for sustainable and efficient urban transportation systems.</description><author>Junqi Shao, Chenhao Zheng, Yuxuan Chen, Yucheng Huang, Rui Zhang</author><pubDate>Wed, 24 Jul 2024 14:17:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.17303v1</guid></item><item><title>Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</title><link>http://arxiv.org/abs/2502.10148v1</link><description>Despite much progress in training distributed artificial intelligence (AI),building cooperative multi-agent systems with multi-agent reinforcementlearning (MARL) faces challenges in sample efficiency, interpretability, andtransferability. Unlike traditional learning-based methods that requireextensive interaction with the environment, large language models (LLMs)demonstrate remarkable capabilities in zero-shot planning and complexreasoning. However, existing LLM-based approaches heavily rely on text-basedobservations and struggle with the non-Markovian nature of multi-agentinteractions under partial observability. We present COMPASS, a novelmulti-agent architecture that integrates vision-language models (VLMs) with adynamic skill library and structured communication for decentralizedclosed-loop decision-making. The skill library, bootstrapped fromdemonstrations, evolves via planner-guided tasks to enable adaptive strategies.COMPASS propagates entity information through multi-hop communication underpartial observability. Evaluations on the improved StarCraft Multi-AgentChallenge (SMACv2) demonstrate COMPASS achieves up to 30\% higher win ratesthan state-of-the-art MARL algorithms in symmetric scenarios.</description><author>Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen</author><pubDate>Fri, 14 Feb 2025 13:23:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.10148v1</guid></item><item><title>Attacking Slicing Network via Side-channel Reinforcement Learning Attack</title><link>http://arxiv.org/abs/2409.11258v1</link><description>Network slicing in 5G and the future 6G networks will enable the creation ofmultiple virtualized networks on a shared physical infrastructure. Thisinnovative approach enables the provision of tailored networks to accommodatespecific business types or industry users, thus delivering more customized andefficient services. However, the shared memory and cache in network slicingintroduce security vulnerabilities that have yet to be fully addressed. In thispaper, we introduce a reinforcement learning-based side-channel cache attackframework specifically designed for network slicing environments. Unliketraditional cache attack methods, our framework leverages reinforcementlearning to dynamically identify and exploit cache locations storing sensitiveinformation, such as authentication keys and user registration data. We assumethat one slice network is compromised and demonstrate how the attacker caninduce another shared slice to send registration requests, thereby estimatingthe cache locations of critical data. By formulating the cache timing channelattack as a reinforcement learning-driven guessing game between the attackslice and the victim slice, our model efficiently explores possible actions topinpoint memory blocks containing sensitive information. Experimental resultsshowcase the superiority of our approach, achieving a success rate ofapproximately 95\% to 98\% in accurately identifying the storage locations ofsensitive data. This high level of accuracy underscores the potential risks inshared network slicing environments and highlights the need for robust securitymeasures to safeguard against such advanced side-channel attacks.</description><author>Wei Shao, Chandra Thapa, Rayne Holland, Sarah Ali Siddiqui, Seyit Camtepe</author><pubDate>Tue, 17 Sep 2024 15:07:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.11258v1</guid></item><item><title>READ: Reinforcement-based Adversarial Learning for Text Classification with Limited Labeled Data</title><link>http://arxiv.org/abs/2501.08035v1</link><description>Pre-trained transformer models such as BERT have shown massive gains acrossmany text classification tasks. However, these models usually need enormouslabeled data to achieve impressive performances. Obtaining labeled data isoften expensive and time-consuming, whereas collecting unlabeled data usingsome heuristics is relatively much cheaper for any task. Therefore, this paperproposes a method that encapsulates reinforcement learning-based textgeneration and semi-supervised adversarial learning approaches in a novel wayto improve the model's performance. Our method READ, Reinforcement-basedAdversarial learning, utilizes an unlabeled dataset to generate diversesynthetic text through reinforcement learning, improving the model'sgeneralization capability using adversarial learning. Our experimental resultsshow that READ outperforms the existing state-of-art methods on multipledatasets.</description><author>Rohit Sharma, Shanu Kumar, Avinash Kumar</author><pubDate>Tue, 14 Jan 2025 11:39:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.08035v1</guid></item><item><title>Efficient Reinforcement Learning for Optimal Control with Natural Images</title><link>http://arxiv.org/abs/2412.08893v1</link><description>Reinforcement learning solves optimal control and sequential decisionproblems widely found in control systems engineering, robotics, and artificialintelligence. This work investigates optimal control over a sequence of naturalimages. The problem is formalized, and general conditions are derived for animage to be sufficient for implementing an optimal policy. Reinforcementlearning is shown to be efficient only for certain types of imagerepresentations. This is demonstrated by developing a reinforcement learningbenchmark that scales easily with number of states and length of horizon, andhas optimal policies that are easily distinguished from suboptimal policies.Image representations given by overcomplete sparse codes are found to becomputationally efficient for optimal control, using fewer computationalresources to learn and evaluate optimal policies. For natural images of fixedsize, representing each image as an overcomplete sparse code in a linearnetwork is shown to increase network storage capacity by orders of magnitudebeyond that possible for any complete code, allowing larger tasks with manymore states to be solved. Sparse codes can be generated by devices with lowenergy requirements and low computational overhead.</description><author>Peter N. Loxley</author><pubDate>Thu, 12 Dec 2024 03:14:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08893v1</guid></item><item><title>AI-Driven Day-to-Day Route Choice</title><link>http://arxiv.org/abs/2412.03338v2</link><description>Understanding travelers' route choices can help policymakers devise optimaloperational and planning strategies for both normal and abnormal circumstances.However, existing choice modeling methods often rely on predefined assumptionsand struggle to capture the dynamic and adaptive nature of travel behavior.Recently, Large Language Models (LLMs) have emerged as a promising alternative,demonstrating remarkable ability to replicate human-like behaviors acrossvarious fields. Despite this potential, their capacity to accurately simulatehuman route choice behavior in transportation contexts remains doubtful. Tosatisfy this curiosity, this paper investigates the potential of LLMs for routechoice modeling by introducing an LLM-empowered agent, "LLMTraveler." Thisagent integrates an LLM as its core, equipped with a memory system that learnsfrom past experiences and makes decisions by balancing retrieved data andpersonality traits. The study systematically evaluates the LLMTraveler'sability to replicate human-like decision-making through two stages ofday-to-day (DTD) congestion games: (1) analyzing its route-switching behaviorin single origin-destination (OD) pair scenarios, where it demonstratespatterns that align with laboratory data but cannot be fully explained bytraditional models, and (2) testing its capacity to model adaptive learningbehaviors in multi-OD scenarios on the Ortuzar and Willumsen (OW) network,producing results comparable to Multinomial Logit (MNL) and ReinforcementLearning (RL) models. These experiments demonstrate that the framework canpartially replicate human-like decision-making in route choice while providingnatural language explanations for its decisions. This capability offersvaluable insights for transportation policymaking, such as simulating travelerresponses to new policies or changes in the network.</description><author>Leizhen Wang, Peibo Duan, Zhengbing He, Cheng Lyu, Xin Chen, Nan Zheng, Li Yao, Zhenliang Ma</author><pubDate>Tue, 31 Dec 2024 14:57:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03338v2</guid></item><item><title>FitLight: Federated Imitation Learning for Plug-and-Play Autonomous Traffic Signal Control</title><link>http://arxiv.org/abs/2502.11937v1</link><description>Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)methods have been extensively studied, their practical applications still raisesome serious issues such as high learning cost and poor generalizability. Thisis because the ``trial-and-error'' training style makes RL agents extremelydependent on the specific traffic environment, which also requires a longconvergence time. To address these issues, we propose a novel FederatedImitation Learning (FIL)-based framework for multi-intersection TSC, namedFitLight, which allows RL agents to plug-and-play for any traffic environmentwithout additional pre-training cost. Unlike existing imitation learningapproaches that rely on pre-training RL agents with demonstrations, FitLightallows real-time imitation learning and seamless transition to reinforcementlearning. Due to our proposed knowledge-sharing mechanism and novel hybridpressure-based agent design, RL agents can quickly find a best control policywith only a few episodes. Moreover, for resource-constrained TSC scenarios,FitLight supports model pruning and heterogeneous model aggregation, such thatRL agents can work on a micro-controller with merely 16{\it KB} RAM and 32{\itKB} ROM. Extensive experiments demonstrate that, compared to state-of-the-artmethods, FitLight not only provides a superior starting point but alsoconverges to a better final solution on both real-world and synthetic datasets,even under extreme resource limitations.</description><author>Yutong Ye, Yingbo Zhou, Zhusen Liu, Xiao Du, Hao Zhou, Xiang Lian, Mingsong Chen</author><pubDate>Mon, 17 Feb 2025 15:48:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.11937v1</guid></item><item><title>Deep Reinforcement Learning for Multi-Objective Optimization: Enhancing Wind Turbine Energy Generation while Mitigating Noise Emissions</title><link>http://arxiv.org/abs/2407.13320v1</link><description>We develop a torque-pitch control framework using deep reinforcement learningfor wind turbines to optimize the generation of wind turbine energy whileminimizing operational noise. We employ a double deep Q-learning, coupled to ablade element momentum solver, to enable precise control over wind turbineparameters. In addition to the blade element momentum, we use the wind turbineacoustic model of Brooks Pope and Marcolini. Through training with simplewinds, the agent learns optimal control policies that allow efficient controlfor complex turbulent winds. Our experiments demonstrate that the reinforcementlearning is able to find optima at the Pareto front, when maximizing energywhile minimizing noise. In addition, the adaptability of the reinforcementlearning agent to changing turbulent wind conditions, underscores its efficacyfor real-world applications. We validate the methodology using a SWT2.3-93 windturbine with a rated power of 2.3 MW. We compare the reinforcement learningcontrol to classic controls to show that they are comparable when not takinginto account noise emissions. When including a maximum limit of 45 dB to thenoise produced (100 meters downwind of the turbine), the extracted yearlyenergy decreases by 22%. The methodology is flexible and allows for easy tuningof the objectives and constraints through the reward definitions, resulting ina flexible multi-objective optimization framework for wind turbine control.Overall, our findings highlight the potential of RL-based control strategies toimprove wind turbine efficiency while mitigating noise pollution, thusadvancing sustainable energy generation technologies</description><author>Martín de Frutos, Oscar A. Marino, David Huergo, Esteban Ferrer</author><pubDate>Thu, 18 Jul 2024 09:21:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.13320v1</guid></item><item><title>Reinforcement learning-based statistical search strategy for an axion model from flavor</title><link>http://arxiv.org/abs/2409.10023v1</link><description>We propose a reinforcement learning-based search strategy to explore newphysics beyond the Standard Model. The reinforcement learning, which is one ofmachine learning methods, is a powerful approach to find model parameters withphenomenological constraints. As a concrete example, we focus on a minimalaxion model with a global $U(1)$ flavor symmetry. Agents of the learningsucceed in finding $U(1)$ charge assignments of quarks and leptons solving theflavor and cosmological puzzles in the Standard Model, and find more than 150realistic solutions for the quark sector taking renormalization effects intoaccount. For the solutions found by the reinforcement learning-based analysis,we discuss the sensitivity of future experiments for the detection of an axionwhich is a Nambu-Goldstone boson of the spontaneously broken $U(1)$. We alsoexamine how fast the reinforcement learning-based searching method finds thebest discrete parameters in comparison with conventional optimization methods.In conclusion, the efficient parameter search based on the reinforcementlearning-based strategy enables us to perform a statistical analysis of thevast parameter space associated with the axion model from flavor.</description><author>Satsuki Nishimura, Coh Miyao, Hajime Otsuka</author><pubDate>Mon, 16 Sep 2024 06:21:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10023v1</guid></item><item><title>Trustworthy Human-AI Collaboration: Reinforcement Learning with Human Feedback and Physics Knowledge for Safe Autonomous Driving</title><link>http://arxiv.org/abs/2409.00858v2</link><description>In the field of autonomous driving, developing safe and trustworthyautonomous driving policies remains a significant challenge. Recently,Reinforcement Learning with Human Feedback (RLHF) has attracted substantialattention due to its potential to enhance training safety and samplingefficiency. Nevertheless, existing RLHF-enabled methods often falter when facedwith imperfect human demonstrations, potentially leading to trainingoscillations or even worse performance than rule-based approaches. Inspired bythe human learning process, we propose Physics-enhanced Reinforcement Learningwith Human Feedback (PE-RLHF). This novel framework synergistically integrateshuman feedback (e.g., human intervention and demonstration) and physicsknowledge (e.g., traffic flow model) into the training loop of reinforcementlearning. The key advantage of PE-RLHF is its guarantee that the learned policywill perform at least as well as the given physics-based policy, even whenhuman feedback quality deteriorates, thus ensuring trustworthy safetyimprovements. PE-RLHF introduces a Physics-enhanced Human-AI (PE-HAI)collaborative paradigm for dynamic action selection between human andphysics-based actions, employs a reward-free approach with a proxy valuefunction to capture human preferences, and incorporates a minimal interventionmechanism to reduce the cognitive load on human mentors. Extensive experimentsacross diverse driving scenarios demonstrate that PE-RLHF significantlyoutperforms traditional methods, achieving state-of-the-art (SOTA) performancein safety, efficiency, and generalizability, even with varying quality of humanfeedback. The philosophy behind PE-RLHF not only advances autonomous drivingtechnology but can also offer valuable insights for other safety-criticaldomains. Demo video and code are available at:\https://zilin-huang.github.io/PE-RLHF-website/</description><author>Zilin Huang, Zihao Sheng, Sikai Chen</author><pubDate>Thu, 05 Sep 2024 08:07:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.00858v2</guid></item><item><title>Multi-Agent Reinforcement Learning-Based UAV Pathfinding for Obstacle Avoidance in Stochastic Environment</title><link>http://arxiv.org/abs/2310.16659v2</link><description>Traditional methods plan feasible paths for multiple agents in the stochasticenvironment. However, the methods' iterations with the changes in theenvironment result in computation complexities, especially for thedecentralized agents without a centralized planner. Although reinforcementlearning provides a plausible solution because of the generalization fordifferent environments, it struggles with enormous agent-environmentinteractions in training. Here, we propose a novel centralized training withdecentralized execution method based on multi-agent reinforcement learning,which is improved based on the idea of model predictive control. In ourapproach, agents communicate only with the centralized planner to makedecentralized decisions online in the stochastic environment. Furthermore,considering the communication constraint with the centralized planner, eachagent plans feasible paths through the extended observation, which combinesinformation on neighboring agents based on the distance-weighted mean fieldapproach. Inspired by the rolling optimization approach of model predictivecontrol, we conduct multi-step value convergence in multi-agent reinforcementlearning to enhance the training efficiency, which reduces the expensiveinteractions in convergence. Experiment results in both comparison, ablation,and real-robot studies validate the effectiveness and generalizationperformance of our method.</description><author>Qizhen Wu, Kexin Liu, Lei Chen, Jinhu Lü</author><pubDate>Fri, 25 Oct 2024 08:56:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16659v2</guid></item><item><title>Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2311.09852v7</link><description>Swarms of autonomous interactive drones can provide compelling sensingcapabilities in Smart City applications, such as traffic monitoring. This paperfocuses on the task assignment problem for large-scale spatio-temporal sensingby a drone swarm. However, existing approaches have distinct challenges:distributed evolutionary optimization, such as collective learning, lackslong-term adaptability in dynamic environments, while deep reinforcementlearning (DRL) is limited to scale effectively due to the curse ofdimensionality. Therefore, this paper proposes a novel synergetic optimizationapproach by integrating long-term DRL and short-term collective learning.Through this approach, each drone independently and proactively determines itsflying direction and recharging location using DRL, while evolving theirnavigation and sensing policies through collective learning based on astructured tree communication model. Extensive experiments with datasetsgenerated from realistic urban mobility demonstrate an outstanding performanceof the proposed solution in complex scenarios. New insights show that thisapproach provides a win-win synthesis of short-term and long-term strategiesfor drone-based traffic monitoring, with short-term methods addressing trainingcomplexity and energy management, while long-term methods preserving highsensing performance.</description><author>Chuhao Qin, Evangelos Pournaras</author><pubDate>Tue, 01 Oct 2024 16:11:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.09852v7</guid></item><item><title>Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function for Real-Time Strategy Tasks</title><link>http://arxiv.org/abs/2501.03824v1</link><description>Effective evaluation of real-time strategy tasks requires adaptive mechanismsto cope with dynamic and unpredictable environments. This study proposes amethod to improve evaluation functions for real-time responsiveness tobattle-field situation changes, utilizing an online reinforcementlearning-based dynam-ic weight adjustment mechanism within the real-timestrategy game. Building on traditional static evaluation functions, the methodemploys gradient descent in online reinforcement learning to update weightsdynamically, incorporating weight decay techniques to ensure stability.Additionally, the AdamW optimizer is integrated to adjust the learning rate anddecay rate of online reinforcement learning in real time, further reducing thedependency on manual parameter tun-ing. Round-robin competition experimentsdemonstrate that this method signifi-cantly enhances the applicationeffectiveness of the Lanchester combat model evaluation function, Simpleevaluation function, and Simple Sqrt evaluation function in planning algorithmsincluding IDABCD, IDRTMinimax, and Port-folio AI. The method achieves a notableimprovement in scores, with the en-hancement becoming more pronounced as themap size increases. Furthermore, the increase in evaluation functioncomputation time induced by this method is kept below 6% for all evaluationfunctions and planning algorithms. The pro-posed dynamic adaptive evaluationfunction demonstrates a promising approach for real-time strategy taskevaluation.</description><author>Weilong Yang, Jie Zhang, Xunyun Liu, Yanqing Ye</author><pubDate>Tue, 07 Jan 2025 14:36:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.03824v1</guid></item><item><title>Robust Real-Time Mortality Prediction in the Intensive Care Unit using Temporal Difference Learning</title><link>http://arxiv.org/abs/2411.04285v1</link><description>The task of predicting long-term patient outcomes using supervised machinelearning is a challenging one, in part because of the high variance of eachpatient's trajectory, which can result in the model over-fitting to thetraining data. Temporal difference (TD) learning, a common reinforcementlearning technique, may reduce variance by generalising learning to the patternof state transitions rather than terminal outcomes. However, in healthcare thismethod requires several strong assumptions about patient states, and thereappears to be limited literature evaluating the performance of TD learningagainst traditional supervised learning methods for long-term health outcomeprediction tasks. In this study, we define a framework for applying TD learningto real-time irregularly sampled time series data using a Semi-Markov RewardProcess. We evaluate the model framework in predicting intensive care mortalityand show that TD learning under this framework can result in improved modelrobustness compared to standard supervised learning methods. and that thisrobustness is maintained even when validated on external datasets. Thisapproach may offer a more reliable method when learning to predict patientoutcomes using high-variance irregular time series data.</description><author>Thomas Frost, Kezhi Li, Steve Harris</author><pubDate>Wed, 06 Nov 2024 22:11:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.04285v1</guid></item><item><title>MathDSL: A Domain-Specific Language for Concise Mathematical Solutions Via Program Synthesis</title><link>http://arxiv.org/abs/2409.17490v1</link><description>We present MathDSL, a Domain-Specific Language (DSL) for mathematicalequation solving, which, when deployed in program synthesis models, outperformsstate-of-the-art reinforcement-learning-based methods. We also introduce aquantitative metric for measuring the conciseness of a mathematical solutionand demonstrate the improvement in the quality of generated solutions comparedto other methods. Our system demonstrates that a program synthesis system(DreamCoder) using MathDSL can generate programs that solve linear equationswith greater accuracy and conciseness than using reinforcement learningsystems. Additionally, we demonstrate that if we use the action spaces ofprevious reinforcement learning systems as DSLs, MathDSL outperforms theaction-space-DSLs. We use DreamCoder to store equation-solving strategies aslearned abstractions in its program library and demonstrate that by usingMathDSL, these can be converted into human-interpretable solution strategiesthat could have applications in mathematical education.</description><author>Sagnik Anupam, Maddy Bowers, Omar Costilla-Reyes, Armando Solar-Lezama</author><pubDate>Thu, 26 Sep 2024 02:54:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17490v1</guid></item><item><title>MathDSL: A Domain-Specific Language for Concise Mathematical Solutions Via Program Synthesis</title><link>http://arxiv.org/abs/2409.17490v3</link><description>We present MathDSL, a Domain-Specific Language (DSL) for mathematicalequation solving, which, when deployed in program synthesis models, outperformsstate-of-the-art reinforcement-learning-based methods. We also introduce aquantitative metric for measuring the conciseness of a mathematical solutionand demonstrate the improvement in the quality of generated solutions comparedto other methods. Our system demonstrates that a program synthesis system(DreamCoder) using MathDSL can generate programs that solve linear equationswith greater accuracy and conciseness than using reinforcement learningsystems. Additionally, we demonstrate that if we use the action spaces ofprevious reinforcement learning systems as DSLs, MathDSL outperforms theaction-space-DSLs. We use DreamCoder to store equation-solving strategies aslearned abstractions in its program library and demonstrate that by usingMathDSL, these can be converted into human-interpretable solution strategiesthat could have applications in mathematical education.</description><author>Sagnik Anupam, Maddy Bowers, Omar Costilla-Reyes, Armando Solar-Lezama</author><pubDate>Wed, 11 Dec 2024 16:31:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17490v3</guid></item><item><title>MathDSL: A Domain-Specific Language for Concise Mathematical Solutions Via Program Synthesis</title><link>http://arxiv.org/abs/2409.17490v2</link><description>We present MathDSL, a Domain-Specific Language (DSL) for mathematicalequation solving, which, when deployed in program synthesis models, outperformsstate-of-the-art reinforcement-learning-based methods. We also introduce aquantitative metric for measuring the conciseness of a mathematical solutionand demonstrate the improvement in the quality of generated solutions comparedto other methods. Our system demonstrates that a program synthesis system(DreamCoder) using MathDSL can generate programs that solve linear equationswith greater accuracy and conciseness than using reinforcement learningsystems. Additionally, we demonstrate that if we use the action spaces ofprevious reinforcement learning systems as DSLs, MathDSL outperforms theaction-space-DSLs. We use DreamCoder to store equation-solving strategies aslearned abstractions in its program library and demonstrate that by usingMathDSL, these can be converted into human-interpretable solution strategiesthat could have applications in mathematical education.</description><author>Sagnik Anupam, Maddy Bowers, Omar Costilla-Reyes, Armando Solar-Lezama</author><pubDate>Thu, 31 Oct 2024 03:28:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.17490v2</guid></item><item><title>Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning</title><link>http://arxiv.org/abs/2409.12045v1</link><description>Safety is one of the key issues preventing the deployment of reinforcementlearning techniques in real-world robots. While most approaches in the SafeReinforcement Learning area do not require prior knowledge of constraints androbot kinematics and rely solely on data, it is often difficult to deploy themin complex real-world settings. Instead, model-based approaches thatincorporate prior knowledge of the constraints and dynamics into the learningframework have proven capable of deploying the learning algorithm directly onthe real robot. Unfortunately, while an approximated model of the robotdynamics is often available, the safety constraints are task-specific and hardto obtain: they may be too complicated to encode analytically, too expensive tocompute, or it may be difficult to envision a priori the long-term safetyrequirements. In this paper, we bridge this gap by extending the safeexploration method, ATACOM, with learnable constraints, with a particular focuson ensuring long-term safety and handling of uncertainty. Our approach iscompetitive or superior to state-of-the-art methods in final performance whilemaintaining safer behavior during training.</description><author>Jonas Günster, Puze Liu, Jan Peters, Davide Tateo</author><pubDate>Wed, 18 Sep 2024 15:08:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12045v1</guid></item><item><title>Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning</title><link>http://arxiv.org/abs/2409.12045v2</link><description>Safety is one of the key issues preventing the deployment of reinforcementlearning techniques in real-world robots. While most approaches in the SafeReinforcement Learning area do not require prior knowledge of constraints androbot kinematics and rely solely on data, it is often difficult to deploy themin complex real-world settings. Instead, model-based approaches thatincorporate prior knowledge of the constraints and dynamics into the learningframework have proven capable of deploying the learning algorithm directly onthe real robot. Unfortunately, while an approximated model of the robotdynamics is often available, the safety constraints are task-specific and hardto obtain: they may be too complicated to encode analytically, too expensive tocompute, or it may be difficult to envision a priori the long-term safetyrequirements. In this paper, we bridge this gap by extending the safeexploration method, ATACOM, with learnable constraints, with a particular focuson ensuring long-term safety and handling of uncertainty. Our approach iscompetitive or superior to state-of-the-art methods in final performance whilemaintaining safer behavior during training.</description><author>Jonas Günster, Puze Liu, Jan Peters, Davide Tateo</author><pubDate>Mon, 23 Sep 2024 12:42:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.12045v2</guid></item><item><title>Reinforcement Learning-based Adaptive Mitigation of Uncorrected DRAM Errors in the Field</title><link>http://arxiv.org/abs/2407.16377v1</link><description>Scaling to larger systems, with current levels of reliability, requirescost-effective methods to mitigate hardware failures. One of the main causes ofhardware failure is an uncorrected error in memory, which terminates thecurrent job and wastes all computation since the last checkpoint. This paperpresents the first adaptive method for triggering uncorrected error mitigation.It uses a prediction approach that considers the likelihood of an uncorrectederror and its current potential cost. The method is based on reinforcementlearning, and the only user-defined parameters are the mitigation cost andwhether the job can be restarted from a mitigation point. We evaluate ourmethod using classical machine learning metrics together with a cost-benefitanalysis, which compares the cost of mitigation actions with the benefits frommitigating some of the errors. On two years of production logs from theMareNostrum supercomputer, our method reduces lost compute time by 54% comparedwith no mitigation and is just 6% below the optimal Oracle method. All sourcecode is open source.</description><author>Isaac Boixaderas, Sergi Moré, Javier Bartolome, David Vicente, Petar Radojković, Paul M. Carpenter, Eduard Ayguadé</author><pubDate>Tue, 23 Jul 2024 11:04:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.16377v1</guid></item><item><title>DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories</title><link>http://arxiv.org/abs/2310.04266v2</link><description>This investigation introduces a novel deep reinforcement learning-based suiteto control floating platforms in both simulated and real-world environments.Floating platforms serve as versatile test-beds to emulate micro-gravityenvironments on Earth, useful to test autonomous navigation systems for spaceapplications. Our approach addresses the system and environmental uncertaintiesin controlling such platforms by training policies capable of precise maneuversamid dynamic and unpredictable conditions. Leveraging Deep ReinforcementLearning (DRL) techniques, our suite achieves robustness, adaptability, andgood transferability from simulation to reality. Our deep reinforcementlearning framework provides advantages such as fast training times, large-scaletesting capabilities, rich visualization options, and ROS bindings forintegration with real-world robotic systems. Being open access, our suiteserves as a comprehensive platform for practitioners who want to replicatesimilar research in their own simulated environments and labs.</description><author>Matteo El-Hariry, Antoine Richard, Vivek Muralidharan, Matthieu Geist, Miguel Olivares-Mendez</author><pubDate>Mon, 16 Sep 2024 09:16:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04266v2</guid></item><item><title>Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control</title><link>http://arxiv.org/abs/2410.08979v1</link><description>Reinforcement learning (RL) is rapidly reaching and surpassing human-levelcontrol capabilities. However, state-of-the-art RL algorithms often requiretimesteps and reaction times significantly faster than human capabilities,which is impractical in real-world settings and typically necessitatesspecialized hardware. Such speeds are difficult to achieve in the real worldand often requires specialized hardware. We introduce Sequence ReinforcementLearning (SRL), an RL algorithm designed to produce a sequence of actions for agiven input state, enabling effective control at lower decision frequencies.SRL addresses the challenges of learning action sequences by employing both amodel and an actor-critic architecture operating at different temporal scales.We propose a "temporal recall" mechanism, where the critic uses the model toestimate intermediate states between primitive actions, providing a learningsignal for each individual action within the sequence. Once training iscomplete, the actor can generate action sequences independently of the model,achieving model-free control at a slower frequency. We evaluate SRL on a suiteof continuous control tasks, demonstrating that it achieves performancecomparable to state-of-the-art algorithms while significantly reducing actorsample complexity. To better assess performance across varying decisionfrequencies, we introduce the Frequency-Averaged Score (FAS) metric. Ourresults show that SRL significantly outperforms traditional RL algorithms interms of FAS, making it particularly suitable for applications requiringvariable decision frequencies. Additionally, we compare SRL with model-basedonline planning, showing that SRL achieves superior FAS while leveraging thesame model during training that online planners use for planning.</description><author>Devdhar Patel, Hava Siegelmann</author><pubDate>Fri, 11 Oct 2024 16:54:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08979v1</guid></item><item><title>Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control</title><link>http://arxiv.org/abs/2410.08979v2</link><description>Reinforcement learning (RL) is rapidly reaching and surpassing human-levelcontrol capabilities. However, state-of-the-art RL algorithms often requiretimesteps and reaction times significantly faster than human capabilities,which is impractical in real-world settings and typically necessitatesspecialized hardware. Such speeds are difficult to achieve in the real worldand often requires specialized hardware. We introduce Sequence ReinforcementLearning (SRL), an RL algorithm designed to produce a sequence of actions for agiven input state, enabling effective control at lower decision frequencies.SRL addresses the challenges of learning action sequences by employing both amodel and an actor-critic architecture operating at different temporal scales.We propose a "temporal recall" mechanism, where the critic uses the model toestimate intermediate states between primitive actions, providing a learningsignal for each individual action within the sequence. Once training iscomplete, the actor can generate action sequences independently of the model,achieving model-free control at a slower frequency. We evaluate SRL on a suiteof continuous control tasks, demonstrating that it achieves performancecomparable to state-of-the-art algorithms while significantly reducing actorsample complexity. To better assess performance across varying decisionfrequencies, we introduce the Frequency-Averaged Score (FAS) metric. Ourresults show that SRL significantly outperforms traditional RL algorithms interms of FAS, making it particularly suitable for applications requiringvariable decision frequencies. Additionally, we compare SRL with model-basedonline planning, showing that SRL achieves superior FAS while leveraging thesame model during training that online planners use for planning.</description><author>Devdhar Patel, Hava Siegelmann</author><pubDate>Fri, 18 Oct 2024 14:35:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.08979v2</guid></item><item><title>Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron</title><link>http://arxiv.org/abs/2409.03749v1</link><description>The ability of a brain or a neural network to efficiently learn dependscrucially on both the task structure and the learning rule. Previous works haveanalyzed the dynamical equations describing learning in the relativelysimplified context of the perceptron under assumptions of a student-teacherframework or a linearized output. While these assumptions have facilitatedtheoretical understanding, they have precluded a detailed understanding of theroles of the nonlinearity and input-data distribution in determining thelearning dynamics, limiting the applicability of the theories to realbiological or artificial neural networks. Here, we use a stochastic-processapproach to derive flow equations describing learning, applying this frameworkto the case of a nonlinear perceptron performing binary classification. Wecharacterize the effects of the learning rule (supervised or reinforcementlearning, SL/RL) and input-data distribution on the perceptron's learning curveand the forgetting curve as subsequent tasks are learned. In particular, wefind that the input-data noise differently affects the learning speed under SLvs. RL, as well as determines how quickly learning of a task is overwritten bysubsequent learning. Additionally, we verify our approach with real data usingthe MNIST dataset. This approach points a way toward analyzing learningdynamics for more-complex circuit architectures.</description><author>Christian Schmid, James M. Murray</author><pubDate>Thu, 05 Sep 2024 17:58:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.03749v1</guid></item><item><title>An introduction to reinforcement learning for neuroscience</title><link>http://arxiv.org/abs/2311.07315v2</link><description>Reinforcement learning has a rich history in neuroscience, from early work ondopamine as a reward prediction error signal for temporal difference learning(Schultz et al., 1997) to recent work suggesting that dopamine could implementa form of 'distributional reinforcement learning' popularized in deep learning(Dabney et al., 2020). Throughout this literature, there has been a tight linkbetween theoretical advances in reinforcement learning and neuroscientificexperiments and findings. As a result, the theories describing our experimentaldata have become increasingly complex and difficult to navigate. In thisreview, we cover the basic theory underlying classical work in reinforcementlearning and build up to an introductory overview of methods in modern deepreinforcement learning that have found applications in systems neuroscience. Westart with an overview of the reinforcement learning problem and classicaltemporal difference algorithms, followed by a discussion of 'model-free' and'model-based' reinforcement learning together with methods such as DYNA andsuccessor representations that fall in between these two extremes. Throughoutthese sections, we highlight the close parallels between such machine learningmethods and related work in both experimental and theoretical neuroscience. Wethen provide an introduction to deep reinforcement learning with examples ofhow these methods have been used to model different learning phenomena insystems neuroscience, such as meta-reinforcement learning (Wang et al., 2018)and distributional reinforcement learning (Dabney et al., 2020). Code thatimplements the methods discussed in this work and generates the figures is alsoprovided.</description><author>Kristopher T. Jensen</author><pubDate>Thu, 01 Aug 2024 16:07:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.07315v2</guid></item><item><title>Improve Vision Language Model Chain-of-thought Reasoning</title><link>http://arxiv.org/abs/2410.16198v1</link><description>Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucialfor improving interpretability and trustworthiness. However, current trainingrecipes lack robust CoT reasoning data, relying on datasets dominated by shortannotations with minimal rationales. In this work, we show that training VLM onshort answers does not generalize well to reasoning tasks that require moredetailed responses. To address this, we propose a two-fold approach. First, wedistill rationales from GPT-4o model to enrich the training data and fine-tuneVLMs, boosting their CoT performance. Second, we apply reinforcement learningto further calibrate reasoning quality. Specifically, we construct positive(correct) and negative (incorrect) pairs of model-generated reasoning chains,by comparing their predictions with annotated short answers. Using thispairwise data, we apply the Direct Preference Optimization algorithm to refinethe model's reasoning abilities. Our experiments demonstrate significantimprovements in CoT reasoning on benchmark datasets and better generalizationto direct answer prediction as well. This work emphasizes the importance ofincorporating detailed rationales in training and leveraging reinforcementlearning to strengthen the reasoning capabilities of VLMs.</description><author>Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, Yiming Yang</author><pubDate>Mon, 21 Oct 2024 17:00:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.16198v1</guid></item><item><title>Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning</title><link>http://arxiv.org/abs/2410.17971v1</link><description>Spectrum access is an essential problem in device-to-device (D2D)communications. However, with the recent growth in the number of mobiledevices, the wireless spectrum is becoming scarce, resulting in low spectralefficiency for D2D communications. To address this problem, this paper aims tointegrate the ambient backscatter communication technology into D2D devices toallow them to backscatter ambient RF signals to transmit their data when theshared spectrum is occupied by mobile users. To obtain the optimal spectrumaccess policy, i.e., stay idle or access the shared spectrum and perform activetransmissions or backscattering ambient RF signals for transmissions, tomaximize the average throughput for D2D users, deep reinforcement learning(DRL) can be adopted. However, DRL-based solutions may require long trainingtime due to the curse of dimensionality issue as well as complex deep neuralnetwork architectures. For that, we develop a novel quantum reinforcementlearning (RL) algorithm that can achieve a faster convergence rate with fewertraining parameters compared to DRL thanks to the quantum superposition andquantum entanglement principles. Specifically, instead of using conventionaldeep neural networks, the proposed quantum RL algorithm uses a parametrizedquantum circuit to approximate an optimal policy. Extensive simulations thendemonstrate that the proposed solution not only can significantly improve theaverage throughput of D2D devices when the shared spectrum is busy but also canachieve much better performance in terms of convergence rate and learningcomplexity compared to existing DRL-based methods.</description><author>Nguyen Van Huynh, Bolun Zhang, Dinh-Hieu Tran, Dinh Thai Hoang, Diep N. Nguyen, Gan Zheng, Dusit Niyato, Quoc-Viet Pham</author><pubDate>Wed, 23 Oct 2024 15:36:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.17971v1</guid></item><item><title>Rationality based Innate-Values-driven Reinforcement Learning</title><link>http://arxiv.org/abs/2411.09160v1</link><description>Innate values describe agents' intrinsic motivations, which reflect theirinherent interests and preferences to pursue goals and drive them to developdiverse skills satisfying their various needs. The essence of reinforcementlearning (RL) is learning from interaction based on reward-driven behaviors,much like natural agents. It is an excellent model to describe theinnate-values-driven (IV) behaviors of AI agents. Especially developing theawareness of the AI agent through balancing internal and external utilitiesbased on its needs in different tasks is a crucial problem for individualslearning to support AI agents integrating human society with safety and harmonyin the long term. This paper proposes a hierarchical compound intrinsic valuereinforcement learning model -- innate-values-driven reinforcement learningtermed IVRL to describe the complex behaviors of AI agents' interaction. Weformulated the IVRL model and proposed two IVRL models: DQN and A2C. Bycomparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in theRole-Playing Game (RPG) reinforcement learning test platform VIZDoom, wedemonstrated that rationally organizing various individual needs caneffectively achieve better performance.</description><author>Qin Yang</author><pubDate>Thu, 14 Nov 2024 03:28:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.09160v1</guid></item><item><title>CRASH: Challenging Reinforcement-Learning Based Adversarial Scenarios For Safety Hardening</title><link>http://arxiv.org/abs/2411.16996v1</link><description>Ensuring the safety of autonomous vehicles (AVs) requires identifying rarebut critical failure cases that on-road testing alone cannot discover.High-fidelity simulations provide a scalable alternative, but automaticallygenerating realistic and diverse traffic scenarios that can effectively stresstest AV motion planners remains a key challenge. This paper introduces CRASH -Challenging Reinforcement-learning based Adversarial scenarios for SafetyHardening - an adversarial deep reinforcement learning framework to addressthis issue. First CRASH can control adversarial Non Player Character (NPC)agents in an AV simulator to automatically induce collisions with the Egovehicle, falsifying its motion planner. We also propose a novel approach, thatwe term safety hardening, which iteratively refines the motion planner bysimulating improvement scenarios against adversarial agents, leveraging thefailure cases to strengthen the AV stack. CRASH is evaluated on a simplifiedtwo-lane highway scenario, demonstrating its ability to falsify both rule-basedand learning-based planners with collision rates exceeding 90%. Additionally,safety hardening reduces the Ego vehicle's collision rate by 26%. Whilepreliminary, these results highlight RL-based safety hardening as a promisingapproach for scenario-driven simulation testing for autonomous vehicles.</description><author>Amar Kulkarni, Shangtong Zhang, Madhur Behl</author><pubDate>Tue, 26 Nov 2024 00:00:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.16996v1</guid></item><item><title>Reward Centering</title><link>http://arxiv.org/abs/2405.09999v2</link><description>We show that discounted methods for solving continuing reinforcement learningproblems can perform significantly better if they center their rewards bysubtracting out the rewards' empirical average. The improvement is substantialat commonly used discount factors and increases further as the discount factorapproaches one. In addition, we show that if a problem's rewards are shifted bya constant, then standard methods perform much worse, whereas methods withreward centering are unaffected. Estimating the average reward isstraightforward in the on-policy setting; we propose a slightly moresophisticated method for the off-policy setting. Reward centering is a generalidea, so we expect almost every reinforcement-learning algorithm to benefit bythe addition of reward centering.</description><author>Abhishek Naik, Yi Wan, Manan Tomar, Richard S. Sutton</author><pubDate>Wed, 30 Oct 2024 14:18:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.09999v2</guid></item><item><title>Learning from Active Human Involvement through Proxy Value Propagation</title><link>http://arxiv.org/abs/2502.03369v1</link><description>Learning from active human involvement enables the human subject to activelyintervene and demonstrate to the AI agent during training. The interaction andcorrective feedback from human brings safety and AI alignment to the learningprocess. In this work, we propose a new reward-free active human involvementmethod called Proxy Value Propagation for policy optimization. Our key insightis that a proxy value function can be designed to express human intents,wherein state-action pairs in the human demonstration are labeled with highvalues, while those agents' actions that are intervened receive low values.Through the TD-learning framework, labeled values of demonstrated state-actionpairs are further propagated to other unlabeled data generated from agents'exploration. The proxy value function thus induces a policy that faithfullyemulates human behaviors. Human-in-the-loop experiments show the generality andefficiency of our method. With minimal modification to existing reinforcementlearning algorithms, our method can learn to solve continuous and discretecontrol tasks with various human control devices, including the challengingtask of driving in Grand Theft Auto V. Demo video and code are available at:https://metadriverse.github.io/pvp</description><author>Zhenghao Peng, Wenjie Mo, Chenda Duan, Quanyi Li, Bolei Zhou</author><pubDate>Wed, 05 Feb 2025 17:07:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.03369v1</guid></item><item><title>Policy Agnostic RL: Offline RL and Online RL Fine-Tuning of Any Class and Backbone</title><link>http://arxiv.org/abs/2412.06685v1</link><description>Recent advances in learning decision-making policies can largely beattributed to training expressive policy models, largely via imitationlearning. While imitation learning discards non-expert data, reinforcementlearning (RL) can still learn from suboptimal data. However, instantiating RLtraining of a new policy class often presents a different challenge: most deepRL machinery is co-developed with assumptions on the policy class and backbone,resulting in poor performance when the policy class changes. For instance, SACutilizes a low-variance reparameterization policy gradient for Gaussianpolicies, but this is unstable for diffusion policies and intractable forautoregressive categorical policies. To address this issue, we develop anoffline RL and online fine-tuning approach called policy-agnostic RL (PA-RL)that can effectively train multiple policy classes, with varying architecturesand sizes. We build off the basic idea that a universal supervised learningloss can replace the policy improvement step in RL, as long as it is applied on"optimized" actions. To obtain these optimized actions, we first samplemultiple actions from a base policy, and run global optimization (i.e.,re-ranking multiple action samples using the Q-function) and local optimization(i.e., running gradient steps on an action sample) to maximize the critic onthese candidates. PA-RL enables fine-tuning diffusion and transformer policieswith either autoregressive tokens or continuous action outputs, at differentsizes, entirely via actor-critic RL. Moreover, PA-RL improves the performanceand sample-efficiency by up to 2 times compared to existing offline RL andonline fine-tuning methods. We show the first result that successfullyfine-tunes OpenVLA, a 7B generalist robot policy, autonomously with Cal-QL, anonline RL fine-tuning algorithm, improving from 40% to 70% in the real world in40 minutes.</description><author>Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar Srirama, Archit Sharma, Chelsea Finn, Aviral Kumar</author><pubDate>Mon, 09 Dec 2024 17:28:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.06685v1</guid></item><item><title>Solving Deep Reinforcement Learning Tasks with Evolution Strategies and Linear Policy Networks</title><link>http://arxiv.org/abs/2402.06912v2</link><description>Although deep reinforcement learning methods can learn effective policies forchallenging problems such as Atari games and robotics tasks, algorithms arecomplex, and training times are often long. This study investigates howEvolution Strategies perform compared to gradient-based deep reinforcementlearning methods. We use Evolution Strategies to optimize the weights of aneural network via neuroevolution, performing direct policy search. Webenchmark both deep policy networks and networks consisting of a single linearlayer from observations to actions for three gradient-based methods, such asProximal Policy Optimization. These methods are evaluated against threeclassical Evolution Strategies and Augmented Random Search, which all uselinear policy networks. Our results reveal that Evolution Strategies can findeffective linear policies for many reinforcement learning benchmark tasks,unlike deep reinforcement learning methods that can only find successfulpolicies using much larger networks, suggesting that current benchmarks areeasier to solve than previously assumed. Interestingly, Evolution Strategiesalso achieve results comparable to gradient-based deep reinforcement learningalgorithms for higher-complexity tasks. Furthermore, we find that by directlyaccessing the memory state of the game, Evolution Strategies can findsuccessful policies in Atari that outperform the policies found by DeepQ-Learning. Evolution Strategies also outperform Augmented Random Search inmost benchmarks, demonstrating superior sample efficiency and robustness intraining linear policy networks.</description><author>Annie Wong, Jacob de Nobel, Thomas Bäck, Aske Plaat, Anna V. Kononova</author><pubDate>Wed, 24 Jul 2024 17:15:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.06912v2</guid></item><item><title>Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning</title><link>http://arxiv.org/abs/2408.03029v2</link><description>Reward shaping addresses the challenge of sparse rewards in reinforcementlearning by constructing denser and more informative reward signals. To achieveself-adaptive and highly efficient reward shaping, we propose a novel methodthat incorporates success rates derived from historical experiences into shapedrewards. Our approach utilizes success rates sampled from Beta distributions,which dynamically evolve from uncertain to reliable values as more data iscollected. Initially, the self-adaptive success rates exhibit more randomnessto encourage exploration. Over time, they become more certain to enhanceexploitation, thus achieving a better balance between exploration andexploitation. We employ Kernel Density Estimation (KDE) combined with RandomFourier Features (RFF) to derive the Beta distributions, resulting in acomputationally efficient implementation in high-dimensional continuous statespaces. This method provides a non-parametric and learning-free approach. Theproposed method is evaluated on a wide range of continuous control tasks withsparse and delayed rewards, demonstrating significant improvements in sampleefficiency and convergence stability compared to relevant baselines.</description><author>Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, Tze-Yun Leong</author><pubDate>Wed, 07 Aug 2024 05:59:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03029v2</guid></item><item><title>Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning</title><link>http://arxiv.org/abs/2408.03029v1</link><description>Reward shaping addresses the challenge of sparse rewards in reinforcementlearning by constructing denser and more informative reward signals. To achieveself-adaptive and highly efficient reward shaping, we propose a novel methodthat incorporates success rates derived from historical experiences into shapedrewards. Our approach utilizes success rates sampled from Beta distributions,which dynamically evolve from uncertain to reliable values as more data iscollected. Initially, the self-adaptive success rates exhibit more randomnessto encourage exploration. Over time, they become more certain to enhanceexploitation, thus achieving a better balance between exploration andexploitation. We employ Kernel Density Estimation (KDE) combined with RandomFourier Features (RFF) to derive the Beta distributions, resulting in acomputationally efficient implementation in high-dimensional continuous statespaces. This method provides a non-parametric and learning-free approach. Theproposed method is evaluated on a wide range of continuous control tasks withsparse and delayed rewards, demonstrating significant improvements in sampleefficiency and convergence stability compared to several baselines.</description><author>Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, Tze-Yun Leong</author><pubDate>Tue, 06 Aug 2024 08:22:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2408.03029v1</guid></item><item><title>Addressing Maximization Bias in Reinforcement Learning with Two-Sample Testing</title><link>http://arxiv.org/abs/2201.08078v4</link><description>Value-based reinforcement-learning algorithms have shown strong results ingames, robotics, and other real-world applications. Overestimation bias is aknown threat to those algorithms and can sometimes lead to dramatic performancedecreases or even complete algorithmic failure. We frame the bias problemstatistically and consider it an instance of estimating the maximum expectedvalue (MEV) of a set of random variables. We propose the $T$-Estimator (TE)based on two-sample testing for the mean, that flexibly interpolates betweenover- and underestimation by adjusting the significance level of the underlyinghypothesis tests. We also introduce a generalization, termed $K$-Estimator(KE), that obeys the same bias and variance bounds as the TE and relies on anearly arbitrary kernel function. We introduce modifications of $Q$-Learningand the Bootstrapped Deep $Q$-Network (BDQN) using the TE and the KE, and proveconvergence in the tabular setting. Furthermore, we propose an adaptive variantof the TE-based BDQN that dynamically adjusts the significance level tominimize the absolute estimation bias. All proposed estimators and algorithmsare thoroughly tested and validated on diverse tasks and environments,illustrating the bias control and performance potential of the TE and KE.</description><author>Martin Waltz, Ostap Okhrin</author><pubDate>Mon, 12 Aug 2024 08:14:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2201.08078v4</guid></item><item><title>Demonstration Selection for In-Context Learning via Reinforcement Learning</title><link>http://arxiv.org/abs/2412.03966v1</link><description>Diversity in demonstration selection is crucial for enhancing modelgeneralization, as it enables a broader coverage of structures and concepts.However, constructing an appropriate set of demonstrations has remained a focalpoint of research. This paper presents the Relevance-Diversity EnhancedSelection (RDES), an innovative approach that leverages reinforcement learningto optimize the selection of diverse reference demonstrations for textclassification tasks using Large Language Models (LLMs), especially in few-shotprompting scenarios. RDES employs a Q-learning framework to dynamicallyidentify demonstrations that maximize both diversity and relevance to theclassification objective by calculating a diversity score based on labeldistribution among selected demonstrations. This method ensures a balancedrepresentation of reference data, leading to improved classification accuracy.Through extensive experiments on four benchmark datasets and involving 12closed-source and open-source LLMs, we demonstrate that RDES significantlyenhances classification accuracy compared to ten established baselines.Furthermore, we investigate the incorporation of Chain-of-Thought (CoT)reasoning in the reasoning process, which further enhances the model'spredictive performance. The results underscore the potential of reinforcementlearning to facilitate adaptive demonstration selection and deepen theunderstanding of classification challenges.</description><author>Xubin Wang, Jianfei Wu, Yichen Yuan, Mingzhe Li, Deyu Cai, Weijia Jia</author><pubDate>Thu, 05 Dec 2024 08:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.03966v1</guid></item><item><title>Structural Design Through Reinforcement Learning</title><link>http://arxiv.org/abs/2407.07288v2</link><description>This paper introduces the Structural Optimization gym (SOgym), a novelopen-source Reinforcement Learning (RL) environment designed to advance machinelearning in Topology Optimization (TO). SOgym enables RL agents to generatephysically viable and structurally robust designs by integrating the physics ofTO into the reward function. To enhance scalability, SOgym leveragesfeature-mapping methods as a mesh-independent interface between the environmentand the agent, allowing efficient interaction with the design variablesregardless of mesh resolution. Baseline results use a model-free ProximalPolicy Optimization agent and a model-based DreamerV3 agent. Three observationspace configurations were tested. The TopOpt game-inspired configuration, aninteractive educational tool that improves students' intuition in designingstructures to minimize compliance under volume constraints, performed best interms of performance and sample efficiency. The 100M parameter version ofDreamerV3 produced structures within 54% of the baseline compliance achieved bytraditional optimization methods and a 0% disconnection rate, an improvementover supervised learning approaches that often struggle with disconnected loadpaths. When comparing the learning rates of the agents to those of engineeringstudents from the TopOpt game experiment, the DreamerV3-100M model shows alearning rate approximately four orders of magnitude lower, an impressive featfor a policy trained from scratch through trial and error. These resultssuggest RL's potential to solve continuous TO problems and its capacity toexplore and learn from diverse design solutions. SOgym provides a platform fordeveloping RL agents for complex structural design challenges and is publiclyavailable to support further research in the field.</description><author>Thomas Rochefort-Beaudoin, Aurelian Vadean, Niels Aage, Sofiane Achiche</author><pubDate>Fri, 12 Jul 2024 14:31:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07288v2</guid></item><item><title>GNN with Model-based RL for Multi-agent Systems</title><link>http://arxiv.org/abs/2407.09249v1</link><description>Multi-agent systems (MAS) constitute a significant role in exploring machineintelligence and advanced applications. In order to deeply investigatecomplicated interactions within MAS scenarios, we originally propose "GNN forMBRL" model, which utilizes a state-spaced Graph Neural Networks withModel-based Reinforcement Learning to address specific MAS missions (e.g.,Billiard-Avoidance, Autonomous Driving Cars). In detail, we firstly used GNNmodel to predict future states and trajectories of multiple agents, thenapplied the Cross-Entropy Method (CEM) optimized Model Predictive Control toassist the ego-agent planning actions and successfully accomplish certain MAStasks.</description><author>Hanxiao Chen</author><pubDate>Fri, 12 Jul 2024 13:21:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09249v1</guid></item><item><title>Enhancing RL Safety with Counterfactual LLM Reasoning</title><link>http://arxiv.org/abs/2409.10188v1</link><description>Reinforcement learning (RL) policies may exhibit unsafe behavior and are hardto explain. We use counterfactual large language model reasoning to enhance RLpolicy safety post-training. We show that our approach improves and helps toexplain the RL policy safety.</description><author>Dennis Gross, Helge Spieker</author><pubDate>Mon, 16 Sep 2024 11:30:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.10188v1</guid></item><item><title>Can large language models explore in-context?</title><link>http://arxiv.org/abs/2403.15371v2</link><description>We investigate the extent to which contemporary Large Language Models (LLMs)can engage in exploration, a core capability in reinforcement learning anddecision making. We focus on native performance of existing LLMs, withouttraining interventions. We deploy LLMs as agents in simple multi-armed banditenvironments, specifying the environment description and interaction historyentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,GPT-4, and Llama2, using a variety of prompt designs, and find that the modelsdo not robustly engage in exploration without substantial interventions: i)Across all of our experiments, only one configuration resulted in satisfactoryexploratory behavior: GPT-4 with chain-of-thought reasoning and an externallysummarized interaction history, presented as sufficient statistics; ii) Allother configurations did not result in robust exploratory behavior, includingthose with chain-of-thought reasoning but unsummarized history. Although thesefindings can be interpreted positively, they suggest that externalsummarization -- which may not be possible in more complex settings -- isimportant for obtaining desirable behavior from LLM agents. We conclude thatnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,may be required to empower LLM-based decision making agents in complexsettings.</description><author>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</author><pubDate>Fri, 12 Jul 2024 14:52:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15371v2</guid></item><item><title>Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning</title><link>http://arxiv.org/abs/2406.08002v2</link><description>Despite the recent successes of multi-agent reinforcement learning (MARL)algorithms, efficiently adapting to co-players in mixed-motive environmentsremains a significant challenge. One feasible approach is to hierarchicallymodel co-players' behavior based on inferring their characteristics. However,these methods often encounter difficulties in efficient reasoning andutilization of inferred information. To address these issues, we proposeHierarchical Opponent modeling and Planning (HOP), a novel multi-agentdecision-making algorithm that enables few-shot adaptation to unseen policiesin mixed-motive environments. HOP is hierarchically composed of two modules: anopponent modeling module that infers others' goals and learns correspondinggoal-conditioned policies, and a planning module that employs Monte Carlo TreeSearch (MCTS) to identify the best response. Our approach improves efficiencyby updating beliefs about others' goals both across and within episodes and byusing information from the opponent modeling module to guide planning.Experimental results demonstrate that in mixed-motive environments, HOPexhibits superior few-shot adaptation capabilities when interacting withvarious unseen agents, and excels in self-play scenarios. Furthermore, theemergence of social intelligence during our experiments underscores thepotential of our approach in complex multi-agent environments.</description><author>Yizhe Huang, Anji Liu, Fanqi Kong, Yaodong Yang, Song-Chun Zhu, Xue Feng</author><pubDate>Fri, 12 Jul 2024 15:13:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.08002v2</guid></item><item><title>Mitigating Partial Observability in Sequential Decision Processes via the Lambda Discrepancy</title><link>http://arxiv.org/abs/2407.07333v1</link><description>Reinforcement learning algorithms typically rely on the assumption that theenvironment dynamics and value function can be expressed in terms of aMarkovian state representation. However, when state information is onlypartially observable, how can an agent learn such a state representation, andhow can it detect when it has found one? We introduce a metric that canaccomplish both objectives, without requiring access to--or knowledge of--anunderlying, unobservable state space. Our metric, the $\lambda$-discrepancy, isthe difference between two distinct temporal difference (TD) value estimates,each computed using TD($\lambda$) with a different value of $\lambda$. SinceTD($\lambda$=0) makes an implicit Markov assumption and TD($\lambda$=1) doesnot, a discrepancy between these estimates is a potential indicator of anon-Markovian state representation. Indeed, we prove that the$\lambda$-discrepancy is exactly zero for all Markov decision processes andalmost always non-zero for a broad class of partially observable environments.We also demonstrate empirically that, once detected, minimizing the$\lambda$-discrepancy can help with learning a memory function to mitigate thecorresponding partial observability. We then train a reinforcement learningagent that simultaneously constructs two recurrent value networks withdifferent $\lambda$ parameters and minimizes the difference between them as anauxiliary loss. The approach scales to challenging partially observabledomains, where the resulting agent frequently performs significantly better(and never performs worse) than a baseline recurrent agent with only a singlevalue network.</description><author>Cameron Allen, Aaron Kirtland, Ruo Yu Tao, Sam Lobel, Daniel Scott, Nicholas Petrocelli, Omer Gottesman, Ronald Parr, Michael L. Littman, George Konidaris</author><pubDate>Wed, 10 Jul 2024 03:04:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07333v1</guid></item><item><title>Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2407.07631v1</link><description>We study risk-sensitive reinforcement learning (RL), a crucial field due toits ability to enhance decision-making in scenarios where it is essential tomanage uncertainty and minimize potential adverse outcomes. Particularly, ourwork focuses on applying the entropic risk measure to RL problems. Whileexisting literature primarily investigates the online setting, there remains alarge gap in understanding how to efficiently derive a near-optimal policybased on this risk measure using only a pre-collected dataset. We center on thelinear Markov Decision Process (MDP) setting, a well-regarded theoreticalframework that has yet to be examined from a risk-sensitive standpoint. Inresponse, we introduce two provably sample-efficient algorithms. We begin bypresenting a risk-sensitive pessimistic value iteration algorithm, offering atight analysis by leveraging the structure of the risk-sensitive performancemeasure. To further improve the obtained bounds, we propose another pessimisticalgorithm that utilizes variance information and reference-advantagedecomposition, effectively improving both the dependence on the space dimension$d$ and the risk-sensitivity factor. To the best of our knowledge, we obtainthe first provably efficient risk-sensitive offline RL algorithms.</description><author>Dake Zhang, Boxiang Lyu, Shuang Qiu, Mladen Kolar, Tong Zhang</author><pubDate>Wed, 10 Jul 2024 13:09:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07631v1</guid></item><item><title>Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control</title><link>http://arxiv.org/abs/2302.11694v4</link><description>Simultaneously accurate and reliable tracking control for quadrotors incomplex dynamic environments is challenging. As aerodynamics derived from dragforces and moment variations are chaotic and difficult to precisely identify,most current quadrotor tracking systems treat them as simple `disturbances' inconventional control approaches. We propose a novel, interpretable trajectorytracker integrating a Distributional Reinforcement Learning disturbanceestimator for unknown aerodynamic effects with a Stochastic Model PredictiveController (SMPC). The proposed estimator `Constrained DistributionalReinforced disturbance estimator' (ConsDRED) accurately identifiesuncertainties between true and estimated values of aerodynamic effects.Simplified Affine Disturbance Feedback is used for control parameterization toguarantee convexity, which we then integrate with a SMPC. We theoreticallyguarantee that ConsDRED achieves at least an optimal global convergence rateand a certain sublinear rate if constraints are violated with an errordecreases as the width and the layer of neural network increase. To demonstratepracticality, we show convergent training in simulation and real-worldexperiments, and empirically verify that ConsDRED is less sensitive tohyperparameter settings compared with canonical constrained RL approaches. Wedemonstrate our system improves accumulative tracking errors by at least 70%compared with the recent art. Importantly, the proposed framework,ConsDRED-SMPC, balances the tradeoff between pursuing high performance andobeying conservative constraints for practical implementations.</description><author>Yanran Wang, David Boyle</author><pubDate>Mon, 15 Jul 2024 13:15:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2302.11694v4</guid></item><item><title>Last-Iterate Global Convergence of Policy Gradients for Constrained Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10775v1</link><description>Constrained Reinforcement Learning (CRL) tackles sequential decision-makingproblems where agents are required to achieve goals by maximizing the expectedreturn while meeting domain-specific constraints, which are often formulated asexpected costs. In this setting, policy-based methods are widely used sincethey come with several advantages when dealing with continuous-controlproblems. These methods search in the policy space with an action-based orparameter-based exploration strategy, depending on whether they learn directlythe parameters of a stochastic policy or those of a stochastic hyperpolicy. Inthis paper, we propose a general framework for addressing CRL problems viagradient-based primal-dual algorithms, relying on an alternate ascent/descentscheme with dual-variable regularization. We introduce an exploration-agnosticalgorithm, called C-PG, which exhibits global last-iterate convergenceguarantees under (weak) gradient domination assumptions, improving andgeneralizing existing results. Then, we design C-PGAE and C-PGPE, theaction-based and the parameter-based versions of C-PG, respectively, and weillustrate how they naturally extend to constraints defined in terms of riskmeasures over the costs, as it is often requested in safety-critical scenarios.Finally, we numerically validate our algorithms on constrained controlproblems, and compare them with state-of-the-art baselines, demonstrating theireffectiveness.</description><author>Alessandro Montenegro, Marco Mussi, Matteo Papini, Alberto Maria Metelli</author><pubDate>Mon, 15 Jul 2024 14:54:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10775v1</guid></item><item><title>A Survey on Neural Architecture Search Based on Reinforcement Learning</title><link>http://arxiv.org/abs/2409.18163v2</link><description>The automation of feature extraction of machine learning has beensuccessfully realized by the explosive development of deep learning. However,the structures and hyperparameters of deep neural network architectures alsomake huge difference on the performance in different tasks. The process ofexploring optimal structures and hyperparameters often involves a lot oftedious human intervene. As a result, a legitimate question is to ask for theautomation of searching for optimal network structures and hyperparameters. Thework of automation of exploring optimal hyperparameters is done byHyperparameter Optimization. Neural Architecture Search is aimed toautomatically find the best network structure given specific tasks. In thispaper, we firstly introduced the overall development of Neural ArchitectureSearch and then focus mainly on providing an overall and understandable surveyabout Neural Architecture Search works that are relevant with reinforcementlearning, including improvements and variants based on the hope of satisfyingmore complex structures and resource-insufficient environment.</description><author>Wenzhu Shao</author><pubDate>Mon, 30 Sep 2024 06:51:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2409.18163v2</guid></item><item><title>Balancing the Scales: Reinforcement Learning for Fair Classification</title><link>http://arxiv.org/abs/2407.10629v1</link><description>Fairness in classification tasks has traditionally focused on bias removalfrom neural representations, but recent trends favor algorithmic methods thatembed fairness into the training process. These methods steer models towardsfair performance, preventing potential elimination of valuable information thatarises from representation manipulation. Reinforcement Learning (RL), with itscapacity for learning through interaction and adjusting reward functions toencourage desired behaviors, emerges as a promising tool in this domain. Inthis paper, we explore the usage of RL to address bias in imbalancedclassification by scaling the reward function to mitigate bias. We employ thecontextual multi-armed bandit framework and adapt three popular RL algorithmsto suit our objectives, demonstrating a novel approach to mitigating bias.</description><author>Leon Eshuijs, Shihan Wang, Antske Fokkens</author><pubDate>Mon, 15 Jul 2024 11:28:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10629v1</guid></item><item><title>Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena</title><link>http://arxiv.org/abs/2407.10627v1</link><description>Assessing the effectiveness of large language models (LLMs) presentssubstantial challenges. The method of conducting human-annotated battles in anonline Chatbot Arena is a highly effective evaluative technique. However, thisapproach is limited by the costs and time required for human annotation. Inthis paper, we introduce Arena Learning, an innovative offline strategydesigned to simulate these arena battles using AI-driven annotations toevaluate battle outcomes, thus facilitating the continuous improvement of thetarget model through both supervised fine-tuning and reinforcement learning.Arena Learning comprises two key elements. First, it ensures preciseevaluations and maintains consistency between offline simulations and onlinecompetitions via WizardArena, a pipeline developed to accurately predict theElo rankings of various models using a meticulously designed offline test set.Our results demonstrate that WizardArena's predictions closely align with thosefrom the online Arena. Second, it involves the continuous improvement oftraining data based on the battle results and the refined model. We establish adata flywheel to iteratively update the training data by highlighting theweaknesses of the target model based on its battle results, enabling it tolearn from the strengths of multiple different models. We apply Arena Learningto train our target model, WizardLM-$\beta$, and demonstrate significantperformance enhancements across various metrics. This fully automated trainingand evaluation pipeline sets the stage for continuous advancements in variousLLMs via post-training. Notably, Arena Learning plays a pivotal role in thesuccess of WizardLM-2, and this paper serves both as an exploration of itsefficacy and a foundational study for future discussions related to WizardLM-2and its derivatives.</description><author>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, Weizhu Chen</author><pubDate>Mon, 15 Jul 2024 11:26:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10627v1</guid></item><item><title>Three Dogmas of Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10583v1</link><description>Modern reinforcement learning has been conditioned by at least three dogmas.The first is the environment spotlight, which refers to our tendency to focuson modeling environments rather than agents. The second is our treatment oflearning as finding the solution to a task, rather than adaptation. The thirdis the reward hypothesis, which states that all goals and purposes can be wellthought of as maximization of a reward signal. These three dogmas shape much ofwhat we think of as the science of reinforcement learning. While each of thedogmas have played an important role in developing the field, it is time webring them to the surface and reflect on whether they belong as basicingredients of our scientific paradigm. In order to realize the potential ofreinforcement learning as a canonical frame for researching intelligent agents,we suggest that it is time we shed dogmas one and two entirely, and embrace anuanced approach to the third.</description><author>David Abel, Mark K. Ho, Anna Harutyunyan</author><pubDate>Mon, 15 Jul 2024 10:03:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10583v1</guid></item><item><title>Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2405.19909v3</link><description>In offline reinforcement learning, the challenge of out-of-distribution (OOD)is pronounced. To address this, existing methods often constrain the learnedpolicy through policy regularization. However, these methods often suffer fromthe issue of unnecessary conservativeness, hampering policy improvement. Thisoccurs due to the indiscriminate use of all actions from the behavior policythat generates the offline dataset as constraints. The problem becomesparticularly noticeable when the quality of the dataset is suboptimal. Thus, wepropose Adaptive Advantage-guided Policy Regularization (A2PR), obtaininghigh-advantage actions from an augmented behavior policy combined with VAE toguide the learned policy. A2PR can select high-advantage actions that differfrom those present in the dataset, while still effectively maintainingconservatism from OOD actions. This is achieved by harnessing the VAE capacityto generate samples matching the distribution of the data points. Wetheoretically prove that the improvement of the behavior policy is guaranteed.Besides, it effectively mitigates value overestimation with a boundedperformance gap. Empirically, we conduct a series of experiments on the D4RLbenchmark, where A2PR demonstrates state-of-the-art performance. Furthermore,experimental results on additional suboptimal mixed datasets reveal that A2PRexhibits superior performance. Code is available athttps://github.com/ltlhuuu/A2PR.</description><author>Tenglong Liu, Yang Li, Yixing Lan, Hao Gao, Wei Pan, Xin Xu</author><pubDate>Mon, 15 Jul 2024 10:55:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2405.19909v3</guid></item><item><title>Context-aware Communication for Multi-agent Reinforcement Learning</title><link>http://arxiv.org/abs/2312.15600v3</link><description>Effective communication protocols in multi-agent reinforcement learning(MARL) are critical to fostering cooperation and enhancing team performance. Toleverage communication, many previous works have proposed to compress localinformation into a single message and broadcast it to all reachable agents.This simplistic messaging mechanism, however, may fail to provide adequate,critical, and relevant information to individual agents, especially in severelybandwidth-limited scenarios. This motivates us to develop context-awarecommunication schemes for MARL, aiming to deliver personalized messages todifferent agents. Our communication protocol, named CACOM, consists of twostages. In the first stage, agents exchange coarse representations in abroadcast fashion, providing context for the second stage. Following this,agents utilize attention mechanisms in the second stage to selectively generatemessages personalized for the receivers. Furthermore, we employ the learnedstep size quantization (LSQ) technique for message quantization to reduce thecommunication overhead. To evaluate the effectiveness of CACOM, we integrate itwith both actor-critic and value-based MARL algorithms. Empirical results oncooperative benchmark tasks demonstrate that CACOM provides evident performancegains over baselines under communication-constrained scenarios. The code ispublicly available at https://github.com/LXXXXR/CACOM.</description><author>Xinran Li, Jun Zhang</author><pubDate>Mon, 15 Jul 2024 08:02:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15600v3</guid></item><item><title>G-PCGRL: Procedural Graph Data Generation via Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10483v1</link><description>Graph data structures offer a versatile and powerful means to modelrelationships and interconnections in various domains, promising substantialadvantages in data representation, analysis, and visualization. In games,graph-based data structures are omnipresent and represent, for example, gameeconomies, skill trees or complex, branching quest lines. With this paper, wepropose G-PCGRL, a novel and controllable method for the procedural generationof graph data using reinforcement learning. Therefore, we frame this problem asmanipulating a graph's adjacency matrix to fulfill a given set of constraints.Our method adapts and extends the Procedural Content Generation viaReinforcement Learning (PCGRL) framework and introduces new representations toframe the problem of graph data generation as a Markov decision process. Wecompare the performance of our method with the original PCGRL, the run timewith a random search and evolutionary algorithm, and evaluate G-PCGRL on twograph data domains in games: game economies and skill trees. The results showthat our method is capable of generating graph-based content quickly andreliably to support and inspire designers in the game creation process. Inaddition, trained models are controllable in terms of the type and number ofnodes to be generated.</description><author>Florian Rupp, Kai Eckert</author><pubDate>Mon, 15 Jul 2024 07:11:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10483v1</guid></item><item><title>SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation</title><link>http://arxiv.org/abs/2407.10481v1</link><description>Physically-simulated models for human motion can generate high-qualityresponsive character animations, often in real-time. Natural language serves asa flexible interface for controlling these models, allowing expert andnon-expert users to quickly create and edit their animations. Many recentphysics-based animation methods, including those that use text interfaces,train control policies using reinforcement learning (RL). However, scalingthese methods beyond several hundred motions has remained challenging.Meanwhile, kinematic animation models are able to successfully learn fromthousands of diverse motions by leveraging supervised learning methods.Inspired by these successes, in this work we introduce SuperPADL, a scalableframework for physics-based text-to-motion that leverages both RL andsupervised learning to train controllers on thousands of diverse motion clips.SuperPADL is trained in stages using progressive distillation, starting with alarge number of specialized experts using RL. These experts are theniteratively distilled into larger, more robust policies using a combination ofreinforcement learning and supervised learning. Our final SuperPADL controlleris trained on a dataset containing over 5000 skills and runs in real time on aconsumer GPU. Moreover, our policy can naturally transition between skills,allowing for users to interactively craft multi-stage animations. Weexperimentally demonstrate that SuperPADL significantly outperforms RL-basedbaselines at this large data scale.</description><author>Jordan Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng</author><pubDate>Mon, 15 Jul 2024 07:07:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10481v1</guid></item><item><title>A Benchmark Environment for Offline Reinforcement Learning in Racing Games</title><link>http://arxiv.org/abs/2407.09415v1</link><description>Offline Reinforcement Learning (ORL) is a promising approach to reduce thehigh sample complexity of traditional Reinforcement Learning (RL) byeliminating the need for continuous environmental interactions. ORL exploits adataset of pre-collected transitions and thus expands the range of applicationof RL to tasks in which the excessive environment queries increase trainingtime and decrease efficiency, such as in modern AAA games. This paperintroduces OfflineMania a novel environment for ORL research. It is inspired bythe iconic TrackMania series and developed using the Unity 3D game engine. Theenvironment simulates a single-agent racing game in which the objective is tocomplete the track through optimal navigation. We provide a variety of datasetsto assess ORL performance. These datasets, created from policies of varyingability and in different sizes, aim to offer a challenging testbed foralgorithm development and evaluation. We further establish a set of baselinesfor a range of Online RL, ORL, and hybrid Offline to Online RL approaches usingour environment.</description><author>Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov</author><pubDate>Fri, 12 Jul 2024 16:44:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.09415v1</guid></item><item><title>Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment</title><link>http://arxiv.org/abs/2407.10804v1</link><description>Adapting general large language models (LLMs) to specialized domains presentsgreat challenges due to varied data distributions. This adaptation typicallyrequires continual pre-training on massive domain-specific corpora tofacilitate knowledge memorization, followed by training to apply this knowledgefollowing human instructions and preferences. However, this method may resultin inefficient knowledge memorization due to a lack of awareness of knowledgeutilization and imposes substantial demands on LLMs to simultaneously learnknowledge utilization and format alignment with limited training samples. Tofacilitate the domain adaptation of LLM, we revise this process and propose anew domain adaptation framework including domain knowledge learning and generalformat alignment, called Mix-CPT. Specifically, we first conduct a knowledgemixture continual pre-training that concurrently focuses on knowledgememorization and utilization, allowing for mutual reinforcement. To avoidcatastrophic forgetting during the continual pre-training process, we furtherincorporate a logit swap self-distillation constraint. Subsequently, leveragingthe knowledge and capabilities acquired during continual pre-training, weefficiently perform instruction tuning and alignment with a few generaltraining samples to achieve format alignment. Extensive experiments demonstratethat our proposed Mix-CPT framework can simultaneously improve the task-solvingcapabilities of LLMs on the target and general domains compared to thetraditional adaptation methods.</description><author>Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen</author><pubDate>Mon, 15 Jul 2024 15:20:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10804v1</guid></item><item><title>GuideLight: "Industrial Solution" Guidance for More Practical Traffic Signal Control Agents</title><link>http://arxiv.org/abs/2407.10811v1</link><description>Currently, traffic signal control (TSC) methods based on reinforcementlearning (RL) have proven superior to traditional methods. However, most RLmethods face difficulties when applied in the real world due to three factors:input, output, and the cycle-flow relation. The industry's observable input ismuch more limited than simulation-based RL methods. For real-world solutions,only flow can be reliably collected, whereas common RL methods need more. Forthe output action, most RL methods focus on acyclic control, which real-worldsignal controllers do not support. Most importantly, industry standards requirea consistent cycle-flow relationship: non-decreasing and different responsestrategies for low, medium, and high-level flows, which is ignored by the RLmethods. To narrow the gap between RL methods and industry standards, weinnovatively propose to use industry solutions to guide the RL agent.Specifically, we design behavior cloning and curriculum learning to guide theagent to mimic and meet industry requirements and, at the same time, leveragethe power of exploration and exploitation in RL for better performance. Wetheoretically prove that such guidance can largely decrease the samplecomplexity to polynomials in the horizon when searching for an optimal policy.Our rigid experiments show that our method has good cycle-flow relation andsuperior performance.</description><author>Haoyuan Jiang, Xuantang Xiong, Ziyue Li, Hangyu Mao, Guanghu Sui, Jingqing Ruan, Yuheng Cheng, Hua Wei, Wolfgang Ketter, Rui Zhao</author><pubDate>Mon, 15 Jul 2024 15:26:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10811v1</guid></item><item><title>Joint Optimization of Age of Information and Energy Consumption in NR-V2X System based on Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2407.08458v1</link><description>Autonomous driving may be the most important application scenario of nextgeneration, the development of wireless access technologies enabling reliableand low-latency vehicle communication becomes crucial. To address this, 3GPPhas developed Vehicle-to-Everything (V2X) specifications based on 5G New Radio(NR) technology, where Mode 2 Side-Link (SL) communication resembles Mode 4 inLTE-V2X, allowing direct communication between vehicles. This supplements SLcommunication in LTE-V2X and represents the latest advancement in cellular V2X(C-V2X) with improved performance of NR-V2X. However, in NR-V2X Mode 2,resource collisions still occur, and thus degrade the age of information (AOI).Therefore, a interference cancellation method is employed to mitigate thisimpact by combining NR-V2X with Non-Orthogonal multiple access (NOMA)technology. In NR-V2X, when vehicles select smaller resource reservationinterval (RRI), higher-frequency transmissions take ore energy to reduce AoI.Hence, it is important to jointly consider AoI and communication energyconsumption based on NR-V2X communication. Then, we formulate such anoptimization problem and employ the Deep Reinforcement Learning (DRL) algorithmto compute the optimal transmission RRI and transmission power for eachtransmitting vehicle to reduce the energy consumption of each transmittingvehicle and the AoI of each receiving vehicle. Extensive simulations havedemonstrated the performance of our proposed algorithm.</description><author>Shulin Song, Zheng Zhang, Qiong Wu, Qiang Fan, Pingyi Fan</author><pubDate>Thu, 11 Jul 2024 12:54:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08458v1</guid></item><item><title>Real-time system optimal traffic routing under uncertainties -- Can physics models boost reinforcement learning?</title><link>http://arxiv.org/abs/2407.07364v1</link><description>System optimal traffic routing can mitigate congestion by assigning routesfor a portion of vehicles so that the total travel time of all vehicles in thetransportation system can be reduced. However, achieving real-time optimalrouting poses challenges due to uncertain demands and unknown system dynamics,particularly in expansive transportation networks. While physics model-basedmethods are sensitive to uncertainties and model mismatches, model-freereinforcement learning struggles with learning inefficiencies andinterpretability issues. Our paper presents TransRL, a novel algorithm thatintegrates reinforcement learning with physics models for enhanced performance,reliability, and interpretability. TransRL begins by establishing adeterministic policy grounded in physics models, from which it learns from andis guided by a differentiable and stochastic teacher policy. During training,TransRL aims to maximize cumulative rewards while minimizing the KullbackLeibler (KL) divergence between the current policy and the teacher policy. Thisapproach enables TransRL to simultaneously leverage interactions with theenvironment and insights from physics models. We conduct experiments on threetransportation networks with up to hundreds of links. The results demonstrateTransRL's superiority over traffic model-based methods for being adaptive andlearning from the actual network data. By leveraging the information fromphysics models, TransRL consistently outperforms state-of-the-art reinforcementlearning algorithms such as proximal policy optimization (PPO) and soft actorcritic (SAC). Moreover, TransRL's actions exhibit higher reliability andinterpretability compared to baseline reinforcement learning approaches likePPO and SAC.</description><author>Zemian Ke, Qiling Zou, Jiachao Liu, Sean Qian</author><pubDate>Wed, 10 Jul 2024 04:53:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07364v1</guid></item><item><title>Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark</title><link>http://arxiv.org/abs/2402.13654v2</link><description>This paper presents a learning-based control strategy for non-linear throttlevalves with an asymmetric hysteresis, leading to a near-optimal controllerwithout requiring any prior knowledge about the environment. We start with acarefully tuned Proportional Integrator (PI) controller and exploit the recentadvances in Reinforcement Learning (RL) with Guides to improve the closed-loopbehavior by learning from the additional interactions with the valve. We testthe proposed control method in various scenarios on three different valves, allhighlighting the benefits of combining both PI and RL frameworks to improvecontrol performance in non-linear stochastic systems. In all the experimentaltest cases, the resulting agent has a better sample efficiency than traditionalRL agents and outperforms the PI controller.</description><author>Paul Daoudi, Bojan Mavkov, Bogdan Robu, Christophe Prieur, Emmanuel Witrant, Merwan Barlier, Ludovic Dos Santos</author><pubDate>Mon, 15 Jul 2024 15:27:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13654v2</guid></item><item><title>A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning</title><link>http://arxiv.org/abs/2312.15474v3</link><description>Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from asource environment to a target environment characterized by distinct yetsimilar dynamics. In this context, traditional RL agents depend excessively onthe dynamics of the source environment, resulting in the discovery of policiesthat excel in this environment but fail to provide reasonable performance inthe target one. In the few-shot framework, a limited number of transitions fromthe target environment are introduced to facilitate a more effective transfer.Addressing this challenge, we propose an innovative approach inspired by recentadvancements in Imitation Learning and conservative RL algorithms. The proposedmethod introduces a penalty to regulate the trajectories generated by thesource-trained policy. We evaluate our method across various environmentsrepresenting diverse off-dynamics conditions, where access to the targetenvironment is extremely limited. These experiments include high-dimensionalsystems relevant to real-world applications. Across most tested scenarios, ourproposed method demonstrates performance improvements compared to existingbaselines.</description><author>Paul Daoudi, Christophe Prieur, Bogdan Robu, Merwan Barlier, Ludovic Dos Santos</author><pubDate>Mon, 15 Jul 2024 15:36:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15474v3</guid></item><item><title>Distributed Deep Reinforcement Learning Based Gradient Quantization for Federated Learning Enabled Vehicle Edge Computing</title><link>http://arxiv.org/abs/2407.08462v1</link><description>Federated Learning (FL) can protect the privacy of the vehicles in vehicleedge computing (VEC) to a certain extent through sharing the gradients ofvehicles' local models instead of local data. The gradients of vehicles' localmodels are usually large for the vehicular artificial intelligence (AI)applications, thus transmitting such large gradients would cause largeper-round latency. Gradient quantization has been proposed as one effectiveapproach to reduce the per-round latency in FL enabled VEC through compressinggradients and reducing the number of bits, i.e., the quantization level, totransmit gradients. The selection of quantization level and thresholdsdetermines the quantization error, which further affects the model accuracy andtraining time. To do so, the total training time and quantization error (QE)become two key metrics for the FL enabled VEC. It is critical to jointlyoptimize the total training time and QE for the FL enabled VEC. However, thetime-varying channel condition causes more challenges to solve this problem. Inthis paper, we propose a distributed deep reinforcement learning (DRL)-basedquantization level allocation scheme to optimize the long-term reward in termsof the total training time and QE. Extensive simulations identify the optimalweighted factors between the total training time and QE, and demonstrate thefeasibility and effectiveness of the proposed scheme.</description><author>Cui Zhang, Wenjun Zhang, Qiong Wu, Pingyi Fan, Qiang Fan, Jiangzhou Wang, Khaled B. Letaief</author><pubDate>Thu, 11 Jul 2024 12:58:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08462v1</guid></item><item><title>Exploration in Knowledge Transfer Utilizing Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10835v1</link><description>The contribution focuses on the problem of exploration within the task ofknowledge transfer. Knowledge transfer refers to the useful application of theknowledge gained while learning the source task in the target task. Theintended benefit of knowledge transfer is to speed up the learning process ofthe target task. The article aims to compare several exploration methods usedwithin a deep transfer learning algorithm, particularly Deep Target Transfer$Q$-learning. The methods used are $\epsilon$-greedy, Boltzmann, and upperconfidence bound exploration. The aforementioned transfer learning algorithmsand exploration methods were tested on the virtual drone problem. The resultshave shown that the upper confidence bound algorithm performs the best out ofthese options. Its sustainability to other applications is to be checked.</description><author>Adam Jedlička, Tatiana Valentine Guy</author><pubDate>Mon, 15 Jul 2024 15:45:29 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10835v1</guid></item><item><title>Offline Reinforcement Learning with Imputed Rewards</title><link>http://arxiv.org/abs/2407.10839v1</link><description>Offline Reinforcement Learning (ORL) offers a robust solution to trainingagents in applications where interactions with the environment must be strictlylimited due to cost, safety, or lack of accurate simulation environments.Despite its potential to facilitate deployment of artificial agents in the realworld, Offline Reinforcement Learning typically requires very manydemonstrations annotated with ground-truth rewards. Consequently,state-of-the-art ORL algorithms can be difficult or impossible to apply indata-scarce scenarios. In this paper we propose a simple but effective RewardModel that can estimate the reward signal from a very limited sample ofenvironment transitions annotated with rewards. Once the reward signal ismodeled, we use the Reward Model to impute rewards for a large sample ofreward-free transitions, thus enabling the application of ORL techniques. Wedemonstrate the potential of our approach on several D4RL continuous locomotiontasks. Our results show that, using only 1\% of reward-labeled transitions fromthe original datasets, our learned reward model is able to impute rewards forthe remaining 99\% of the transitions, from which performant agents can belearned using Offline Reinforcement Learning.</description><author>Carlo Romeo, Andrew D. Bagdanov</author><pubDate>Mon, 15 Jul 2024 15:53:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10839v1</guid></item><item><title>TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations</title><link>http://arxiv.org/abs/2407.08464v1</link><description>Unsupervised goal-conditioned reinforcement learning (GCRL) is a promisingparadigm for developing diverse robotic skills without external supervision.However, existing unsupervised GCRL methods often struggle to cover a widerange of states in complex environments due to their limited exploration andsparse or noisy rewards for GCRL. To overcome these challenges, we propose anovel unsupervised GCRL method that leverages TemporaL Distance-awareRepresentations (TLDR). TLDR selects faraway goals to initiate exploration andcomputes intrinsic exploration rewards and goal-reaching rewards, based ontemporal distance. Specifically, our exploration policy seeks states with largetemporal distances (i.e. covering a large state space), while thegoal-conditioned policy learns to minimize the temporal distance to the goal(i.e. reaching the goal). Our experimental results in six simulated roboticlocomotion environments demonstrate that our method significantly outperformsprevious unsupervised GCRL methods in achieving a wide variety of states.</description><author>Junik Bae, Kwanyoung Park, Youngwoon Lee</author><pubDate>Thu, 11 Jul 2024 13:01:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08464v1</guid></item><item><title>CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate Confirmation Bias</title><link>http://arxiv.org/abs/2407.07454v1</link><description>In human decision-making tasks, individuals learn through trials andprediction errors. When individuals learn the task, some are more influenced bygood outcomes, while others weigh bad outcomes more heavily. Such confirmationbias can lead to different learning effects. In this study, we propose a newalgorithm in Deep Reinforcement Learning, CM-DQN, which applies the idea ofdifferent update strategies for positive or negative prediction errors, tosimulate the human decision-making process when the task's states arecontinuous while the actions are discrete. We test in Lunar Lander environmentwith confirmatory, disconfirmatory bias and non-biased to observe the learningeffects. Moreover, we apply the confirmation model in a multi-armed banditproblem (environment in discrete states and discrete actions), which utilizesthe same idea as our proposed algorithm, as a contrast experiment toalgorithmically simulate the impact of different confirmation bias indecision-making process. In both experiments, confirmatory bias indicates abetter learning effect. Our code can be found herehttps://github.com/Patrickhshs/CM-DQN.</description><author>Jiacheng Shen, Lihan Feng</author><pubDate>Wed, 10 Jul 2024 08:16:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07454v1</guid></item><item><title>Language-Conditioned Offline RL for Multi-Robot Navigation</title><link>http://arxiv.org/abs/2407.20164v1</link><description>We present a method for developing navigation policies for multi-robot teamsthat interpret and follow natural language instructions. We condition thesepolicies on embeddings from pretrained Large Language Models (LLMs), and trainthem via offline reinforcement learning with as little as 20 minutes ofrandomly-collected data. Experiments on a team of five real robots show thatthese policies generalize well to unseen commands, indicating an understandingof the LLM latent space. Our method requires no simulators or environmentmodels, and produces low-latency control policies that can be deployed directlyto real robots without finetuning. We provide videos of our experiments athttps://sites.google.com/view/llm-marl.</description><author>Steven Morad, Ajay Shankar, Jan Blumenkamp, Amanda Prorok</author><pubDate>Mon, 29 Jul 2024 16:49:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.20164v1</guid></item><item><title>Reinforcement Learning of Adaptive Acquisition Policies for Inverse Problems</title><link>http://arxiv.org/abs/2407.07794v1</link><description>A promising way to mitigate the expensive process of obtaining ahigh-dimensional signal is to acquire a limited number of low-dimensionalmeasurements and solve an under-determined inverse problem by utilizing thestructural prior about the signal. In this paper, we focus on adaptiveacquisition schemes to save further the number of measurements. To this end, wepropose a reinforcement learning-based approach that sequentially collectsmeasurements to better recover the underlying signal by acquiring fewermeasurements. Our approach applies to general inverse problems with continuousaction spaces and jointly learns the recovery algorithm. Using insightsobtained from theoretical analysis, we also provide a probabilistic design forour methods using variational formulation. We evaluate our approach on multipledatasets and with two measurement spaces (Gaussian, Radon). Our results confirmthe benefits of adaptive strategies in low-acquisition horizon settings.</description><author>Gianluigi Silvestri, Fabio Valerio Massoli, Tribhuvanesh Orekondy, Afshin Abdi, Arash Behboodi</author><pubDate>Wed, 10 Jul 2024 16:12:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.07794v1</guid></item><item><title>Dissecting Deep RL with High Update Ratios: Combatting Value Divergence</title><link>http://arxiv.org/abs/2403.05996v2</link><description>We show that deep reinforcement learning algorithms can retain their abilityto learn without resetting network parameters in settings where the number ofgradient updates greatly exceeds the number of environment samples bycombatting value function divergence. Under large update-to-data ratios, arecent study by Nikishin et al. (2022) suggested the emergence of a primacybias, in which agents overfit early interactions and downplay later experience,impairing their ability to learn. In this work, we investigate the phenomenaleading to the primacy bias. We inspect the early stages of training that wereconjectured to cause the failure to learn and find that one fundamentalchallenge is a long-standing acquaintance: value function divergence.Overinflated Q-values are found not only on out-of-distribution but alsoin-distribution data and can be linked to overestimation on unseen actionprediction propelled by optimizer momentum. We employ a simple unit-ballnormalization that enables learning under large update ratios, show itsefficacy on the widely used dm_control suite, and obtain strong performance onthe challenging dog tasks, competitive with model-based approaches. Our resultsquestion, in parts, the prior explanation for sub-optimal learning due tooverfitting early data.</description><author>Marcel Hussing, Claas Voelcker, Igor Gilitschenski, Amir-massoud Farahmand, Eric Eaton</author><pubDate>Mon, 15 Jul 2024 17:08:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05996v2</guid></item><item><title>Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation</title><link>http://arxiv.org/abs/2401.05675v2</link><description>Recent works have demonstrated that using reinforcement learning (RL) withmultiple quality rewards can improve the quality of generated images intext-to-image (T2I) generation. However, manually adjusting reward weightsposes challenges and may cause over-optimization in certain metrics. To solvethis, we propose Parrot, which addresses the issue through multi-objectiveoptimization and introduces an effective multi-reward optimization strategy toapproximate Pareto optimal. Utilizing batch-wise Pareto optimal selection,Parrot automatically identifies the optimal trade-off among different rewards.We use the novel multi-reward optimization algorithm to jointly optimize theT2I model and a prompt expansion network, resulting in significant improvementof image quality and also allow to control the trade-off of different rewardsusing a reward related prompt during inference. Furthermore, we introduceoriginal prompt-centered guidance at inference time, ensuring fidelity to userinput after prompt expansion. Extensive experiments and a user study validatethe superiority of Parrot over several baselines across various qualitycriteria, including aesthetics, human preference, text-image alignment, andimage sentiment.</description><author>Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, Gang Li, Sangpil Kim, Irfan Essa, Feng Yang</author><pubDate>Mon, 15 Jul 2024 17:19:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.05675v2</guid></item><item><title>Differentiated Federated Reinforcement Learning Based Traffic Offloading on Space-Air-Ground Integrated Networks</title><link>http://arxiv.org/abs/2212.02075v3</link><description>The Space-Air-Ground Integrated Network (SAGIN) plays a pivotal role as acomprehensive foundational network communication infrastructure, presentingopportunities for highly efficient global data transmission. Nonetheless, givenSAGIN's unique characteristics as a dynamically heterogeneous network,conventional network optimization methodologies encounter challenges insatisfying the stringent requirements for network latency and stabilityinherent to data transmission within this network environment. Therefore, thispaper proposes the use of differentiated federated reinforcement learning(DFRL) to solve the traffic offloading problem in SAGIN, i.e., using multipleagents to generate differentiated traffic offloading policies. Considering thedifferentiated characteristics of each region of SAGIN, DFRL models the trafficoffloading policy optimization process as the process of solving theDecentralized Partially Observable Markov Decision Process (DEC-POMDP) problem.The paper proposes a novel Differentiated Federated Soft Actor-Critic (DFSAC)algorithm to solve the problem. The DFSAC algorithm takes the network packetdelay as the joint reward value and introduces the global trend model as thejoint target action-value function of each agent to guide the update of eachagent's policy. The simulation results demonstrate that the traffic offloadingpolicy based on the DFSAC algorithm achieves better performance in terms ofnetwork throughput, packet loss rate, and packet delay compared to thetraditional federated reinforcement learning approach and other baselineapproaches.</description><author>Yeguang Qin, Yilin Yang, Fengxiao Tang, Xin Yao, Ming Zhao, Nei Kato</author><pubDate>Thu, 11 Jul 2024 14:11:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.02075v3</guid></item><item><title>BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10967v1</link><description>Offline model-based reinforcement learning (MBRL) enhances data efficiency byutilizing pre-collected datasets to learn models and policies, especially inscenarios where exploration is costly or infeasible. Nevertheless, itsperformance often suffers from the objective mismatch between model and policylearning, resulting in inferior performance despite accurate model predictions.This paper first identifies the primary source of this mismatch comes from theunderlying confounders present in offline data for MBRL. Subsequently, weintroduce \textbf{B}ilin\textbf{E}ar \textbf{CAUS}alr\textbf{E}presentation~(BECAUSE), an algorithm to capture causalrepresentation for both states and actions to reduce the influence of thedistribution shift, thus mitigating the objective mismatch problem.Comprehensive evaluations on 18 tasks that vary in data quality and environmentcontext demonstrate the superior performance of BECAUSE over existing offlineRL algorithms. We show the generalizability and robustness of BECAUSE underfewer samples or larger numbers of confounders. Additionally, we offertheoretical analysis of BECAUSE to prove its error bound and sample efficiencywhen integrating causal representation into offline MBRL.</description><author>Haohong Lin, Wenhao Ding, Jian Chen, Laixi Shi, Jiacheng Zhu, Bo Li, Ding Zhao</author><pubDate>Mon, 15 Jul 2024 17:59:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10967v1</guid></item><item><title>Walking the Values in Bayesian Inverse Reinforcement Learning</title><link>http://arxiv.org/abs/2407.10971v1</link><description>The goal of Bayesian inverse reinforcement learning (IRL) is recovering aposterior distribution over reward functions using a set of demonstrations froman expert optimizing for a reward unknown to the learner. The resultingposterior over rewards can then be used to synthesize an apprentice policy thatperforms well on the same or a similar task. A key challenge in Bayesian IRL isbridging the computational gap between the hypothesis space of possible rewardsand the likelihood, often defined in terms of Q values: vanilla Bayesian IRLneeds to solve the costly forward planning problem - going from rewards to theQ values - at every step of the algorithm, which may need to be done thousandsof times. We propose to solve this by a simple change: instead of focusing onprimarily sampling in the space of rewards, we can focus on primarily workingin the space of Q-values, since the computation required to go from Q-values toreward is radically cheaper. Furthermore, this reversion of the computationmakes it easy to compute the gradient allowing efficient sampling usingHamiltonian Monte Carlo. We propose ValueWalk - a new Markov chain Monte Carlomethod based on this insight - and illustrate its advantages on several tasks.</description><author>Ondrej Bajgar, Alessandro Abate, Konstantinos Gatsis, Michael A. Osborne</author><pubDate>Mon, 15 Jul 2024 17:59:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.10971v1</guid></item><item><title>HACMan++: Spatially-Grounded Motion Primitives for Manipulation</title><link>http://arxiv.org/abs/2407.08585v1</link><description>Although end-to-end robot learning has shown some success for robotmanipulation, the learned policies are often not sufficiently robust tovariations in object pose or geometry. To improve the policy generalization, weintroduce spatially-grounded parameterized motion primitives in our methodHACMan++. Specifically, we propose an action representation consisting of threecomponents: what primitive type (such as grasp or push) to execute, where theprimitive will be grounded (e.g. where the gripper will make contact with theworld), and how the primitive motion is executed, such as parameters specifyingthe push direction or grasp orientation. These three components define a noveldiscrete-continuous action space for reinforcement learning. Our frameworkenables robot agents to learn to chain diverse motion primitives together andselect appropriate primitive parameters to complete long-horizon manipulationtasks. By grounding the primitives on a spatial location in the environment,our method is able to effectively generalize across object shape and posevariations. Our approach significantly outperforms existing methods,particularly in complex scenarios demanding both high-level sequentialreasoning and object generalization. With zero-shot sim-to-real transfer, ourpolicy succeeds in challenging real-world manipulation tasks, withgeneralization to unseen objects. Videos can be found on the project website:https://sgmp-rss2024.github.io.</description><author>Bowen Jiang, Yilin Wu, Wenxuan Zhou, Chris Paxton, David Held</author><pubDate>Thu, 11 Jul 2024 15:10:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08585v1</guid></item><item><title>Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding</title><link>http://arxiv.org/abs/2403.07559v2</link><description>Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding(MAPF) has recently gained attention due to its efficiency and scalability.Several MARL-MAPF methods choose to use communication to enrich the informationone agent can perceive. However, existing works still struggle in structuredenvironments with high obstacle density and a high number of agents. To furtherimprove the performance of the communication-based MARL-MAPF solvers, wepropose a new method, Ensembling Prioritized Hybrid Policies (EPH). We firstpropose a selective communication block to gather richer information for betteragent coordination within multi-agent environments and train the model with a Qlearning-based algorithm. We further introduce three advanced inferencestrategies aimed at bolstering performance during the execution phase. First,we hybridize the neural policy with single-agent expert guidance for navigatingconflict-free zones. Secondly, we propose Q value-based methods for prioritizedresolution of conflicts as well as deadlock situations. Finally, we introduce arobust ensemble method that can efficiently collect the best out of multiplepossible solutions. We empirically evaluate EPH in complex multi-agentenvironments and demonstrate competitive performance against state-of-the-artneural methods for MAPF. We open-source our code athttps://github.com/ai4co/eph-mapf.</description><author>Huijie Tang, Federico Berto, Jinkyoo Park</author><pubDate>Wed, 10 Jul 2024 08:36:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.07559v2</guid></item><item><title>A Meta-Learning Approach for Multi-Objective Reinforcement Learning in Sustainable Home Environments</title><link>http://arxiv.org/abs/2407.11489v1</link><description>Effective residential appliance scheduling is crucial for sustainable living.While multi-objective reinforcement learning (MORL) has proven effective inbalancing user preferences in appliance scheduling, traditional MORL struggleswith limited data in non-stationary residential settings characterized byrenewable generation variations. Significant context shifts that can invalidatepreviously learned policies. To address these challenges, we extendstate-of-the-art MORL algorithms with the meta-learning paradigm, enablingrapid, few-shot adaptation to shifting contexts. Additionally, we employ anauto-encoder (AE)-based unsupervised method to detect environment contextchanges. We have also developed a residential energy environment to evaluateour method using real-world data from London residential settings. This studynot only assesses the application of MORL in residential appliance schedulingbut also underscores the effectiveness of meta-learning in energy management.Our top-performing method significantly surpasses the best baseline, while thetrained model saves 3.28% on electricity bills, a 2.74% increase in usercomfort, and a 5.9% improvement in expected utility. Additionally, it reducesthe sparsity of solutions by 62.44%. Remarkably, these gains were accomplishedusing 96.71% less training data and 61.1% fewer training steps.</description><author>Junlin Lu, Patrick Mannion, Karl Mason</author><pubDate>Tue, 16 Jul 2024 08:23:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11489v1</guid></item><item><title>Reasoning with Large Language Models, a Survey</title><link>http://arxiv.org/abs/2407.11511v1</link><description>Scaling up language models to billions of parameters has opened uppossibilities for in-context learning, allowing instruction tuning and few-shotlearning on tasks that the model was not specifically trained for. This hasachieved breakthrough performance on language tasks such as translation,summarization, and question-answering. Furthermore, in addition to theseassociative "System 1" tasks, recent advances in Chain-of-thought promptlearning have demonstrated strong "System 2" reasoning abilities, answering aquestion in the field of artificial general intelligence whether LLMs canreason. The field started with the question whether LLMs can solve grade schoolmath word problems. This paper reviews the rapidly expanding field ofprompt-based reasoning with LLMs. Our taxonomy identifies different ways togenerate, evaluate, and control multi-step reasoning. We provide an in-depthcoverage of core approaches and open problems, and we propose a research agendafor the near future. Finally, we highlight the relation between reasoning andprompt-based learning, and we discuss the relation between reasoning,sequential decision processes, and reinforcement learning. We find thatself-improvement, self-reflection, and some metacognitive abilities of thereasoning processes are possible through the judicious use of prompts. Trueself-improvement and self-reasoning, to go from reasoning with LLMs toreasoning by LLMs, remains future work.</description><author>Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</author><pubDate>Tue, 16 Jul 2024 08:49:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11511v1</guid></item><item><title>PID Accelerated Temporal Difference Algorithms</title><link>http://arxiv.org/abs/2407.08803v1</link><description>Long-horizon tasks, which have a large discount factor, pose a challenge formost conventional reinforcement learning (RL) algorithms. Algorithms such asValue Iteration and Temporal Difference (TD) learning have a slow convergencerate and become inefficient in these tasks. When the transition distributionsare given, PID VI was recently introduced to accelerate the convergence ofValue Iteration using ideas from control theory. Inspired by this, we introducePID TD Learning and PID Q-Learning algorithms for the RL setting in which onlysamples from the environment are available. We give theoretical analysis oftheir convergence and acceleration compared to their traditional counterparts.We also introduce a method for adapting PID gains in the presence of noise andempirically verify its effectiveness.</description><author>Mark Bedaywi, Amin Rakhsha, Amir-massoud Farahmand</author><pubDate>Thu, 11 Jul 2024 18:23:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.08803v1</guid></item><item><title>REVEAL-IT: REinforcement learning with Visibility of Evolving Agent poLicy for InTerpretability</title><link>http://arxiv.org/abs/2406.14214v4</link><description>Understanding the agent's learning process, particularly the factors thatcontribute to its success or failure post-training, is crucial forcomprehending the rationale behind the agent's decision-making process. Priormethods clarify the learning process by creating a structural causal model(SCM) or visually representing the distribution of value functions.Nevertheless, these approaches have constraints as they exclusively function in2D-environments or with uncomplicated transition dynamics. Understanding theagent's learning process in complicated environments or tasks is morechallenging. In this paper, we propose REVEAL-IT, a novel framework forexplaining the learning process of an agent in complex environments. Initially,we visualize the policy structure and the agent's learning process for varioustraining tasks. By visualizing these findings, we can understand how much aparticular training task or stage affects the agent's performance in test.Then, a GNN-based explainer learns to highlight the most important section ofthe policy, providing a more clear and robust explanation of the agent'slearning process. The experiments demonstrate that explanations derived fromthis framework can effectively help in the optimization of the training tasks,resulting in improved learning efficiency and final performance.</description><author>Shuang Ao, Simon Khan, Haris Aziz, Flora D. Salim</author><pubDate>Tue, 16 Jul 2024 08:13:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.14214v4</guid></item><item><title>DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems</title><link>http://arxiv.org/abs/2407.11472v1</link><description>Learning an effective policy to control high-dimensional, overactuatedsystems is a significant challenge for deep reinforcement learning algorithms.Such control scenarios are often observed in the neural control of vertebratemusculoskeletal systems. The study of these control mechanisms will provideinsights into the control of high-dimensional, overactuated systems. Thecoordination of actuators, known as muscle synergies in neuromechanics, isconsidered a presumptive mechanism that simplifies the generation of motorcommands. The dynamical structure of a system is the basis of its function,allowing us to derive a synergistic representation of actuators. Motivated bythis theory, we propose the Dynamical Synergistic Representation (DynSyn)algorithm. DynSyn aims to generate synergistic representations from dynamicalstructures and perform task-specific, state-dependent adaptation to therepresentations to improve motor control. We demonstrate DynSyn's efficiencyacross various tasks involving different musculoskeletal models, achievingstate-of-the-art sample efficiency and robustness compared to baselinealgorithms. DynSyn generates interpretable synergistic representations thatcapture the essential features of dynamical structures and demonstratesgeneralizability across diverse motor tasks.</description><author>Kaibo He, Chenhui Zuo, Chengtian Ma, Yanan Sui</author><pubDate>Tue, 16 Jul 2024 08:09:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.11472v1</guid></item><item><title>Contrastive Adversarial Training for Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2407.12782v1</link><description>Domain adversarial training has shown its effective capability for findingdomain invariant feature representations and been successfully adopted forvarious domain adaptation tasks. However, recent advances of large models(e.g., vision transformers) and emerging of complex adaptation scenarios (e.g.,DomainNet) make adversarial training being easily biased towards source domainand hardly adapted to target domain. The reason is twofold: relying on largeamount of labelled data from source domain for large model training and lackingof labelled data from target domain for fine-tuning. Existing approaches widelyfocused on either enhancing discriminator or improving the training stabilityfor the backbone networks. Due to unbalanced competition between the featureextractor and the discriminator during the adversarial training, existingsolutions fail to function well on complex datasets. To address this issue, weproposed a novel contrastive adversarial training (CAT) approach that leveragesthe labeled source domain samples to reinforce and regulate the featuregeneration for target domain. Typically, the regulation forces the targetfeature distribution being similar to the source feature distribution. CATaddressed three major challenges in adversarial learning: 1) ensure the featuredistributions from two domains as indistinguishable as possible for thediscriminator, resulting in a more robust domain-invariant feature generation;2) encourage target samples moving closer to the source in the feature space,reducing the requirement for generalizing classifier trained on the labeledsource domain to unlabeled target domain; 3) avoid directly aligning unpairedsource and target samples within mini-batch. CAT can be easily plugged intoexisting models and exhibits significant performance improvements.</description><author>Jiahong Chen, Zhilin Zhang, Lucy Li, Behzad Shahrasbi, Arjun Mishra</author><pubDate>Wed, 17 Jul 2024 17:59:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12782v1</guid></item><item><title>Warm-Start Variational Quantum Policy Iteration</title><link>http://arxiv.org/abs/2404.10546v2</link><description>Reinforcement learning is a powerful framework aiming to determine optimalbehavior in highly complex decision-making scenarios. This objective can beachieved using policy iteration, which requires to solve a typically largelinear system of equations. We propose the variational quantum policy iteration(VarQPI) algorithm, realizing this step with a NISQ-compatible quantum-enhancedsubroutine. Its scalability is supported by an analysis of the structure ofgeneric reinforcement learning environments, laying the foundation forpotential quantum advantage with utility-scale quantum computers. Furthermore,we introduce the warm-start initialization variant (WS-VarQPI) thatsignificantly reduces resource overhead. The algorithm solves a largeFrozenLake environment with an underlying 256x256-dimensional linear system,indicating its practical robustness.</description><author>Nico Meyer, Jakob Murauer, Alexander Popov, Christian Ufrecht, Axel Plinge, Christopher Mutschler, Daniel D. Scherer</author><pubDate>Wed, 17 Jul 2024 15:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.10546v2</guid></item><item><title>Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2406.13542v2</link><description>One core capability of large language models (LLMs) is to follow naturallanguage instructions. However, the issue of automatically constructinghigh-quality training data to enhance the complex instruction-followingabilities of LLMs without manual annotation remains unresolved. In this paper,we introduce AutoIF, the first scalable and reliable method for automaticallygenerating instruction-following training data. AutoIF transforms thevalidation of instruction-following data quality into code verification,requiring LLMs to generate instructions, the corresponding code to check thecorrectness of the instruction responses, and unit test samples to verify thecode's correctness. Then, execution feedback-based rejection sampling cangenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning fromHuman Feedback (RLHF) training. AutoIF achieves significant improvements acrossthree training algorithms, SFT, Offline DPO, and Online DPO, when applied tothe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment andstrong-to-weak distillation settings. Our code is publicly available athttps://github.com/QwenLM/AutoIF.</description><author>Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou</author><pubDate>Wed, 17 Jul 2024 14:33:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13542v2</guid></item><item><title>A new economic and financial theory of money</title><link>http://arxiv.org/abs/2310.04986v6</link><description>This paper fundamentally reformulates economic and financial theory toinclude electronic currencies. The valuation of the electronic currencies willbe based on macroeconomic theory and the fundamental equation of monetarypolicy, not the microeconomic theory of discounted cash flows. The view ofelectronic currency as a transactional equity associated with tangible assetsof a sub-economy will be developed, in contrast to the view of stock as anequity associated mostly with intangible assets of a sub-economy. The view willbe developed of the electronic currency management firm as an entityresponsible for coordinated monetary (electronic currency supply and valuestabilization) and fiscal (investment and operational) policies of asubstantial (for liquidity of the electronic currency) sub-economy. The riskmodel used in the valuations and the decision-making will not be theubiquitous, yet inappropriate, exponential risk model that leads to discountrates, but will be multi time scale models that capture the true risk. Thedecision-making will be approached from the perspective of true systems controlbased on a system response function given by the multi scale risk model andsystem controllers that utilize the Deep Reinforcement Learning, GenerativePretrained Transformers, and other methods of Generative ArtificialIntelligence (genAI). Finally, the sub-economy will be viewed as a nonlinearcomplex physical system with both stable equilibriums that are associated withshort-term exploitation, and unstable equilibriums that need to be stabilizedwith active nonlinear control based on the multi scale system responsefunctions and genAI.</description><author>Michael E. Glinsky, Sharon Sievert</author><pubDate>Wed, 17 Jul 2024 13:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04986v6</guid></item><item><title>Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models</title><link>http://arxiv.org/abs/2407.12532v1</link><description>Effective collaboration in multi-agent systems requires communicating goalsand intentions between agents. Current agent frameworks often suffer fromdependencies on single-agent execution and lack robust inter-modulecommunication, frequently leading to suboptimal multi-agent reinforcementlearning (MARL) policies and inadequate task coordination. To address thesechallenges, we present a framework for training large language models (LLMs) ascollaborative agents to enable coordinated behaviors in cooperative MARL. Eachagent maintains a private intention consisting of its current goal andassociated sub-tasks. Agents broadcast their intentions periodically, allowingother agents to infer coordination tasks. A propagation network transformsbroadcast intentions into teammate-specific communication messages, sharingrelevant goals with designated teammates. The architecture of our framework isstructured into planning, grounding, and execution modules. During execution,multiple agents interact in a downstream environment and communicate intentionsto enable coordinated behaviors. The grounding module dynamically adaptscomprehension strategies based on emerging coordination patterns, whilefeedback from execution agents influnces the planning module, enabling thedynamic re-planning of sub-tasks. Results in collaborative environmentsimulation demonstrate intention propagation reduces miscoordination errors byaligning sub-task dependencies between agents. Agents learn when to communicateintentions and which teammates require task details, resulting in emergentcoordinated behaviors. This demonstrates the efficacy of intention sharing forcooperative multi-agent RL based on LLMs.</description><author>Xihe Qiu, Haoyu Wang, Xiaoyu Tan, Chao Qu, Yujie Xiong, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi</author><pubDate>Wed, 17 Jul 2024 13:14:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12532v1</guid></item><item><title>Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments</title><link>http://arxiv.org/abs/2407.12505v1</link><description>Learning policies for multi-entity systems in 3D environments is far morecomplicated against single-entity scenarios, due to the exponential expansionof the global state space as the number of entities increases. One potentialsolution of alleviating the exponential complexity is dividing the global spaceinto independent local views that are invariant to transformations includingtranslations and rotations. To this end, this paper proposes SubequivariantHierarchical Neural Networks (SHNN) to facilitate multi-entity policy learning.In particular, SHNN first dynamically decouples the global space into localentity-level graphs via task assignment. Second, it leverages subequivariantmessage passing over the local entity-level graphs to devise local referenceframes, remarkably compressing the representation redundancy, particularly ingravity-affected environments. Furthermore, to overcome the limitations ofexisting benchmarks in capturing the subtleties of multi-entity systems underthe Euclidean symmetry, we propose the Multi-entity Benchmark (MEBEN), a newsuite of environments tailored for exploring a wide range of multi-entityreinforcement learning. Extensive experiments demonstrate significantadvancements of SHNN on the proposed benchmarks compared to existing methods.Comprehensive ablations are conducted to verify the indispensability of taskassignment and subequivariance.</description><author>Runfa Chen, Ling Wang, Yu Du, Tianrui Xue, Fuchun Sun, Jianwei Zhang, Wenbing Huang</author><pubDate>Wed, 17 Jul 2024 11:37:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12505v1</guid></item><item><title>Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms</title><link>http://arxiv.org/abs/2406.11481v3</link><description>Reinforcement Learning (RL) serves as a versatile framework for sequentialdecision-making, finding applications across diverse domains such as robotics,autonomous driving, recommendation systems, supply chain optimization, biology,mechanics, and finance. The primary objective in these applications is tomaximize the average reward. Real-world scenarios often necessitate adherenceto specific constraints during the learning process. This monograph focuses on the exploration of various model-based andmodel-free approaches for Constrained RL within the context of average rewardMarkov Decision Processes (MDPs). The investigation commences with anexamination of model-based strategies, delving into two foundational methods -optimism in the face of uncertainty and posterior sampling. Subsequently, thediscussion transitions to parametrized model-free approaches, where theprimal-dual policy gradient-based algorithm is explored as a solution forconstrained MDPs. The monograph provides regret guarantees and analyzesconstraint violation for each of the discussed setups. For the above exploration, we assume the underlying MDP to be ergodic.Further, this monograph extends its discussion to encompass results tailoredfor weakly communicating MDPs, thereby broadening the scope of its findings andtheir relevance to a wider range of practical scenarios.</description><author>Vaneet Aggarwal, Washim Uddin Mondal, Qinbo Bai</author><pubDate>Wed, 17 Jul 2024 11:32:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.11481v3</guid></item><item><title>Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models</title><link>http://arxiv.org/abs/2406.13542v3</link><description>One core capability of large language models (LLMs) is to follow naturallanguage instructions. However, the issue of automatically constructinghigh-quality training data to enhance the complex instruction-followingabilities of LLMs without manual annotation remains unresolved. In this paper,we introduce AutoIF, the first scalable and reliable method for automaticallygenerating instruction-following training data. AutoIF transforms thevalidation of instruction-following data quality into code verification,requiring LLMs to generate instructions, the corresponding code to check thecorrectness of the instruction responses, and unit test samples to verify thecode's correctness. Then, execution feedback-based rejection sampling cangenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning fromHuman Feedback (RLHF) training. AutoIF achieves significant improvements acrossthree training algorithms, SFT, Offline DPO, and Online DPO, when applied tothe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment andstrong-to-weak distillation settings. Our code is publicly available athttps://github.com/QwenLM/AutoIF.</description><author>Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou</author><pubDate>Thu, 18 Jul 2024 09:00:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.13542v3</guid></item><item><title>Estimating Reaction Barriers with Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2407.12453v1</link><description>Stable states in complex systems correspond to local minima on the associatedpotential energy surface. Transitions between these local minima govern thedynamics of such systems. Precisely determining the transition pathways incomplex and high-dimensional systems is challenging because these transitionsare rare events, and isolating the relevant species in experiments isdifficult. Most of the time, the system remains near a local minimum, withrare, large fluctuations leading to transitions between minima. The probabilityof such transitions decreases exponentially with the height of the energybarrier, making the system's dynamics highly sensitive to the calculated energybarriers. This work aims to formulate the problem of finding the minimum energybarrier between two stable states in the system's state space as acost-minimization problem. We propose solving this problem using reinforcementlearning algorithms. The exploratory nature of reinforcement learning agentsenables efficient sampling and determination of the minimum energy barrier fortransitions.</description><author>Adittya Pal</author><pubDate>Wed, 17 Jul 2024 10:02:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12453v1</guid></item><item><title>Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement Learning</title><link>http://arxiv.org/abs/2407.12448v1</link><description>Combining offline and online reinforcement learning (RL) techniques is indeedcrucial for achieving efficient and safe learning where data acquisition isexpensive. Existing methods replay offline data directly in the online phase,resulting in a significant challenge of data distribution shift andsubsequently causing inefficiency in online fine-tuning. To address this issue,we introduce an innovative approach, \textbf{E}nergy-guided \textbf{DI}ffusion\textbf{S}ampling (EDIS), which utilizes a diffusion model to extract priorknowledge from the offline dataset and employs energy functions to distill thisknowledge for enhanced data generation in the online phase. The theoreticalanalysis demonstrates that EDIS exhibits reduced suboptimality compared tosolely utilizing online data or directly reusing offline data. EDIS is aplug-in approach and can be combined with existing methods in offline-to-onlineRL setting. By implementing EDIS to off-the-shelf methods Cal-QL and IQL, weobserve a notable 20% average improvement in empirical performance on MuJoCo,AntMaze, and Adroit environments. Code is available at\url{https://github.com/liuxhym/EDIS}.</description><author>Xu-Hui Liu, Tian-Shuo Liu, Shengyi Jiang, Ruifeng Chen, Zhilong Zhang, Xinwei Chen, Yang Yu</author><pubDate>Wed, 17 Jul 2024 09:56:51 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12448v1</guid></item><item><title>Variable-Agnostic Causal Exploration for Reinforcement Learning</title><link>http://arxiv.org/abs/2407.12437v1</link><description>Modern reinforcement learning (RL) struggles to capture real-worldcause-and-effect dynamics, leading to inefficient exploration due to extensivetrial-and-error actions. While recent efforts to improve agent exploration haveleveraged causal discovery, they often make unrealistic assumptions of causalvariables in the environments. In this paper, we introduce a novel framework,Variable-Agnostic Causal Exploration for Reinforcement Learning (VACERL),incorporating causal relationships to drive exploration in RL withoutspecifying environmental causal variables. Our approach automaticallyidentifies crucial observation-action steps associated with key variables usingattention mechanisms. Subsequently, it constructs the causal graph connectingthese steps, which guides the agent towards observation-action pairs withgreater causal influence on task completion. This can be leveraged to generateintrinsic rewards or establish a hierarchy of subgoals to enhance explorationefficiency. Experimental results showcase a significant improvement in agentperformance in grid-world, 2d games and robotic domains, particularly inscenarios with sparse rewards and noisy actions, such as the notorious Noisy-TVenvironments.</description><author>Minh Hoang Nguyen, Hung Le, Svetha Venkatesh</author><pubDate>Wed, 17 Jul 2024 09:45:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2407.12437v1</guid></item><item><title>Empowering Safe Reinforcement Learning for Power System Control with CommonPower</title><link>http://arxiv.org/abs/2406.03231v2</link><description>The growing complexity of power system management has led to an increasedinterest in reinforcement learning (RL). However, vanilla RL controllers cannotthemselves ensure satisfaction of system constraints. Therefore, combining themwith formally correct safeguarding mechanisms is an important aspect whenstudying RL for power system management. Integrating safeguarding into complexuse cases requires tool support. To address this need, we introduce the Pythontool CommonPower. CommonPower's unique contribution lies in its symbolicmodeling approach, which enables flexible, model-based safeguarding of RLcontrollers. Moreover, CommonPower offers a unified interface for single-agentRL, multi-agent RL, and optimal control, with seamless integration of differentforecasting methods. This allows users to validate the effectiveness of safe RLcontrollers across a large variety of case studies and investigate theinfluence of specific aspects on overall performance. We demonstrateCommonPower's versatility through a numerical case study that compares RLagents featuring different safeguards with a model predictive controller in thecontext of building energy management.</description><author>Michael Eichelbeck, Hannah Markgraf, Matthias Althoff</author><pubDate>Tue, 16 Jul 2024 09:48:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.03231v2</guid></item></channel></rss>