<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivreinforcement learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 18 Aug 2025 22:01:24 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</title><link>http://arxiv.org/abs/2508.11503v1</link><description>Reliable autonomous navigation across the unstructured terrains of distantplanetary surfaces is a critical enabler for future space exploration. However,the deployment of learning-based controllers is hindered by the inherentsim-to-real gap, particularly for the complex dynamics of wheel interactionswith granular media. This work presents a complete sim-to-real framework fordeveloping and validating robust control policies for dynamic waypoint trackingon such challenging surfaces. We leverage massively parallel simulation totrain reinforcement learning agents across a vast distribution of procedurallygenerated environments with randomized physics. These policies are thentransferred zero-shot to a physical wheeled rover operating in a lunar-analoguefacility. Our experiments systematically compare multiple reinforcementlearning algorithms and action smoothing filters to identify the most effectivecombinations for real-world deployment. Crucially, we provide strong empiricalevidence that agents trained with procedural diversity achieve superiorzero-shot performance compared to those trained on static scenarios. We alsoanalyze the trade-offs of fine-tuning with high-fidelity particle physics,which offers minor gains in low-speed precision at a significant computationalcost. Together, these contributions establish a validated workflow for creatingreliable learning-based navigation systems, marking a critical step towardsdeploying autonomous robots in the final frontier.</description><author>Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez</author><pubDate>Fri, 15 Aug 2025 14:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11503v1</guid></item><item><title>SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</title><link>http://arxiv.org/abs/2508.11553v1</link><description>We introduce SeamlessFlow, a server based reinforcement learning (RL)framework that addresses two core challenges in industrial scale RL: (1)decoupling RL training from the complex execution flow of agents; (2)maximizing GPU utilization with minimal idle time while preserving thestability and scalability required for large-scale deployments. First,SeamlessFlow introduces a data plane that decouples the RL trainer fromdiverse, complex agent implementations while sustaining high throughput. Acentral trajectory manager maintains complete interaction histories andsupports partial rollout, allowing rollout to pause for weight updates andresume seamlessly, keeping agents unaware of service interruptions. Second, wepropose a tag driven scheduling paradigm that abstracts hardware intocapability tagged resources, unifying colocated and disaggregatedarchitectures. Based on this, SeamlessFlow introduces a spatiotemporalmultiplexing pipeline that dynamically reassigns idle training nodes to rolloutin a train rollout separated setup, eliminating pipeline bubbles and fullyexploiting heterogeneous cluster resources. By combining these innovations,SeamlessFlow delivers both stability and high performance, making it wellsuited for multi agent, long horizon, and other complex RL tasks.</description><author>Jinghui Wang, Shaojie Wang, Yinghan Cui, Xuxing Chen, Chao Wang, Xiaojiang Zhang, Minglei Zhang, Jiarong Zhang, Wenhao Zhuang, Yuchen Cao, Wankang Bao, Haimo Li, Zheng Lin, Huiming Wang, Haoyang Huang, Zongxian Feng, Zizheng Zhan, Ken Deng, Wen Xiang, Huaixi Tang, Kun Wu, Mengtong Li, Mengfei Xie, Junyi Peng, Haotian Zhang, Bin Chen, Bing Yu</author><pubDate>Fri, 15 Aug 2025 15:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11553v1</guid></item><item><title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title><link>http://arxiv.org/abs/2508.11538v1</link><description>Video reasoning segmentation (VRS) endeavors to delineate referred objects invideos guided by implicit instructions that encapsulate human intent andtemporal logic. Previous approaches leverage large vision language models(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.However, this paradigm suffers from limited interpretability during inferenceand suboptimal performance due to inadequate spatiotemporal reasoning. Drawinginspiration from seminal breakthroughs in reinforcement learning, we introduceVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning insegmentation. Veason-R1 is trained through Group Relative Policy Optimization(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, wecurate high-quality CoT training data to instill structured reasoningtrajectories, bridging video-level semantics and frame-level spatial grounding,yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPOfine-tuning encourages efficient exploration of the reasoning space byoptimizing reasoning chains. To this end, we incorporate a holistic rewardmechanism that synergistically enhances spatial alignment and temporalconsistency, bolstering keyframe localization and fine-grained grounding.Comprehensive empirical evaluations demonstrate that Veason-R1 achievesstate-of-the-art performance on multiple benchmarks, surpassing prior art bysignificant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),while exhibiting robustness to hallucinations (+8.8 R). Our code and modelweights will be available at Veason-R1.</description><author>Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</author><pubDate>Fri, 15 Aug 2025 15:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11538v1</guid></item><item><title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title><link>http://arxiv.org/abs/2503.01307v2</link><description>Test-time inference has emerged as a powerful paradigm for enabling languagemodels to ``think'' longer and more carefully about complex challenges, muchlike skilled human experts. While reinforcement learning (RL) can driveself-improvement in language models on verifiable tasks, some models exhibitsubstantial gains while others quickly plateau. For instance, we find thatQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the gameof Countdown. This discrepancy raises a critical question: what intrinsicproperties enable effective self-improvement? We introduce a framework toinvestigate this question by analyzing four key cognitive behaviors --verification, backtracking, subgoal setting, and backward chaining -- that bothexpert human problem solvers and successful language models employ. Our studyreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llamainitially lacks them. In systematic experimentation with controlled behavioraldatasets, we find that priming Llama with examples containing these reasoningbehaviors enables substantial improvements during RL, matching or exceedingQwen's performance. Importantly, the presence of reasoning behaviors, ratherthan correctness of answers, proves to be the critical factor -- models primedwith incorrect solutions containing proper reasoning patterns achievecomparable performance to those trained on correct solutions. Finally,leveraging continued pretraining with OpenWebMath data, filtered to amplifyreasoning behaviors, enables the Llama model to match Qwen's self-improvementtrajectory. Our findings establish a fundamental relationship between initialreasoning behaviors and the capacity for improvement, explaining why somelanguage models effectively utilize additional computation while othersplateau.</description><author>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</author><pubDate>Fri, 15 Aug 2025 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.01307v2</guid></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2507.01006v5</link><description>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models(VLMs) designed to advance general-purpose multimodal understanding andreasoning. In this report, we share our key findings in the development of thereasoning-centric training framework. We first develop a capable visionfoundation model with significant potential through large-scale pre-training,which arguably sets the upper bound for the final performance. We then proposeReinforcement Learning with Curriculum Sampling (RLCS) to unlock the fullpotential of the model, leading to comprehensive capability enhancement acrossa diverse range of tasks, including STEM problem solving, video understanding,content recognition, coding, grounding, GUI-based agents, and long documentinterpretation. In a comprehensive evaluation across 42 public benchmarks,GLM-4.5V achieves state-of-the-art performance on nearly all tasks amongopen-source models of similar size, and demonstrates competitive or evensuperior results compared to closed-source models such as Gemini-2.5-Flash onchallenging tasks including Coding and GUI Agents. Meanwhile, the smallerGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results tothe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source bothGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information arereleased at https://github.com/zai-org/GLM-V.</description><author>GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan</author><pubDate>Fri, 15 Aug 2025 13:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01006v5</guid></item><item><title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title><link>http://arxiv.org/abs/2508.09736v2</link><description>We introduce M3-Agent, a novel multimodal agent framework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual andauditory inputs to build and update its long-term memory. Beyond episodicmemory, it also develops semantic memory, enabling it to accumulate worldknowledge over time. Its memory is organized in an entity-centric, multimodalformat, allowing deeper and more consistent understanding of the environment.Given an instruction, M3-Agent autonomously performs multi-turn, iterativereasoning and retrieves relevant information from memory to accomplish thetask. To evaluate memory effectiveness and memory-based reasoning in multimodalagents, we develop M3-Bench, a new long-video question answering benchmark.M3-Bench comprises 100 newly recorded real-world videos captured from a robot'sperspective (M3-Bench-robot) and 920 web-sourced videos across diversescenarios (M3-Bench-web). We annotate question-answer pairs designed to testkey capabilities essential for agent applications, such as human understanding,general knowledge extraction, and cross-modal reasoning. Experimental resultsshow that M3-Agent, trained via reinforcement learning, outperforms thestrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-weband VideoMME-long, respectively. Our work advances the multimodal agents towardmore human-like long-term memory and provides insights into their practicaldesign. Model, code and data are available athttps://github.com/bytedance-seed/m3-agent</description><author>Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</author><pubDate>Fri, 15 Aug 2025 13:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09736v2</guid></item><item><title>Embedding Safety into RL: A New Take on Trust Region Methods</title><link>http://arxiv.org/abs/2411.02957v4</link><description>Reinforcement Learning (RL) agents can solve diverse tasks but often exhibitunsafe behavior. Constrained Markov Decision Processes (CMDPs) address this byenforcing safety constraints, yet existing methods either sacrifice rewardmaximization or allow unsafe training. We introduce Constrained Trust RegionPolicy Optimization (C-TRPO), which reshapes the policy space geometry toensure trust regions contain only safe policies, guaranteeing constraintsatisfaction throughout training. We analyze its theoretical properties andconnections to TRPO, Natural Policy Gradient (NPG), and Constrained PolicyOptimization (CPO). Experiments show that C-TRPO reduces constraint violationswhile maintaining competitive returns.</description><author>Nikola Milosevic, Johannes Müller, Nico Scherf</author><pubDate>Fri, 15 Aug 2025 12:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02957v4</guid></item><item><title>Exploring Superior Function Calls via Reinforcement Learning</title><link>http://arxiv.org/abs/2508.05118v3</link><description>Function calling capabilities are crucial for deploying Large Language Modelsin real-world applications, yet current training approaches fail to developrobust reasoning strategies. Supervised fine-tuning produces models that relyon superficial pattern matching, while standard reinforcement learning methodsstruggle with the complex action space of structured function calls. We presenta novel reinforcement learning framework designed to enhance group relativepolicy optimization through strategic entropy based exploration specificallytailored for function calling tasks. Our approach addresses three criticalchallenges in function calling: insufficient exploration during policylearning, lack of structured reasoning in chain-of-thought generation, andinadequate verification of parameter extraction. Our two-stage data preparationpipeline ensures high-quality training samples through iterative LLM evaluationand abstract syntax tree validation. Extensive experiments on the BerkeleyFunction Calling Leaderboard demonstrate that this framework achievesstate-of-the-art performance among open-source models with 86.02\% overallaccuracy, outperforming standard GRPO by up to 6\% on complex multi-functionscenarios. Notably, our method shows particularly strong improvements oncode-pretrained models, suggesting that structured language generationcapabilities provide an advantageous starting point for reinforcement learningin function calling tasks. We will release all the code, models and dataset tobenefit the community.</description><author>Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</author><pubDate>Fri, 15 Aug 2025 12:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05118v3</guid></item><item><title>On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting</title><link>http://arxiv.org/abs/2508.11408v1</link><description>Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are twoprominent post-training paradigms for refining the capabilities and aligningthe behavior of Large Language Models (LLMs). Existing approaches thatintegrate SFT and RL often face the risk of disrupting established modelpatterns and inducing overfitting to expert data. To address this, we present anovel investigation into the unified view of SFT and RL through an off-policyversus on-policy lens. We propose CHORD, a framework for the ControllableHarmonization of On- and Off-Policy Reinforcement Learning via DynamicWeighting, which reframes SFT not as a separate stage but as a dynamicallyweighted auxiliary objective within the on-policy RL process. Based on ananalysis of off-policy expert data's influence at both holistic and granularlevels, we incorporate a dual-control mechanism in CHORD. Specifically, theframework first employs a global coefficient to holistically guide thetransition from off-policy imitation to on-policy exploration, and then appliesa token-wise weighting function that enables granular learning from experttokens, which preserves on-policy exploration and mitigates disruption fromoff-policy data. We conduct extensive experiments on widely used benchmarks,providing empirical evidence that CHORD achieves a stable and efficientlearning process. By effectively harmonizing off-policy expert data withon-policy exploration, CHORD demonstrates significant improvements overbaselines. We release the implementation athttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord toinspire further research.</description><author>Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou</author><pubDate>Fri, 15 Aug 2025 11:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11408v1</guid></item><item><title>Sketch Decompositions for Classical Planning via Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2412.08574v2</link><description>In planning and reinforcement learning, the identification of common subgoalstructures across problems is important when goals are to be achieved over longhorizons. Recently, it has been shown that such structures can be expressed asfeature-based rules, called sketches, over a number of classical planningdomains. These sketches split problems into subproblems which then becomesolvable in low polynomial time by a greedy sequence of IW$(k)$ searches.Methods for learning sketches using feature pools and min-SAT solvers have beendeveloped, yet they face two key limitations: scalability and expressivity. Inthis work, we address these limitations by formulating the problem of learningsketch decompositions as a deep reinforcement learning (DRL) task, wheregeneral policies are sought in a modified planning problem where the successorstates of a state s are defined as those reachable from s through an IW$(k)$search. The sketch decompositions obtained through this method areexperimentally evaluated across various domains, and problems are regarded assolved by the decomposition when the goal is reached through a greedy sequenceof IW$(k)$ searches. While our DRL approach for learning sketch decompositionsdoes not yield interpretable sketches in the form of rules, we demonstrate thatthe resulting decompositions can often be understood in a crisp manner.</description><author>Michael Aichmüller, Hector Geffner</author><pubDate>Fri, 15 Aug 2025 10:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08574v2</guid></item><item><title>Fusing Rewards and Preferences in Reinforcement Learning</title><link>http://arxiv.org/abs/2508.11363v1</link><description>We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm thatfuses both individual rewards and pairwise preferences (if available) into asingle update rule. DFA uses the policy's log-probabilities directly to modelthe preference probability, avoiding a separate reward-modeling step.Preferences can be provided by human-annotators (at state-level ortrajectory-level) or be synthesized online from Q-values stored in anoff-policy replay buffer. Under a Bradley-Terry model, we prove that minimizingDFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)policy. Our simulation results show that DFA trained on generated preferencesmatches or exceeds SAC on six control environments and demonstrates a morestable training process. With only a semi-synthetic preference dataset underBradley-Terry model, our algorithm outperforms reward-modeling reinforcementlearning from human feedback (RLHF) baselines in a stochastic GridWorld andapproaches the performance of an oracle with true rewards.</description><author>Sadegh Khorasani, Saber Salehkaleybar, Negar Kiyavash, Matthias Grossglauser</author><pubDate>Fri, 15 Aug 2025 09:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11363v1</guid></item><item><title>CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</title><link>http://arxiv.org/abs/2508.11360v1</link><description>As autonomous agents become adept at understanding and interacting withgraphical user interface (GUI) environments, a new era of automated taskexecution is emerging. Recent studies have demonstrated that ReinforcementLearning (RL) can effectively enhance agents' performance in dynamicinteractive GUI environments. However, these methods face two key limitations:(1) they overlook the significant variation in difficulty across different GUItasks by treating the entire training data as a uniform set, which hampers theagent's ability to adapt its learning process; and (2) most approaches collapsetask-specific nuances into a single, coarse reward, leaving the agent with auniform signal that yields inefficient policy updates. To address theselimitations, we propose CRAFT-GUI, a curriculum learning framework based onGroup Relative Policy Optimization (GRPO) that explicitly accounts for thevarying difficulty across trajectories. To enable more fine-grained policyoptimization, we design a reward function that combines simple rule-basedsignals with model-judged evaluation, providing richer and more nuancedfeedback during training. Experimental results demonstrate that our methodachieves significant improvements over previous state-of-the-art approaches,outperforming them by 5.6% on public benchmarks Android Control and 10.3% onour internal online benchmarks, respectively. These findings empiricallyvalidate the effectiveness of integrating reinforcement learning withcurriculum learning in GUI interaction tasks.</description><author>Songqin Nong, Jingxuan Xu, Sheng Zhou, Jianfeng Chen, Xiaoxuan Tang, Tao Jiang, Wenhao Xu</author><pubDate>Fri, 15 Aug 2025 09:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11360v1</guid></item><item><title>ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism</title><link>http://arxiv.org/abs/2508.11356v1</link><description>Recent advancements in Large Language Models have yielded significantimprovements in complex reasoning tasks such as mathematics and programming.However, these models remain heavily dependent on annotated data and exhibitlimited adaptability in unsupervised scenarios. To address these limitations,test-time reinforcement learning (TTRL) has been proposed, which enablesself-optimization by leveraging model-generated pseudo-labels. Despite itspromise, TTRL faces several key challenges, including high inference costs dueto parallel rollouts and early-stage estimation bias that fostersoverconfidence, reducing output diversity and causing performance plateaus. Toaddress these challenges, we introduce an entropy-based mechanism to enhancethe exploration-exploitation balance in test-time reinforcement learningthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) andEntropy-based Advantage Reshaping (EAR). Compared with the baseline, ourapproach enables Llama3.1-8B to achieve a 68 percent relative improvement inPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent ofthe rollout tokens budget. This highlights our method's ability to effectivelyoptimize the trade-off between inference efficiency, diversity, and estimationrobustness, thereby advancing unsupervised reinforcement learning foropen-domain reasoning tasks.</description><author>Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, ShaoGuo Liu, TingTing Gao</author><pubDate>Fri, 15 Aug 2025 09:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11356v1</guid></item><item><title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title><link>http://arxiv.org/abs/2506.07468v2</link><description>Conventional language model (LM) safety alignment relies on a reactive,disjoint procedure: attackers exploit a static model, followed by defensivefine-tuning to patch exposed vulnerabilities. This sequential approach createsa mismatch -- attackers overfit to obsolete defenses, while defendersperpetually lag behind emerging threats. To address this, we proposeSelf-RedTeam, an online self-play reinforcement learning algorithm where anattacker and defender agent co-evolve through continuous interaction. We castsafety alignment as a two-player zero-sum game, where a single model alternatesbetween attacker and defender roles -- generating adversarial prompts andsafeguarding against them -- while a reward LM adjudicates outcomes. Thisenables dynamic co-adaptation. Grounded in the game-theoretic framework ofzero-sum games, we establish a theoretical safety guarantee which motivates thedesign of our method: if self-play converges to a Nash Equilibrium, thedefender will reliably produce safe responses to any adversarial input.Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) comparedto attackers trained against static defenders and achieves higher robustness onsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trainedagainst static attackers. We further propose hidden Chain-of-Thought, allowingagents to plan privately, which boosts adversarial diversity and reducesover-refusals. Our results motivate a shift from reactive patching to proactiveco-evolution in LM safety training, enabling scalable, autonomous, and robustself-improvement of LMs via multi-agent reinforcement learning (MARL).</description><author>Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</author><pubDate>Fri, 15 Aug 2025 09:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07468v2</guid></item><item><title>HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</title><link>http://arxiv.org/abs/2508.11350v1</link><description>Understanding and recognizing human-object interaction (HOI) is a pivotalapplication in AR/VR and robotics. Recent open-vocabulary HOI detectionapproaches depend exclusively on large language models for richer textualprompts, neglecting their inherent 3D spatial understanding capabilities. Toaddress this shortcoming, we introduce HOID-R1, the first HOI detectionframework that integrates chain-of-thought (CoT) guided supervised fine-tuning(SFT) with group relative policy optimization (GRPO) within a reinforcementlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the modelwith essential reasoning capabilities, forcing the model to articulate itsthought process in the output. Subsequently, we integrate GRPO to leveragemulti-reward signals for policy optimization, thereby enhancing alignmentacross diverse modalities. To mitigate hallucinations in the CoT reasoning, weintroduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,further improving generalization. Extensive experiments show that HOID-R1achieves state-of-the-art performance on HOI detection benchmarks andoutperforms existing methods in open-world generalization to novel scenarios.</description><author>Zhenhao Zhang, Hanqing Wang, Xiangyu Zeng, Ziyu Cheng, Jiaxin Liu, Haoyu Yan, Zhirui Liu, Kaiyang Ji, Tianxiang Gui, Ke Hu, Kangyi Chen, Yahao Fan, Mokai Pan</author><pubDate>Fri, 15 Aug 2025 09:28:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11350v1</guid></item><item><title>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model</title><link>http://arxiv.org/abs/2508.06571v3</link><description>Vision-Language-Action (VLA) models have demonstrated potential in autonomousdriving. However, two critical challenges hinder their development: (1)Existing VLA architectures are typically based on imitation learning inopen-loop setup which tends to capture the recorded behaviors in the dataset,leading to suboptimal and constrained performance, (2) Close-loop trainingrelies heavily on high-fidelity sensor simulation, where domain gaps andcomputational inefficiencies pose significant barriers. In this paper, weintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world modelwith a self-built VLA approach. Our framework proceeds in a three-stageparadigm: In the first stage, we propose a VLA architecture and pretrain theVLA policy via imitation learning. In the second stage, we construct alightweight reward world model via inverse reinforcement learning to enableefficient close-loop reward computation. To further enhance planningperformance, finally, we design specialized reward world model guidencereinforcement learning via PPO(Proximal Policy Optimization) to effectivelybalance the safety incidents, comfortable driving, and traffic efficiency. Ourapproach achieves state-of-the-art performance in NAVSIM v2 end-to-end drivingbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope thatour framework will accelerate VLA research in close-loop autonomous driving.</description><author>Anqing Jiang, Yu Gao, Yiru Wang, Zhigang Sun, Shuo Wang, Yuwen Heng, Hao Sun, Shichen Tang, Lijuan Zhu, Jinhao Chai, Jijun Wang, Zichong Gu, Hao Jiang, Li Sun</author><pubDate>Fri, 15 Aug 2025 05:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06571v3</guid></item><item><title>Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</title><link>http://arxiv.org/abs/2508.11204v1</link><description>Sampling efficiency is critical for deploying visuomotor learning inreal-world robotic manipulation. While task symmetry has emerged as a promisinginductive bias to improve efficiency, most prior work is limited to isometricsymmetries -- applying the same group transformation to all task objects acrossall timesteps. In this work, we explore non-isometric symmetries, applyingmultiple independent group transformations across spatial and temporaldimensions to relax these constraints. We introduce a novel formulation of thepartially observable Markov decision process (POMDP) that incorporates thenon-isometric symmetry structures, and propose a simple yet effective dataaugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrateMEA with offline reinforcement learning to enhance sampling efficiency, andintroduce a voxel-based visual representation that preserves translationalequivariance. Extensive simulation and real-robot experiments across twomanipulation domains demonstrate the effectiveness of our approach.</description><author>Hongbin Lin, Juan Rojas, Kwok Wai Samuel Au</author><pubDate>Fri, 15 Aug 2025 04:30:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11204v1</guid></item><item><title>RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System</title><link>http://arxiv.org/abs/2508.09186v2</link><description>The proliferation of AI-powered cameras in Intelligent Transportation Systems(ITS) creates a severe conflict between the need for rich visual data and theright to privacy. Existing privacy-preserving methods, such as blurring orencryption, are often insufficient due to creating an undesirable trade-offwhere either privacy is compromised against advanced reconstruction attacks ordata utility is critically degraded. To resolve this challenge, we proposeRL-MoE, a novel framework that transforms sensitive visual data intoprivacy-preserving textual descriptions, eliminating the need for direct imagetransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecturefor nuanced, multi-aspect scene decomposition with a Reinforcement Learning(RL) agent that optimizes the generated text for a dual objective of semanticaccuracy and privacy preservation. Extensive experiments demonstrate thatRL-MoE provides superior privacy protection, reducing the success rate ofreplay attacks to just 9.4\% on the CFP-FP dataset, while simultaneouslygenerating richer textual content than baseline methods. Our work provides apractical and scalable solution for building trustworthy AI systems inprivacy-sensitive domains, paving the way for more secure smart city andautonomous vehicle networks.</description><author>Abdolazim Rezaei, Mehdi Sookhak, Mahboobeh Haghparast</author><pubDate>Fri, 15 Aug 2025 04:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09186v2</guid></item><item><title>UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</title><link>http://arxiv.org/abs/2508.11196v1</link><description>Recent advances in vision-language models (VLMs) have demonstrated stronggeneralization in natural image tasks. However, their performance oftendegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which featureshigh resolution, complex spatial semantics, and strict real-time constraints.These challenges limit the applicability of general-purpose VLMs to structuredaerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, alightweight VLM explicitly designed for aerial visual reasoning. It is trainedusing a hybrid method that combines supervised fine-tuning (SFT) andmulti-stage reinforcement learning (RL). We leverage the group relative policyoptimization (GRPO) algorithm to promote structured and interpretable reasoningthrough rule-guided rewards and intra-group policy alignment. To support modeltraining and evaluation, we introduce a high-resolution visual questionanswering dataset named HRVQA-VL, which consists of 50,019 annotated samplescovering eight UAV-relevant reasoning tasks, including object counting,transportation recognition, and spatial scene inference. Experimental resultsshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than theQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, whichis 36x larger, on multiple tasks. Ablation studies reveal that while SFTimproves semantic alignment, it may reduce reasoning diversity in mathematicaltasks. GRPO-based RL compensates for this limitation by enhancing logicalflexibility and the robustness of inference. Additionally, UAV-VL-R1 requiresonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB withINT8, supporting real-time deployment on resource-constrained UAV platforms.</description><author>Jiajin Guan, Haibo Mei, Bonan Zhang, Dan Liu, Yuanshuang Fu, Yue Zhang</author><pubDate>Fri, 15 Aug 2025 04:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11196v1</guid></item><item><title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title><link>http://arxiv.org/abs/2508.00344v2</link><description>Large Language Models (LLMs) have shown remarkable advancements in tacklingagent-oriented tasks. Despite their potential, existing work faces challengeswhen deploying LLMs in agent-based environments. The widely adopted agentparadigm ReAct centers on integrating single-step reasoning with immediateaction execution, which limits its effectiveness in complex tasks requiringlong-term strategic planning. Furthermore, the coordination between the plannerand executor during problem-solving is also a critical factor to consider inagent design. Additionally, current approaches predominantly rely on supervisedfine-tuning, which often leads models to memorize established task completiontrajectories, thereby restricting their generalization ability when confrontedwith novel problem contexts. To address these challenges, we introduce anadaptive global plan-based agent paradigm AdaPlan, aiming to synergizehigh-level explicit guidance with execution to support effective long-horizondecision-making. Based on the proposed paradigm, we further put forwardPilotRL, a global planning-guided training framework for LLM agents driven byprogressive reinforcement learning. We first develop the model's ability tofollow explicit guidance from global plans when addressing agent tasks.Subsequently, based on this foundation, we focus on optimizing the quality ofgenerated plans. Finally, we conduct joint optimization of the model's planningand execution coordination. Experiments indicate that PilotRL could achievestate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassingclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%comparing to GPT-4o-mini at a comparable parameter scale.</description><author>Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang</author><pubDate>Fri, 15 Aug 2025 03:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.00344v2</guid></item><item><title>Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward</title><link>http://arxiv.org/abs/2508.11143v1</link><description>Existing reinforcement learning (RL) methods struggle with long-horizonrobotic manipulation tasks, particularly those involving sparse rewards. Whileaction chunking is a promising paradigm for robotic manipulation, using RL todirectly learn continuous action chunks in a stable and data-efficient mannerremains a critical challenge. This paper introduces AC3 (Actor-Critic forContinuous Chunks), a novel RL framework that learns to generatehigh-dimensional, continuous action sequences. To make this learning processstable and data-efficient, AC3 incorporates targeted stabilization mechanismsfor both the actor and the critic. First, to ensure reliable policyimprovement, the actor is trained with an asymmetric update rule, learningexclusively from successful trajectories. Second, to enable effective valuelearning despite sparse rewards, the critic's update is stabilized usingintra-chunk $n$-step returns and further enriched by a self-supervised moduleproviding intrinsic rewards at anchor points aligned with each action chunk. Weconducted extensive experiments on 25 tasks from the BiGym and RLBenchbenchmarks. Results show that by using only a few demonstrations and a simplemodel architecture, AC3 achieves superior success rates on most tasks,validating its effective design.</description><author>Jiarui Yang, Bin Zhu, Jingjing Chen, Yu-Gang Jiang</author><pubDate>Fri, 15 Aug 2025 01:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11143v1</guid></item><item><title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title><link>http://arxiv.org/abs/2508.10501v2</link><description>Existing tool-augmented agentic systems are limited in the real world by (i)black-box reasoning steps that undermine trust of decision-making and posesafety risks, (ii) poor multimodal integration, which is inherently criticalfor healthcare tasks, and (iii) rigid and computationally inefficient agenticpipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), thefirst multimodal framework to address these challenges in the context of ChestX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over amulti-tool graph, yielding decision paths annotated with interpretableprobabilities. Given the complex CXR reasoning task with multimodal medicaldata, PASS leverages its learned task-conditioned distribution over the agenticsupernet. Thus, it adaptively selects the most suitable tool at each supernetlayer, offering probability-annotated trajectories for post-hoc audits anddirectly enhancing medical AI safety. PASS also continuously compresses salientfindings into an evolving personalized memory, while dynamically decidingwhether to deepen its reasoning path or invoke an early exit for efficiency. Tooptimize a Pareto frontier balancing performance and cost, we design a novelthree-stage training procedure, including expert knowledge warm-up, contrastivepath-ranking, and cost-aware reinforcement learning. To facilitate rigorousevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,safety-critical, free-form CXR reasoning. Experiments across various benchmarksvalidate that PASS significantly outperforms strong baselines in multiplemetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,pushing a new paradigm shift towards interpretable, adaptive, and multimodalmedical agentic systems.</description><author>Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu</author><pubDate>Fri, 15 Aug 2025 01:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10501v2</guid></item><item><title>GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning</title><link>http://arxiv.org/abs/2508.11049v1</link><description>Recent advances have shown that video generation models can enhance robotlearning by deriving effective robot actions through inverse dynamics. However,these methods heavily depend on the quality of generated data and struggle withfine-grained manipulation due to the lack of environment feedback. Whilevideo-based reinforcement learning improves policy robustness, it remainsconstrained by the uncertainty of video generation and the challenges ofcollecting large-scale robot datasets for training diffusion models. To addressthese limitations, we propose GenFlowRL, which derives shaped rewards fromgenerated flow trained from diverse cross-embodiment datasets. This enableslearning generalizable and robust policies from diverse demonstrations usinglow-dimensional, object-centric features. Experiments on 10 manipulation tasks,both in simulation and real-world cross-embodiment evaluations, demonstratethat GenFlowRL effectively leverages manipulation features extracted fromgenerated object-centric flow, consistently achieving superior performanceacross diverse and challenging scenarios. Our Project Page:https://colinyu1.github.io/genflowrl</description><author>Kelin Yu, Sheng Zhang, Harshit Soora, Furong Huang, Heng Huang, Pratap Tokekar, Ruohan Gao</author><pubDate>Thu, 14 Aug 2025 20:19:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11049v1</guid></item><item><title>Sophisticated Learning: A novel algorithm for active learning during model-based planning</title><link>http://arxiv.org/abs/2308.08029v2</link><description>We introduce Sophisticated Learning (SL), a planning-to-learn algorithm thatembeds active parameter learning inside the Sophisticated Inference (SI)tree-search framework of Active Inference. Unlike SI -- which optimizes beliefsabout hidden states -- SL also updates beliefs about model parameters withineach simulated branch, enabling counterfactual reasoning about how futureobservations would improve subsequent planning. We compared SL with Bayes-adaptive Reinforcement Learning (BARL) agents aswell as with its parent algorithm, SI. Using a biologically inspired seasonalforaging task in which resources shift probabilistically over a 10x10 grid, wedesigned experiments that forced agents to balance probabilistic rewardharvesting against information gathering. In early trials, where rapid learning is vital, SL agents survive, onaverage, 8.2% longer than SI and 35% longer than Bayes-adaptive ReinforcementLearning. While both SL and SI showed equal convergence performance, SL reachedthis convergence 40% faster than SI. Additionally, SL showed robustout-performance of other algorithms in altered environment configurations. Our results show that incorporating active learning into multi-step planningmaterially improves decision making under radical uncertainty, and reinforcesthe broader utility of Active Inference for modeling biologically relevantbehavior.</description><author>Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith</author><pubDate>Thu, 14 Aug 2025 20:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08029v2</guid></item><item><title>Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</title><link>http://arxiv.org/abs/2508.11618v1</link><description>Carbon capture and storage (CCS) projects typically involve a diverse arrayof stakeholders or players from public, private, and regulatory sectors, eachwith different objectives and responsibilities. Given the complexity, scale,and long-term nature of CCS operations, determining whether individualstakeholders can independently maximize their interests or whethercollaborative coalition agreements are needed remains a central question foreffective CCS project planning and management. CCS projects are oftenimplemented in geologically connected sites, where shared geological featuressuch as pressure space and reservoir pore capacity can lead to competitivebehavior among stakeholders. Furthermore, CO2 storage sites are often locatedin geologically mature basins that previously served as sites for hydrocarbonextraction or wastewater disposal in order to leverage existinginfrastructures, which makes unilateral optimization even more complicated andunrealistic. In this work, we propose a paradigm based on Markov games to quantitativelyinvestigate how different coalition structures affect the goals ofstakeholders. We frame this multi-stakeholder multi-site problem as amulti-agent reinforcement learning problem with safety constraints. Ourapproach enables agents to learn optimal strategies while compliant with safetyregulations. We present an example where multiple operators are injecting CO2into their respective project areas in a geologically connected basin. Toaddress the high computational cost of repeated simulations of high-fidelitymodels, a previously developed surrogate model based on the Embed-to-Control(E2C) framework is employed. Our results demonstrate the effectiveness of theproposed framework in addressing optimal management of CO2 storage whenmultiple stakeholders with various objectives and goals are involved.</description><author>Jungang Chen, Seyyed A. Hosseini</author><pubDate>Fri, 15 Aug 2025 17:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11618v1</guid></item><item><title>CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention</title><link>http://arxiv.org/abs/2508.11016v1</link><description>Recent advances in Reinforcement Learning with Verified Reward (RLVR) havedriven the emergence of more sophisticated cognitive behaviors in largelanguage models (LLMs), thereby enhancing their reasoning capabilities.However, in prior RLVR pipelines, the repeated use of static initial-statesampling drawn exactly from the dataset distribution during each sampling phaseproduced overly deterministic, low diversity model behavior, which manifestedas rapid entropy collapse and hindered sustained performance gains duringprolonged training. To address this issue, we introduce CURE(Critical-token-gUided Re concatenation for Entropy-collapse prevention), atwo-stage framework that balances exploration and exploitation. Specifically,in the first stage, to deliberately steer the model toward novel yet coherentcontexts, we re-generate at high-entropy critical tokens and jointly optimizethe original and the branched trajectories. The further comparison with vanillaDAPO shows that the regeneration process achieves a better performance on mathreasoning tasks while sustaining a high-level entropy degree for exploration.In the second stage, we continue training with static initial-state sampling byDAPO, intentionally placing the model in a familiar state to graduallystrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,compared to other RLVR methods, CURE achieves a 5% performance gain across sixmath benchmarks, establishing state-of-the-art performance in both entropy andaccuracy. A series of experiments further validate the effectiveness of ourapproach. Code is available at https://github.com/CURE-Project/CURE.</description><author>Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang</author><pubDate>Thu, 14 Aug 2025 18:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11016v1</guid></item></channel></rss>