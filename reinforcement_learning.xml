<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivreinforcement learning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 25 Aug 2025 13:00:10 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</title><link>http://arxiv.org/abs/2508.11503v1</link><description>Reliable autonomous navigation across the unstructured terrains of distantplanetary surfaces is a critical enabler for future space exploration. However,the deployment of learning-based controllers is hindered by the inherentsim-to-real gap, particularly for the complex dynamics of wheel interactionswith granular media. This work presents a complete sim-to-real framework fordeveloping and validating robust control policies for dynamic waypoint trackingon such challenging surfaces. We leverage massively parallel simulation totrain reinforcement learning agents across a vast distribution of procedurallygenerated environments with randomized physics. These policies are thentransferred zero-shot to a physical wheeled rover operating in a lunar-analoguefacility. Our experiments systematically compare multiple reinforcementlearning algorithms and action smoothing filters to identify the most effectivecombinations for real-world deployment. Crucially, we provide strong empiricalevidence that agents trained with procedural diversity achieve superiorzero-shot performance compared to those trained on static scenarios. We alsoanalyze the trade-offs of fine-tuning with high-fidelity particle physics,which offers minor gains in low-speed precision at a significant computationalcost. Together, these contributions establish a validated workflow for creatingreliable learning-based navigation systems, marking a critical step towardsdeploying autonomous robots in the final frontier.</description><author>Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez</author><pubDate>Fri, 15 Aug 2025 14:30:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11503v1</guid></item><item><title>SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling</title><link>http://arxiv.org/abs/2508.11553v1</link><description>We introduce SeamlessFlow, a server based reinforcement learning (RL)framework that addresses two core challenges in industrial scale RL: (1)decoupling RL training from the complex execution flow of agents; (2)maximizing GPU utilization with minimal idle time while preserving thestability and scalability required for large-scale deployments. First,SeamlessFlow introduces a data plane that decouples the RL trainer fromdiverse, complex agent implementations while sustaining high throughput. Acentral trajectory manager maintains complete interaction histories andsupports partial rollout, allowing rollout to pause for weight updates andresume seamlessly, keeping agents unaware of service interruptions. Second, wepropose a tag driven scheduling paradigm that abstracts hardware intocapability tagged resources, unifying colocated and disaggregatedarchitectures. Based on this, SeamlessFlow introduces a spatiotemporalmultiplexing pipeline that dynamically reassigns idle training nodes to rolloutin a train rollout separated setup, eliminating pipeline bubbles and fullyexploiting heterogeneous cluster resources. By combining these innovations,SeamlessFlow delivers both stability and high performance, making it wellsuited for multi agent, long horizon, and other complex RL tasks.</description><author>Jinghui Wang, Shaojie Wang, Yinghan Cui, Xuxing Chen, Chao Wang, Xiaojiang Zhang, Minglei Zhang, Jiarong Zhang, Wenhao Zhuang, Yuchen Cao, Wankang Bao, Haimo Li, Zheng Lin, Huiming Wang, Haoyang Huang, Zongxian Feng, Zizheng Zhan, Ken Deng, Wen Xiang, Huaixi Tang, Kun Wu, Mengtong Li, Mengfei Xie, Junyi Peng, Haotian Zhang, Bin Chen, Bing Yu</author><pubDate>Fri, 15 Aug 2025 15:55:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11553v1</guid></item><item><title>Reinforcing Video Reasoning Segmentation to Think Before It Segments</title><link>http://arxiv.org/abs/2508.11538v1</link><description>Video reasoning segmentation (VRS) endeavors to delineate referred objects invideos guided by implicit instructions that encapsulate human intent andtemporal logic. Previous approaches leverage large vision language models(LVLMs) to encode object semantics into &lt;SEG&gt; tokens for mask prediction.However, this paradigm suffers from limited interpretability during inferenceand suboptimal performance due to inadequate spatiotemporal reasoning. Drawinginspiration from seminal breakthroughs in reinforcement learning, we introduceVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning insegmentation. Veason-R1 is trained through Group Relative Policy Optimization(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, wecurate high-quality CoT training data to instill structured reasoningtrajectories, bridging video-level semantics and frame-level spatial grounding,yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPOfine-tuning encourages efficient exploration of the reasoning space byoptimizing reasoning chains. To this end, we incorporate a holistic rewardmechanism that synergistically enhances spatial alignment and temporalconsistency, bolstering keyframe localization and fine-grained grounding.Comprehensive empirical evaluations demonstrate that Veason-R1 achievesstate-of-the-art performance on multiple benchmarks, surpassing prior art bysignificant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS),while exhibiting robustness to hallucinations (+8.8 R). Our code and modelweights will be available at Veason-R1.</description><author>Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</author><pubDate>Fri, 15 Aug 2025 15:34:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11538v1</guid></item><item><title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title><link>http://arxiv.org/abs/2503.01307v2</link><description>Test-time inference has emerged as a powerful paradigm for enabling languagemodels to ``think'' longer and more carefully about complex challenges, muchlike skilled human experts. While reinforcement learning (RL) can driveself-improvement in language models on verifiable tasks, some models exhibitsubstantial gains while others quickly plateau. For instance, we find thatQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the gameof Countdown. This discrepancy raises a critical question: what intrinsicproperties enable effective self-improvement? We introduce a framework toinvestigate this question by analyzing four key cognitive behaviors --verification, backtracking, subgoal setting, and backward chaining -- that bothexpert human problem solvers and successful language models employ. Our studyreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llamainitially lacks them. In systematic experimentation with controlled behavioraldatasets, we find that priming Llama with examples containing these reasoningbehaviors enables substantial improvements during RL, matching or exceedingQwen's performance. Importantly, the presence of reasoning behaviors, ratherthan correctness of answers, proves to be the critical factor -- models primedwith incorrect solutions containing proper reasoning patterns achievecomparable performance to those trained on correct solutions. Finally,leveraging continued pretraining with OpenWebMath data, filtered to amplifyreasoning behaviors, enables the Llama model to match Qwen's self-improvementtrajectory. Our findings establish a fundamental relationship between initialreasoning behaviors and the capacity for improvement, explaining why somelanguage models effectively utilize additional computation while othersplateau.</description><author>Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D. Goodman</author><pubDate>Fri, 15 Aug 2025 15:21:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.01307v2</guid></item><item><title>GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</title><link>http://arxiv.org/abs/2507.01006v5</link><description>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models(VLMs) designed to advance general-purpose multimodal understanding andreasoning. In this report, we share our key findings in the development of thereasoning-centric training framework. We first develop a capable visionfoundation model with significant potential through large-scale pre-training,which arguably sets the upper bound for the final performance. We then proposeReinforcement Learning with Curriculum Sampling (RLCS) to unlock the fullpotential of the model, leading to comprehensive capability enhancement acrossa diverse range of tasks, including STEM problem solving, video understanding,content recognition, coding, grounding, GUI-based agents, and long documentinterpretation. In a comprehensive evaluation across 42 public benchmarks,GLM-4.5V achieves state-of-the-art performance on nearly all tasks amongopen-source models of similar size, and demonstrates competitive or evensuperior results compared to closed-source models such as Gemini-2.5-Flash onchallenging tasks including Coding and GUI Agents. Meanwhile, the smallerGLM-4.1V-9B-Thinking remains highly competitive-achieving superior results tothe much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source bothGLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information arereleased at https://github.com/zai-org/GLM-V.</description><author>GLM-V Team, :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan</author><pubDate>Fri, 15 Aug 2025 13:23:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.01006v5</guid></item><item><title>Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory</title><link>http://arxiv.org/abs/2508.09736v2</link><description>We introduce M3-Agent, a novel multimodal agent framework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual andauditory inputs to build and update its long-term memory. Beyond episodicmemory, it also develops semantic memory, enabling it to accumulate worldknowledge over time. Its memory is organized in an entity-centric, multimodalformat, allowing deeper and more consistent understanding of the environment.Given an instruction, M3-Agent autonomously performs multi-turn, iterativereasoning and retrieves relevant information from memory to accomplish thetask. To evaluate memory effectiveness and memory-based reasoning in multimodalagents, we develop M3-Bench, a new long-video question answering benchmark.M3-Bench comprises 100 newly recorded real-world videos captured from a robot'sperspective (M3-Bench-robot) and 920 web-sourced videos across diversescenarios (M3-Bench-web). We annotate question-answer pairs designed to testkey capabilities essential for agent applications, such as human understanding,general knowledge extraction, and cross-modal reasoning. Experimental resultsshow that M3-Agent, trained via reinforcement learning, outperforms thestrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-weband VideoMME-long, respectively. Our work advances the multimodal agents towardmore human-like long-term memory and provides insights into their practicaldesign. Model, code and data are available athttps://github.com/bytedance-seed/m3-agent</description><author>Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li</author><pubDate>Fri, 15 Aug 2025 13:40:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09736v2</guid></item><item><title>Embedding Safety into RL: A New Take on Trust Region Methods</title><link>http://arxiv.org/abs/2411.02957v4</link><description>Reinforcement Learning (RL) agents can solve diverse tasks but often exhibitunsafe behavior. Constrained Markov Decision Processes (CMDPs) address this byenforcing safety constraints, yet existing methods either sacrifice rewardmaximization or allow unsafe training. We introduce Constrained Trust RegionPolicy Optimization (C-TRPO), which reshapes the policy space geometry toensure trust regions contain only safe policies, guaranteeing constraintsatisfaction throughout training. We analyze its theoretical properties andconnections to TRPO, Natural Policy Gradient (NPG), and Constrained PolicyOptimization (CPO). Experiments show that C-TRPO reduces constraint violationswhile maintaining competitive returns.</description><author>Nikola Milosevic, Johannes Müller, Nico Scherf</author><pubDate>Fri, 15 Aug 2025 12:29:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2411.02957v4</guid></item><item><title>Exploring Superior Function Calls via Reinforcement Learning</title><link>http://arxiv.org/abs/2508.05118v3</link><description>Function calling capabilities are crucial for deploying Large Language Modelsin real-world applications, yet current training approaches fail to developrobust reasoning strategies. Supervised fine-tuning produces models that relyon superficial pattern matching, while standard reinforcement learning methodsstruggle with the complex action space of structured function calls. We presenta novel reinforcement learning framework designed to enhance group relativepolicy optimization through strategic entropy based exploration specificallytailored for function calling tasks. Our approach addresses three criticalchallenges in function calling: insufficient exploration during policylearning, lack of structured reasoning in chain-of-thought generation, andinadequate verification of parameter extraction. Our two-stage data preparationpipeline ensures high-quality training samples through iterative LLM evaluationand abstract syntax tree validation. Extensive experiments on the BerkeleyFunction Calling Leaderboard demonstrate that this framework achievesstate-of-the-art performance among open-source models with 86.02\% overallaccuracy, outperforming standard GRPO by up to 6\% on complex multi-functionscenarios. Notably, our method shows particularly strong improvements oncode-pretrained models, suggesting that structured language generationcapabilities provide an advantageous starting point for reinforcement learningin function calling tasks. We will release all the code, models and dataset tobenefit the community.</description><author>Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</author><pubDate>Fri, 15 Aug 2025 12:14:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.05118v3</guid></item><item><title>On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting</title><link>http://arxiv.org/abs/2508.11408v1</link><description>Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are twoprominent post-training paradigms for refining the capabilities and aligningthe behavior of Large Language Models (LLMs). Existing approaches thatintegrate SFT and RL often face the risk of disrupting established modelpatterns and inducing overfitting to expert data. To address this, we present anovel investigation into the unified view of SFT and RL through an off-policyversus on-policy lens. We propose CHORD, a framework for the ControllableHarmonization of On- and Off-Policy Reinforcement Learning via DynamicWeighting, which reframes SFT not as a separate stage but as a dynamicallyweighted auxiliary objective within the on-policy RL process. Based on ananalysis of off-policy expert data's influence at both holistic and granularlevels, we incorporate a dual-control mechanism in CHORD. Specifically, theframework first employs a global coefficient to holistically guide thetransition from off-policy imitation to on-policy exploration, and then appliesa token-wise weighting function that enables granular learning from experttokens, which preserves on-policy exploration and mitigates disruption fromoff-policy data. We conduct extensive experiments on widely used benchmarks,providing empirical evidence that CHORD achieves a stable and efficientlearning process. By effectively harmonizing off-policy expert data withon-policy exploration, CHORD demonstrates significant improvements overbaselines. We release the implementation athttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord toinspire further research.</description><author>Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou</author><pubDate>Fri, 15 Aug 2025 11:20:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11408v1</guid></item><item><title>Sketch Decompositions for Classical Planning via Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2412.08574v2</link><description>In planning and reinforcement learning, the identification of common subgoalstructures across problems is important when goals are to be achieved over longhorizons. Recently, it has been shown that such structures can be expressed asfeature-based rules, called sketches, over a number of classical planningdomains. These sketches split problems into subproblems which then becomesolvable in low polynomial time by a greedy sequence of IW$(k)$ searches.Methods for learning sketches using feature pools and min-SAT solvers have beendeveloped, yet they face two key limitations: scalability and expressivity. Inthis work, we address these limitations by formulating the problem of learningsketch decompositions as a deep reinforcement learning (DRL) task, wheregeneral policies are sought in a modified planning problem where the successorstates of a state s are defined as those reachable from s through an IW$(k)$search. The sketch decompositions obtained through this method areexperimentally evaluated across various domains, and problems are regarded assolved by the decomposition when the goal is reached through a greedy sequenceof IW$(k)$ searches. While our DRL approach for learning sketch decompositionsdoes not yield interpretable sketches in the form of rules, we demonstrate thatthe resulting decompositions can often be understood in a crisp manner.</description><author>Michael Aichmüller, Hector Geffner</author><pubDate>Fri, 15 Aug 2025 10:24:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2412.08574v2</guid></item><item><title>Fusing Rewards and Preferences in Reinforcement Learning</title><link>http://arxiv.org/abs/2508.11363v1</link><description>We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm thatfuses both individual rewards and pairwise preferences (if available) into asingle update rule. DFA uses the policy's log-probabilities directly to modelthe preference probability, avoiding a separate reward-modeling step.Preferences can be provided by human-annotators (at state-level ortrajectory-level) or be synthesized online from Q-values stored in anoff-policy replay buffer. Under a Bradley-Terry model, we prove that minimizingDFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)policy. Our simulation results show that DFA trained on generated preferencesmatches or exceeds SAC on six control environments and demonstrates a morestable training process. With only a semi-synthetic preference dataset underBradley-Terry model, our algorithm outperforms reward-modeling reinforcementlearning from human feedback (RLHF) baselines in a stochastic GridWorld andapproaches the performance of an oracle with true rewards.</description><author>Sadegh Khorasani, Saber Salehkaleybar, Negar Kiyavash, Matthias Grossglauser</author><pubDate>Fri, 15 Aug 2025 09:56:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11363v1</guid></item><item><title>CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</title><link>http://arxiv.org/abs/2508.11360v1</link><description>As autonomous agents become adept at understanding and interacting withgraphical user interface (GUI) environments, a new era of automated taskexecution is emerging. Recent studies have demonstrated that ReinforcementLearning (RL) can effectively enhance agents' performance in dynamicinteractive GUI environments. However, these methods face two key limitations:(1) they overlook the significant variation in difficulty across different GUItasks by treating the entire training data as a uniform set, which hampers theagent's ability to adapt its learning process; and (2) most approaches collapsetask-specific nuances into a single, coarse reward, leaving the agent with auniform signal that yields inefficient policy updates. To address theselimitations, we propose CRAFT-GUI, a curriculum learning framework based onGroup Relative Policy Optimization (GRPO) that explicitly accounts for thevarying difficulty across trajectories. To enable more fine-grained policyoptimization, we design a reward function that combines simple rule-basedsignals with model-judged evaluation, providing richer and more nuancedfeedback during training. Experimental results demonstrate that our methodachieves significant improvements over previous state-of-the-art approaches,outperforming them by 5.6% on public benchmarks Android Control and 10.3% onour internal online benchmarks, respectively. These findings empiricallyvalidate the effectiveness of integrating reinforcement learning withcurriculum learning in GUI interaction tasks.</description><author>Songqin Nong, Jingxuan Xu, Sheng Zhou, Jianfeng Chen, Xiaoxuan Tang, Tao Jiang, Wenhao Xu</author><pubDate>Fri, 15 Aug 2025 09:55:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11360v1</guid></item><item><title>ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism</title><link>http://arxiv.org/abs/2508.11356v1</link><description>Recent advancements in Large Language Models have yielded significantimprovements in complex reasoning tasks such as mathematics and programming.However, these models remain heavily dependent on annotated data and exhibitlimited adaptability in unsupervised scenarios. To address these limitations,test-time reinforcement learning (TTRL) has been proposed, which enablesself-optimization by leveraging model-generated pseudo-labels. Despite itspromise, TTRL faces several key challenges, including high inference costs dueto parallel rollouts and early-stage estimation bias that fostersoverconfidence, reducing output diversity and causing performance plateaus. Toaddress these challenges, we introduce an entropy-based mechanism to enhancethe exploration-exploitation balance in test-time reinforcement learningthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) andEntropy-based Advantage Reshaping (EAR). Compared with the baseline, ourapproach enables Llama3.1-8B to achieve a 68 percent relative improvement inPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent ofthe rollout tokens budget. This highlights our method's ability to effectivelyoptimize the trade-off between inference efficiency, diversity, and estimationrobustness, thereby advancing unsupervised reinforcement learning foropen-domain reasoning tasks.</description><author>Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, ShaoGuo Liu, TingTing Gao</author><pubDate>Fri, 15 Aug 2025 09:49:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11356v1</guid></item><item><title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title><link>http://arxiv.org/abs/2506.07468v2</link><description>Conventional language model (LM) safety alignment relies on a reactive,disjoint procedure: attackers exploit a static model, followed by defensivefine-tuning to patch exposed vulnerabilities. This sequential approach createsa mismatch -- attackers overfit to obsolete defenses, while defendersperpetually lag behind emerging threats. To address this, we proposeSelf-RedTeam, an online self-play reinforcement learning algorithm where anattacker and defender agent co-evolve through continuous interaction. We castsafety alignment as a two-player zero-sum game, where a single model alternatesbetween attacker and defender roles -- generating adversarial prompts andsafeguarding against them -- while a reward LM adjudicates outcomes. Thisenables dynamic co-adaptation. Grounded in the game-theoretic framework ofzero-sum games, we establish a theoretical safety guarantee which motivates thedesign of our method: if self-play converges to a Nash Equilibrium, thedefender will reliably produce safe responses to any adversarial input.Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) comparedto attackers trained against static defenders and achieves higher robustness onsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trainedagainst static attackers. We further propose hidden Chain-of-Thought, allowingagents to plan privately, which boosts adversarial diversity and reducesover-refusals. Our results motivate a shift from reactive patching to proactiveco-evolution in LM safety training, enabling scalable, autonomous, and robustself-improvement of LMs via multi-agent reinforcement learning (MARL).</description><author>Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</author><pubDate>Fri, 15 Aug 2025 09:30:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.07468v2</guid></item><item><title>HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model</title><link>http://arxiv.org/abs/2508.11350v1</link><description>Understanding and recognizing human-object interaction (HOI) is a pivotalapplication in AR/VR and robotics. Recent open-vocabulary HOI detectionapproaches depend exclusively on large language models for richer textualprompts, neglecting their inherent 3D spatial understanding capabilities. Toaddress this shortcoming, we introduce HOID-R1, the first HOI detectionframework that integrates chain-of-thought (CoT) guided supervised fine-tuning(SFT) with group relative policy optimization (GRPO) within a reinforcementlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the modelwith essential reasoning capabilities, forcing the model to articulate itsthought process in the output. Subsequently, we integrate GRPO to leveragemulti-reward signals for policy optimization, thereby enhancing alignmentacross diverse modalities. To mitigate hallucinations in the CoT reasoning, weintroduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,further improving generalization. Extensive experiments show that HOID-R1achieves state-of-the-art performance on HOI detection benchmarks andoutperforms existing methods in open-world generalization to novel scenarios.</description><author>Zhenhao Zhang, Hanqing Wang, Xiangyu Zeng, Ziyu Cheng, Jiaxin Liu, Haoyu Yan, Zhirui Liu, Kaiyang Ji, Tianxiang Gui, Ke Hu, Kangyi Chen, Yahao Fan, Mokai Pan</author><pubDate>Fri, 15 Aug 2025 09:28:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11350v1</guid></item><item><title>IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model</title><link>http://arxiv.org/abs/2508.06571v3</link><description>Vision-Language-Action (VLA) models have demonstrated potential in autonomousdriving. However, two critical challenges hinder their development: (1)Existing VLA architectures are typically based on imitation learning inopen-loop setup which tends to capture the recorded behaviors in the dataset,leading to suboptimal and constrained performance, (2) Close-loop trainingrelies heavily on high-fidelity sensor simulation, where domain gaps andcomputational inefficiencies pose significant barriers. In this paper, weintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world modelwith a self-built VLA approach. Our framework proceeds in a three-stageparadigm: In the first stage, we propose a VLA architecture and pretrain theVLA policy via imitation learning. In the second stage, we construct alightweight reward world model via inverse reinforcement learning to enableefficient close-loop reward computation. To further enhance planningperformance, finally, we design specialized reward world model guidencereinforcement learning via PPO(Proximal Policy Optimization) to effectivelybalance the safety incidents, comfortable driving, and traffic efficiency. Ourapproach achieves state-of-the-art performance in NAVSIM v2 end-to-end drivingbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope thatour framework will accelerate VLA research in close-loop autonomous driving.</description><author>Anqing Jiang, Yu Gao, Yiru Wang, Zhigang Sun, Shuo Wang, Yuwen Heng, Hao Sun, Shichen Tang, Lijuan Zhu, Jinhao Chai, Jijun Wang, Zichong Gu, Hao Jiang, Li Sun</author><pubDate>Fri, 15 Aug 2025 05:19:30 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.06571v3</guid></item><item><title>Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation</title><link>http://arxiv.org/abs/2508.11204v1</link><description>Sampling efficiency is critical for deploying visuomotor learning inreal-world robotic manipulation. While task symmetry has emerged as a promisinginductive bias to improve efficiency, most prior work is limited to isometricsymmetries -- applying the same group transformation to all task objects acrossall timesteps. In this work, we explore non-isometric symmetries, applyingmultiple independent group transformations across spatial and temporaldimensions to relax these constraints. We introduce a novel formulation of thepartially observable Markov decision process (POMDP) that incorporates thenon-isometric symmetry structures, and propose a simple yet effective dataaugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrateMEA with offline reinforcement learning to enhance sampling efficiency, andintroduce a voxel-based visual representation that preserves translationalequivariance. Extensive simulation and real-robot experiments across twomanipulation domains demonstrate the effectiveness of our approach.</description><author>Hongbin Lin, Juan Rojas, Kwok Wai Samuel Au</author><pubDate>Fri, 15 Aug 2025 04:30:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11204v1</guid></item><item><title>RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System</title><link>http://arxiv.org/abs/2508.09186v2</link><description>The proliferation of AI-powered cameras in Intelligent Transportation Systems(ITS) creates a severe conflict between the need for rich visual data and theright to privacy. Existing privacy-preserving methods, such as blurring orencryption, are often insufficient due to creating an undesirable trade-offwhere either privacy is compromised against advanced reconstruction attacks ordata utility is critically degraded. To resolve this challenge, we proposeRL-MoE, a novel framework that transforms sensitive visual data intoprivacy-preserving textual descriptions, eliminating the need for direct imagetransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecturefor nuanced, multi-aspect scene decomposition with a Reinforcement Learning(RL) agent that optimizes the generated text for a dual objective of semanticaccuracy and privacy preservation. Extensive experiments demonstrate thatRL-MoE provides superior privacy protection, reducing the success rate ofreplay attacks to just 9.4\% on the CFP-FP dataset, while simultaneouslygenerating richer textual content than baseline methods. Our work provides apractical and scalable solution for building trustworthy AI systems inprivacy-sensitive domains, paving the way for more secure smart city andautonomous vehicle networks.</description><author>Abdolazim Rezaei, Mehdi Sookhak, Mahboobeh Haghparast</author><pubDate>Fri, 15 Aug 2025 04:36:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.09186v2</guid></item><item><title>UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</title><link>http://arxiv.org/abs/2508.11196v1</link><description>Recent advances in vision-language models (VLMs) have demonstrated stronggeneralization in natural image tasks. However, their performance oftendegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which featureshigh resolution, complex spatial semantics, and strict real-time constraints.These challenges limit the applicability of general-purpose VLMs to structuredaerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, alightweight VLM explicitly designed for aerial visual reasoning. It is trainedusing a hybrid method that combines supervised fine-tuning (SFT) andmulti-stage reinforcement learning (RL). We leverage the group relative policyoptimization (GRPO) algorithm to promote structured and interpretable reasoningthrough rule-guided rewards and intra-group policy alignment. To support modeltraining and evaluation, we introduce a high-resolution visual questionanswering dataset named HRVQA-VL, which consists of 50,019 annotated samplescovering eight UAV-relevant reasoning tasks, including object counting,transportation recognition, and spatial scene inference. Experimental resultsshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than theQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, whichis 36x larger, on multiple tasks. Ablation studies reveal that while SFTimproves semantic alignment, it may reduce reasoning diversity in mathematicaltasks. GRPO-based RL compensates for this limitation by enhancing logicalflexibility and the robustness of inference. Additionally, UAV-VL-R1 requiresonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB withINT8, supporting real-time deployment on resource-constrained UAV platforms.</description><author>Jiajin Guan, Haibo Mei, Bonan Zhang, Dan Liu, Yuanshuang Fu, Yue Zhang</author><pubDate>Fri, 15 Aug 2025 04:06:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11196v1</guid></item><item><title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title><link>http://arxiv.org/abs/2508.00344v2</link><description>Large Language Models (LLMs) have shown remarkable advancements in tacklingagent-oriented tasks. Despite their potential, existing work faces challengeswhen deploying LLMs in agent-based environments. The widely adopted agentparadigm ReAct centers on integrating single-step reasoning with immediateaction execution, which limits its effectiveness in complex tasks requiringlong-term strategic planning. Furthermore, the coordination between the plannerand executor during problem-solving is also a critical factor to consider inagent design. Additionally, current approaches predominantly rely on supervisedfine-tuning, which often leads models to memorize established task completiontrajectories, thereby restricting their generalization ability when confrontedwith novel problem contexts. To address these challenges, we introduce anadaptive global plan-based agent paradigm AdaPlan, aiming to synergizehigh-level explicit guidance with execution to support effective long-horizondecision-making. Based on the proposed paradigm, we further put forwardPilotRL, a global planning-guided training framework for LLM agents driven byprogressive reinforcement learning. We first develop the model's ability tofollow explicit guidance from global plans when addressing agent tasks.Subsequently, based on this foundation, we focus on optimizing the quality ofgenerated plans. Finally, we conduct joint optimization of the model's planningand execution coordination. Experiments indicate that PilotRL could achievestate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassingclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%comparing to GPT-4o-mini at a comparable parameter scale.</description><author>Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang</author><pubDate>Fri, 15 Aug 2025 03:07:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.00344v2</guid></item><item><title>Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward</title><link>http://arxiv.org/abs/2508.11143v1</link><description>Existing reinforcement learning (RL) methods struggle with long-horizonrobotic manipulation tasks, particularly those involving sparse rewards. Whileaction chunking is a promising paradigm for robotic manipulation, using RL todirectly learn continuous action chunks in a stable and data-efficient mannerremains a critical challenge. This paper introduces AC3 (Actor-Critic forContinuous Chunks), a novel RL framework that learns to generatehigh-dimensional, continuous action sequences. To make this learning processstable and data-efficient, AC3 incorporates targeted stabilization mechanismsfor both the actor and the critic. First, to ensure reliable policyimprovement, the actor is trained with an asymmetric update rule, learningexclusively from successful trajectories. Second, to enable effective valuelearning despite sparse rewards, the critic's update is stabilized usingintra-chunk $n$-step returns and further enriched by a self-supervised moduleproviding intrinsic rewards at anchor points aligned with each action chunk. Weconducted extensive experiments on 25 tasks from the BiGym and RLBenchbenchmarks. Results show that by using only a few demonstrations and a simplemodel architecture, AC3 achieves superior success rates on most tasks,validating its effective design.</description><author>Jiarui Yang, Bin Zhu, Jingjing Chen, Yu-Gang Jiang</author><pubDate>Fri, 15 Aug 2025 01:27:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11143v1</guid></item><item><title>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning</title><link>http://arxiv.org/abs/2508.10501v2</link><description>Existing tool-augmented agentic systems are limited in the real world by (i)black-box reasoning steps that undermine trust of decision-making and posesafety risks, (ii) poor multimodal integration, which is inherently criticalfor healthcare tasks, and (iii) rigid and computationally inefficient agenticpipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), thefirst multimodal framework to address these challenges in the context of ChestX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over amulti-tool graph, yielding decision paths annotated with interpretableprobabilities. Given the complex CXR reasoning task with multimodal medicaldata, PASS leverages its learned task-conditioned distribution over the agenticsupernet. Thus, it adaptively selects the most suitable tool at each supernetlayer, offering probability-annotated trajectories for post-hoc audits anddirectly enhancing medical AI safety. PASS also continuously compresses salientfindings into an evolving personalized memory, while dynamically decidingwhether to deepen its reasoning path or invoke an early exit for efficiency. Tooptimize a Pareto frontier balancing performance and cost, we design a novelthree-stage training procedure, including expert knowledge warm-up, contrastivepath-ranking, and cost-aware reinforcement learning. To facilitate rigorousevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,safety-critical, free-form CXR reasoning. Experiments across various benchmarksvalidate that PASS significantly outperforms strong baselines in multiplemetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,pushing a new paradigm shift towards interpretable, adaptive, and multimodalmedical agentic systems.</description><author>Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu</author><pubDate>Fri, 15 Aug 2025 01:18:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10501v2</guid></item><item><title>GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning</title><link>http://arxiv.org/abs/2508.11049v1</link><description>Recent advances have shown that video generation models can enhance robotlearning by deriving effective robot actions through inverse dynamics. However,these methods heavily depend on the quality of generated data and struggle withfine-grained manipulation due to the lack of environment feedback. Whilevideo-based reinforcement learning improves policy robustness, it remainsconstrained by the uncertainty of video generation and the challenges ofcollecting large-scale robot datasets for training diffusion models. To addressthese limitations, we propose GenFlowRL, which derives shaped rewards fromgenerated flow trained from diverse cross-embodiment datasets. This enableslearning generalizable and robust policies from diverse demonstrations usinglow-dimensional, object-centric features. Experiments on 10 manipulation tasks,both in simulation and real-world cross-embodiment evaluations, demonstratethat GenFlowRL effectively leverages manipulation features extracted fromgenerated object-centric flow, consistently achieving superior performanceacross diverse and challenging scenarios. Our Project Page:https://colinyu1.github.io/genflowrl</description><author>Kelin Yu, Sheng Zhang, Harshit Soora, Furong Huang, Heng Huang, Pratap Tokekar, Ruohan Gao</author><pubDate>Thu, 14 Aug 2025 20:19:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11049v1</guid></item><item><title>Sophisticated Learning: A novel algorithm for active learning during model-based planning</title><link>http://arxiv.org/abs/2308.08029v2</link><description>We introduce Sophisticated Learning (SL), a planning-to-learn algorithm thatembeds active parameter learning inside the Sophisticated Inference (SI)tree-search framework of Active Inference. Unlike SI -- which optimizes beliefsabout hidden states -- SL also updates beliefs about model parameters withineach simulated branch, enabling counterfactual reasoning about how futureobservations would improve subsequent planning. We compared SL with Bayes-adaptive Reinforcement Learning (BARL) agents aswell as with its parent algorithm, SI. Using a biologically inspired seasonalforaging task in which resources shift probabilistically over a 10x10 grid, wedesigned experiments that forced agents to balance probabilistic rewardharvesting against information gathering. In early trials, where rapid learning is vital, SL agents survive, onaverage, 8.2% longer than SI and 35% longer than Bayes-adaptive ReinforcementLearning. While both SL and SI showed equal convergence performance, SL reachedthis convergence 40% faster than SI. Additionally, SL showed robustout-performance of other algorithms in altered environment configurations. Our results show that incorporating active learning into multi-step planningmaterially improves decision making under radical uncertainty, and reinforcesthe broader utility of Active Inference for modeling biologically relevantbehavior.</description><author>Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith</author><pubDate>Thu, 14 Aug 2025 20:04:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.08029v2</guid></item><item><title>CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention</title><link>http://arxiv.org/abs/2508.11016v1</link><description>Recent advances in Reinforcement Learning with Verified Reward (RLVR) havedriven the emergence of more sophisticated cognitive behaviors in largelanguage models (LLMs), thereby enhancing their reasoning capabilities.However, in prior RLVR pipelines, the repeated use of static initial-statesampling drawn exactly from the dataset distribution during each sampling phaseproduced overly deterministic, low diversity model behavior, which manifestedas rapid entropy collapse and hindered sustained performance gains duringprolonged training. To address this issue, we introduce CURE(Critical-token-gUided Re concatenation for Entropy-collapse prevention), atwo-stage framework that balances exploration and exploitation. Specifically,in the first stage, to deliberately steer the model toward novel yet coherentcontexts, we re-generate at high-entropy critical tokens and jointly optimizethe original and the branched trajectories. The further comparison with vanillaDAPO shows that the regeneration process achieves a better performance on mathreasoning tasks while sustaining a high-level entropy degree for exploration.In the second stage, we continue training with static initial-state sampling byDAPO, intentionally placing the model in a familiar state to graduallystrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,compared to other RLVR methods, CURE achieves a 5% performance gain across sixmath benchmarks, establishing state-of-the-art performance in both entropy andaccuracy. A series of experiments further validate the effectiveness of ourapproach. Code is available at https://github.com/CURE-Project/CURE.</description><author>Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang</author><pubDate>Thu, 14 Aug 2025 18:40:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11016v1</guid></item><item><title>MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models</title><link>http://arxiv.org/abs/2508.13148v1</link><description>Diffusion language models, as a promising alternative to traditionalautoregressive (AR) models, enable faster generation and richer conditioning onbidirectional context. However, they suffer from a key discrepancy betweentraining and inference: during inference, MDLMs progressively reveal thestructure of the generated sequence by producing fewer and fewer masked tokens,whereas this structure is ignored in training as tokens are masked at random.Although this discrepancy between training and inference can lead to suboptimalperformance, it has been largely overlooked by previous works, leaving closingthis gap between the two stages an open problem. To address this, we frame theproblem of learning effective denoising trajectories as a sequentialdecision-making problem and use the resulting framework to apply reinforcementlearning. We propose a novel Masked Diffusion Policy Optimization (MDPO) toexploit the Markov property diffusion possesses and explicitly train the modelunder the same progressive refining schedule used at inference. MDPO matchesthe performance of the previous state-of-the-art (SOTA) method with 60x fewergradient updates, while achieving average improvements of 9.6% on MATH500 and54.2% on Countdown over SOTA when trained within the same number of weightupdates. Additionally, we improve the remasking strategy of MDLMs as a plug-ininference replacement to overcome the limitation that the model cannot refinetokens flexibly. This simple yet effective training-free strategy, what werefer to as RCR, consistently improves performance and yields additional gainswhen combined with MDPO. Our findings establish great potential forinvestigating the discrepancy between pre-training and inference of MDLMs.Code: https://github.com/autonomousvision/mdpo. Project Page:https://cli212.github.io/MDPO/.</description><author>Haoyu He, Katrin Renz, Yong Cao, Andreas Geiger</author><pubDate>Mon, 18 Aug 2025 17:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13148v1</guid></item><item><title>Improving Detection of Watermarked Language Models</title><link>http://arxiv.org/abs/2508.13131v1</link><description>Watermarking has recently emerged as an effective strategy for detecting thegenerations of large language models (LLMs). The strength of a watermarktypically depends strongly on the entropy afforded by the language model andthe set of input prompts. However, entropy can be quite limited in practice,especially for models that are post-trained, for example via instruction tuningor reinforcement learning from human feedback (RLHF), which makes detectionbased on watermarking alone challenging. In this work, we investigate whetherdetection can be improved by combining watermark detectors with non-watermarkones. We explore a number of hybrid schemes that combine the two, observingperformance gains over either class of detector under a wide range ofexperimental conditions.</description><author>Dara Bahri, John Wieting</author><pubDate>Mon, 18 Aug 2025 17:43:06 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13131v1</guid></item><item><title>LLMs Are In-Context Bandit Reinforcement Learners</title><link>http://arxiv.org/abs/2410.05362v3</link><description>Large Language Models (LLMs) excel at in-context learning (ICL), a supervisedlearning technique that relies on adding annotated examples to the modelcontext. We investigate a contextual bandit version of in-context reinforcementlearning (ICRL), where models learn in-context, online, from external reward,instead of supervised data. We show that LLMs effectively demonstrate suchlearning, and provide a detailed study of the phenomena, experimenting withchallenging classification tasks and models of sizes from 500M to 70Bparameters. This includes identifying and addressing the instability of theprocess, demonstrating learning with both semantic and abstract labels, andshowing scaling trends. Our findings highlight ICRL capabilities in LLMs, whilealso underscoring fundamental limitations in their implicit reasoning abouterrors.</description><author>Giovanni Monea, Antoine Bosselut, Kianté Brantley, Yoav Artzi</author><pubDate>Mon, 18 Aug 2025 16:38:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.05362v3</guid></item><item><title>CaRL: Learning Scalable Planning Policies with Simple Rewards</title><link>http://arxiv.org/abs/2504.17838v2</link><description>We investigate reinforcement learning (RL) for privileged planning inautonomous driving. State-of-the-art approaches for this task are rule-based,but these methods do not scale to the long tail. RL, on the other hand, isscalable and does not suffer from compounding errors like imitation learning.Contemporary RL approaches for driving use complex shaped rewards that summultiple individual rewards, \eg~progress, position, or orientation rewards. Weshow that PPO fails to optimize a popular version of these rewards when themini-batch size is increased, which limits the scalability of these approaches.Instead, we propose a new reward design based primarily on optimizing a singleintuitive reward term: route completion. Infractions are penalized byterminating the episode or multiplicatively reducing route completion. We findthat PPO scales well with higher mini-batch sizes when trained with our simplereward, even improving performance. Training with large mini-batch sizesenables efficient scaling via distributed data parallelism. We scale PPO to300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. Theresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,outperforming other RL methods with more complex rewards by a large margin.Requiring only minimal adaptations from its use in CARLA, the same method isthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and90.6 in reactive traffic on the Val14 benchmark while being an order ofmagnitude faster than prior work.</description><author>Bernhard Jaeger, Daniel Dauner, Jens Beißwenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger</author><pubDate>Mon, 18 Aug 2025 16:46:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17838v2</guid></item><item><title>Reinforced Context Order Recovery for Adaptive Reasoning and Planning</title><link>http://arxiv.org/abs/2508.13070v1</link><description>Modern causal language models, followed by rapid developments in discretediffusion models, can now produce a wide variety of interesting and usefulcontent. However, these families of models are predominantly trained to outputtokens with a fixed (left-to-right) or random order, which may deviate from thelogical order in which tokens are generated originally. In this paper, weobserve that current causal and diffusion models encounter difficulties inproblems that require adaptive token generation orders to solve tractably,which we characterize with the $\mathcal{V}$-information framework. Motivatedby this, we propose Reinforced Context Order Recovery (ReCOR), areinforcement-learning-based framework to extract adaptive, data-dependenttoken generation orders from text data without annotations. Self-supervised bytoken prediction statistics, ReCOR estimates the hardness of predicting everyunfilled token and adaptively selects the next token during both training andinference. Experiments on challenging reasoning and planning datasetsdemonstrate the superior performance of ReCOR compared with baselines,sometimes outperforming oracle models supervised with the ground-truth order.</description><author>Long Ma, Fangwei Zhong, Yizhou Wang</author><pubDate>Mon, 18 Aug 2025 16:42:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13070v1</guid></item><item><title>Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</title><link>http://arxiv.org/abs/2508.13037v1</link><description>Recent studies have demonstrated that Large Language Models (LLMs) havestrong mathematical reasoning abilities but rely on hundreds of billions ofparameters. To tackle the challenge of poor reasoning in Small Language Models(SLMs), existing methods typically leverage LLMs to generate massive amounts ofdata for cramming training. In psychology, they are akin to System 1 thinking,which resolves reasoning problems rapidly based on experience and intuition.However, human learning also requires System 2 thinking, where knowledge isfirst acquired and then reinforced through practice. Inspired by such twodistinct modes of thinking, we propose a novel method based on the multi-LoRAInteraction for mathematical reasoning Distillation (LoRID). First, we inputthe question and reasoning of each sample into an LLM to createknowledge-enhanced datasets. Subsequently, we train a LoRA block on the studentmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughtsfor problem-solving. Then, to imitate System 2 thinking, we train the KnowledgeGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs onlyknowledge after receiving problems, while the latter uses that knowledge toperform reasoning. Finally, to address the randomness in the generation of IRand DR, we evaluate whether their outputs are consistent, and the inferenceprocess needs to be iterated if not. This step can enhance the mathematicalreasoning ability of SLMs through mutual feedback. Experimental results showthat LoRID achieves state-of-the-art performance, especially on the GSM8Kdataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,12.3%, and 1.8% accuracy across the five base models, respectively.</description><author>Xinhe Li, Jiajun Liu, Peng Wang</author><pubDate>Mon, 18 Aug 2025 15:56:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13037v1</guid></item><item><title>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</title><link>http://arxiv.org/abs/2508.13023v1</link><description>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhancedthe reasoning abilities of large language models (LLMs). Its success, however,largely depends on strong base models with rich world knowledge, yielding onlymodest improvements for small-size language models (SLMs). To address thislimitation, we investigate Guided GRPO, which injects ground-truth reasoningsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.Through a comprehensive study of various guidance configurations, we find thatnaively adding guidance delivers limited gains. These insights motivateG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strengthin response to the model's evolving training dynamics. Experiments onmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-Asubstantially outperforms vanilla GRPO. Our code and models are available athttps://github.com/T-Lab-CUHKSZ/G2RPO-A.</description><author>Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</author><pubDate>Mon, 18 Aug 2025 15:41:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.13023v1</guid></item><item><title>An MRP Formulation for Supervised Learning: Generalized Temporal Difference Learning Models</title><link>http://arxiv.org/abs/2404.15518v4</link><description>In traditional statistical learning, data points are usually assumed to beindependently and identically distributed (i.i.d.) following an unknownprobability distribution. This paper presents a contrasting viewpoint,perceiving data points as interconnected and employing a Markov reward process(MRP) for data modeling. We reformulate the typical supervised learning as anon-policy policy evaluation problem within reinforcement learning (RL),introducing a generalized temporal difference (TD) learning algorithm as aresolution. Theoretically, our analysis establishes connections between thesolutions of linear TD learning and ordinary least squares (OLS). Underspecific conditions -- particularly when the noise is correlated -- the TDsolution serves as a more effective estimator than OLS. Furthermore, we showthat when our algorithm is applied with many commonly used loss functions --such as those found in generalized linear models -- it corresponds to theapplication of a novel and generalized Bellman operator. We prove that thisoperator admits a unique fixed point, and based on this, we establishconvergence guarantees for our generalized TD algorithm under linear functionapproximation. Empirical studies verify our theoretical results, examine thevital design of our TD algorithm and show practical utility across variousdatasets, encompassing tasks such as regression and image classification withdeep learning.</description><author>Yangchen Pan, Junfeng Wen, Chenjun Xiao, Philip Torr</author><pubDate>Mon, 18 Aug 2025 15:20:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.15518v4</guid></item><item><title>Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</title><link>http://arxiv.org/abs/2508.15766v1</link><description>Recent efforts have extended the capabilities of transformers in logicalreasoning and symbolic computations. In this work, we investigate theircapacity for non-linear latent pattern discovery in the context of functionaldecomposition, focusing on the challenging algebraic task of multivariatepolynomial decomposition. This problem, with widespread applications in scienceand engineering, is proved to be NP-hard, and demands both precision andinsight. Our contributions are threefold: First, we develop a synthetic datageneration pipeline providing fine-grained control over problem complexity.Second, we train transformer models via supervised learning and evaluate themacross four key dimensions involving scaling behavior and generalizability.Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), arank-aware reinforcement learning method suitable for hard algebraic problems.Finetuning with BGRPO improves accuracy while reducing beam width by up tohalf, resulting in approximately 75% lower inference compute. Additionally, ourmodel demonstrates competitive performance in polynomial simplification,outperforming Mathematica in various cases.</description><author>Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU</author><pubDate>Thu, 21 Aug 2025 17:58:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15766v1</guid></item><item><title>Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</title><link>http://arxiv.org/abs/2508.15764v1</link><description>We address the problem of detecting adversarial attacks against cooperativemulti-agent reinforcement learning with continuous action space. We propose adecentralized detector that relies solely on the local observations of theagents and makes use of a statistical characterization of the normal behaviorof observable agents. The proposed detector utilizes deep neural networks toapproximate the normal behavior of agents as parametric multivariate Gaussiandistributions. Based on the predicted density functions, we define a normalityscore and provide a characterization of its mean and variance. Thischaracterization allows us to employ a two-sided CUSUM procedure for detectingdeviations of the normality score from its mean, serving as a detector ofanomalous behavior in real-time. We evaluate our scheme on various multi-agentPettingZoo benchmarks against different state-of-the-art attack methods, andour results demonstrate the effectiveness of our method in detecting impactfuladversarial attacks. Particularly, it outperforms the discrete counterpart byachieving AUC-ROC scores of over 0.95 against the most impactful attacks in allevaluated environments.</description><author>Kiarash Kazari, Ezzeldin Shereen, György Dán</author><pubDate>Thu, 21 Aug 2025 17:58:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15764v1</guid></item><item><title>Intern-S1: A Scientific Multimodal Foundation Model</title><link>http://arxiv.org/abs/2508.15763v1</link><description>In recent years, a plethora of open-source foundation models have emerged,achieving remarkable progress in some widely attended fields, with performancebeing quite close to that of closed-source models. However, in high-value butmore challenging scientific professional fields, either the fields still relyon expert models, or the progress of general foundation models lagssignificantly compared to those in popular areas, far from sufficient fortransforming scientific research and leaving substantial gap betweenopen-source models and closed-source models in these scientific domains. Tomitigate this gap and explore a step further toward Artificial GeneralIntelligence (AGI), we introduce Intern-S1, a specialized generalist equippedwith general understanding and reasoning capabilities with expertise to analyzemultiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)model with 28 billion activated parameters and 241 billion total parameters,continually pre-trained on 5T tokens, including over 2.5T tokens fromscientific domains. In the post-training stage, Intern-S1 undergoes offline andthen online reinforcement learning (RL) in InternBootCamp, where we proposeMixture-of-Rewards (MoR) to synergize the RL training on more than 1000 taskssimultaneously. Through integrated innovations in algorithms, data, andtraining systems, Intern-S1 achieved top-tier performance in online RLtraining.On comprehensive evaluation benchmarks, Intern-S1 demonstratescompetitive performance on general reasoning tasks among open-source models andsignificantly outperforms open-source models in scientific domains, surpassingclosed-source state-of-the-art models in professional tasks, such as molecularsynthesis planning, reaction condition prediction, predicting thermodynamicstabilities for crystals. Our models are available athttps://huggingface.co/internlm/Intern-S1.</description><author>Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Ju</author><pubDate>Thu, 21 Aug 2025 17:58:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15763v1</guid></item><item><title>NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</title><link>http://arxiv.org/abs/2508.15693v1</link><description>We present NiceWebRL, a research tool that enables researchers to use machinereinforcement learning (RL) environments for online human subject experiments.NiceWebRL is a Python library that allows any Jax-based environment to betransformed into an online interface, supporting both single-agent andmulti-agent environments. As such, NiceWebRL enables AI researchers to comparetheir algorithms to human performance, cognitive scientists to test MLalgorithms as theories for human cognition, and multi-agent researchers todevelop algorithms for human-AI collaboration. We showcase NiceWebRL with 3case studies that demonstrate its potential to help develop Human-like AI,Human-compatible AI, and Human-assistive AI. In the first case study(Human-like AI), NiceWebRL enables the development of a novel RL model ofcognition. Here, NiceWebRL facilitates testing this model against humanparticipants in both a grid world and Craftax, a 2D Minecraft domain. In oursecond case study (Human-compatible AI), NiceWebRL enables the development of anovel multi-agent RL algorithm that can generalize to human partners in theOvercooked domain. Finally, in our third case study (Human-assistive AI), weshow how NiceWebRL can allow researchers to study how an LLM can assist humanson complex tasks in XLand-Minigrid, an environment with millions ofhierarchical tasks. The library is available athttps://github.com/KempnerInstitute/nicewebrl.</description><author>Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha</author><pubDate>Thu, 21 Aug 2025 16:18:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15693v1</guid></item><item><title>Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</title><link>http://arxiv.org/abs/2508.15680v1</link><description>This paper argues that a techno-philosophical reading of the EU AI Actprovides insight into the long-term dynamics of data in AI systems,specifically, how the lifecycle from ingestion to deployment generatesrecursive value chains that challenge existing frameworks for Responsible AI.We introduce a conceptual tool to frame the AI pipeline, spanning data,training regimes, architectures, feature stores, and transfer learning. Usingcross-disciplinary methods, we develop a technically grounded andphilosophically coherent analysis of regulatory blind spots. Our central claimis that what remains absent from policymaking is an account of the dynamic ofbecoming that underpins both the technical operation and economic logic of AI.To address this, we advance a formal reading of AI inspired by Simondonianphilosophy of technology, reworking his concept of individuation to model theAI lifecycle, including the pre-individual milieu, individuation, andindividuated AI. To translate these ideas, we introduce futurity: theself-reinforcing lifecycle of AI, where more data enhances performance, deepenspersonalisation, and expands application domains. Futurity highlights therecursively generative, non-rivalrous nature of data, underpinned byinfrastructures like feature stores that enable feedback, adaptation, andtemporal recursion. Our intervention foregrounds escalating power asymmetries,particularly the tech oligarchy whose infrastructures of capture, training, anddeployment concentrate value and decision-making. We argue that effectiveregulation must address these infrastructural and temporal dynamics, andpropose measures including lifecycle audits, temporal traceability, feedbackaccountability, recursion transparency, and a right to contest recursive reuse.</description><author>Mark Cote, Susana Aires</author><pubDate>Thu, 21 Aug 2025 16:00:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15680v1</guid></item><item><title>Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation</title><link>http://arxiv.org/abs/2504.14716v2</link><description>Large Language Models (LLMs) are widely used as proxies for human labelers inboth training (Reinforcement Learning from AI Feedback) and large-scaleresponse evaluation (LLM-as-a-judge). Alignment and evaluation are criticalcomponents in the development of reliable LLMs, and the choice of feedbackprotocol plays a central role in both but remains understudied. In this work,we show that the choice of feedback protocol for evaluation (absolute scoresversus relative preferences) can significantly affect evaluation reliabilityand induce systematic biases. In the context of LLM-as-a-judge evaluation, weshow that pairwise protocols are more vulnerable to distracted evaluation.Generator models can exploit spurious attributes (or distractor features)favored by the LLM judge, resulting in inflated scores for lower-qualityoutputs. We find that absolute scoring is more robust to such manipulation,producing judgments that better reflect response quality and are lessinfluenced by distractor features. Our results demonstrate that generatormodels can flip preferences by embedding distractor features, skewingLLM-as-a-judge comparisons and leading to inaccurate conclusions about modelquality in benchmark evaluations. Pairwise preferences flip in about 35% of thecases, compared to only 9% for absolute scores. We offer recommendations forchoosing feedback protocols based on dataset characteristics and evaluationobjectives.</description><author>Tuhina Tripathi, Manya Wadhwa, Greg Durrett, Scott Niekum</author><pubDate>Thu, 21 Aug 2025 15:48:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.14716v2</guid></item><item><title>Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</title><link>http://arxiv.org/abs/2508.15652v1</link><description>To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it iscrucial to understand individual agent behaviors within a team. While priorwork typically evaluates overall team performance based on explicit rewardsignals or learned value functions, it is unclear how to infer agentcontributions in the absence of any value feedback. In this work, weinvestigate whether meaningful insights into agent behaviors can be extractedthat are consistent with the underlying value functions, solely by analyzingthe policy distribution. Inspired by the phenomenon that intelligent agentstend to pursue convergent instrumental values, which generally increase thelikelihood of task success, we introduce Intended Cooperation Values (ICVs), amethod based on information-theoretic Shapley values for quantifying eachagent's causal influence on their co-players' instrumental empowerment.Specifically, ICVs measure an agent's action effect on its teammates' policiesby assessing their decision uncertainty and preference alignment. The analysisacross cooperative and competitive MARL environments reveals the extent towhich agents adopt similar or diverse strategies. By comparing action effectsbetween policies and value functions, our method identifies which agentbehaviors are beneficial to team success, either by fostering deterministicdecisions or by preserving flexibility for future action choices. Our proposedmethod offers novel insights into cooperation dynamics and enhancesexplainability in MARL systems.</description><author>Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci</author><pubDate>Thu, 21 Aug 2025 15:35:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15652v1</guid></item><item><title>A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification</title><link>http://arxiv.org/abs/2508.15588v1</link><description>The application of reinforcement learning to safety-critical systems islimited by the lack of formal methods for verifying the robustness and safetyof learned policies. This paper introduces a novel framework that addressesthis gap by analyzing the combination of an RL agent and its environment as adiscrete-time autonomous dynamical system. By leveraging tools from dynamicalsystems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), weidentify and visualize Lagrangian Coherent Structures (LCS) that act as thehidden "skeleton" governing the system's behavior. We demonstrate thatrepelling LCS function as safety barriers around unsafe regions, whileattracting LCS reveal the system's convergence properties and potential failuremodes, such as unintended "trap" states. To move beyond qualitativevisualization, we introduce a suite of quantitative metrics, Mean BoundaryRepulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), andTemporally-Aware Spurious Attractor Strength (TASAS), to formally measure apolicy's safety margin and robustness. We further provide a method for derivinglocal stability guarantees and extend the analysis to handle model uncertainty.Through experiments in both discrete and continuous control environments, weshow that this framework provides a comprehensive and interpretable assessmentof policy behavior, successfully identifying critical flaws in policies thatappear successful based on reward alone.</description><author>Ahmed Nasir, Abdelhafid Zenati</author><pubDate>Thu, 21 Aug 2025 14:00:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15588v1</guid></item><item><title>Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning</title><link>http://arxiv.org/abs/2508.15507v1</link><description>Large Language Models (LLMs) with chains-of-thought have demonstrated strongperformance on an increasing range of tasks, particularly those involvingcomplex logical reasoning. However, excessively long chains can lead tooverthinking, causing computational waste and slower responses. This raises aquestion: can LLMs dynamically adjust the length of their reasoning processesbased on task complexity? To address this, we propose the Think in Blocksframework, which enables adaptive reasoning-from zero to deep reasoning-bypartitioning the reasoning process into a tunable number of blocks. Our maincontributions are: (1) Establishing an explicit block-structured paradigm inwhich the model first predicts an integer reasoning budget-the number ofblocks-and then partitions its reasoning accordingly; (2) Training an adaptivemodel through a three-stage pipeline-Supervised Fine-Tuning, reward-guidedDirect Preference Optimization, and Reinforcement Learning-that adjusts itsreasoning depth to problem difficulty; (3) Exploiting the explicit block countto dynamically control reasoning depth at inference time, allowing flexibleadjustment of chain-of-thought length during deployment.</description><author>Yekun Zhu, Guang Chen, Chengjun Mao</author><pubDate>Thu, 21 Aug 2025 12:32:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15507v1</guid></item><item><title>ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction</title><link>http://arxiv.org/abs/2508.08170v2</link><description>Reinforcement learning for training end-to-end autonomous driving models inclosed-loop simulations is gaining growing attention. However, most simulationenvironments differ significantly from real-world conditions, creating asubstantial simulation-to-reality (sim2real) gap. To bridge this gap, someapproaches utilize scene reconstruction techniques to create photorealisticenvironments as a simulator. While this improves realistic sensor simulation,these methods are inherently constrained by the distribution of the trainingdata, making it difficult to render high-quality sensor data for noveltrajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, aframework designed to integrate video diffusion priors into scenereconstruction to aid reinforcement learning, thereby enhancing end-to-endautonomous driving training. Specifically, in ReconDreamer-RL, we introduceReconSimulator, which combines the video diffusion prior for appearancemodeling and incorporates a kinematic model for physical modeling, therebyreconstructing driving scenarios from real-world data. This narrows thesim2real gap for closed-loop evaluation and reinforcement learning. To covermore corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),which adjusts the trajectories of surrounding vehicles relative to the egovehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issueof training data distribution, which is often biased toward simplestraight-line movements. Experiments show that ReconDreamer-RL improvesend-to-end autonomous driving training, outperforming imitation learningmethods with a 5x reduction in the Collision Ratio.</description><author>Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Xinze Chen, Guanghong Jia, Guan Huang, Wenjun Mei</author><pubDate>Thu, 21 Aug 2025 11:45:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.08170v2</guid></item><item><title>Human-Object Interaction from Human-Level Instructions</title><link>http://arxiv.org/abs/2406.17840v3</link><description>Intelligent agents must autonomously interact with the environments toperform daily tasks based on human-level instructions. They need a foundationalunderstanding of the world to accurately interpret these instructions, alongwith precise low-level movement and interaction skills to execute the derivedactions. In this work, we propose the first complete system for synthesizingphysically plausible, long-horizon human-object interactions for objectmanipulation in contextual environments, driven by human-level instructions. Weleverage large language models (LLMs) to interpret the input instructions intodetailed execution plans. Unlike prior work, our system is capable ofgenerating detailed finger-object interactions, in seamless coordination withfull-body movements. We also train a policy to track generated motions inphysics simulation via reinforcement learning (RL) to ensure physicalplausibility of the motion. Our experiments demonstrate the effectiveness ofour system in synthesizing realistic interactions with diverse objects incomplex environments, highlighting its potential for real-world applications.</description><author>Zhen Wu, Jiaman Li, Pei Xu, C. Karen Liu</author><pubDate>Thu, 21 Aug 2025 08:23:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2406.17840v3</guid></item><item><title>Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning</title><link>http://arxiv.org/abs/2508.15327v1</link><description>Offline reinforcement learning refers to the process of learning policiesfrom fixed datasets, without requiring additional environment interaction.However, it often relies on well-defined reward functions, which are difficultand expensive to design. Human feedback is an appealing alternative, but itstwo common forms, expert demonstrations and preferences, have complementarylimitations. Demonstrations provide stepwise supervision, but they are costlyto collect and often reflect limited expert behavior modes. In contrast,preferences are easier to collect, but it is unclear which parts of a behaviorcontribute most to a trajectory segment, leaving credit assignment unresolved.In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme tounify these two feedback sources. For each transition in a preference labeledtrajectory, SPW searches for the most similar state-action pairs from expertdemonstrations and directly derives stepwise importance weights based on theirsimilarity scores. These weights are then used to guide standard preferencelearning, enabling more accurate credit assignment that traditional approachesstruggle to achieve. We demonstrate that SPW enables effective joint learningfrom preferences and demonstrations, outperforming prior methods that leverageboth feedback types on challenging robot manipulation tasks.</description><author>Xiancheng Gao, Yufeng Shi, Wengang Zhou, Houqiang Li</author><pubDate>Thu, 21 Aug 2025 07:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15327v1</guid></item><item><title>Breaking (Global) Barriers in Parallel Stochastic Optimization with Wait-Avoiding Group Averaging</title><link>http://arxiv.org/abs/2005.00124v5</link><description>Deep learning at scale is dominated by communication time. Distributingsamples across nodes usually yields the best performance, but poses scalingchallenges due to global information dissemination and load imbalance acrossuneven sample lengths. State-of-the-art decentralized optimizers mitigate theproblem, but require more iterations to achieve the same accuracy as theirglobally-communicating counterparts. We present Wait-Avoiding Group ModelAveraging (WAGMA) SGD, a wait-avoiding stochastic optimizer that reduces globalcommunication via subgroup weight exchange. The key insight is a combination ofalgorithmic changes to the averaging scheme and the use of a group allreduceoperation. We prove the convergence of WAGMA-SGD, and empirically show that itretains convergence rates similar to Allreduce-SGD. For evaluation, we trainResNet-50 on ImageNet; Transformer for machine translation; and deepreinforcement learning for navigation at scale. Compared with state-of-the-artdecentralized SGD variants, WAGMA-SGD significantly improves trainingthroughput (e.g., 2.1x on 1,024 GPUs for reinforcement learning), and achievesthe fastest time-to-solution (e.g., the highest score using the shortesttraining time for Transformer).</description><author>Shigang Li, Tal Ben-Nun, Giorgi Nadiradze, Salvatore Di Girolamo, Nikoli Dryden, Dan Alistarh, Torsten Hoefler</author><pubDate>Thu, 21 Aug 2025 04:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2005.00124v5</guid></item><item><title>Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning</title><link>http://arxiv.org/abs/2508.15207v1</link><description>Existing approaches in reinforcement learning train an agent to learn desiredoptimal behavior in an environment with rule based surrounding agents. Insafety critical applications such as autonomous driving it is crucial that therule based agents are modelled properly. Several behavior modelling strategiesand IDM models are used currently to model the surrounding agents. We present alearning based method to derive the adversarial behavior for the rule basedagents to cause failure scenarios. We evaluate our adversarial agent againstall the rule based agents and show the decrease in cumulative reward.</description><author>Arjun Srinivasan, Anubhav Paras, Aniket Bera</author><pubDate>Thu, 21 Aug 2025 03:38:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15207v1</guid></item><item><title>Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models</title><link>http://arxiv.org/abs/2508.15202v1</link><description>Process Reward Models (PRMs) have emerged as a promising framework forsupervising intermediate reasoning in large language models (LLMs), yetexisting PRMs are primarily trained on general or Science, Technology,Engineering, and Mathematics (STEM) domains and fall short in domain-specificcontexts such as finance, where reasoning is more structured, symbolic, andsensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM},a domain-specialized, trajectory-aware PRM tailored to evaluate intermediatereasoning steps in financial tasks. Fin-PRM integrates step-level andtrajectory-level reward supervision, enabling fine-grained evaluation ofreasoning traces aligned with financial logic. We apply Fin-PRM in both offlineand online reward learning settings, supporting three key applications: (i)selecting high-quality reasoning trajectories for distillation-based supervisedfine-tuning, (ii) providing dense process-level rewards for reinforcementlearning, and (iii) guiding reward-informed Best-of-N inference at test time.Experimental results on financial reasoning benchmarks, including CFLUE andFinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMsand strong domain baselines in trajectory selection quality. Downstream modelstrained with Fin-PRM yield substantial improvements with baselines, with gainsof 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% intest-time performance. These findings highlight the value of domain-specializedreward modeling for aligning LLMs with expert-level financial reasoning. Ourproject resources will be available at https://github.com/aliyun/qwen-dianjin.</description><author>Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang</author><pubDate>Thu, 21 Aug 2025 03:31:11 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15202v1</guid></item><item><title>VLASCD: A Visual Language Action Model for Simultaneous Chatting and Decision Making</title><link>http://arxiv.org/abs/2410.15885v2</link><description>Although current mainstream pre-trained large models, such as LLM modelsrepresented by ChatGPT and VLA models represented by OpenVLA, have achievedsignificant progress in multimodal tasks through a "Multiple-Input,Single-Output" (MISO) architecture. However, our investigation reveals that theMISO architecture exhibits fundamental limitations in "Multiple-Input,Multiple-Output" (MIMO) (e.g., parallel multi-tasks output processing): thearchitecture generates task mutual exclusion effects, leading to resourcecontention among different tasks when sharing output channels, and consequentlyresulting in optimization imbalance and performance degradation. In contrast,human MIMO processing inherently enables concurrent task execution (e.g., whiledialogue and decision-making) without interference. Inspired by this, in thiswork, we propose a unified MIMO training model with parallel multi-tasks outputcapabilities termed Visual Language Action Model for Simultaneously Chattingand Decision Making. We refer to this method as VLASCD or MIMO-VLA, and in thefollowing, we will use these two names interchangeably. We evaluate the modelon the CARLA autonomous driving platform. The results show that, compared toLLM models with MISO dialogue capabilities, reinforcement learning models, andVLA models with MISO decision-making capabilities, MIMO-VLA significantlyoutperforms existing MISO models in simultaneously handling dialogue generationand decision-making tasks within the MIMO scenario.</description><author>Zuojin Tang, Bin Hu, Chenyang Zhao, De Ma, Gang Pan, Bin Liu</author><pubDate>Thu, 21 Aug 2025 02:12:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2410.15885v2</guid></item><item><title>GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy</title><link>http://arxiv.org/abs/2505.12355v3</link><description>Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloudcomputing, focusing on devising an effective scheduling policy to efficientlyschedule dynamically arriving workflow tasks, represented as Directed AcyclicGraphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning(DRL) has been widely employed for automated scheduling policy design. However,the performance of DRL is heavily influenced by the design of theproblem-tailored policy network and is highly sensitive to hyperparameters andthe design of reward feedback. Considering the above-mentioned issues, thisstudy proposes a novel DRL method combining Graph Attention Networks-basedpolicy network and Evolution Strategy, referred to as GATES. The contributionsof GATES are summarized as follows: (1) GATES can capture the impact of currenttask scheduling on subsequent tasks by learning the topological relationshipsbetween tasks in a DAG. (2) GATES can assess the importance of each VM to theready task, enabling it to adapt to dynamically changing VM resources. (3)Utilizing Evolution Strategy's robustness, exploratory nature, and tolerancefor delayed rewards, GATES achieves stable policy learning in CADWS. Extensiveexperimental results demonstrate the superiority of the proposed GATES inCADWS, outperforming several state-of-the-art algorithms. The source code isavailable at: https://github.com/YaShen998/GATES.</description><author>Ya Shen, Gang Chen, Hui Ma, Mengjie Zhang</author><pubDate>Thu, 21 Aug 2025 01:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.12355v3</guid></item><item><title>Universal Reinforcement Learning in Coalgebras: Asynchronous Stochastic Computation via Conduction</title><link>http://arxiv.org/abs/2508.15128v1</link><description>In this paper, we introduce a categorial generalization of RL, termeduniversal reinforcement learning (URL), building on powerful mathematicalabstractions from the study of coinduction on non-well-founded sets anduniversal coalgebras, topos theory, and categorial models of asynchronousparallel distributed computation. In the first half of the paper, we review thebasic RL framework, illustrate the use of categories and functors in RL,showing how they lead to interesting insights. In particular, we also introducea standard model of asynchronous distributed minimization proposed by Bertsekasand Tsitsiklis, and describe the relationship between metric coinduction andtheir proof of the Asynchronous Convergence Theorem. The space of algorithmsfor MDPs or PSRs can be modeled as a functor category, where the co-domaincategory forms a topos, which admits all (co)limits, possesses a subobjectclassifier, and has exponential objects. In the second half of the paper, wemove on to universal coalgebras. Dynamical system models, such as Markovdecision processes (MDPs), partially observed MDPs (POMDPs), a predictive staterepresentation (PSRs), and linear dynamical systems (LDSs) are all specialtypes of coalgebras. We describe a broad family of universal coalgebras,extending the dynamic system models studied previously in RL. The core problemin finding fixed points in RL to determine the exact or approximate (action)value function is generalized in URL to determining the final coalgebraasynchronously in a parallel distributed manner.</description><author>Sridhar Mahadevan</author><pubDate>Wed, 20 Aug 2025 23:37:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15128v1</guid></item><item><title>One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning</title><link>http://arxiv.org/abs/2508.01561v3</link><description>Generalizing to complex and temporally extended task objectives and safetyconstraints remains a critical challenge in reinforcement learning (RL). Lineartemporal logic (LTL) offers a unified formalism to specify such requirements,yet existing methods are limited in their abilities to handle nestedlong-horizon tasks and safety constraints, and cannot identify situations whena subgoal is not satisfiable and an alternative should be sought. In thispaper, we introduce GenZ-LTL, a method that enables zero-shot generalization toarbitrary LTL specifications. GenZ-LTL leverages the structure of B\"uchiautomata to decompose an LTL task specification into sequences of reach-avoidsubgoals. Contrary to the current state-of-the-art method that conditions onsubgoal sequences, we show that it is more effective to achieve zero-shotgeneralization by solving these reach-avoid problems \textit{one subgoal at atime} through proper safe RL formulations. In addition, we introduce a novelsubgoal-induced observation reduction technique that can mitigate theexponential complexity of subgoal-state combinations under realisticassumptions. Empirical results show that GenZ-LTL substantially outperformsexisting methods in zero-shot generalization to unseen LTL specifications.</description><author>Zijian Guo, İlker Işık, H. M. Sabbir Ahmad, Wenchao Li</author><pubDate>Wed, 20 Aug 2025 22:06:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.01561v3</guid></item><item><title>Deceptive Sequential Decision-Making via Regularized Policy Optimization</title><link>http://arxiv.org/abs/2501.18803v2</link><description>Autonomous systems are increasingly expected to operate in the presence ofadversaries, though adversaries may infer sensitive information simply byobserving a system. Therefore, present a deceptive sequential decision-makingframework that not only conceals sensitive information, but actively misleadsadversaries about it. We model autonomous systems as Markov decision processes,with adversaries using inverse reinforcement learning to recover rewardfunctions. To counter them, we present three regularization strategies forpolicy synthesis problems that actively deceive an adversary about a system'sreward. ``Diversionary deception'' leads an adversary to draw any falseconclusion about the system's reward function. ``Targeted deception'' leads anadversary to draw a specific false conclusion about the system's rewardfunction. ``Equivocal deception'' leads an adversary to infer that the realreward and a false reward both explain the system's behavior. We show how eachform of deception can be implemented in policy optimization problems andanalytically bound the loss in total accumulated reward induced by deception.Next, we evaluate these developments in a multi-agent setting. We show thatdiversionary, targeted, and equivocal deception all steer the adversary tofalse beliefs while still attaining a total accumulated reward that is at least97% of its optimal, non-deceptive value.</description><author>Yerin Kim, Alexander Benvenuti, Bo Chen, Mustafa Karabag, Abhishek Kulkarni, Nathaniel D. Bastian, Ufuk Topcu, Matthew Hale</author><pubDate>Wed, 20 Aug 2025 20:19:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2501.18803v2</guid></item><item><title>Goals and the Structure of Experience</title><link>http://arxiv.org/abs/2508.15013v1</link><description>Purposeful behavior is a hallmark of natural and artificial intelligence. Itsacquisition is often believed to rely on world models, comprising bothdescriptive (what is) and prescriptive (what is desirable) aspects thatidentify and evaluate state of affairs in the world, respectively. Canonicalcomputational accounts of purposeful behavior, such as reinforcement learning,posit distinct components of a world model comprising a state representation(descriptive aspect) and a reward function (prescriptive aspect). However, analternative possibility, which has not yet been computationally formulated, isthat these two aspects instead co-emerge interdependently from an agent's goal.Here, we describe a computational framework of goal-directed staterepresentation in cognitive agents, in which the descriptive and prescriptiveaspects of a world model co-emerge from agent-environment interactionsequences, or experiences. Drawing on Buddhist epistemology, we introduce aconstruct of goal-directed, or telic, states, defined as classes ofgoal-equivalent experience distributions. Telic states provide a parsimoniousaccount of goal-directed learning in terms of the statistical divergencebetween behavioral policies and desirable experience features. We reviewempirical and theoretical literature supporting this novel perspective anddiscuss its potential to provide a unified account of behavioral,phenomenological and neural dimensions of purposeful behaviors across diversesubstrates.</description><author>Nadav Amir, Stas Tiomkin, Angela Langdon</author><pubDate>Wed, 20 Aug 2025 19:05:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.15013v1</guid></item><item><title>CaRL: Learning Scalable Planning Policies with Simple Rewards</title><link>http://arxiv.org/abs/2504.17838v3</link><description>We investigate reinforcement learning (RL) for privileged planning inautonomous driving. State-of-the-art approaches for this task are rule-based,but these methods do not scale to the long tail. RL, on the other hand, isscalable and does not suffer from compounding errors like imitation learning.Contemporary RL approaches for driving use complex shaped rewards that summultiple individual rewards, \eg~progress, position, or orientation rewards. Weshow that PPO fails to optimize a popular version of these rewards when themini-batch size is increased, which limits the scalability of these approaches.Instead, we propose a new reward design based primarily on optimizing a singleintuitive reward term: route completion. Infractions are penalized byterminating the episode or multiplicatively reducing route completion. We findthat PPO scales well with higher mini-batch sizes when trained with our simplereward, even improving performance. Training with large mini-batch sizesenables efficient scaling via distributed data parallelism. We scale PPO to300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. Theresulting model achieves 64 DS on the CARLA longest6 v2 benchmark,outperforming other RL methods with more complex rewards by a large margin.Requiring only minimal adaptations from its use in CARLA, the same method isthe best learning-based approach on nuPlan. It scores 91.3 in non-reactive and90.6 in reactive traffic on the Val14 benchmark while being an order ofmagnitude faster than prior work.</description><author>Bernhard Jaeger, Daniel Dauner, Jens Beißwenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger</author><pubDate>Wed, 20 Aug 2025 19:00:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.17838v3</guid></item><item><title>Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal CAPTCHA System</title><link>http://arxiv.org/abs/2508.14976v1</link><description>Aura-CAPTCHA was developed as a multi-modal CAPTCHA system to addressvulnerabilities in traditional methods that are increasingly bypassed by AItechnologies, such as Optical Character Recognition (OCR) and adversarial imageprocessing. The design integrated Generative Adversarial Networks (GANs) forgenerating dynamic image challenges, Reinforcement Learning (RL) for adaptivedifficulty tuning, and Large Language Models (LLMs) for creating text and audioprompts. Visual challenges included 3x3 grid selections with at least threecorrect images, while audio challenges combined randomized numbers and wordsinto a single task. RL adjusted difficulty based on incorrect attempts,response time, and suspicious user behavior. Evaluations on real-world trafficdemonstrated a 92% human success rate and a 10% bot bypass rate, significantlyoutperforming existing CAPTCHA systems. The system provided a robust andscalable approach for securing online applications while remaining accessibleto users, addressing gaps highlighted in previous research.</description><author>Joydeep Chandra, Prabal Manhas, Ramanjot Kaur, Rashi Sahay</author><pubDate>Wed, 20 Aug 2025 18:00:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14976v1</guid></item><item><title>Compute-Optimal Scaling for Value-Based Deep RL</title><link>http://arxiv.org/abs/2508.14881v1</link><description>As models grow larger and training them becomes expensive, it becomesincreasingly important to scale training recipes not just to larger models andmore data, but to do so in a compute-optimal manner that extracts maximalperformance per unit of compute. While such scaling has been well studied forlanguage modeling, reinforcement learning (RL) has received less attention inthis regard. In this paper, we investigate compute scaling for online,value-based deep RL. These methods present two primary axes for computeallocation: model capacity and the update-to-data (UTD) ratio. Given a fixedcompute budget, we ask: how should resources be partitioned across these axesto maximize sample efficiency? Our analysis reveals a nuanced interplay betweenmodel size, batch size, and UTD. In particular, we identify a phenomenon wecall TD-overfitting: increasing the batch quickly harms Q-function accuracy forsmall models, but this effect is absent in large models, enabling effective useof large batch size at scale. We provide a mental model for understanding thisphenomenon and build guidelines for choosing batch size and UTD to optimizecompute usage. Our findings provide a grounded starting point forcompute-optimal scaling in deep RL, mirroring studies in supervised learningbut adapted to TD learning.</description><author>Preston Fu, Oleh Rybkin, Zhiyuan Zhou, Michal Nauman, Pieter Abbeel, Sergey Levine, Aviral Kumar</author><pubDate>Wed, 20 Aug 2025 17:54:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14881v1</guid></item><item><title>MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title><link>http://arxiv.org/abs/2508.14880v2</link><description>Recent developments in Large Language Model (LLM)-based agents have shownimpressive capabilities spanning multiple domains, exemplified by deep researchsystems that demonstrate superior performance on complex information-seekingand synthesis tasks. While general-purpose deep research agents have shownimpressive capabilities, they struggle significantly with medical domainchallenges, as evidenced by leading proprietary systems achieving limitedaccuracy on complex medical benchmarks. The key limitations are: (1) the modellacks sufficient dense medical knowledge for clinical reasoning, and (2) theframework is constrained by the absence of specialized retrieval tools tailoredfor medical contexts. We present a medical deep research agent that addressesthese challenges through two core innovations. First, we develop a novel datasynthesis framework using medical knowledge graphs, extracting the longestchains from subgraphs around rare medical entities to generate complexmulti-hop question-answer pairs. Second, we integrate a custom-built privatemedical retrieval engine alongside general-purpose tools, enabling accuratemedical information synthesis. Our approach generates 2100+ diversetrajectories across 12 medical specialties, each averaging 4.2 toolinteractions. Through a two-stage training paradigm combining supervisedfine-tuning and online reinforcement learning with composite rewards, ourMedResearcher-R1-32B model demonstrates exceptional performance, establishingnew state-of-the-art results on medical benchmarks while maintainingcompetitive performance on general deep research tasks. Our work demonstratesthat strategic domain-specific innovations in architecture, tool design, andtraining data construction can enable smaller open-source models to outperformmuch larger proprietary systems in specialized domains.</description><author>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</author><pubDate>Thu, 21 Aug 2025 18:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14880v2</guid></item><item><title>Endo-FASt3r: Endoscopic Foundation model Adaptation for Structure from motion</title><link>http://arxiv.org/abs/2503.07204v4</link><description>Accurate depth and camera pose estimation is essential for achievinghigh-quality 3D visualisations in robotic-assisted surgery. Despite recentadvancements in foundation model adaptation to monocular depth estimation ofendoscopic scenes via self-supervised learning (SSL), no prior work hasexplored their use for pose estimation. These methods rely on low rank-basedadaptation approaches, which constrain model updates to a low-rank space. Wepropose Endo-FASt3r, the first monocular SSL depth and pose estimationframework that uses foundation models for both tasks. We extend the Reloc3rrelative pose estimation foundation model by designing Reloc3rX, introducingmodifications necessary for convergence in SSL. We also present DoMoRA, a noveladaptation technique that enables higher-rank updates and faster convergence.Experiments on the SCARED dataset show that Endo-FASt3r achieves a substantial$10\%$ improvement in pose estimation and a $2\%$ improvement in depthestimation over prior work. Similar performance gains on the Hamlyn andStereoMIS datasets reinforce the generalisability of Endo-FASt3r acrossdifferent datasets.</description><author>Mona Sheikh Zeinoddin, Mobarak I. Hoque, Zafer Tandogdu, Greg Shaw, Matthew J. Clarkson, Evangelos Mazomenos, Danail Stoyanov</author><pubDate>Wed, 20 Aug 2025 16:41:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.07204v4</guid></item><item><title>Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress</title><link>http://arxiv.org/abs/2506.23036v2</link><description>This paper explores Reinforcement learning (RL) policy robustness bysystematically analyzing network parameters under internal and externalstresses. Inspired by synaptic plasticity in neuroscience, synaptic filteringintroduces internal stress by selectively perturbing parameters, whileadversarial attacks apply external stress through modified agent observations.This dual approach enables the classification of parameters as fragile, robust,or antifragile, based on their influence on policy performance in clean andadversarial settings. Parameter scores are defined to quantify thesecharacteristics, and the framework is validated on PPO-trained agents in Mujococontinuous control environments. The results highlight the presence ofantifragile parameters that enhance policy performance under stress,demonstrating the potential of targeted filtering techniques to improve RLpolicy adaptability. These insights provide a foundation for futureadvancements in the design of robust and antifragile RL systems.</description><author>Zain ul Abdeen, Ming Jin</author><pubDate>Wed, 20 Aug 2025 16:21:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.23036v2</guid></item><item><title>Quantum Long Short-term Memory with Differentiable Architecture Search</title><link>http://arxiv.org/abs/2508.14955v1</link><description>Recent advances in quantum computing and machine learning have given rise toquantum machine learning (QML), with growing interest in learning fromsequential data. Quantum recurrent models like QLSTM are promising fortime-series prediction, NLP, and reinforcement learning. However, designingeffective variational quantum circuits (VQCs) remains challenging and oftentask-specific. To address this, we propose DiffQAS-QLSTM, an end-to-enddifferentiable framework that optimizes both VQC parameters and architectureselection during training. Our results show that DiffQAS-QLSTM consistentlyoutperforms handcrafted baselines, achieving lower loss across diverse testsettings. This approach opens the door to scalable and adaptive quantumsequence learning.</description><author>Samuel Yen-Chi Chen, Prayag Tiwari</author><pubDate>Wed, 20 Aug 2025 16:15:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14955v1</guid></item><item><title>PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning</title><link>http://arxiv.org/abs/2508.14765v1</link><description>Designing therapeutic peptides with tailored properties is hindered by thevastness of sequence space, limited experimental data, and poorinterpretability of current generative models. To address these challenges, weintroduce PepThink-R1, a generative framework that integrates large languagemodels (LLMs) with chain-of-thought (CoT) supervised fine-tuning andreinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitlyreasons about monomer-level modifications during sequence generation, enablinginterpretable design choices while optimizing for multiple pharmacologicalproperties. Guided by a tailored reward function balancing chemical validityand property improvements, the model autonomously explores diverse sequencevariants. We demonstrate that PepThink-R1 generates cyclic peptides withsignificantly enhanced lipophilicity, stability, and exposure, outperformingexisting general LLMs (e.g., GPT-5) and domain-specific baseline in bothoptimization success and interpretability. To our knowledge, this is the firstLLM-based peptide design framework that combines explicit reasoning withRL-driven property control, marking a step toward reliable and transparentpeptide optimization for therapeutic discovery.</description><author>Ruheng Wang, Hang Zhang, Trieu Nguyen, Shasha Feng, Hao-Wei Pang, Xiang Yu, Li Xiao, Peter Zhiping Zhang</author><pubDate>Wed, 20 Aug 2025 15:13:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14765v1</guid></item><item><title>Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided Refinement</title><link>http://arxiv.org/abs/2504.02906v2</link><description>Translating chart images into executable plotting scripts-referred to as thechart-to-code generation task-requires Multimodal Large Language Models (MLLMs)to perform fine-grained visual parsing, precise code synthesis, and robustcross-modal reasoning. However, this task is inherently under-constrained:multiple valid code implementations can produce the same visual chart, andevaluation must consider both code correctness and visual fidelity acrossdiverse dimensions. This makes it difficult to learn accurate and generalizablemappings through standard supervised fine-tuning. To address these challenges,we propose a dual preference-guided refinement framework that combines afeedback-driven, dual-modality reward mechanism with iterative preferencelearning. Our approach introduces a structured variant generation strategy anda visual reward model to efficiently produce high-quality, aspect-awarepreference pairs-making preference collection scalable and supervision moretargeted. These preferences are used in an offline reinforcement learning setupto optimize the model toward multi-dimensional fidelity. Experimental resultsshow that our framework significantly enhances the performance ofgeneral-purpose open-source MLLMs, enabling them to generate high-qualityplotting code that rivals specialized chart-centric models and even someproprietary systems. The code and datasets are publicly available athttps://github.com/Zhihan72/Chart2Code.</description><author>Zhihan Zhang, Yixin Cao, Lizi Liao</author><pubDate>Wed, 20 Aug 2025 14:56:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.02906v2</guid></item><item><title>HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents</title><link>http://arxiv.org/abs/2508.14751v1</link><description>Open-ended AI agents need to be able to learn efficiently goals of increasingcomplexity, abstraction and heterogeneity over their lifetime. Beyond samplingefficiently their own goals, autotelic agents specifically need to be able tokeep the growing complexity of goals under control, limiting the associatedgrowth in sample and computational complexity. To adress this challenge, recentapproaches have leveraged hierarchical reinforcement learning (HRL) andlanguage, capitalizing on its compositional and combinatorial generalizationcapabilities to acquire temporally extended reusable behaviours. Existingapproaches use expert defined spaces of subgoals over which they instantiate ahierarchy, and often assume pre-trained associated low-level policies. Suchdesigns are inadequate in open-ended scenarios, where goal spaces naturallydiversify across a broad spectrum of difficulties. We introduce HERAKLES, aframework that enables a two-level hierarchical autotelic agent to continuouslycompile mastered goals into the low-level policy, executed by a small, fastneural network, dynamically expanding the set of subgoals available to thehigh-level policy. We train a Large Language Model (LLM) to serve as thehigh-level controller, exploiting its strengths in goal decomposition andgeneralization to operate effectively over this evolving subgoal space. Weevaluate HERAKLES in the open-ended Crafter environment and show that it scaleseffectively with goal complexity, improves sample efficiency through skillcompilation, and enables the agent to adapt robustly to novel challenges overtime.</description><author>Thomas Carta, Clément Romac, Loris Gaven, Pierre-Yves Oudeyer, Olivier Sigaud, Sylvain Lamprier</author><pubDate>Wed, 20 Aug 2025 14:50:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14751v1</guid></item><item><title>AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</title><link>http://arxiv.org/abs/2508.14734v1</link><description>In many real-world scenarios, acquiring all features of a data instance canbe expensive or impractical due to monetary cost, latency, or privacy concerns.Active Feature Acquisition (AFA) addresses this challenge by dynamicallyselecting a subset of informative features for each data instance, tradingpredictive performance against acquisition cost. While numerous methods havebeen proposed for AFA, ranging from greedy information-theoretic strategies tonon-myopic reinforcement learning approaches, fair and systematic evaluation ofthese methods has been hindered by the lack of standardized benchmarks. In thispaper, we introduce AFABench, the first benchmark framework for AFA. Ourbenchmark includes a diverse set of synthetic and real-world datasets, supportsa wide range of acquisition policies, and provides a modular design thatenables easy integration of new methods and tasks. We implement and evaluaterepresentative algorithms from all major categories, including static, greedy,and reinforcement learning-based approaches. To test the lookahead capabilitiesof AFA policies, we introduce a novel synthetic dataset, AFAContext, designedto expose the limitations of greedy selection. Our results highlight keytrade-offs between different AFA strategies and provide actionable insights forfuture research. The benchmark code is available at:https://github.com/Linusaronsson/AFA-Benchmark.</description><author>Valter Schütz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani</author><pubDate>Wed, 20 Aug 2025 14:29:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14734v1</guid></item><item><title>Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving</title><link>http://arxiv.org/abs/2505.07773v4</link><description>Large Language Models (LLMs) often struggle with mathematical reasoning tasksrequiring precise, verifiable computation. While Reinforcement Learning (RL)from outcome-based rewards enhances text-based reasoning, understanding howagents autonomously learn to leverage external tools like code executionremains crucial. We investigate RL from outcome-based rewards forTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneouslygenerate and execute Python code for mathematical problems without supervisedtool-use examples. Our central contribution is we demonstrate that as RLtraining progresses, key metrics scale predictably. Specifically, we observestrong positive correlations where increased training steps lead to increasesin the spontaneous code execution frequency, the average response length, and,critically, the final task accuracy. This suggests a quantifiable relationshipbetween computational effort invested in training and the emergence ofeffective, tool-augmented reasoning strategies. We implement a robust frameworkfeaturing a decoupled code execution environment and validate our findingsacross standard RL algorithms and frameworks. Experiments show ZeroTIRsignificantly surpasses non-tool ZeroRL baselines on challenging mathbenchmarks. Our findings provide a foundational understanding of how autonomoustool use is acquired and scales within Agent RL, offering a reproduciblebenchmark for future studies. Code is released at\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\_async\_pipline}.</description><author>Xinji Mai, Haotian Xu, Zhong-Zhi Li, Xing W, Weinong Wang, Jian Hu, Yingying Zhang, Wenqiang Zhang</author><pubDate>Wed, 20 Aug 2025 12:20:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.07773v4</guid></item><item><title>OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service</title><link>http://arxiv.org/abs/2508.14646v1</link><description>Local life service is a vital scenario in Kuaishou App, where videorecommendation is intrinsically linked with store's location information. Thus,recommendation in our scenario is challenging because we should take intoaccount user's interest and real-time location at the same time. In the face ofsuch complex scenarios, end-to-end generative recommendation has emerged as anew paradigm, such as OneRec in the short video scenario, OneSug in the searchscenario, and EGA in the advertising scenario. However, in local life service,an end-to-end generative recommendation model has not yet been developed asthere are some key challenges to be solved. The first challenge is how to makefull use of geographic information. The second challenge is how to balancemultiple objectives, including user interests, the distance between user andstores, and some other business objectives. To address the challenges, wepropose OneLoc. Specifically, we leverage geographic information from differentperspectives: (1) geo-aware semantic ID incorporates both video and geographicinformation for tokenization, (2) geo-aware self-attention in the encoderleverages both video location similarity and user's real-time location, and (3)neighbor-aware prompt captures rich context information surrounding users forgeneration. To balance multiple objectives, we use reinforcement learning andpropose two reward functions, i.e., geographic reward and GMV reward. With theabove design, OneLoc achieves outstanding offline and online performance. Infact, OneLoc has been deployed in local life service of Kuaishou App. It serves400 million active users daily, achieving 21.016% and 17.891% improvements interms of gross merchandise value (GMV) and orders numbers.</description><author>Zhipeng Wei, Kuo Cai, Junda She, Jie Chen, Minghao Chen, Yang Zeng, Qiang Luo, Wencong Zeng, Ruiming Tang, Kun Gai, Guorui Zhou</author><pubDate>Wed, 20 Aug 2025 11:57:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14646v1</guid></item><item><title>Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback</title><link>http://arxiv.org/abs/2506.03106v5</link><description>Recent advances in reinforcement learning (RL) with numerical feedback, suchas scalar rewards, have significantly enhanced the complex reasoningcapabilities of large language models (LLMs). Despite this success, we identifythree key challenges encountered by RL with solely numerical feedback:performance plateaus, limited effectiveness of spontaneous self-reflection, andpersistent failures. We then demonstrate that RL-finetuned models, even afterexhibiting performance plateaus, can generate correct refinements onpersistently failed problems by leveraging natural language feedback in theform of critiques. Building on this insight, we propose Critique-GRPO, anonline RL framework that integrates both natural language and numericalfeedback for effective policy optimization. Critique-GRPO enables LLMs to learnfrom initial responses and critique-guided self-refinements simultaneouslywhile maintaining exploration. Additionally, we employ a shaping function toamplify learning from correct, especially unfamiliar, refinements and penalizeincorrect ones. Extensive experiments with Qwen2.5-7B-Base,Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistentlyoutperforms supervised learning and RL-based fine-tuning methods across eightchallenging mathematical, STEM, and general reasoning tasks. Specifically,Critique-GRPO improves average pass@1 scores across all compared methods byapproximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably,Critique-GRPO enables effective self-improvement through self-critiquing,achieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME2024.</description><author>Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, Helen Meng</author><pubDate>Wed, 20 Aug 2025 09:10:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.03106v5</guid></item><item><title>Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks</title><link>http://arxiv.org/abs/2508.14536v1</link><description>The performance of Deep Q-Networks (DQN) is critically dependent on theability of its underlying neural network to accurately approximate theaction-value function. Standard function approximators, such as multi-layerperceptrons, may struggle to efficiently represent the complex value landscapesinherent in many reinforcement learning problems. This paper introduces a novelarchitecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshevpolynomial basis into the DQN framework to create a more effective featurerepresentation. By leveraging the powerful function approximation properties ofChebyshev polynomials, we hypothesize that the Ch-DQN can learn moreefficiently and achieve higher performance. We evaluate our proposed model onthe CartPole-v1 benchmark and compare it against a standard DQN with acomparable number of parameters. Our results demonstrate that the Ch-DQN with amoderate polynomial degree (N=4) achieves significantly better asymptoticperformance, outperforming the baseline by approximately 39\%. However, we alsofind that the choice of polynomial degree is a critical hyperparameter, as ahigh degree (N=8) can be detrimental to learning. This work validates thepotential of using orthogonal polynomial bases in deep reinforcement learningwhile also highlighting the trade-offs involved in model complexity.</description><author>Saman Yazdannik, Morteza Tayefi, Shamim Sanisales</author><pubDate>Wed, 20 Aug 2025 08:41:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14536v1</guid></item><item><title>Deep Exploration with PAC-Bayes</title><link>http://arxiv.org/abs/2402.03055v5</link><description>Reinforcement learning (RL) for continuous control under delayed rewards isan under-explored problem despite its significance in real-world applications.Many complex skills are based on intermediate ones as prerequisites. Forinstance, a humanoid locomotor must learn how to stand before it can learn towalk. To cope with delayed reward, an agent must perform deep exploration.However, existing deep exploration methods are designed for small discreteaction spaces, and their generalization to state-of-the-art continuous controlremains unproven. We address the deep exploration problem for the first timefrom a PAC-Bayesian perspective in the context of actor-critic learning. To dothis, we quantify the error of the Bellman operator through a PAC-Bayes bound,where a bootstrapped ensemble of critic networks represents the posteriordistribution, and their targets serve as a data-informed function-space prior.We derive an objective function from this bound and use it to train the criticensemble. Each critic trains an individual soft actor network, implemented as ashared trunk and critic-specific heads. The agent performs deep exploration byacting epsilon-softly on a randomly chosen actor head. Our proposed algorithm,named {\it PAC-Bayesian Actor-Critic (PBAC)}, is the only algorithm toconsistently discover delayed rewards on continuous control tasks with varyingdifficulty.</description><author>Bahareh Tasdighi, Manuel Haussmann, Nicklas Werge, Yi-Shan Wu, Melih Kandemir</author><pubDate>Wed, 20 Aug 2025 07:52:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.03055v5</guid></item><item><title>LaViPlan : Language-Guided Visual Path Planning with RLVR</title><link>http://arxiv.org/abs/2507.12911v4</link><description>Out-of-distribution (OOD) scenarios in autonomous driving pose criticalchallenges, as planners often fail to generalize beyond their trainingexperience, leading to unsafe or unexpected behavior. Vision-Language Models(VLMs) have shown promise in handling such scenarios by providing high-levelscene understanding and user-aligned decisions. However, existing VLMs oftenexhibit a misalignment between their language-based reasoning and the low-leveltrajectories required for action-level planning. In this paper, we proposeLaViPlan, a framework that leverages Reinforcement Learning with VerifiableRewards (RLVR) to fine-tune VLMs using planning-oriented metrics. Experimentalresults show that LaViPlan improves planning performance across both in-domainand out-of-domain datasets. While linguistic fidelity slightly decreases afterRLVR-based fine-tuning, qualitative evaluation indicates that the outputsremain coherent. We also conduct ablation studies to analyze the effects ofsampling ratio and reasoning guidance, highlighting how these design choicesinfluence performance. These findings demonstrate the potential of RLVR as apost-training paradigm for aligning language-guided reasoning with action-levelplanning in autonomous driving.</description><author>Hayeon Oh</author><pubDate>Wed, 20 Aug 2025 06:32:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.12911v4</guid></item><item><title>DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization</title><link>http://arxiv.org/abs/2508.14460v1</link><description>We present DuPO, a dual learning-based preference optimization framework thatgenerates annotation-free feedback via a generalized duality. DuPO addressestwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'sreliance on costly labels and applicability restricted to verifiable tasks, andtraditional dual learning's restriction to strictly dual task pairs (e.g.,translation and back-translation). Specifically, DuPO decomposes a primaltask's input into known and unknown components, then constructs its dual taskto reconstruct the unknown part using the primal output and known information(e.g., reversing math solutions to recover hidden variables), broadeningapplicability to non-invertible tasks. The quality of this reconstructionserves as a self-supervised reward to optimize the primal task, synergizingwith LLMs' ability to instantiate both tasks via a single model. Empirically,DuPO achieves substantial gains across diverse tasks: it enhances the averagetranslation quality by 2.13 COMET over 756 directions, boosts the mathematicalreasoning accuracy by an average of 6.4 points on three challenge benchmarks,and enhances performance by 9.3 points as an inference-time reranker (tradingcomputation for accuracy). These results position DuPO as a scalable, general,and annotation-free paradigm for LLM optimization.</description><author>Shuaijie She, Yu Bao, Yu Lu, Lu Xu, Tao Li, Wenhao Zhu, Shujian Huang, Shanbo Cheng, Lu Lu, Yuxuan Wang</author><pubDate>Wed, 20 Aug 2025 06:31:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14460v1</guid></item><item><title>When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs</title><link>http://arxiv.org/abs/2508.03365v2</link><description>As large language models become increasingly integrated into daily life,audio has emerged as a key interface for human-AI interaction. However, thisconvenience also introduces new vulnerabilities, making audio a potentialattack surface for adversaries. Our research introduces WhisperInject, atwo-stage adversarial audio attack framework that can manipulatestate-of-the-art audio language models to generate harmful content. Our methoduses imperceptible perturbations in audio inputs that remain benign to humanlisteners. The first stage uses a novel reward-based optimization method,Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide thetarget model to circumvent its own safety protocols and generate harmful nativeresponses. This native harmful response then serves as the target for Stage 2,Payload Injection, where we use Projected Gradient Descent (PGD) to optimizesubtle perturbations that are embedded into benign audio carriers, such asweather queries or greeting messages. Validated under the rigorousStrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluationframework, our experiments demonstrate a success rate exceeding 86% acrossQwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates anew class of practical, audio-native threats, moving beyond theoreticalexploits to reveal a feasible and covert method for manipulating AI behavior.</description><author>Bodam Kim, Hiskias Dingeto, Taeyoun Kwon, Dasol Choi, DongGeon Lee, Haon Park, JaeHoon Lee, Jongho Shin</author><pubDate>Wed, 20 Aug 2025 06:08:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.03365v2</guid></item><item><title>DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</title><link>http://arxiv.org/abs/2508.14391v1</link><description>Relation extraction enables the construction of structured knowledge for manydownstream applications. While large language models (LLMs) have shown greatpromise in this domain, most existing methods concentrate on relationclassification, which predicts the semantic relation type between a relatedentity pair. However, we observe that LLMs often struggle to reliably determinewhether a relation exists, especially in cases involving complex sentencestructures or intricate semantics, which leads to spurious predictions. Suchhallucinations can introduce noisy edges in knowledge graphs, compromising theintegrity of structured knowledge and downstream reliability. To address thesechallenges, we propose DEPTH, a framework that integrates Dependency-awaresEntence simPlification and Two-tiered Hierarchical refinement into therelation extraction pipeline. Given a sentence and its candidate entity pairs,DEPTH operates in two stages: (1) the Grounding module extracts relations foreach pair by leveraging their shortest dependency path, distilling the sentenceinto a minimal yet coherent relational context that reduces syntactic noisewhile preserving key semantics; (2) the Refinement module aggregates all localpredictions and revises them based on a holistic understanding of the sentence,correcting omissions and inconsistencies. We further introduce acausality-driven reward model that mitigates reward hacking by disentanglingspurious correlations, enabling robust fine-tuning via reinforcement learningwith human feedback. Experiments on six benchmarks demonstrate that DEPTHreduces the average hallucination rate to 7.0\% while achieving a 17.2\%improvement in average F1 score over state-of-the-art baselines.</description><author>Yupei Yang, Fan Feng, Lin Yang, Wanxi Deng, Lin Qu, Biwei Huang, Shikui Tu, Lei Xu</author><pubDate>Wed, 20 Aug 2025 03:35:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14391v1</guid></item><item><title>TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network</title><link>http://arxiv.org/abs/2508.14373v1</link><description>Computer-aided surgical simulation is a critical component of orthognathicsurgical planning, where accurately simulating face-bone shape transformationsis significant. The traditional biomechanical simulation methods are limited bytheir computational time consumption levels, labor-intensive data processingstrategies and low accuracy. Recently, deep learning-based simulation methodshave been proposed to view this problem as a point-to-point transformationbetween skeletal and facial point clouds. However, these approaches cannotprocess large-scale points, have limited receptive fields that lead to noisypoints, and employ complex preprocessing and postprocessing operations based onregistration. These shortcomings limit the performance and widespreadapplicability of such methods. Therefore, we propose a Transformer-basedcoarse-to-fine point movement network (TCFNet) to learn unique, complicatedcorrespondences at the patch and point levels for dense face-bone point cloudtransformations. This end-to-end framework adopts a Transformer-based networkand a local information aggregation network (LIA-Net) in the first and secondstages, respectively, which reinforce each other to generate precise pointmovement paths. LIA-Net can effectively compensate for the neighborhoodprecision loss of the Transformer-based network by modeling local geometricstructures (edges, orientations and relative position features). The previousglobal features are employed to guide the local displacement using a gatedrecurrent unit. Inspired by deformable medical image registration, we proposean auxiliary loss that can utilize expert knowledge for reconstructing criticalorgans.Compared with the existing state-of-the-art (SOTA) methods on gathereddatasets, TCFNet achieves outstanding evaluation metrics and visualizationresults. The code is available at https://github.com/Runshi-Zhang/TCFNet.</description><author>Runshi Zhang, Bimeng Jie, Yang He, Junchen Wang</author><pubDate>Wed, 20 Aug 2025 03:02:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14373v1</guid></item><item><title>CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</title><link>http://arxiv.org/abs/2508.02091v2</link><description>Approximate nearest-neighbor search (ANNS) algorithms have becomeincreasingly critical for recent AI applications, particularly inretrieval-augmented generation (RAG) and agent-based LLM applications. In thispaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNSoptimization as a reinforcement learning problem where execution speed servesas the reward signal. This approach enables the automatic generation ofprogressively faster ANNS implementations while maintaining accuracyconstraints. Our experimental evaluation demonstrates CRINN's effectivenessacross six widely-used NNS benchmark datasets. When compared againststate-of-the-art open-source ANNS algorithms, CRINN achieves best performanceon three of them (GIST-960-Euclidean, MNIST-784-Euclidean, andGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclideanand GloVe-25-angular). The implications of CRINN's success reach well beyondANNS optimization: It validates that LLMs augmented with reinforcement learningcan function as an effective tool for automating sophisticated algorithmicoptimizations that demand specialized knowledge and labor-intensive manualrefinement. Code can be found at https://github.com/deepreinforce-ai/CRINN</description><author>Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li</author><pubDate>Wed, 20 Aug 2025 01:47:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.02091v2</guid></item><item><title>CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning</title><link>http://arxiv.org/abs/2507.14111v7</link><description>The exponential growth in demand for GPU computing resources has created anurgent need for automated CUDA optimization strategies. While recent advancesin LLMs show promise for code generation, current SOTA models achieve lowsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, anautomated reinforcement learning framework for CUDA optimization that employs anovel contrastive RL algorithm. CUDA-L1 achieves significant performance improvements on the CUDAoptimization task: trained on A100, it delivers an average speedup of x3.12with a median speedup of x1.42 against default baselines over across all 250CUDA kernels of KernelBench, with peak speedups reaching x120. In addition tothe default baseline provided by KernelBench, CUDA-L1 demonstrates x2.77 overTorch Compile, x2.88 over Torch Compile with reduce overhead, x2.81 over CUDAGraph implementations, and remarkably x7.72 over cuDNN libraries. Furthermore,the model also demonstrates portability across different GPU architectures. Beyond these benchmark results, CUDA-L1 demonstrates several properties: it1) discovers a variety of CUDA optimization techniques and learns to combinethem strategically to achieve optimal performance; 2) uncovers fundamentalprinciples of CUDA optimization, such as the multiplicative nature ofoptimizations; 3) identifies non-obvious performance bottlenecks and rejectsseemingly beneficial optimizations that actually harm performance. Thecapabilities demonstrate that, RL can transform an initially poor-performingLLM into an effective CUDA optimizer through speedup-based reward signalsalone, without human expertise or domain knowledge. This paradigm openspossibilities for automated optimization of CUDA operations, and holds promiseto substantially promote GPU efficiency and alleviate the rising pressure onGPU computing resources.</description><author>Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum</author><pubDate>Wed, 20 Aug 2025 01:41:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14111v7</guid></item><item><title>Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems</title><link>http://arxiv.org/abs/2508.16574v1</link><description>This paper presents a hierarchical decision-making framework for autonomousnavigation in four-wheel independent steering and driving (4WISD) systems. Theproposed approach integrates deep reinforcement learning (DRL) for high-levelnavigation with fuzzy logic for low-level control to ensure both taskperformance and physical feasibility. The DRL agent generates global motioncommands, while the fuzzy logic controller enforces kinematic constraints toprevent mechanical strain and wheel slippage. Simulation experimentsdemonstrate that the proposed framework outperforms traditional navigationmethods, offering enhanced training efficiency and stability and mitigatingerratic behaviors compared to purely DRL-based solutions. Real-worldvalidations further confirm the framework's ability to navigate safely andeffectively in dynamic industrial settings. Overall, this work provides ascalable and reliable solution for deploying 4WISD mobile robots in complex,real-world scenarios.</description><author>Yizhi Wang, Degang Xu, Yongfang Xie, Shuzhong Tan, Xianan Zhou, Peng Chen</author><pubDate>Fri, 22 Aug 2025 17:57:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16574v1</guid></item><item><title>RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs</title><link>http://arxiv.org/abs/2508.16546v1</link><description>Training large language models (LLMs) from scratch is increasinglyimpractical, making post-training methods such as supervised fine-tuning (SFT)and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modernpractice. Using an out-of-distribution (OOD) variant of the 24-point card gameand new spectrum-based diagnostics, we revisit how these two stages reshapemodel representation and OOD performance. Our key findings are- (1) RL-FT canrestore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting anda clear distribution shift, RL-FT cannot fully recover OOD performance. (2)Direction shifts of singular vectors matter more than singular valuemagnitudes. These shifts concentrate on directions linked to the largest andsmallest singular values, leaving the bulk spectrum intact. (3) Low-rank andshallow recovery is effective: restoring singular vector directions for the top20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)Stronger SFT checkpoints enable better recovery by RL, while overfitted onesresist restoration. These results reconcile prior reports of RL superior OODperformance: RL primarily counteracts SFT-induced directional drift rather thanfinding new solutions. Our spectrum-aware analysis highlights inexpensiverecovery knobs low-rank UV merging and shallow-layer resets that practitionerscan use before costly RL fine-tuning.</description><author>Hangzhan Jin, Sicheng Lv, Sifan Wu, Mohammad Hamdaqa</author><pubDate>Fri, 22 Aug 2025 17:10:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16546v1</guid></item><item><title>A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning</title><link>http://arxiv.org/abs/2507.14295v2</link><description>Multi-turn problem solving is critical yet challenging for Large ReasoningModels (LRMs) to reflect on their reasoning and revise from feedback. ExistingReinforcement Learning (RL) methods train large reasoning models on asingle-turn paradigm with verifiable rewards. However, we observe that modelstrained with existing RL paradigms often lose their ability to solve problemsacross multiple turns and struggle to revise answers based on contextualfeedback, leading to repetitive responses. We ask: can LRMs learn to reflecttheir answers in a multi-turn context? In this work, we find that trainingmodels with multi-turn RL using only unary feedback (e.g., "Let's try again")after wrong answers can improve both single-turn performance and multi-turnreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcementlearning, which uses minimal yet common unary user feedback during iterativeproblem solving. It can be easily applied to existing single-turn RL trainingsetups. Experimental results show that RL training with UFO keeps single-turnperformance and improves multi-turn reasoning accuracy by up to 14%, enablinglanguage models to better react to feedback in multi-turn problem solving. Tofurther minimize the number of turns needed for a correct answer whileencouraging diverse reasoning when mistakes occur, we design reward structuresthat guide models to produce careful and deliberate answers in each turn. Code:https://github.com/lichengliu03/unary-feedback</description><author>Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, Manling Li</author><pubDate>Fri, 22 Aug 2025 16:49:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2507.14295v2</guid></item><item><title>Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation</title><link>http://arxiv.org/abs/2508.16521v1</link><description>Generating physically realistic 3D molecular structures remains a corechallenge in molecular generative modeling. While diffusion models equippedwith equivariant neural networks have made progress in capturing moleculargeometries, they often struggle to produce equilibrium structures that adhereto physical principles such as force field consistency. To bridge this gap, wepropose Reinforcement Learning with Physical Feedback (RLPF), a novel frameworkthat extends Denoising Diffusion Policy Optimization to 3D moleculargeneration. RLPF formulates the task as a Markov decision process and appliesproximal policy optimization to fine-tune equivariant diffusion models.Crucially, RLPF introduces reward functions derived from force-fieldevaluations, providing direct physical feedback to guide the generation towardenergetically stable and physically meaningful structures. Experiments on theQM9 and GEOM-drug datasets demonstrate that RLPF significantly improvesmolecular stability compared to existing methods. These results highlight thevalue of incorporating physics-based feedback into generative modeling. Thecode is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.</description><author>Zhijian Zhou, Junyi An, Zongkai Liu, Yunfei Shi, Xuan Zhang, Fenglei Cao, Chao Qu, Yuan Qi</author><pubDate>Fri, 22 Aug 2025 16:44:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16521v1</guid></item><item><title>On Zero-Shot Reinforcement Learning</title><link>http://arxiv.org/abs/2508.16496v1</link><description>Modern reinforcement learning (RL) systems capture deep truths about general,human problem-solving. In domains where new data can be simulated cheaply,these systems uncover sequential decision-making policies that far exceed theability of any human. Society faces many problems whose solutions require thisskill, but they are often in domains where new data cannot be cheaplysimulated. In such scenarios, we can learn simulators from existing data, butthese will only ever be approximately correct, and can be pathologicallyincorrect when queried outside of their training distribution. As a result, amisalignment between the environments in which we train our agents and thereal-world in which we wish to deploy our agents is inevitable. Dealing withthis misalignment is the primary concern of zero-shot reinforcement learning, aproblem setting where the agent must generalise to a new task or domain withzero practice shots. Whilst impressive progress has been made on methods thatperform zero-shot RL in idealised settings, new work is needed if these resultsare to be replicated in real-world settings. In this thesis, we argue thatdoing so requires us to navigate (at least) three constraints. First, the dataquality constraint: real-world datasets are small and homogeneous. Second, theobservability constraint: states, dynamics and rewards in the real-world areoften only partially observed. And third, the data availability constraint: apriori access to data cannot always be assumed. This work proposes a suite ofmethods that perform zero-shot RL subject to these constraints. In a series ofempirical studies we expose the failings of existing methods, and justify ourtechniques for remedying them. We believe these designs take us a step closerto RL methods that can be deployed to solve real-world problems.</description><author>Scott Jeen</author><pubDate>Fri, 22 Aug 2025 16:20:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16496v1</guid></item><item><title>Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)</title><link>http://arxiv.org/abs/2508.16474v1</link><description>This work presents a novel reinforcement learning (RL) algorithm based onY-wise Affine Neural Networks (YANNs). YANNs provide an interpretable neuralnetwork which can exactly represent known piecewise affine functions ofarbitrary input and output dimensions defined on any amount of polytopicsubdomains. One representative application of YANNs is to reformulate explicitsolutions of multi-parametric linear model predictive control. Built on this,we propose the use of YANNs to initialize RL actor and critic networks, whichenables the resulting YANN-RL control algorithm to start with the confidence oflinear optimal control. The YANN-actor is initialized by representing themulti-parametric control solutions obtained via offline computation using anapproximated linear system model. The YANN-critic represents the explicit formof the state-action value function for the linear system and the rewardfunction as the objective in an optimal control problem (OCP). Additionalnetwork layers are injected to extend YANNs for nonlinear expressions, whichcan be trained online by directly interacting with the true complex nonlinearsystem. In this way, both the policy and state-value functions exactlyrepresent a linear OCP initially and are able to eventually learn the solutionof a general nonlinear OCP. Continuous policy improvement is also implementedto provide heuristic confidence that the linear OCP solution serves as aneffective lower bound to the performance of RL policy. The YANN-RL algorithm isdemonstrated on a clipped pendulum and a safety-critical chemical-reactivesystem. Our results show that YANN-RL significantly outperforms the modern RLalgorithm using deep deterministic policy gradient, especially when consideringsafety constraints.</description><author>Austin Braniff, Yuhe Tian</author><pubDate>Fri, 22 Aug 2025 15:42:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16474v1</guid></item><item><title>Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS</title><link>http://arxiv.org/abs/2508.14313v2</link><description>Test-time scaling (TTS) for large language models (LLMs) has thus far falleninto two largely separate paradigms: (1) reinforcement learning (RL) methodsthat optimize sparse outcome-based rewards, yet suffer from instability and lowsample efficiency; and (2) search-based techniques guided by independentlytrained, static process reward models (PRMs), which require expensive human- orLLM-generated labels and often degrade under distribution shifts. In thispaper, we introduce AIRL-S, the first natural unification of RL-based andsearch-based TTS. Central to AIRL-S is the insight that the reward functionlearned during RL training inherently represents the ideal PRM for guidingdownstream search. Specifically, we leverage adversarial inverse reinforcementlearning (AIRL) combined with group relative policy optimization (GRPO) tolearn a dense, dynamic PRM directly from correct reasoning traces, entirelyeliminating the need for labeled intermediate process data. At inference, theresulting PRM simultaneously serves as the critic for RL rollouts and as aheuristic to effectively guide search procedures, facilitating robust reasoningchain extension, mitigating reward hacking, and enhancing cross-taskgeneralization. Experimental results across eight benchmarks, includingmathematics, scientific reasoning, and code generation, demonstrate that ourunified approach improves performance by 9 % on average over the base model,matching GPT-4o. Furthermore, when integrated into multiple search algorithms,our PRM consistently outperforms all baseline PRMs trained with labeled data.These results underscore that, indeed, your reward function for RL is your bestPRM for search, providing a robust and cost-effective solution to complexreasoning tasks in LLMs.</description><author>Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas</author><pubDate>Fri, 22 Aug 2025 15:37:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.14313v2</guid></item><item><title>Integrated Noise and Safety Management in UAM via A Unified Reinforcement Learning Framework</title><link>http://arxiv.org/abs/2508.16440v1</link><description>Urban Air Mobility (UAM) envisions the widespread use of small aerialvehicles to transform transportation in dense urban environments. However, UAMfaces critical operational challenges, particularly the balance betweenminimizing noise exposure and maintaining safe separation in low-altitude urbanairspace, two objectives that are often addressed separately. We propose areinforcement learning (RL)-based air traffic management system that integratesboth noise and safety considerations within a unified, decentralized framework.Under this scalable air traffic coordination solution, agents operate in astructured, multi-layered airspace and learn altitude adjustment policies tojointly manage noise impact and separation constraints. The system demonstratesstrong performance across both objectives and reveals tradeoffs amongseparation, noise exposure, and energy efficiency under high traffic density.The findings highlight the potential of RL and multi-objective coordinationstrategies in enhancing the safety, quietness, and efficiency of UAMoperations.</description><author>Surya Murthy, Zhenyu Gao, John-Paul Clarke, Ufuk Topcu</author><pubDate>Fri, 22 Aug 2025 14:56:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16440v1</guid></item><item><title>OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval</title><link>http://arxiv.org/abs/2508.16438v1</link><description>Recent advances in large language models (LLMs) and dense retrievers havedriven significant progress in retrieval-augmented generation (RAG). However,existing approaches face significant challenges in complex reasoning-orientedmulti-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Priormethods struggle to generate robust multi-step plans for complex queries, asrule-based decomposers perform poorly on out-of-template questions. 2)Suboptimal reasoning-driven retrieval: Related methods employ limited queryreformulation, leading to iterative retrieval loops that often fail to locategolden documents. 3) Insufficient reasoning-guided filtering: Prevailingmethods lack the fine-grained reasoning to effectively filter salientinformation from noisy results, hindering utilization of retrieved knowledge.Fundamentally, these limitations all stem from the weak coupling betweenretrieval and reasoning in current RAG architectures. We introduce theOrchestrated Planner-Executor Reasoning Architecture (OPERA), a novelreasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM)decomposes questions into sub-goals, which are executed by a Reason-ExecuteModule (REM) with specialized components for precise reasoning and effectiveretrieval. To train OPERA, we propose Multi-Agents Progressive Group RelativePolicy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complexmulti-hop benchmarks show OPERA's superior performance, validating both theMAPGRPO method and OPERA's design. Code is available athttps://github.com/Ameame1/OPERA.</description><author>Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma</author><pubDate>Fri, 22 Aug 2025 14:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16438v1</guid></item><item><title>Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning</title><link>http://arxiv.org/abs/2508.16420v1</link><description>Offline reinforcement learning (RL) has achieved significant advances indomains such as robotic control, autonomous driving, and medicaldecision-making. Most existing methods primarily focus on training policiesthat maximize cumulative returns from a given dataset. However, many real-worldapplications require precise control over policy performance levels, ratherthan simply pursuing the best possible return. Reinforcement learning viasupervised learning (RvS) frames offline RL as a sequence modeling task,enabling the extraction of diverse policies by conditioning on differentdesired returns. Yet, existing RvS-based transformers, such as DecisionTransformer (DT), struggle to reliably align the actual achieved returns withspecified target returns, especially when interpolating within underrepresentedreturns or extrapolating beyond the dataset. To address this limitation, wepropose Doctor, a novel approach that Double Checks the Transformer with targetalignment for Offline RL. Doctor achieves superior target alignment both withinand beyond the dataset, while enabling accurate and flexible control overpolicy performance. Notably, on the dynamic treatment regime benchmark,EpiCare, our approach effectively modulates treatment policy aggressiveness,balancing therapeutic returns against adverse event risk.</description><author>Yue Pei, Hongming Zhang, Chao Gao, Martin Müller, Mengxiao Zhu, Hao Sheng, Haogang Zhu, Liang Lin</author><pubDate>Fri, 22 Aug 2025 14:30:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.16420v1</guid></item><item><title>Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</title><link>http://arxiv.org/abs/2508.11618v1</link><description>Carbon capture and storage (CCS) projects typically involve a diverse arrayof stakeholders or players from public, private, and regulatory sectors, eachwith different objectives and responsibilities. Given the complexity, scale,and long-term nature of CCS operations, determining whether individualstakeholders can independently maximize their interests or whethercollaborative coalition agreements are needed remains a central question foreffective CCS project planning and management. CCS projects are oftenimplemented in geologically connected sites, where shared geological featuressuch as pressure space and reservoir pore capacity can lead to competitivebehavior among stakeholders. Furthermore, CO2 storage sites are often locatedin geologically mature basins that previously served as sites for hydrocarbonextraction or wastewater disposal in order to leverage existinginfrastructures, which makes unilateral optimization even more complicated andunrealistic. In this work, we propose a paradigm based on Markov games to quantitativelyinvestigate how different coalition structures affect the goals ofstakeholders. We frame this multi-stakeholder multi-site problem as amulti-agent reinforcement learning problem with safety constraints. Ourapproach enables agents to learn optimal strategies while compliant with safetyregulations. We present an example where multiple operators are injecting CO2into their respective project areas in a geologically connected basin. Toaddress the high computational cost of repeated simulations of high-fidelitymodels, a previously developed surrogate model based on the Embed-to-Control(E2C) framework is employed. Our results demonstrate the effectiveness of theproposed framework in addressing optimal management of CO2 storage whenmultiple stakeholders with various objectives and goals are involved.</description><author>Jungang Chen, Seyyed A. Hosseini</author><pubDate>Fri, 15 Aug 2025 17:36:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11618v1</guid></item><item><title>Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms</title><link>http://arxiv.org/abs/2506.09457v2</link><description>Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization(DPO) and Simple Preference Optimization (SimPO), have emerged as efficientalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithmsfor aligning large language models (LLMs) with human preferences. However, DAAssuffer from a fundamental limitation we identify as the "reward-generation gap"-- a misalignment between optimization objectives during training and actualgeneration performance during inference. In this paper, we find a contributorto the reward-generation gap is the mismatch between the inherent importance ofprefix tokens during the LLM generation process and how this importance isreflected in the implicit reward functions of DAAs. To bridge the gap, we adopta token-level MDP perspective of DAAs to analyze its limitations and introducea simple yet effective approach called Prefix-Oriented Equal-length Training(POET), which truncates both preferred and dispreferred responses to match theshorter one's length. Training with \mname, where both responses in each sampleare truncated to equal length, resulting in diverse truncated lengths acrosssamples, the optimization of DAAs objective is implicitly constrained toconverge across all timesteps of token-level MDP, thus paying more attention toprefix tokens than the standard DAAs. We conduct experiments with DPO andSimPO, two representative DAAs, demonstrating that POET improves over theirstandard implementations, achieving up to 15.6 points in AlpacaEval 2 andoverall improvements across downstream tasks. Our results highlight theimportance of addressing the misalignment between reward optimization andgeneration performance in DAAs.</description><author>Zeguan Xiao, Yun Chen, Guanhua Chen, Ke Tang</author><pubDate>Fri, 22 Aug 2025 14:16:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2506.09457v2</guid></item></channel></rss>