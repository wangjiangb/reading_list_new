<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Thu, 28 Aug 2025 13:00:11 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title><link>http://arxiv.org/abs/2508.11598v1</link><description>We introduce AuriStream, a biologically inspired model for encoding speechvia a two-stage framework inspired by the human auditory processing hierarchy.The first stage transforms raw audio into a time-frequency representation basedon the human cochlea, from which we extract discrete \textbf{cochlear tokens}.The second stage applies an autoregressive sequence model over the cochleartokens. AuriStream learns meaningful phoneme and word representations, andstate-of-the-art lexical semantics. AuriStream shows competitive performance ondiverse downstream SUPERB speech tasks. Complementing AuriStream's strongrepresentational capabilities, it generates continuations of audio which can bevisualized in a spectrogram space and decoded back into audio, providinginsights into the model's predictions. In summary, we present a two-stageframework for speech representation learning to advance the development of morehuman-like models that efficiently handle a range of speech-based tasks.</description><author>Greta Tuckute, Klemen Kotar, Evelina Fedorenko, Daniel L. K. Yamins</author><pubDate>Fri, 15 Aug 2025 17:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11598v1</guid></item><item><title>When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing</title><link>http://arxiv.org/abs/2508.10482v2</link><description>In the study of trustworthy Natural Language Processing (NLP), a number ofimportant research fields have emerged, including that of explainability andprivacy. While research interest in both explainable and privacy-preserving NLPhas increased considerably in recent years, there remains a lack ofinvestigation at the intersection of the two. This leaves a considerable gap inunderstanding of whether achieving both explainability and privacy is possible,or whether the two are at odds with each other. In this work, we conduct anempirical investigation into the privacy-explainability trade-off in thecontext of NLP, guided by the popular overarching methods of DifferentialPrivacy (DP) and Post-hoc Explainability. Our findings include a view into theintricate relationship between privacy and explainability, which is formed by anumber of factors, including the nature of the downstream task and choice ofthe text privatization and explainability method. In this, we highlight thepotential for privacy and explainability to co-exist, and we summarize ourfindings in a collection of practical recommendations for future work at thisimportant intersection.</description><author>Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci</author><pubDate>Fri, 15 Aug 2025 13:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10482v2</guid></item><item><title>A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design</title><link>http://arxiv.org/abs/2508.10899v1</link><description>AI-driven discovery can greatly reduce design time and enhance newtherapeutics' effectiveness. Models using simulators explore broad designspaces but risk violating implicit constraints due to a lack of experimentalpriors. For example, in a new analysis we performed on a diverse set of modelson the GuacaMol benchmark using supervised classifiers, over 60\% of moleculesproposed had high probability of being mutagenic. In this work, we introduce\ourdataset, a dataset of priors for design problems extracted from literaturedescribing compounds used in lab settings. It is constructed with LLM pipelinesfor discovering therapeutic entities in relevant paragraphs and summarizinginformation in concise fair-use facts. \ourdataset~ consists of 32.3 millionpairs of natural language facts, and appropriate entity representations (i.e.SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,CLIP, and LLava architectures to reason jointly about text and design targetsand evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~ishighly effective for creating models with strong priors: in supervisedprediction problems that use our data as pretraining, our best models with 15Mlearnable parameters outperform larger 2B TxGemma on both regression andclassification TDC tasks, and perform comparably to 9B models on average.Models built with \ourdataset~can be used as constraints while optimizing fornovel molecules in GuacaMol, resulting in proposals that are safer and nearlyas effective. We release our dataset at\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},and will provide expanded versions as available literature grows.</description><author>Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar</author><pubDate>Thu, 14 Aug 2025 17:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10899v1</guid></item><item><title>Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models</title><link>http://arxiv.org/abs/2504.19061v3</link><description>Clinical summarization is crucial in healthcare as it distills complexmedical data into digestible information, enhancing patient understanding andcare management. Large language models (LLMs) have shown significant potentialin automating and improving the accuracy of such summarizations due to theiradvanced natural language understanding capabilities. These models areparticularly applicable in the context of summarizing medical/clinical texts,where precise and concise information transfer is essential. In this paper, weinvestigate the effectiveness of open-source LLMs in extracting key events fromdischarge reports, including admission reasons, major in-hospital events, andcritical follow-up actions. In addition, we also assess the prevalence ofvarious types of hallucinations in the summaries produced by these models.Detecting hallucinations is vital as it directly influences the reliability ofthe information, potentially affecting patient care and treatment outcomes. Weconduct comprehensive simulations to rigorously evaluate the performance ofthese models, further probing the accuracy and fidelity of the extractedcontent in clinical summarization. Our results reveal that while the LLMs(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admissionreasons and hospitalization events, they are generally less consistent when itcomes to identifying follow-up recommendations, highlighting broader challengesin leveraging LLMs for comprehensive summarization.</description><author>Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib</author><pubDate>Wed, 20 Aug 2025 14:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19061v3</guid></item><item><title>Neither Valid nor Reliable? Investigating the Use of LLMs as Judges</title><link>http://arxiv.org/abs/2508.18076v1</link><description>Evaluating natural language generation (NLG) systems remains a core challengeof natural language processing (NLP), further complicated by the rise of largelanguage models (LLMs) that aims to be general-purpose. Recently, largelanguage models as judges (LLJs) have emerged as a promising alternative totraditional metrics, but their validity remains underexplored. This positionpaper argues that the current enthusiasm around LLJs may be premature, as theiradoption has outpaced rigorous scrutiny of their reliability and validity asevaluators. Drawing on measurement theory from the social sciences, we identifyand critically assess four core assumptions underlying the use of LLJs: theirability to act as proxies for human judgment, their capabilities as evaluators,their scalability, and their cost-effectiveness. We examine how each of theseassumptions may be challenged by the inherent limitations of LLMs, LLJs, orcurrent practices in NLG evaluation. To ground our analysis, we explore threeapplications of LLJs: text summarization, data annotation, and safetyalignment. Finally, we highlight the need for more responsible evaluationpractices in LLJs evaluation, to ensure that their growing role in the fieldsupports, rather than undermines, progress in NLG.</description><author>Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, Golnoosh Farnadi</author><pubDate>Mon, 25 Aug 2025 14:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18076v1</guid></item><item><title>Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications</title><link>http://arxiv.org/abs/2502.13358v3</link><description>Large Language Models (LLMs) have significantly advanced natural languageprocessing, demonstrating strong capabilities in tasks such as text generation,summarization, and reasoning. Recently, their potential for automating precisetext editing tasks across specialized domains, such as programming code, LaTeX,and structured database languages, has gained attention. However, currentstate-of-the-art LLMs still struggle with executing precise, instruction-drivenedits, particularly when structural accuracy and strict adherence to domainconventions are required. To address these challenges, we introduceInstrEditBench, an automated benchmark dataset comprising over 30,000structured editing tasks spanning diverse domains, including Wikipediaarticles, LaTeX documents, source code, and database languages. Using thisbenchmark, we develop FineEdit, a specialized editing model explicitly trainedfor accurate, context-aware text modifications. Experimental evaluationsdemonstrate that FineEdit outperforms state-of-the-art models, achievingimprovements of approximately 10\% over Gemini models on single-turn edits, upto 30\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance byover 40\% on direct editing tasks. FineEdit also effectively generalizes torealistic multi-turn editing scenarios, highlighting its practicalapplicability. To facilitate further research and reproducibility, we releaseFineEdit at https://github.com/StuRinDQB/FineEdit} andhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench.</description><author>Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu</author><pubDate>Tue, 26 Aug 2025 17:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.13358v3</guid></item><item><title>SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?</title><link>http://arxiv.org/abs/2503.06029v2</link><description>Large Language Models (LLMs) have become integral to daily life, especiallyadvancing as intelligent assistants through on-device deployment onsmartphones. However, existing LLM evaluation benchmarks predominantly focus onobjective tasks like mathematics and coding in English, which do notnecessarily reflect the practical use cases of on-device LLMs in real-worldmobile scenarios, especially for Chinese users. To address these gaps, weintroduce SmartBench, the first benchmark designed to evaluate the capabilitiesof on-device LLMs in Chinese mobile contexts. We analyze functionalitiesprovided by representative smartphone manufacturers and divide them into fivecategories: text summarization, text Q&amp;A, information extraction, contentcreation, and notification management, further detailed into 20 specific tasks.For each task, we construct high-quality datasets comprising 50 to 200question-answer pairs that reflect everyday mobile interactions, and we developautomated evaluation criteria tailored for these tasks. We conductcomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and alsoassess their performance after quantized deployment on real smartphone NPUs.Our contributions provide a standardized framework for evaluating on-deviceLLMs in Chinese, promoting further development and optimization in thiscritical area. Code and data will be available athttps://github.com/vivo-ai-lab/SmartBench.</description><author>Xudong Lu, Haohao Gao, Renshou Wu, Shuai Ren, Xiaoxin Chen, Hongsheng Li, Fangyuan Li</author><pubDate>Tue, 26 Aug 2025 14:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.06029v2</guid></item><item><title>LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination</title><link>http://arxiv.org/abs/2508.18791v1</link><description>Despite the remarkable progress of modern machine translation (MT) systems ongeneral-domain texts, translating structured LaTeX-formatted documents remainsa significant challenge. These documents typically interleave natural languagewith domain-specific syntax, such as mathematical equations, tables, figures,and cross-references, all of which must be accurately preserved to maintainsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, acollaborative multi-agent system designed to address this challenge. LaTeXTransensures format preservation, structural fidelity, and terminology consistencythrough six specialized agents: 1) a Parser that decomposes LaTeX intotranslation-friendly units via placeholder substitution and syntax filtering;2) a Translator, Validator, Summarizer, and Terminology Extractor that workcollaboratively to ensure context-aware, self-correcting, andterminology-consistent translations; 3) a Generator that reconstructs thetranslated content into well-structured LaTeX documents. Experimental resultsdemonstrate that LaTeXTrans can outperform mainstream MT systems in bothtranslation accuracy and structural fidelity, offering an effective andpractical solution for translating LaTeX-formatted documents.</description><author>Ziming Zhu, Chenglong Wang, Shunjie Xing, Yifu Huo, Fengning Tian, Quan Du, Di Yang, Chunliang Zhang, Tong Xiao, Jingbo Zhu</author><pubDate>Tue, 26 Aug 2025 08:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18791v1</guid></item></channel></rss>