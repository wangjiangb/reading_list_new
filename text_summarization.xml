<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Wed, 10 Sep 2025 13:00:21 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title><link>http://arxiv.org/abs/2508.11598v1</link><description>We introduce AuriStream, a biologically inspired model for encoding speechvia a two-stage framework inspired by the human auditory processing hierarchy.The first stage transforms raw audio into a time-frequency representation basedon the human cochlea, from which we extract discrete \textbf{cochlear tokens}.The second stage applies an autoregressive sequence model over the cochleartokens. AuriStream learns meaningful phoneme and word representations, andstate-of-the-art lexical semantics. AuriStream shows competitive performance ondiverse downstream SUPERB speech tasks. Complementing AuriStream's strongrepresentational capabilities, it generates continuations of audio which can bevisualized in a spectrogram space and decoded back into audio, providinginsights into the model's predictions. In summary, we present a two-stageframework for speech representation learning to advance the development of morehuman-like models that efficiently handle a range of speech-based tasks.</description><author>Greta Tuckute, Klemen Kotar, Evelina Fedorenko, Daniel L. K. Yamins</author><pubDate>Fri, 15 Aug 2025 17:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11598v1</guid></item><item><title>When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing</title><link>http://arxiv.org/abs/2508.10482v2</link><description>In the study of trustworthy Natural Language Processing (NLP), a number ofimportant research fields have emerged, including that of explainability andprivacy. While research interest in both explainable and privacy-preserving NLPhas increased considerably in recent years, there remains a lack ofinvestigation at the intersection of the two. This leaves a considerable gap inunderstanding of whether achieving both explainability and privacy is possible,or whether the two are at odds with each other. In this work, we conduct anempirical investigation into the privacy-explainability trade-off in thecontext of NLP, guided by the popular overarching methods of DifferentialPrivacy (DP) and Post-hoc Explainability. Our findings include a view into theintricate relationship between privacy and explainability, which is formed by anumber of factors, including the nature of the downstream task and choice ofthe text privatization and explainability method. In this, we highlight thepotential for privacy and explainability to co-exist, and we summarize ourfindings in a collection of practical recommendations for future work at thisimportant intersection.</description><author>Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci</author><pubDate>Fri, 15 Aug 2025 13:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10482v2</guid></item><item><title>A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design</title><link>http://arxiv.org/abs/2508.10899v1</link><description>AI-driven discovery can greatly reduce design time and enhance newtherapeutics' effectiveness. Models using simulators explore broad designspaces but risk violating implicit constraints due to a lack of experimentalpriors. For example, in a new analysis we performed on a diverse set of modelson the GuacaMol benchmark using supervised classifiers, over 60\% of moleculesproposed had high probability of being mutagenic. In this work, we introduce\ourdataset, a dataset of priors for design problems extracted from literaturedescribing compounds used in lab settings. It is constructed with LLM pipelinesfor discovering therapeutic entities in relevant paragraphs and summarizinginformation in concise fair-use facts. \ourdataset~ consists of 32.3 millionpairs of natural language facts, and appropriate entity representations (i.e.SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,CLIP, and LLava architectures to reason jointly about text and design targetsand evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~ishighly effective for creating models with strong priors: in supervisedprediction problems that use our data as pretraining, our best models with 15Mlearnable parameters outperform larger 2B TxGemma on both regression andclassification TDC tasks, and perform comparably to 9B models on average.Models built with \ourdataset~can be used as constraints while optimizing fornovel molecules in GuacaMol, resulting in proposals that are safer and nearlyas effective. We release our dataset at\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},and will provide expanded versions as available literature grows.</description><author>Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar</author><pubDate>Thu, 14 Aug 2025 17:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10899v1</guid></item><item><title>Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models</title><link>http://arxiv.org/abs/2504.19061v3</link><description>Clinical summarization is crucial in healthcare as it distills complexmedical data into digestible information, enhancing patient understanding andcare management. Large language models (LLMs) have shown significant potentialin automating and improving the accuracy of such summarizations due to theiradvanced natural language understanding capabilities. These models areparticularly applicable in the context of summarizing medical/clinical texts,where precise and concise information transfer is essential. In this paper, weinvestigate the effectiveness of open-source LLMs in extracting key events fromdischarge reports, including admission reasons, major in-hospital events, andcritical follow-up actions. In addition, we also assess the prevalence ofvarious types of hallucinations in the summaries produced by these models.Detecting hallucinations is vital as it directly influences the reliability ofthe information, potentially affecting patient care and treatment outcomes. Weconduct comprehensive simulations to rigorously evaluate the performance ofthese models, further probing the accuracy and fidelity of the extractedcontent in clinical summarization. Our results reveal that while the LLMs(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admissionreasons and hospitalization events, they are generally less consistent when itcomes to identifying follow-up recommendations, highlighting broader challengesin leveraging LLMs for comprehensive summarization.</description><author>Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib</author><pubDate>Wed, 20 Aug 2025 14:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2504.19061v3</guid></item><item><title>Neither Valid nor Reliable? Investigating the Use of LLMs as Judges</title><link>http://arxiv.org/abs/2508.18076v1</link><description>Evaluating natural language generation (NLG) systems remains a core challengeof natural language processing (NLP), further complicated by the rise of largelanguage models (LLMs) that aims to be general-purpose. Recently, largelanguage models as judges (LLJs) have emerged as a promising alternative totraditional metrics, but their validity remains underexplored. This positionpaper argues that the current enthusiasm around LLJs may be premature, as theiradoption has outpaced rigorous scrutiny of their reliability and validity asevaluators. Drawing on measurement theory from the social sciences, we identifyand critically assess four core assumptions underlying the use of LLJs: theirability to act as proxies for human judgment, their capabilities as evaluators,their scalability, and their cost-effectiveness. We examine how each of theseassumptions may be challenged by the inherent limitations of LLMs, LLJs, orcurrent practices in NLG evaluation. To ground our analysis, we explore threeapplications of LLJs: text summarization, data annotation, and safetyalignment. Finally, we highlight the need for more responsible evaluationpractices in LLJs evaluation, to ensure that their growing role in the fieldsupports, rather than undermines, progress in NLG.</description><author>Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, Golnoosh Farnadi</author><pubDate>Mon, 25 Aug 2025 14:43:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18076v1</guid></item><item><title>Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications</title><link>http://arxiv.org/abs/2502.13358v3</link><description>Large Language Models (LLMs) have significantly advanced natural languageprocessing, demonstrating strong capabilities in tasks such as text generation,summarization, and reasoning. Recently, their potential for automating precisetext editing tasks across specialized domains, such as programming code, LaTeX,and structured database languages, has gained attention. However, currentstate-of-the-art LLMs still struggle with executing precise, instruction-drivenedits, particularly when structural accuracy and strict adherence to domainconventions are required. To address these challenges, we introduceInstrEditBench, an automated benchmark dataset comprising over 30,000structured editing tasks spanning diverse domains, including Wikipediaarticles, LaTeX documents, source code, and database languages. Using thisbenchmark, we develop FineEdit, a specialized editing model explicitly trainedfor accurate, context-aware text modifications. Experimental evaluationsdemonstrate that FineEdit outperforms state-of-the-art models, achievingimprovements of approximately 10\% over Gemini models on single-turn edits, upto 30\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance byover 40\% on direct editing tasks. FineEdit also effectively generalizes torealistic multi-turn editing scenarios, highlighting its practicalapplicability. To facilitate further research and reproducibility, we releaseFineEdit at https://github.com/StuRinDQB/FineEdit} andhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench.</description><author>Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu</author><pubDate>Tue, 26 Aug 2025 17:11:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2502.13358v3</guid></item><item><title>SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?</title><link>http://arxiv.org/abs/2503.06029v2</link><description>Large Language Models (LLMs) have become integral to daily life, especiallyadvancing as intelligent assistants through on-device deployment onsmartphones. However, existing LLM evaluation benchmarks predominantly focus onobjective tasks like mathematics and coding in English, which do notnecessarily reflect the practical use cases of on-device LLMs in real-worldmobile scenarios, especially for Chinese users. To address these gaps, weintroduce SmartBench, the first benchmark designed to evaluate the capabilitiesof on-device LLMs in Chinese mobile contexts. We analyze functionalitiesprovided by representative smartphone manufacturers and divide them into fivecategories: text summarization, text Q&amp;A, information extraction, contentcreation, and notification management, further detailed into 20 specific tasks.For each task, we construct high-quality datasets comprising 50 to 200question-answer pairs that reflect everyday mobile interactions, and we developautomated evaluation criteria tailored for these tasks. We conductcomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and alsoassess their performance after quantized deployment on real smartphone NPUs.Our contributions provide a standardized framework for evaluating on-deviceLLMs in Chinese, promoting further development and optimization in thiscritical area. Code and data will be available athttps://github.com/vivo-ai-lab/SmartBench.</description><author>Xudong Lu, Haohao Gao, Renshou Wu, Shuai Ren, Xiaoxin Chen, Hongsheng Li, Fangyuan Li</author><pubDate>Tue, 26 Aug 2025 14:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2503.06029v2</guid></item><item><title>LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination</title><link>http://arxiv.org/abs/2508.18791v1</link><description>Despite the remarkable progress of modern machine translation (MT) systems ongeneral-domain texts, translating structured LaTeX-formatted documents remainsa significant challenge. These documents typically interleave natural languagewith domain-specific syntax, such as mathematical equations, tables, figures,and cross-references, all of which must be accurately preserved to maintainsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, acollaborative multi-agent system designed to address this challenge. LaTeXTransensures format preservation, structural fidelity, and terminology consistencythrough six specialized agents: 1) a Parser that decomposes LaTeX intotranslation-friendly units via placeholder substitution and syntax filtering;2) a Translator, Validator, Summarizer, and Terminology Extractor that workcollaboratively to ensure context-aware, self-correcting, andterminology-consistent translations; 3) a Generator that reconstructs thetranslated content into well-structured LaTeX documents. Experimental resultsdemonstrate that LaTeXTrans can outperform mainstream MT systems in bothtranslation accuracy and structural fidelity, offering an effective andpractical solution for translating LaTeX-formatted documents.</description><author>Ziming Zhu, Chenglong Wang, Shunjie Xing, Yifu Huo, Fengning Tian, Quan Du, Di Yang, Chunliang Zhang, Tong Xiao, Jingbo Zhu</author><pubDate>Tue, 26 Aug 2025 08:17:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.18791v1</guid></item><item><title>Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts</title><link>http://arxiv.org/abs/2508.19578v1</link><description>We introduce HAMLET, a holistic and automated framework for evaluating thelong-context comprehension of large language models (LLMs). HAMLET structuressource texts into a three-level key-fact hierarchy at root-, branch-, andleaf-levels, and employs query-focused summarization to evaluate how wellmodels recall and faithfully represent information at each level. To validatethe reliability of our fully automated pipeline, we conduct a systematic humanstudy, showing that our automatic evaluation achieves over 90% agreement withexpert human judgments, while reducing the cost by up to 25 times. HAMLETreveals that LLMs struggle with fine-grained comprehension, especially at theleaf level, and are sensitive to positional effects like thelost-in-the-middle. Analytical queries pose greater challenges than narrativeones, and consistent performance gaps emerge between open-source andproprietary models, as well as across model scales. Our code and dataset arepublicly available at https://github.com/DISL-Lab/HAMLET.</description><author>Jiaqi Deng, Yuho Lee, Nicole Hee-Yeon Kim, Hyangsuk Min, Taewon Yun, Minjeong Ban, Kim Yul, Hwanjun Song</author><pubDate>Wed, 27 Aug 2025 05:23:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.19578v1</guid></item><item><title>Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision</title><link>http://arxiv.org/abs/2508.20729v1</link><description>Large language models (LLMs) serve as an active and promising field ofgenerative artificial intelligence and have demonstrated abilities to performcomplex tasks in multiple domains, including mathematical and scientificreasoning. In this work, we construct a novel agent framework for solvingrepresentative problems in scientific computing. The proposed agent,incorporating a "rewriting-resolution-review-revision" logical chain via threereasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,respectively), is integrated in a collaborative and interactive manner. TheConsultant module endows the agent with knowledge transfer capabilities to linkproblems to professional domain insights, thereby rewriting problemdescriptions through text augmentation. The Programmer module is responsiblefor generating and executing well-structured code to deliver the problemresolution. The Reviewer module equips the agent with the capacity forself-debugging and self-refinement through interactive feedback with coderuntime outputs. By leveraging the end-to-end review mechanism, the executablecode provided by the Programmer attains the iterative revision. A comprehensiveevaluation is conducted on the performance of the proposed agent framework insolving PDEs, ill-conditioned linear systems, and data-driven physical analysisproblems. Compared to single-model, this collaborative framework significantlyimproves the bug-free code generation rate and reduces the occurrence ofnon-physical solutions, thereby establishing a highly reliable framework forautonomous code generation based on natural language descriptions. The reviewmechanism improved the average execution success (bug-free code and non-NaNsolutions) rate of the latest reasoning models. In summary, our agent frameworkestablishes automatic code generation and review as a promising scientificcomputing paradigm.</description><author>Ao Cheng, Lei Zhang, Guowei He</author><pubDate>Thu, 28 Aug 2025 12:50:48 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.20729v1</guid></item><item><title>PreGenie: An Agentic Framework for High-quality Visual Presentation Generation</title><link>http://arxiv.org/abs/2505.21660v2</link><description>Visual presentations are vital for effective communication. Early attempts toautomate their creation using deep learning often faced issues such as poorlyorganized layouts, inaccurate text summarization, and a lack of imageunderstanding, leading to mismatched visuals and text. These limitationsrestrict their application in formal contexts like business and scientificresearch. To address these challenges, we propose PreGenie, an agentic andmodular framework powered by multimodal large language models (MLLMs) forgenerating high-quality visual presentations. PreGenie is built on the Slidev presentation framework, where slides arerendered from Markdown code. It operates in two stages: (1) Analysis andInitial Generation, which summarizes multimodal input and generates initialcode, and (2) Review and Re-generation, which iteratively reviews intermediatecode and rendered slides to produce final, high-quality presentations. Eachstage leverages multiple MLLMs that collaborate and share information.Comprehensive experiments demonstrate that PreGenie excels in multimodalunderstanding, outperforming existing models in both aesthetics and contentconsistency, while aligning more closely with human design preferences.</description><author>Xiaojie Xu, Xinli Xu, Sirui Chen, Haoyu Chen, Fan Zhang, Ying-Cong Chen</author><pubDate>Sun, 31 Aug 2025 00:34:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2505.21660v2</guid></item><item><title>One Flight Over the Gap: A Survey from Perspective to Panoramic Vision</title><link>http://arxiv.org/abs/2509.04444v2</link><description>Driven by the demand for spatial intelligence and holistic scene perception,omnidirectional images (ODIs), which provide a complete 360\textdegree{} fieldof view, are receiving growing attention across diverse applications such asvirtual reality, autonomous driving, and embodied robotics. Despite theirunique characteristics, ODIs exhibit remarkable differences from perspectiveimages in geometric projection, spatial distribution, and boundary continuity,making it challenging for direct domain adaption from perspective methods. Thissurvey reviews recent panoramic vision techniques with a particular emphasis onthe perspective-to-panorama adaptation. We first revisit the panoramic imagingpipeline and projection methods to build the prior knowledge required foranalyzing the structural disparities. Then, we summarize three challenges ofdomain adaptation: severe geometric distortions near the poles, non-uniformsampling in Equirectangular Projection (ERP), and periodic boundary continuity.Building on this, we cover 20+ representative tasks drawn from more than 300research papers in two dimensions. On one hand, we present a cross-methodanalysis of representative strategies for addressing panoramic specificchallenges across different tasks. On the other hand, we conduct a cross-taskcomparison and classify panoramic vision into four major categories: visualquality enhancement and assessment, visual understanding, multimodalunderstanding, and visual generation. In addition, we discuss open challengesand future directions in data, models, and applications that will drive theadvancement of panoramic vision research. We hope that our work can provide newinsight and forward looking perspectives to advance the development ofpanoramic vision technologies. Our project page ishttps://insta360-research-team.github.io/Survey-of-Panorama</description><author>Xin Lin, Xian Ge, Dizhe Zhang, Zhaoliang Wan, Xianshun Wang, Xiangtai Li, Wenjie Jiang, Bo Du, Dacheng Tao, Ming-Hsuan Yang, Lu Qi</author><pubDate>Tue, 09 Sep 2025 15:29:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.04444v2</guid></item><item><title>Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems</title><link>http://arxiv.org/abs/2509.07817v1</link><description>Textual response generation is pivotal for multimodal \mbox{task-oriented}dialog systems, which aims to generate proper textual responses based on themultimodal context. While existing efforts have demonstrated remarkableprogress, there still exist the following limitations: 1) \textit{neglect ofunstructured review knowledge} and 2) \textit{underutilization of largelanguage models (LLMs)}. Inspired by this, we aim to fully utilize dualknowledge (\textit{i.e., } structured attribute and unstructured reviewknowledge) with LLMs to promote textual response generation in multimodaltask-oriented dialog systems. However, this task is non-trivial due to two keychallenges: 1) \textit{dynamic knowledge type selection} and 2)\textit{intention-response decoupling}. To address these challenges, we proposea novel dual knowledge-enhanced two-stage reasoner by adapting LLMs formultimodal dialog systems (named DK2R). To be specific, DK2R first extractsboth structured attribute and unstructured review knowledge from externalknowledge base given the dialog context. Thereafter, DK2R uses an LLM toevaluate each knowledge type's utility by analyzing LLM-generated provisionalprobe responses. Moreover, DK2R separately summarizes the intention-orientedkey clues via dedicated reasoning, which are further used as auxiliary signalsto enhance LLM-based textual response generation. Extensive experimentsconducted on a public dataset verify the superiority of DK2R. We have releasedthe codes and parameters.</description><author>Xiaolin Chen, Xuemeng Song, Haokun Wen, Weili Guan, Xiangyu Zhao, Liqiang Nie</author><pubDate>Tue, 09 Sep 2025 14:55:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07817v1</guid></item><item><title>HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention</title><link>http://arxiv.org/abs/2509.07475v1</link><description>Detecting content that contradicts or is unsupported by a given source textis a critical challenge for the safe deployment of generative language models.We introduce HALT-RAG, a post-hoc verification system designed to identifyhallucinations in the outputs of Retrieval-Augmented Generation (RAG)pipelines. Our flexible and task-adaptable framework uses a universal featureset derived from an ensemble of two frozen, off-the-shelf Natural LanguageInference (NLI) models and lightweight lexical signals. These features are usedto train a simple, calibrated, and task-adapted meta-classifier. Using arigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage andproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.By pairing our universal feature set with a lightweight, task-adaptedclassifier and a precision-constrained decision policy, HALT-RAG achievesstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,and dialogue tasks, respectively. The system's well-calibrated probabilitiesenable a practical abstention mechanism, providing a reliable tool forbalancing model performance with safety requirements.</description><author>Saumya Goswami, Siddharth Kurra</author><pubDate>Tue, 09 Sep 2025 07:58:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2509.07475v1</guid></item></channel></rss>