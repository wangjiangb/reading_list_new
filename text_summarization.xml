<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivtext summarization</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Mon, 18 Aug 2025 22:07:19 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title><link>http://arxiv.org/abs/2508.11598v1</link><description>We introduce AuriStream, a biologically inspired model for encoding speechvia a two-stage framework inspired by the human auditory processing hierarchy.The first stage transforms raw audio into a time-frequency representation basedon the human cochlea, from which we extract discrete \textbf{cochlear tokens}.The second stage applies an autoregressive sequence model over the cochleartokens. AuriStream learns meaningful phoneme and word representations, andstate-of-the-art lexical semantics. AuriStream shows competitive performance ondiverse downstream SUPERB speech tasks. Complementing AuriStream's strongrepresentational capabilities, it generates continuations of audio which can bevisualized in a spectrogram space and decoded back into audio, providinginsights into the model's predictions. In summary, we present a two-stageframework for speech representation learning to advance the development of morehuman-like models that efficiently handle a range of speech-based tasks.</description><author>Greta Tuckute, Klemen Kotar, Evelina Fedorenko, Daniel L. K. Yamins</author><pubDate>Fri, 15 Aug 2025 17:06:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.11598v1</guid></item><item><title>When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing</title><link>http://arxiv.org/abs/2508.10482v2</link><description>In the study of trustworthy Natural Language Processing (NLP), a number ofimportant research fields have emerged, including that of explainability andprivacy. While research interest in both explainable and privacy-preserving NLPhas increased considerably in recent years, there remains a lack ofinvestigation at the intersection of the two. This leaves a considerable gap inunderstanding of whether achieving both explainability and privacy is possible,or whether the two are at odds with each other. In this work, we conduct anempirical investigation into the privacy-explainability trade-off in thecontext of NLP, guided by the popular overarching methods of DifferentialPrivacy (DP) and Post-hoc Explainability. Our findings include a view into theintricate relationship between privacy and explainability, which is formed by anumber of factors, including the nature of the downstream task and choice ofthe text privatization and explainability method. In this, we highlight thepotential for privacy and explainability to co-exist, and we summarize ourfindings in a collection of practical recommendations for future work at thisimportant intersection.</description><author>Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci</author><pubDate>Fri, 15 Aug 2025 13:25:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10482v2</guid></item><item><title>A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design</title><link>http://arxiv.org/abs/2508.10899v1</link><description>AI-driven discovery can greatly reduce design time and enhance newtherapeutics' effectiveness. Models using simulators explore broad designspaces but risk violating implicit constraints due to a lack of experimentalpriors. For example, in a new analysis we performed on a diverse set of modelson the GuacaMol benchmark using supervised classifiers, over 60\% of moleculesproposed had high probability of being mutagenic. In this work, we introduce\ourdataset, a dataset of priors for design problems extracted from literaturedescribing compounds used in lab settings. It is constructed with LLM pipelinesfor discovering therapeutic entities in relevant paragraphs and summarizinginformation in concise fair-use facts. \ourdataset~ consists of 32.3 millionpairs of natural language facts, and appropriate entity representations (i.e.SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM,CLIP, and LLava architectures to reason jointly about text and design targetsand evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~ishighly effective for creating models with strong priors: in supervisedprediction problems that use our data as pretraining, our best models with 15Mlearnable parameters outperform larger 2B TxGemma on both regression andclassification TDC tasks, and perform comparably to 9B models on average.Models built with \ourdataset~can be used as constraints while optimizing fornovel molecules in GuacaMol, resulting in proposals that are safer and nearlyas effective. We release our dataset at\href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex},and will provide expanded versions as available literature grows.</description><author>Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar</author><pubDate>Thu, 14 Aug 2025 17:59:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2508.10899v1</guid></item></channel></rss>