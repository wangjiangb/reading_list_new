<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo captioning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Tue, 09 Apr 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Streaming Dense Video Captioning</title><link>http://arxiv.org/abs/2404.01297v1</link><description>An ideal model for dense video captioning -- predicting captions localizedtemporally in a video -- should be able to handle long input videos, predictrich, detailed textual descriptions, and be able to produce outputs beforeprocessing the entire video. Current state-of-the-art models, however, processa fixed number of downsampled frames, and make a single full prediction afterseeing the whole video. We propose a streaming dense video captioning modelthat consists of two novel components: First, we propose a new memory module,based on clustering incoming tokens, which can handle arbitrarily long videosas the memory is of a fixed size. Second, we develop a streaming decodingalgorithm that enables our model to make predictions before the entire videohas been processed. Our model achieves this streaming ability, andsignificantly improves the state-of-the-art on three dense video captioningbenchmarks: ActivityNet, YouCook2 and ViTT. Our code is released athttps://github.com/google-research/scenic.</description><author>Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, Cordelia Schmid</author><pubDate>Mon, 01 Apr 2024 18:59:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01297v1</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v3</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Wed, 28 Feb 2024 13:55:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v3</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v1</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Tue, 20 Feb 2024 18:58:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v1</guid></item><item><title>Video ReCap: Recursive Captioning of Hour-Long Videos</title><link>http://arxiv.org/abs/2402.13250v2</link><description>Most video captioning models are designed to process short video clips of fewseconds and output text describing low-level visual concepts (e.g., objects,scenes, atomic actions). However, most real-world videos last for minutes orhours and have a complex hierarchical structure spanning different temporalgranularities. We propose Video ReCap, a recursive video captioning model thatcan process video inputs of dramatically different lengths (from 1 second to 2hours) and output video captions at multiple hierarchy levels. The recursivevideo-language architecture exploits the synergy between different videohierarchies and can process hour-long videos efficiently. We utilize acurriculum learning training scheme to learn the hierarchical structure ofvideos, starting from clip-level captions describing atomic actions, thenfocusing on segment-level descriptions, and concluding with generatingsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset byaugmenting Ego4D with 8,267 manually collected long-range video summaries. Ourrecursive model can flexibly generate captions at different hierarchy levelswhile also being useful for other complex video understanding tasks, such asVideoQA on EgoSchema. Data, code, and models are available at:https://sites.google.com/view/vidrecap</description><author>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius</author><pubDate>Wed, 21 Feb 2024 22:19:35 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13250v2</guid></item><item><title>MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning</title><link>http://arxiv.org/abs/2402.17680v1</link><description>To address the problem of catastrophic forgetting due to the invisibility ofold categories in sequential input, existing work based on relatively simplecategorization tasks has made some progress. In contrast, video captioning is amore complex task in multimodal scenario, which has not been explored in thefield of incremental learning. After identifying this stability-plasticityproblem when analyzing video with sequential input, we originally propose amethod to Mitigate Catastrophic Forgetting in class-incremental learning formultimodal Video Captioning (MCF-VC). As for effectively maintaining goodperformance on old tasks at the macro level, we design Fine-grained SensitivitySelection (FgSS) based on the Mask of Linear's Parameters and FisherSensitivity to pick useful knowledge from old tasks. Further, in order tobetter constrain the knowledge characteristics of old and new tasks at thespecific feature level, we have created the Two-stage Knowledge Distillation(TsKD), which is able to learn the new task well while weighing the old task.Specifically, we design two distillation losses, which constrain the crossmodal semantic information of semantic attention feature map and the textualinformation of the final outputs respectively, so that the inter-model andintra-model stylized knowledge of the old class is retained while learning thenew class. In order to illustrate the ability of our model to resistforgetting, we designed a metric CIDER_t to detect the stage forgetting rate.Our experiments on the public dataset MSR-VTT show that the proposed methodsignificantly resists the forgetting of previous tasks without replaying oldsamples, and performs well on the new task.</description><author>Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li</author><pubDate>Tue, 27 Feb 2024 16:54:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17680v1</guid></item><item><title>DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement</title><link>http://arxiv.org/abs/2404.02755v1</link><description>We present Dive Into the BoundarieS (DIBS), a novel pretraining framework fordense video captioning (DVC), that elaborates on improving the quality of thegenerated event captions and their associated pseudo event boundaries fromunlabeled videos. By leveraging the capabilities of diverse large languagemodels (LLMs), we generate rich DVC-oriented caption candidates and optimizethe corresponding pseudo boundaries under several meticulously designedobjectives, considering diversity, event-centricity, temporal ordering, andcoherence. Moreover, we further introduce a novel online boundary refinementstrategy that iteratively improves the quality of pseudo boundaries duringtraining. Comprehensive experiments have been conducted to examine theeffectiveness of the proposed technique components. By leveraging a substantialamount of unlabeled video data, such as HowTo100M, we achieve a remarkableadvancement on standard DVC datasets like YouCook2 and ActivityNet. Weoutperform the previous state-of-the-art Vid2Seq across a majority of metrics,achieving this with just 0.4% of the unlabeled video data used for pre-trainingby Vid2Seq.</description><author>Hao Wu, Huabin Liu, Yu Qiao, Xiao Sun</author><pubDate>Wed, 03 Apr 2024 14:57:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.02755v1</guid></item><item><title>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</title><link>http://arxiv.org/abs/2402.19479v1</link><description>The quality of the data and annotation upper-bounds the quality of adownstream model. While there exist large text corpora and image-text pairs,high-quality video-text data is much harder to collect. First of all, manuallabeling is more time-consuming, as it requires an annotator to watch an entirevideo. Second, videos have a temporal dimension, consisting of several scenesstacked together, and showing multiple actions. Accordingly, to establish avideo dataset with high-quality captions, we propose an automatic approachleveraging multimodal inputs, such as textual video description, subtitles, andindividual video frames. Specifically, we curate 3.8M high-resolution videosfrom the publicly available HD-VILA-100M dataset. We then split them intosemantically consistent video clips, and apply multiple cross-modality teachermodels to obtain captions for each video. Next, we finetune a retrieval modelon a small subset where the best caption of each video is manually selected andthen employ the model in the whole dataset to select the best caption as theannotation. In this way, we get 70M videos paired with high-quality textcaptions. We dub the dataset as Panda-70M. We show the value of the proposeddataset on three downstream tasks: video captioning, video and text retrieval,and text-driven video generation. The models trained on the proposed data scoresubstantially better on the majority of metrics across all the tasks.</description><author>Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, Sergey Tulyakov</author><pubDate>Thu, 29 Feb 2024 18:59:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.19479v1</guid></item><item><title>The MSR-Video to Text Dataset with Clean Annotations</title><link>http://arxiv.org/abs/2102.06448v4</link><description>Video captioning automatically generates short descriptions of the videocontent, usually in form of a single sentence. Many methods have been proposedfor solving this task. A large dataset called MSR Video to Text (MSR-VTT) isoften used as the benchmark dataset for testing the performance of the methods.However, we found that the human annotations, i.e., the descriptions of videocontents in the dataset are quite noisy, e.g., there are many duplicatecaptions and many captions contain grammatical problems. These problems maypose difficulties to video captioning models for learning underlying patterns.We cleaned the MSR-VTT annotations by removing these problems, then testedseveral typical video captioning models on the cleaned dataset. Experimentalresults showed that data cleaning boosted the performances of the modelsmeasured by popular quantitative metrics. We recruited subjects to evaluate theresults of a model trained on the original and cleaned datasets. The humanbehavior experiment demonstrated that trained on the cleaned dataset, the modelgenerated captions that were more coherent and more relevant to the contents ofthe video clips.</description><author>Haoran Chen, Jianmin Li, Simone Frintrop, Xiaolin Hu</author><pubDate>Sun, 25 Feb 2024 09:04:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2102.06448v4</guid></item><item><title>OW-VISCap: Open-World Video Instance Segmentation and Captioning</title><link>http://arxiv.org/abs/2404.03657v1</link><description>Open-world video instance segmentation is an important video understandingtask. Yet most methods either operate in a closed-world setting, require anadditional user-input, or use classic region-based proposals to identify neverbefore seen objects. Further, these methods only assign a one-word label todetected objects, and don't generate rich object-centric descriptions. Theyalso often suffer from highly overlapping predictions. To address these issues,we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),an approach to jointly segment, track, and caption previously seen or unseenobjects in a video. For this, we introduce open-world object queries todiscover never before seen objects without additional user-input. We generaterich and descriptive object-centric captions for each detected object via amasked attention augmented LLM input. We introduce an inter-query contrastiveloss to ensure that the object queries differ from one another. Our generalizedapproach matches or surpasses state-of-the-art on three tasks: open-world videoinstance segmentation on the BURST dataset, dense video object captioning onthe VidSTG dataset, and closed-world video instance segmentation on the OVISdataset.</description><author>Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing</author><pubDate>Thu, 04 Apr 2024 18:59:58 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.03657v1</guid></item><item><title>Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality</title><link>http://arxiv.org/abs/2403.19221v1</link><description>Video paragraph captioning (VPC) involves generating detailed narratives forlong videos, utilizing supportive modalities such as speech and eventboundaries. However, the existing models are constrained by the assumption ofconstant availability of a single auxiliary modality, which is impracticalgiven the diversity and unpredictable nature of real-world scenarios. To thisend, we propose a Missing-Resistant framework MR-VPC that effectively harnessesall available auxiliary inputs and maintains resilience even in the absence ofcertain modalities. Under this framework, we propose the Multimodal VPC (MVPC)architecture integrating video, speech, and event boundary inputs in a unifiedmanner to process various auxiliary inputs. Moreover, to fortify the modelagainst incomplete data, we introduce DropAM, a data augmentation strategy thatrandomly omits auxiliary inputs, paired with DistillAM, a regularization targetthat distills knowledge from teacher models trained on modality-complete data,enabling efficient learning in modality-deficient environments. Throughexhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC hasproven to deliver superior performance on modality-complete andmodality-missing test data. This work highlights the significance of developingresilient VPC models and paves the way for more adaptive, robust multimodalvideo understanding.</description><author>Sishuo Chen, Lei Li, Shuhuai Ren, Rundong Gao, Yuanxin Liu, Xiaohan Bi, Xu Sun, Lu Hou</author><pubDate>Thu, 28 Mar 2024 09:35:46 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.19221v1</guid></item><item><title>Beyond MOT: Semantic Multi-Object Tracking</title><link>http://arxiv.org/abs/2403.05021v1</link><description>Current multi-object tracking (MOT) aims to predict trajectories of targets(i.e.,"where") in videos. Yet, knowing merely "where" is insufficient in manycrucial applications. In comparison, semantic understanding such asfine-grained behaviors, interactions, and overall summarized captions (i.e.,"what") from videos, associated with "where", is highly-desired forcomprehensive video analysis. Thus motivated, we introduce SemanticMulti-Object Tracking (SMOT), that aims to estimate object trajectories andmeanwhile understand semantic details of associated trajectories includinginstance captions, instance interactions, and overall video captions,integrating "where" and "what" for tracking. In order to foster the explorationof SMOT, we propose BenSMOT, a large-scale Benchmark for Semantic MOT.Specifically, BenSMOT comprises 3,292 videos with 151K frames, covering variousscenarios for semantic tracking of humans. BenSMOT provides annotations for thetrajectories of targets, along with associated instance captions in naturallanguage, instance interactions, and overall caption for each video sequence.To our best knowledge, BenSMOT is the first publicly available benchmark forSMOT. Besides, to encourage future research, we present a novel tracker namedSMOTer, which is specially designed and end-to-end trained for SMOT, showingpromising performance. By releasing BenSMOT, we expect to go beyondconventional MOT by predicting "where" and "what" for SMOT, opening up a newdirection in tracking for video understanding. Our BenSMOT and SMOTer will bereleased.</description><author>Yunhao Li, Hao Wang, Qin Li, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, Libo Zhang</author><pubDate>Fri, 08 Mar 2024 03:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05021v1</guid></item><item><title>OmniVid: A Generative Framework for Universal Video Understanding</title><link>http://arxiv.org/abs/2403.17935v1</link><description>The core of video understanding tasks, such as recognition, captioning, andtracking, is to automatically detect objects or actions in a video and analyzetheir temporal evolution. Despite sharing a common goal, different tasks oftenrely on distinct model architectures and annotation formats. In contrast,natural language processing benefits from a unified output space, i.e., textsequences, which simplifies the training of powerful foundational languagemodels, such as GPT-3, with extensive training corpora. Inspired by this, weseek to unify the output space of video understanding tasks by using languagesas labels and additionally introducing time and box tokens. In this way, avariety of video tasks could be formulated as video-grounded token generation.This enables us to address various types of video tasks, includingclassification (such as action recognition), captioning (covering clipcaptioning, video question answering, and dense video captioning), andlocalization tasks (such as visual object tracking) within a fully sharedencoder-decoder architecture, following a generative framework. Throughcomprehensive experiments, we demonstrate such a simple and straightforwardidea is quite effective and can achieve state-of-the-art or competitive resultson seven video benchmarks, providing a novel perspective for more universalvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.</description><author>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang</author><pubDate>Tue, 26 Mar 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.17935v1</guid></item><item><title>Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation</title><link>http://arxiv.org/abs/2403.05131v1</link><description>Text-to-video generation marks a significant frontier in the rapidly evolvingdomain of generative AI, integrating advancements in text-to-image synthesis,video captioning, and text-guided editing. This survey critically examines theprogression of text-to-video technologies, focusing on the shift fromtraditional generative models to the cutting-edge Sora model, highlightingdevelopments in scalability and generalizability. Distinguishing our analysisfrom prior works, we offer an in-depth exploration of the technologicalframeworks and evolutionary pathways of these models. Additionally, we delveinto practical applications and address ethical and technological challengessuch as the inability to perform multiple entity handling, comprehendcausal-effect learning, understand physical interaction, perceive objectscaling and proportioning, and combat object hallucination which is also along-standing problem in generative models. Our comprehensive discussion coversthe topic of enablement of text-to-video generation models as human-assistivetools and world models, as well as eliciting model's shortcomings andsummarizing future improvement direction that mainly centers around trainingdatasets and evaluation metrics (both automatic and human-centered). Aimed atboth newcomers and seasoned researchers, this survey seeks to catalyze furtherinnovation and discussion in the growing field of text-to-video generation,paving the way for more reliable and practical generative artificialintelligence technologies.</description><author>Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, Chaoning Zhang</author><pubDate>Fri, 08 Mar 2024 07:58:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.05131v1</guid></item><item><title>VideoPrism: A Foundational Visual Encoder for Video Understanding</title><link>http://arxiv.org/abs/2402.13217v1</link><description>We introduce VideoPrism, a general-purpose video encoder that tackles diversevideo understanding tasks with a single frozen model. We pretrain VideoPrism ona heterogeneous corpus containing 36M high-quality video-caption pairs and 582Mvideo clips with noisy parallel text (e.g., ASR transcripts). The pretrainingapproach improves upon masked autoencoding by global-local distillation ofsemantic video embeddings and a token shuffling scheme, enabling VideoPrism tofocus primarily on the video modality while leveraging the invaluable textassociated with videos. We extensively test VideoPrism on four broad groups ofvideo understanding tasks, from web video question answering to CV for science,achieving state-of-the-art performance on 30 out of 33 video understandingbenchmarks.</description><author>Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong</author><pubDate>Tue, 20 Feb 2024 18:29:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.13217v1</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v2</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Fri, 08 Mar 2024 18:28:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v2</guid></item><item><title>Pix2Gif: Motion-Guided Diffusion for GIF Generation</title><link>http://arxiv.org/abs/2403.04634v1</link><description>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)generation. We tackle this problem differently by formulating the task as animage translation problem steered by text and motion magnitude prompts, asshown in teaser fig. To ensure that the model adheres to motion guidance, wepropose a new motion-guided warping module to spatially transform the featuresof the source image conditioned on the two types of prompts. Furthermore, weintroduce a perceptual loss to ensure the transformed feature map remainswithin the same space as the target image, ensuring content consistency andcoherence. In preparation for the model training, we meticulously curated databy extracting coherent image frames from the TGIF video-caption dataset, whichprovides rich information about the temporal changes of subjects. Afterpretraining, we apply our model in a zero-shot manner to a number of videodatasets. Extensive qualitative and quantitative experiments demonstrate theeffectiveness of our model -- it not only captures the semantic prompt fromtext but also the spatial ones from motion guidance. We train all our modelsusing a single node of 16xV100 GPUs. Code, dataset and models are made publicat: https://hiteshk03.github.io/Pix2Gif/.</description><author>Hitesh Kandala, Jianfeng Gao, Jianwei Yang</author><pubDate>Thu, 07 Mar 2024 16:18:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.04634v1</guid></item><item><title>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</title><link>http://arxiv.org/abs/2402.10698v1</link><description>We present Q-ViD, a simple approach for video question answering (video QA),that unlike prior methods, which are based on complex architectures,computationally expensive pipelines or use closed models like GPTs, Q-ViDrelies on a single instruction-aware open vision-language model (InstructBLIP)to tackle videoQA using frame descriptions. Specifically, we create captioninginstruction prompts that rely on the target questions about the videos andleverage InstructBLIP to obtain video frame captions that are useful to thetask at hand. Subsequently, we form descriptions of the whole video using thequestion-dependent frame captions, and feed that information, along with aquestion-answering prompt, to a large language model (LLM). The LLM is ourreasoning module, and performs the final step of multiple-choice QA. Our simpleQ-ViD framework achieves competitive or even higher performances than currentstate of the art models on a diverse range of videoQA benchmarks, includingNExT-QA, STAR, How2QA, TVQA and IntentQA.</description><author>David Romero, Thamar Solorio</author><pubDate>Fri, 16 Feb 2024 13:59:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10698v1</guid></item><item><title>LVCHAT: Facilitating Long Video Comprehension</title><link>http://arxiv.org/abs/2402.12079v1</link><description>Enabling large language models (LLMs) to read videos is vital for multimodalLLMs. Existing works show promise on short videos whereas long video (longerthan e.g.~1 minute) comprehension remains challenging. The major problem liesin the over-compression of videos, i.e., the encoded video representations arenot enough to represent the whole video. To address this issue, we propose LongVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced todynamically adjust the number of embeddings in alignment with the duration ofthe video to ensure long videos are not overly compressed into a fewembeddings. To deal with long videos whose length is beyond videos seen duringtraining, we propose Interleaved Frame Encoding (IFE), repeating positionalembedding and interleaving multiple groups of videos to enable long videoinput, avoiding performance degradation due to overly long videos. Experimentalresults show that LVChat significantly outperforms existing methods by up to27\% in accuracy on long-video QA datasets and long-video captioningbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.</description><author>Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He</author><pubDate>Mon, 19 Feb 2024 11:59:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.12079v1</guid></item><item><title>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</title><link>http://arxiv.org/abs/2404.05726v1</link><description>With the success of large language models (LLMs), integrating the visionmodel into LLMs to build vision-language foundation models has gained much moreinterest recently. However, existing LLM-based large multimodal models (e.g.,Video-LLaMA, VideoChat) can only take in a limited number of frames for shortvideo understanding. In this study, we mainly focus on designing an efficientand effective model for long-term video understanding. Instead of trying toprocess more frames simultaneously like most existing work, we propose toprocess videos in an online manner and store past video information in a memorybank. This allows our model to reference historical video content for long-termanalysis without exceeding LLMs' context length constraints or GPU memorylimits. Our memory bank can be seamlessly integrated into current multimodalLLMs in an off-the-shelf manner. We conduct extensive experiments on variousvideo understanding tasks, such as long-video understanding, video questionanswering, and video captioning, and our model can achieve state-of-the-artperformances across multiple datasets. Code available athttps://boheumd.github.io/MA-LMM/.</description><author>Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim</author><pubDate>Mon, 08 Apr 2024 18:59:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05726v1</guid></item><item><title>Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT</title><link>http://arxiv.org/abs/2402.15746v1</link><description>With the rise of short video platforms represented by TikTok, the trend ofusers expressing their creativity through photos and videos has increaseddramatically. However, ordinary users lack the professional skills to producehigh-quality videos using professional creation software. To meet the demandfor intelligent and user-friendly video creation tools, we propose the DynamicVisual Composition (DVC) task, an interesting and challenging task that aims toautomatically integrate various media elements based on user requirements andcreate storytelling videos. We propose an Intelligent Director framework,utilizing LENS to generate descriptions for images and video frames andcombining ChatGPT to generate coherent captions while recommending appropriatemusic names. Then, the best-matched music is obtained through music retrieval.Then, materials such as captions, images, videos, and music are integrated toseamlessly synthesize the video. Finally, we apply AnimeGANv2 for styletransfer. We construct UCF101-DVC and Personal Album datasets and verified theeffectiveness of our framework in solving DVC through qualitative andquantitative comparisons, along with user studies, demonstrating itssubstantial potential.</description><author>Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu</author><pubDate>Sat, 24 Feb 2024 06:58:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15746v1</guid></item><item><title>Improved Baselines for Data-efficient Perceptual Augmentation of LLMs</title><link>http://arxiv.org/abs/2403.13499v1</link><description>The abilities of large language models (LLMs) have recently progressed tounprecedented levels, paving the way to novel applications in a wide variety ofareas. In computer vision, LLMs can be used to prime vision-language tasks suchimage captioning and visual question answering when coupled with pre-trainedvision backbones. While different approaches have been explored to interfaceLLMs with ``perceptual backbones'' that process, e.g., visual or audio data,they are often explored for different tasks, different datasets, and usingdifferent perceptual backbones and language models, hindering direct comparisonof the interfacing mechanisms. To remedy this lack of comparability betweenmethods, we present an extensive experimental evaluation of differentinterfacing mechanisms, across multiple tasks (including image, video, andaudio captioning as well as visual question answering), datasets and backbones,paying special attention to low-data settings. We find improved performanceusing existing mechanisms over state-of-the-art results, and identify a newinterfacing mechanism that yields (near) optimal results across differenttasks, while obtaining a 4x reduction in training time.</description><author>Th√©ophane Vallaeys, Mustafa Shukor, Matthieu Cord, Jakob Verbeek</author><pubDate>Wed, 20 Mar 2024 11:57:17 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.13499v1</guid></item><item><title>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</title><link>http://arxiv.org/abs/2403.15377v1</link><description>We introduce InternVideo2, a new video foundation model (ViFM) that achievesthe state-of-the-art performance in action recognition, video-text tasks, andvideo-centric dialogue. Our approach employs a progressive training paradigmthat unifies the different self- or weakly-supervised learning frameworks ofmasked video token reconstruction, cross-modal contrastive learning, and nexttoken prediction. Different training stages would guide our model to capturedifferent levels of structure and semantic information through differentpretext tasks. At the data level, we prioritize the spatiotemporal consistencyby semantically segmenting videos and generating video-audio-speech captions.This improves the alignment between video and text. We scale both data andmodel size for our InternVideo2. Through extensive experiments, we validate ourdesigns and demonstrate the state-of-the-art performance on over 60 video andaudio tasks. Notably, our model outperforms others on various video-relatedcaptioning, dialogue, and long video understanding benchmarks, highlighting itsability to reason and comprehend long temporal contexts. Code and models areavailable at https://github.com/OpenGVLab/InternVideo2/.</description><author>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</author><pubDate>Fri, 22 Mar 2024 18:57:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.15377v1</guid></item><item><title>Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward</title><link>http://arxiv.org/abs/2404.01258v2</link><description>Preference modeling techniques, such as direct preference optimization (DPO),has shown effective in enhancing the generalization abilities of large languagemodel (LLM). However, in tasks involving video instruction-following, providinginformative feedback, especially for detecting hallucinations in generatedresponses, remains a significant challenge. Previous studies have exploredusing large large multimodal models (LMMs) as reward models to guide preferencemodeling, but their ability to accurately assess the factuality of generatedresponses compared to corresponding videos has not been conclusivelyestablished. This paper introduces a novel framework that utilizes detailedvideo captions as a proxy of video content, enabling language models toincorporate this information as supporting evidence for scoring video QuestionAnswering (QA) predictions. Our approach demonstrates robust alignment withOpenAI GPT-4V model's reward mechanism, which directly takes video frames asinput. Furthermore, we show that applying this tailored reward through DPOsignificantly improves the performance of video LMMs on video QA tasks.</description><author>Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, Yiming Yang</author><pubDate>Tue, 02 Apr 2024 13:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01258v2</guid></item><item><title>HawkEye: Training Video-Text LLMs for Grounding Text in Videos</title><link>http://arxiv.org/abs/2403.10228v1</link><description>Video-text Large Language Models (video-text LLMs) have shown remarkableperformance in answering questions and holding conversations on simple videos.However, they perform almost the same as random on grounding text queries inlong and complicated videos, having little ability to understand and reasonabout temporal information, which is the most fundamental difference betweenvideos and images. In this paper, we propose HawkEye, one of the firstvideo-text LLMs that can perform temporal video grounding in a fullytext-to-text manner. To collect training data that is applicable for temporalvideo grounding, we construct InternVid-G, a large-scale video-text corpus withsegment-level captions and negative spans, with which we introduce two newtime-aware training objectives to video-text LLMs. We also propose acoarse-grained method of representing segments in videos, which is more robustand easier for LLMs to learn and follow than other alternatives. Extensiveexperiments show that HawkEye is better at temporal video grounding andcomparable on other video-text tasks with existing video-text LLMs, whichverifies its superior video-text multi-modal understanding abilities.</description><author>Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao</author><pubDate>Fri, 15 Mar 2024 12:58:18 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.10228v1</guid></item><item><title>Learn Suspected Anomalies from Event Prompts for Video Anomaly Detection</title><link>http://arxiv.org/abs/2403.01169v1</link><description>Most models for weakly supervised video anomaly detection (WS-VAD) rely onmultiple instance learning, aiming to distinguish normal and abnormal snippetswithout specifying the type of anomaly. The ambiguous nature of anomalydefinitions across contexts introduces bias in detecting abnormal and normalsnippets within the abnormal bag. Taking the first step to show the model whyit is anomalous, a novel framework is proposed to guide the learning ofsuspected anomalies from event prompts. Given a textual prompt dictionary ofpotential anomaly events and the captions generated from anomaly videos, thesemantic anomaly similarity between them could be calculated to identify thesuspected anomalous events for each video snippet. It enables a newmulti-prompt learning process to constrain the visual-semantic features acrossall videos, as well as provides a new way to label pseudo anomalies forself-training. To demonstrate effectiveness, comprehensive experiments anddetailed ablation studies are conducted on four datasets, namely XD-Violence,UCF-Crime, TAD, and ShanghaiTech. Our proposed model outperforms moststate-of-the-art methods in terms of AP or AUC (82.6\%, 87.7\%, 93.1\%, and97.4\%). Furthermore, it shows promising performance in open-set andcross-dataset cases.</description><author>Chenchen Tao, Chong Wang, Yuexian Zou, Xiaohao Peng, Jiafei Wu, Jiangbo Qian</author><pubDate>Sat, 02 Mar 2024 10:42:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.01169v1</guid></item><item><title>End-to-End Dense Video Grounding via Parallel Regression</title><link>http://arxiv.org/abs/2109.11265v5</link><description>Video grounding aims to localize the corresponding video moment in anuntrimmed video given a language query. Existing methods often address thistask in an indirect way, by casting it as a proposal-and-match orfusion-and-detection problem. Solving these surrogate problems often requiressophisticated label assignment during training and hand-crafted removal ofnear-duplicate results. Meanwhile, existing works typically focus on sparsevideo grounding with a single sentence as input, which could result inambiguous localization due to its unclear description. In this paper, we tacklea new problem of dense video grounding, by simultaneously localizing multiplemoments with a paragraph as input. From a perspective on video grounding aslanguage conditioned regression, we present an end-to-end parallel decodingparadigm by re-purposing a Transformer-alike architecture (PRVG). The keydesign in our PRVG is to use languages as queries, and directly regress themoment boundaries based on language-modulated visual representations. Thanks toits simplicity in design, our PRVG framework can be applied in differenttesting schemes (sparse or dense grounding) and allows for efficient inferencewithout any post-processing technique. In addition, we devise a robustproposal-level attention loss to guide the training of PRVG, which is invariantto moment duration and contributes to model convergence. We perform experimentson two video grounding benchmarks of ActivityNet Captions and TACoS,demonstrating that our PRVG can significantly outperform previous methods. Wealso perform in-depth studies to investigate the effectiveness of parallelregression paradigm on video grounding.</description><author>Fengyuan Shi, Weilin Huang, Limin Wang</author><pubDate>Wed, 28 Feb 2024 13:04:24 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2109.11265v5</guid></item><item><title>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</title><link>http://arxiv.org/abs/2402.10896v1</link><description>This paper demonstrates that a progressively aligned language model caneffectively bridge frozen vision encoders and large language models (LLMs).While the fundamental architecture and pre-training methods of vision encodersand LLMs have been extensively studied, the architecture and training strategyof vision-language adapters vary significantly across recent works. Ourresearch undertakes a thorough exploration of the state-of-the-art perceiverresampler architecture and builds a strong baseline. However, we observe thatthe vision-language alignment with perceiver resampler exhibits slowconvergence and limited scalability with a lack of direct supervision. Toaddress this issue, we propose PaLM2-VAdapter, employing a progressivelyaligned language model as the vision-language adapter. Compared to the strongbaseline with perceiver resampler, our method empirically shows fasterconvergence, higher performance, and stronger scalability. Extensiveexperiments across various Visual Question Answering (VQA) and captioning taskson both images and videos demonstrate that our model exhibits state-of-the-artvisual understanding and multi-modal reasoning capabilities. Notably, ourmethod achieves these advancements with 30~70% fewer parameters than thestate-of-the-art large vision-language models, marking a significant efficiencyimprovement.</description><author>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang</author><pubDate>Fri, 16 Feb 2024 18:54:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.10896v1</guid></item><item><title>Harnessing Large Language Models for Training-free Video Anomaly Detection</title><link>http://arxiv.org/abs/2404.01014v1</link><description>Video anomaly detection (VAD) aims to temporally locate abnormal events in avideo. Existing works mostly rely on training deep models to learn thedistribution of normality with either video-level supervision, one-classsupervision, or in an unsupervised setting. Training-based methods are prone tobe domain-specific, thus being costly for practical deployment as any domainchange will involve data collection and model training. In this paper, weradically depart from previous efforts and propose LAnguage-based VAD (LAVAD),a method tackling VAD in a novel, training-free paradigm, exploiting thecapabilities of pre-trained large language models (LLMs) and existingvision-language models (VLMs). We leverage VLM-based captioning models togenerate textual descriptions for each frame of any test video. With thetextual scene description, we then devise a prompting mechanism to unlock thecapability of LLMs in terms of temporal aggregation and anomaly scoreestimation, turning LLMs into an effective video anomaly detector. We furtherleverage modality-aligned VLMs and propose effective techniques based oncross-modal similarity for cleaning noisy captions and refining the LLM-basedanomaly scores. We evaluate LAVAD on two large datasets featuring real-worldsurveillance scenarios (UCF-Crime and XD-Violence), showing that it outperformsboth unsupervised and one-class methods without requiring any training or datacollection.</description><author>Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, Elisa Ricci</author><pubDate>Mon, 01 Apr 2024 10:34:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.01014v1</guid></item><item><title>Self-Explainable Affordance Learning with Embodied Caption</title><link>http://arxiv.org/abs/2404.05603v1</link><description>In the field of visual affordance learning, previous methods mainly usedabundant images or videos that delineate human behavior patterns to identifyaction possibility regions for object manipulation, with a variety ofapplications in robotic tasks. However, they encounter a main challenge ofaction ambiguity, illustrated by the vagueness like whether to beat or carry adrum, and the complexities involved in processing intricate scenes. Moreover,it is important for human intervention to rectify robot errors in time. Toaddress these issues, we introduce Self-Explainable Affordance learning (SEA)with embodied caption. This innovation enables robots to articulate theirintentions and bridge the gap between explainable vision-language caption andvisual affordance learning. Due to a lack of appropriate dataset, we unveil apioneering dataset and metrics tailored for this task, which integrates images,heatmaps, and embodied captions. Furthermore, we propose a novel model toeffectively combine affordance grounding with self-explanation in a simple butefficient manner. Extensive quantitative and qualitative experimentsdemonstrate our method's effectiveness.</description><author>Zhipeng Zhang, Zhimin Wei, Guolei Sun, Peng Wang, Luc Van Gool</author><pubDate>Mon, 08 Apr 2024 16:22:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05603v1</guid></item><item><title>VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT</title><link>http://arxiv.org/abs/2403.02076v1</link><description>Video temporal grounding (VTG) aims to locate specific temporal segments froman untrimmed video based on a linguistic query. Most existing VTG models aretrained on extensive annotated video-text pairs, a process that not onlyintroduces human biases from the queries but also incurs significantcomputational costs. To tackle these challenges, we propose VTG-GPT, aGPT-based method for zero-shot VTG without training or fine-tuning. To reduceprejudice in the original query, we employ Baichuan2 to generate debiasedqueries. To lessen redundant information in videos, we apply MiniGPT-v2 totransform visual content into more precise captions. Finally, we devise theproposal generator and post-processing to produce accurate segments fromdebiased queries and image captions. Extensive experiments demonstrate thatVTG-GPT significantly outperforms SOTA methods in zero-shot settings andsurpasses unsupervised approaches. More notably, it achieves competitiveperformance comparable to supervised methods. The code is available onhttps://github.com/YoucanBaby/VTG-GPT</description><author>Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, Sidan Du</author><pubDate>Mon, 04 Mar 2024 14:22:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.02076v1</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v2</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness oflanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 28 Feb 2024 02:48:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v2</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v3</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness of thelanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 20 Mar 2024 15:49:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v3</guid></item><item><title>OSCaR: Object State Captioning and State Change Representation</title><link>http://arxiv.org/abs/2402.17128v4</link><description>The capability of intelligent models to extrapolate and comprehend changes inobject states is a crucial yet demanding aspect of AI research, particularlythrough the lens of human interaction in real-world settings. This taskinvolves describing complex visual environments, identifying active objects,and interpreting their changes as conveyed through language. Traditionalmethods, which isolate object captioning and state change detection, offer alimited view of dynamic environments. Moreover, relying on a small set ofsymbolic words to represent changes has restricted the expressiveness of thelanguage. To address these challenges, in this paper, we introduce the ObjectState Captioning and State Change Representation (OSCaR) dataset and benchmark.OSCaR consists of 14,084 annotated video segments with nearly 1,000 uniqueobjects from various egocentric video collections. It sets a new testbed forevaluating multimodal large language models (MLLMs). Our experimentsdemonstrate that while MLLMs show some skill, they lack a full understanding ofobject state changes. The benchmark includes a fine-tuned model that, despiteinitial capabilities, requires significant improvements in accuracy andgeneralization ability for effective understanding of these changes. Our codeand dataset are available at https://github.com/nguyennm1024/OSCaR.</description><author>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</author><pubDate>Wed, 03 Apr 2024 00:14:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.17128v4</guid></item><item><title>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</title><link>http://arxiv.org/abs/2312.02051v2</link><description>This work proposes TimeChat, a time-sensitive multimodal large language modelspecifically designed for long video understanding. Our model incorporates twokey architectural contributions: (1) a timestamp-aware frame encoder that bindsvisual content with the timestamp of each frame, and (2) a sliding videoQ-Former that produces a video token sequence of varying lengths to accommodatevideos of various durations. Additionally, we construct an instruction-tuningdataset, encompassing 6 tasks and a total of 125K instances, to further enhanceTimeChat's instruction-following performance. Experiment results across variousvideo understanding tasks, such as dense captioning, temporal grounding, andhighlight detection, demonstrate TimeChat's strong zero-shot temporallocalization and reasoning capabilities. For example, it achieves +9.2 F1 scoreand +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5)on Charades-STA, compared to state-of-the-art video large language models,holding the potential to serve as a versatile video assistant for long-formvideo comprehension tasks and satisfy realistic user requirements.</description><author>Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou</author><pubDate>Thu, 28 Mar 2024 13:41:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.02051v2</guid></item><item><title>EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition</title><link>http://arxiv.org/abs/2310.16640v2</link><description>Facial Expression Recognition (FER) is a crucial task in affective computing,but its conventional focus on the seven basic emotions limits its applicabilityto the complex and expanding emotional spectrum. To address the issue of newand unseen emotions present in dynamic in-the-wild FER, we propose a novelvision-language model that utilises sample-level text descriptions (i.e.captions of the context, expressions or emotional cues) as natural languagesupervision, aiming to enhance the learning of rich latent representations, forzero-shot classification. To test this, we evaluate using zero-shotclassification of the model trained on sample-level descriptions on fourpopular dynamic FER datasets. Our findings show that this approach yieldssignificant improvements when compared to baseline methods. Specifically, forzero-shot video FER, we outperform CLIP by over 10\% in terms of WeightedAverage Recall and 5\% in terms of Unweighted Average Recall on severaldatasets. Furthermore, we evaluate the representations obtained from thenetwork trained using sample-level descriptions on the downstream task ofmental health symptom estimation, achieving performance comparable or superiorto state-of-the-art methods and strong agreement with human experts. Namely, weachieve a Pearson's Correlation Coefficient of up to 0.85 on schizophreniasymptom severity estimation, which is comparable to human experts' agreement.The code is publicly available at: https://github.com/NickyFot/EmoCLIP.</description><author>Niki Maria Foteinopoulou, Ioannis Patras</author><pubDate>Mon, 18 Mar 2024 10:07:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.16640v2</guid></item><item><title>Test-Time Zero-Shot Temporal Action Localization</title><link>http://arxiv.org/abs/2404.05426v1</link><description>Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locateactions in untrimmed videos unseen during training. Existing ZS-TAL methodsinvolve fine-tuning a model on a large amount of annotated training data. Whileeffective, training-based ZS-TAL approaches assume the availability of labeleddata for supervised learning, which can be impractical in some applications.Furthermore, the training process naturally induces a domain bias into thelearned model, which may adversely affect the model's generalization ability toarbitrary videos. These considerations prompt us to approach the ZS-TAL problemfrom a radically novel perspective, relaxing the requirement for training data.To this aim, we introduce a novel method that performs Test-Time adaptation forTemporal Action Localization (T3AL). In a nutshell, T3AL adapts a pre-trainedVision and Language Model (VLM). T3AL operates in three steps. First, avideo-level pseudo-label of the action category is computed by aggregatinginformation from the entire video. Then, action localization is performedadopting a novel procedure inspired by self-supervised learning. Finally,frame-level textual descriptions extracted with a state-of-the-art captioningmodel are employed for refining the action region proposals. We validate theeffectiveness of T3AL by conducting experiments on the THUMOS14 and theActivityNet-v1.3 datasets. Our results demonstrate that T3AL significantlyoutperforms zero-shot baselines based on state-of-the-art VLMs, confirming thebenefit of a test-time adaptation approach.</description><author>Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci</author><pubDate>Mon, 08 Apr 2024 12:54:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2404.05426v1</guid></item><item><title>Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding</title><link>http://arxiv.org/abs/2403.14174v1</link><description>Inspired by the activity-silent and persistent activity mechanisms in humanvisual perception biology, we design a Unified Static and Dynamic Network(UniSDNet), to learn the semantic association between the video and text/audioqueries in a cross-modal environment for efficient video grounding. For staticmodeling, we devise a novel residual structure (ResMLP) to boost the globalcomprehensive interaction between the video segments and queries, achievingmore effective semantic enhancement/supplement. For dynamic modeling, weeffectively exploit three characteristics of the persistent activity mechanismin our network design for a better video context comprehension. Specifically,we construct a diffusely connected video clip graph on the basis of 2D sparsetemporal masking to reflect the "short-term effect" relationship. Weinnovatively consider the temporal distance and relevance as the joint"auxiliary evidence clues" and design a multi-kernel Temporal Gaussian Filterto expand the context clue into high-dimensional space, simulating the "complexvisual perception", and then conduct element level filtering convolutionoperations on neighbour clip nodes in message passing stage for finallygenerating and ranking the candidate proposals. Our UniSDNet is applicable toboth Natural Language Video Grounding (NLVG) and Spoken Language VideoGrounding (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widelyused datasets for NLVG, as well as three datasets for SLVG, e.g., reporting newrecords at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 onTACoS. To facilitate this field, we collect two new datasets (Charades-STASpeech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of ourUniSDNet is 1.56$\times$ faster than the strong multi-query benchmark. Code isavailable at: https://github.com/xian-sh/UniSDNet.</description><author>Jingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, Meng Wang</author><pubDate>Thu, 21 Mar 2024 07:53:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2403.14174v1</guid></item><item><title>CI w/o TN: Context Injection without Task Name for Procedure Planning</title><link>http://arxiv.org/abs/2402.15579v1</link><description>This paper explores the challenge of procedure planning in instructionalvideos, which involves creating goal-directed plans based on visual start andgoal observations from videos. Previous research has tackled this problem withgradually weaker training supervision, from heavy intermediate visualobservations or language instructions to task class supervision. However, withthe advent of large language models, even given only the task name, thesemodels can produce a detailed plan. In this study, we propose a much weakersetting without task name as supervision, which is not currently solvable byexisting large language models since they require good prompts with sufficientinformation. Specifically, we hypothesize that previous intermediatesupervisions can serve as context information, and we use captions of visualstart and goal observations as a much cheaper form of supervision. Thisapproach greatly reduces the labeling cost since the captions can be easilyobtained by large pre-trained vision-language foundation models. Technically,we apply BLIP to generate captions as supervision to train the context featurewith contrastive learning loss. Afterward, the context feature is fed into thegenerator to aid in plan generation. Our experiments on two datasets withvarying scales demonstrate that our model can achieve comparable performance onmultiple metrics, which validates our hypothesis.</description><author>Xinjie Li</author><pubDate>Fri, 23 Feb 2024 19:34:47 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2402.15579v1</guid></item></channel></rss>