<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Arxivvideo captioning</title><link></link><description>Arxiv paper</description><language>en-US</language><lastBuildDate>Sun, 07 Jan 2024 14:00:04 GMT</lastBuildDate><generator>rfeed v1.0.0</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Accurate and Fast Compressed Video Captioning</title><link>http://arxiv.org/abs/2309.12867v1</link><description>Existing video captioning approaches typically require to first sample videoframes from a decoded video and then conduct a subsequent process (e.g.,feature extraction and/or captioning model learning). In this pipeline, manualframe sampling may ignore key information in videos and thus degradeperformance. Additionally, redundant information in the sampled frames mayresult in low efficiency in the inference of video captioning. Addressing this,we study video captioning from a different perspective in compressed domain,which brings multi-fold advantages over the existing pipeline: 1) Compared toraw images from the decoded video, the compressed video, consisting ofI-frames, motion vectors and residuals, is highly distinguishable, which allowsus to leverage the entire video for learning without manual sampling through aspecialized model design; 2) The captioning model is more efficient ininference as smaller and less redundant information is processed. We propose asimple yet effective end-to-end transformer in the compressed domain for videocaptioning that enables learning from the compressed video for captioning. Weshow that even with a simple design, our method can achieve state-of-the-artperformance on different benchmarks while running almost 2x faster thanexisting approaches. Code is available at https://github.com/acherstyx/CoCap.</description><author>Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, Libo Zhang</author><pubDate>Fri, 22 Sep 2023 14:43:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12867v1</guid></item><item><title>Accurate and Fast Compressed Video Captioning</title><link>http://arxiv.org/abs/2309.12867v2</link><description>Existing video captioning approaches typically require to first sample videoframes from a decoded video and then conduct a subsequent process (e.g.,feature extraction and/or captioning model learning). In this pipeline, manualframe sampling may ignore key information in videos and thus degradeperformance. Additionally, redundant information in the sampled frames mayresult in low efficiency in the inference of video captioning. Addressing this,we study video captioning from a different perspective in compressed domain,which brings multi-fold advantages over the existing pipeline: 1) Compared toraw images from the decoded video, the compressed video, consisting ofI-frames, motion vectors and residuals, is highly distinguishable, which allowsus to leverage the entire video for learning without manual sampling through aspecialized model design; 2) The captioning model is more efficient ininference as smaller and less redundant information is processed. We propose asimple yet effective end-to-end transformer in the compressed domain for videocaptioning that enables learning from the compressed video for captioning. Weshow that even with a simple design, our method can achieve state-of-the-artperformance on different benchmarks while running almost 2x faster thanexisting approaches. Code is available at https://github.com/acherstyx/CoCap.</description><author>Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, Libo Zhang</author><pubDate>Wed, 03 Jan 2024 08:29:03 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.12867v2</guid></item><item><title>Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023)</title><link>http://arxiv.org/abs/2312.07418v2</link><description>Video captioning in Nepali, a language written in the Devanagari script,presents a unique challenge due to the lack of existing academic work in thisdomain. This work develops a novel encoder-decoder paradigm for Nepali videocaptioning to tackle this difficulty. LSTM and GRU sequence-to-sequence modelsare used in the model to produce related textual descriptions based on featuresretrieved from video frames using CNNs. Using Google Translate and manualpost-editing, a Nepali video captioning dataset is generated from the MicrosoftResearch Video Description Corpus (MSVD) dataset created using GoogleTranslate, and manual post-editing work. The efficacy of the model forDevanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGEmeasures, which are used to assess its performance.</description><author>Kabita Parajuli, Shashidhar Ram Joshi</author><pubDate>Tue, 02 Jan 2024 12:24:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07418v2</guid></item><item><title>Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023)</title><link>http://arxiv.org/abs/2312.07418v1</link><description>Video captioning in Nepali, a language written in the Devanagari script,presents a unique challenge due to the lack of existing academic work in thisdomain. This work develops a novel encoder-decoder paradigm for Nepali videocaptioning to tackle this difficulty. LSTM and GRU sequence-to-sequence modelsare used in the model to produce related textual descriptions based on featuresretrieved from video frames using CNNs. Using Google Translate and manualpost-editing, a Nepali video captioning dataset is generated from the MicrosoftResearch Video Description Corpus (MSVD) dataset created using GoogleTranslate, and manual post-editing work. The efficacy of the model forDevanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGEmeasures, which are used to assess its performance.</description><author>Kabita Parajuli, Shashidhar Ram Joshi</author><pubDate>Tue, 12 Dec 2023 16:39:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.07418v1</guid></item><item><title>Retrieval-Augmented Egocentric Video Captioning</title><link>http://arxiv.org/abs/2401.00789v2</link><description>Understanding human actions from videos of first-person view posessignificant challenges. Most prior approaches explore representation learningon egocentric videos only, while overlooking the potential benefit ofexploiting existing large-scale third-person videos. In this paper, (1) wedevelop EgoInstructor, a retrieval-augmented multimodal captioning model thatautomatically retrieves semantically relevant third-person instructional videosto enhance the video captioning of egocentric videos. (2) For training thecross-view retrieval module, we devise an automatic pipeline to discoverego-exo video pairs from distinct large-scale egocentric and exocentricdatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCEloss that pulls egocentric and exocentric video features closer by aligningthem to shared text features that describe similar actions. (4) Throughextensive experiments, our cross-view retrieval module demonstrates superiorperformance across seven benchmarks. Regarding egocentric video captioning,EgoInstructor exhibits significant improvements by leveraging third-personvideos as references.</description><author>Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie</author><pubDate>Wed, 03 Jan 2024 05:08:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00789v2</guid></item><item><title>Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos</title><link>http://arxiv.org/abs/2311.16444v1</link><description>We propose a novel benchmark for cross-view knowledge transfer of dense videocaptioning, adapting models from web instructional videos with exocentric viewsto an egocentric view. While dense video captioning (predicting time segmentsand their captions) is primarily studied with exocentric videos (e.g.,YouCook2), benchmarks with egocentric videos are restricted due to datascarcity. To overcome the limited video availability, transferring knowledgefrom abundant exocentric web videos is demanded as a practical approach.However, learning the correspondence between exocentric and egocentric views isdifficult due to their dynamic view changes. The web videos contain mixed viewsfocusing on either human body actions or close-up hand-object interactions,while the egocentric view is constantly shifting as the camera wearer moves.This necessitates the in-depth study of cross-view transfer under complex viewchanges. In this work, we first create a real-life egocentric dataset (EgoYC2)whose captions are shared with YouCook2, enabling transfer learning betweenthese datasets assuming their ground-truth is accessible. To bridge the viewgaps, we propose a view-invariant learning method using adversarial training inboth the pre-training and fine-tuning stages. While the pre-training isdesigned to learn invariant features against the mixed views in the web videos,the view-invariant fine-tuning further mitigates the view gaps between bothdatasets. We validate our proposed method by studying how effectively itovercomes the view change problem and efficiently transfers the knowledge tothe egocentric domain. Our benchmark pushes the study of the cross-viewtransfer into a new task domain of dense video captioning and will envisionmethodologies to describe egocentric videos in natural language.</description><author>Takehiko Ohkawa, Takuma Yagi, Taichi Nishimura, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato</author><pubDate>Tue, 28 Nov 2023 02:51:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16444v1</guid></item><item><title>Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos</title><link>http://arxiv.org/abs/2311.16444v2</link><description>We propose a novel benchmark for cross-view knowledge transfer of dense videocaptioning, adapting models from web instructional videos with exocentric viewsto an egocentric view. While dense video captioning (predicting time segmentsand their captions) is primarily studied with exocentric videos (e.g.,YouCook2), benchmarks with egocentric videos are restricted due to datascarcity. To overcome the limited video availability, transferring knowledgefrom abundant exocentric web videos is demanded as a practical approach.However, learning the correspondence between exocentric and egocentric views isdifficult due to their dynamic view changes. The web videos contain mixed viewsfocusing on either human body actions or close-up hand-object interactions,while the egocentric view is constantly shifting as the camera wearer moves.This necessitates the in-depth study of cross-view transfer under complex viewchanges. In this work, we first create a real-life egocentric dataset (EgoYC2)whose captions are shared with YouCook2, enabling transfer learning betweenthese datasets assuming their ground-truth is accessible. To bridge the viewgaps, we propose a view-invariant learning method using adversarial training inboth the pre-training and fine-tuning stages. While the pre-training isdesigned to learn invariant features against the mixed views in the web videos,the view-invariant fine-tuning further mitigates the view gaps between bothdatasets. We validate our proposed method by studying how effectively itovercomes the view change problem and efficiently transfers the knowledge tothe egocentric domain. Our benchmark pushes the study of the cross-viewtransfer into a new task domain of dense video captioning and will envisionmethodologies to describe egocentric videos in natural language.</description><author>Takehiko Ohkawa, Takuma Yagi, Taichi Nishimura, Ryosuke Furuta, Atsushi Hashimoto, Yoshitaka Ushiku, Yoichi Sato</author><pubDate>Wed, 29 Nov 2023 06:01:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.16444v2</guid></item><item><title>Exploring the Role of Audio in Video Captioning</title><link>http://arxiv.org/abs/2306.12559v1</link><description>Recent focus in video captioning has been on designing architectures that canconsume both video and text modalities, and using large-scale video datasetswith text transcripts for pre-training, such as HowTo100M. Though theseapproaches have achieved significant improvement, the audio modality is oftenignored in video captioning. In this work, we present an audio-visualframework, which aims to fully exploit the potential of the audio modality forcaptioning. Instead of relying on text transcripts extracted via automaticspeech recognition (ASR), we argue that learning with raw audio signals can bemore beneficial, as audio has additional information including acoustic events,speaker identity, etc. Our contributions are twofold. First, we observed thatthe model overspecializes to the audio modality when pre-training with bothvideo and audio modality, since the ground truth (i.e., text transcripts) canbe solely predicted using audio. We proposed a Modality Balanced Pre-training(MBP) loss to mitigate this issue and significantly improve the performance ondownstream tasks. Second, we slice and dice different design choices of thecross-modal module, which may become an information bottleneck and generateinferior results. We proposed new local-global fusion mechanisms to improveinformation exchange across audio and video. We demonstrate significantimprovements by leveraging the audio modality on four datasets, and evenoutperform the state of the art on some metrics without relying on the textmodality as the input.</description><author>Yuhan Shen, Linjie Yang, Longyin Wen, Haichao Yu, Ehsan Elhamifar, Heng Wang</author><pubDate>Wed, 21 Jun 2023 21:54:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.12559v1</guid></item><item><title>GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation</title><link>http://arxiv.org/abs/2303.14655v2</link><description>Despite the recent emergence of video captioning models, how to generatevivid, fine-grained video descriptions based on the background knowledge (i.e.,long and informative commentary about the domain-specific scenes withappropriate reasoning) is still far from being solved, which however has greatapplications such as automatic sports narrative. In this paper, we presentGOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42kknowledge triples for proposing a challenging new task setting asKnowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimentaladaption of existing methods to show the difficulty and potential directionsfor solving this valuable and applicable task. Our data and code are availableat https://github.com/THU-KEG/goal.</description><author>Ji Qi, Jifan Yu, Teng Tu, Kunyu Gao, Yifan Xu, Xinyu Guan, Xiaozhi Wang, Yuxiao Dong, Bin Xu, Lei Hou, Juanzi Li, Jie Tang, Weidong Guo, Hui Liu, Yu Xu</author><pubDate>Thu, 05 Oct 2023 07:55:13 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.14655v2</guid></item><item><title>Video Captioning with Aggregated Features Based on Dual Graphs and Gated Fusion</title><link>http://arxiv.org/abs/2308.06685v1</link><description>The application of video captioning models aims at translating the content ofvideos by using accurate natural language. Due to the complex nature inbetweenobject interaction in the video, the comprehensive understanding ofspatio-temporal relations of objects remains a challenging task. Existingmethods often fail in generating sufficient feature representations of videocontent. In this paper, we propose a video captioning model based on dualgraphs and gated fusion: we adapt two types of graphs to generate featurerepresentations of video content and utilize gated fusion to further understandthese different levels of information. Using a dual-graphs model to generateappearance features and motion features respectively can utilize the contentcorrelation in frames to generate various features from multiple perspectives.Among them, dual-graphs reasoning can enhance the content correlation in framesequences to generate advanced semantic features; The gated fusion, on theother hand, aggregates the information in multiple feature representations forcomprehensive video content understanding. The experiments conducted on worldlyused datasets MSVD and MSR-VTT demonstrate state-of-the-art performance of ourproposed approach.</description><author>Yutao Jin, Bin Liu, Jing Wang</author><pubDate>Sun, 13 Aug 2023 06:18:08 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.06685v1</guid></item><item><title>Set Prediction Guided by Semantic Concepts for Diverse Video Captioning</title><link>http://arxiv.org/abs/2312.15720v1</link><description>Diverse video captioning aims to generate a set of sentences to describe thegiven video in various aspects. Mainstream methods are trained with independentpairs of a video and a caption from its ground-truth set without exploiting theintra-set relationship, resulting in low diversity of generated captions.Different from them, we formulate diverse captioning into asemantic-concept-guided set prediction (SCG-SP) problem by fitting thepredicted caption set to the ground-truth set, where the set-level relationshipis fully captured. Specifically, our set prediction consists of two synergistictasks, i.e., caption generation and an auxiliary task of concept combinationprediction providing extra semantic supervision. Each caption in the set isattached to a concept combination indicating the primary semantic content ofthe caption and facilitating element alignment in set prediction. Furthermore,we apply a diversity regularization term on concepts to encourage the model togenerate semantically diverse captions with various concept combinations. Thesetwo tasks share multiple semantics-specific encodings as input, which areobtained by iterative interaction between visual features and conceptualqueries. The correspondence between the generated captions and specific conceptcombinations further guarantees the interpretability of our model. Extensiveexperiments on benchmark datasets show that the proposed SCG-SP achievesstate-of-the-art (SOTA) performance under both relevance and diversity metrics.</description><author>Yifan Lu, Ziqi Zhang, Chunfeng Yuan, Peng Li, Yan Wang, Bing Li, Weiming Hu</author><pubDate>Mon, 25 Dec 2023 13:13:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15720v1</guid></item><item><title>Nepali Video Captioning using CNN-RNN Architecture</title><link>http://arxiv.org/abs/2311.02699v1</link><description>This article presents a study on Nepali video captioning using deep neuralnetworks. Through the integration of pre-trained CNNs and RNNs, the researchfocuses on generating precise and contextually relevant captions for Nepalivideos. The approach involves dataset collection, data preprocessing, modelimplementation, and evaluation. By enriching the MSVD dataset with Nepalicaptions via Google Translate, the study trains various CNN-RNN architectures.The research explores the effectiveness of CNNs (e.g., EfficientNetB0,ResNet101, VGG16) paired with different RNN decoders like LSTM, GRU, andBiLSTM. Evaluation involves BLEU and METEOR metrics, with the best model beingEfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 scoreof 17 and METEOR score of 46. The article also outlines challenges and futuredirections for advancing Nepali video captioning, offering a crucial resourcefor further research in this area.</description><author>Bipesh Subedi, Saugat Singh, Bal Krishna Bal</author><pubDate>Sun, 05 Nov 2023 16:09:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.02699v1</guid></item><item><title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</title><link>http://arxiv.org/abs/2307.02682v1</link><description>Dense video captioning, a task of localizing meaningful moments andgenerating relevant captions for videos, often requires a large, expensivecorpus of annotated video segments paired with text. In an effort to minimizethe annotation cost, we propose ZeroTA, a novel method for dense videocaptioning in a zero-shot manner. Our method does not require any videos orannotations for training; instead, it localizes and describes events withineach input video at test time by optimizing solely on the input. This isaccomplished by introducing a soft moment mask that represents a temporalsegment in the video and jointly optimizing it with the prefix parameters of alanguage model. This joint optimization aligns a frozen language generationmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,CLIP) by maximizing the matching score between the generated text and a momentwithin the video. We also introduce a pairwise temporal IoU loss to let a setof soft moment masks capture multiple distinct events within the video. Ourmethod effectively discovers diverse significant events within the video, withthe resulting captions appropriately describing these events. The empiricalresults demonstrate that ZeroTA surpasses zero-shot baselines and evenoutperforms the state-of-the-art few-shot method on the widely-used benchmarkActivityNet Captions. Moreover, our method shows greater robustness compared tosupervised methods when evaluated in out-of-domain scenarios. This researchprovides insight into the potential of aligning widely-used models, such aslanguage generation models and vision-language models, to unlock a newcapability: understanding temporal aspects of videos.</description><author>Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo</author><pubDate>Thu, 06 Jul 2023 00:01:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02682v1</guid></item><item><title>Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment</title><link>http://arxiv.org/abs/2307.02682v2</link><description>Dense video captioning, a task of localizing meaningful moments andgenerating relevant captions for videos, often requires a large, expensivecorpus of annotated video segments paired with text. In an effort to minimizethe annotation cost, we propose ZeroTA, a novel method for dense videocaptioning in a zero-shot manner. Our method does not require any videos orannotations for training; instead, it localizes and describes events withineach input video at test time by optimizing solely on the input. This isaccomplished by introducing a soft moment mask that represents a temporalsegment in the video and jointly optimizing it with the prefix parameters of alanguage model. This joint optimization aligns a frozen language generationmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,CLIP) by maximizing the matching score between the generated text and a momentwithin the video. We also introduce a pairwise temporal IoU loss to let a setof soft moment masks capture multiple distinct events within the video. Ourmethod effectively discovers diverse significant events within the video, withthe resulting captions appropriately describing these events. The empiricalresults demonstrate that ZeroTA surpasses zero-shot baselines and evenoutperforms the state-of-the-art few-shot method on the widely-used benchmarkActivityNet Captions. Moreover, our method shows greater robustness compared tosupervised methods when evaluated in out-of-domain scenarios. This researchprovides insight into the potential of aligning widely-used models, such aslanguage generation models and vision-language models, to unlock a newcapability: understanding temporal aspects of videos.</description><author>Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo</author><pubDate>Tue, 11 Jul 2023 05:10:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.02682v2</guid></item><item><title>Event and Entity Extraction from Generated Video Captions</title><link>http://arxiv.org/abs/2211.02982v3</link><description>Annotation of multimedia data by humans is time-consuming and costly, whilereliable automatic generation of semantic metadata is a major challenge. Wepropose a framework to extract semantic metadata from automatically generatedvideo captions. As metadata, we consider entities, the entities' properties,relations between entities, and the video category. We employ twostate-of-the-art dense video captioning models with masked transformer (MT) andparallel decoding (PVDC) to generate captions for videos of the ActivityNetCaptions dataset. Our experiments show that it is possible to extract entities,their properties, relations between entities, and the video category from thegenerated captions. We observe that the quality of the extracted information ismainly influenced by the quality of the event localization in the video as wellas the performance of the event caption generation.</description><author>Johannes Scherer, Ansgar Scherp, Deepayan Bhowmik</author><pubDate>Wed, 13 Sep 2023 15:49:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2211.02982v3</guid></item><item><title>CoVR: Learning Composed Video Retrieval from Web Video Captions</title><link>http://arxiv.org/abs/2308.14746v1</link><description>Composed Image Retrieval (CoIR) has recently gained popularity as a task thatconsiders both text and image queries together, to search for relevant imagesin a database. Most CoIR approaches require manually annotated datasets,comprising image-text-image triplets, where the text describes a modificationfrom the query image to the target image. However, manual curation of CoIRtriplets is expensive and prevents scalability. In this work, we insteadpropose a scalable automatic dataset creation methodology that generatestriplets given video-caption pairs, while also expanding the scope of the taskto include composed video retrieval (CoVR). To this end, we mine paired videoswith a similar caption from a large database, and leverage a large languagemodel to generate the corresponding modification text. Applying thismethodology to the extensive WebVid2M collection, we automatically constructour WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, weintroduce a new benchmark for CoVR with a manually annotated evaluation set,along with baseline results. Our experiments further demonstrate that traininga CoVR model on our dataset effectively transfers to CoIR, leading to improvedstate-of-the-art performance in the zero-shot setup on both the CIRR andFashionIQ benchmarks. Our code, datasets, and models are publicly available athttps://imagine.enpc.fr/~ventural/covr.</description><author>Lucas Ventura, Antoine Yang, Cordelia Schmid, GÃ¼l Varol</author><pubDate>Mon, 28 Aug 2023 18:55:33 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.14746v1</guid></item><item><title>Collaborative Three-Stream Transformers for Video Captioning</title><link>http://arxiv.org/abs/2309.09611v1</link><description>As the most critical components in a sentence, subject, predicate and objectrequire special attention in the video captioning task. To implement this idea,we design a novel framework, named COllaborative three-Stream Transformers(COST), to model the three parts separately and complement each other forbetter representation. Specifically, COST is formed by three branches oftransformers to exploit the visual-linguistic interactions of differentgranularities in spatial-temporal domain between videos and text, detectedobjects and text, and actions and text. Meanwhile, we propose across-granularity attention module to align the interactions modeled by thethree branches of transformers, then the three branches of transformers cansupport each other to exploit the most discriminative semantic information ofdifferent granularities for accurate predictions of captions. The whole modelis trained in an end-to-end fashion. Extensive experiments conducted on threelarge-scale challenging datasets, i.e., YouCookII, ActivityNet Captions andMSVD, demonstrate that the proposed method performs favorably against thestate-of-the-art methods.</description><author>Hao Wang, Libo Zhang, Heng Fan, Tiejian Luo</author><pubDate>Mon, 18 Sep 2023 10:33:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09611v1</guid></item><item><title>Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks</title><link>http://arxiv.org/abs/2306.04362v1</link><description>To promote the development of Vision-Language Pre-training (VLP) andmultimodal Large Language Model (LLM) in the Chinese community, we firstlyrelease the largest public Chinese high-quality video-language dataset namedYouku-mPLUG, which is collected from Youku, a well-known Chinese video-sharingwebsite, with strict criteria of safety, diversity, and quality. Youku-mPLUGcontains 10 million Chinese video-text pairs filtered from 400 million rawvideos across a wide range of 45 diverse categories for large-scalepre-training. In addition, to facilitate a comprehensive evaluation ofvideo-language models, we carefully build the largest human-annotated Chinesebenchmarks covering three popular video-language tasks of cross-modalretrieval, video captioning, and video category classification. Youku-mPLUG canenable researchers to conduct more in-depth multimodal research and developbetter applications in the future. Furthermore, we release popularvideo-language pre-training models, ALPRO and mPLUG-2, and our proposedmodularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG.Experiments show that models pre-trained on Youku-mPLUG gain up to 23.1%improvement in video category classification. Besides, mPLUG-video achieves anew state-of-the-art result on these benchmarks with 80.5% top-1 accuracy invideo category classification and 68.9 CIDEr score in video captioning,respectively. Finally, we scale up mPLUG-video based on the frozen Bloomz withonly 1.7% trainable parameters as Chinese multimodal LLM, and demonstrateimpressive instruction and video understanding ability. The zero-shotinstruction understanding experiment indicates that pretraining withYouku-mPLUG can enhance the ability to comprehend overall and detailed visualsemantics, recognize scene text, and leverage open-domain knowledge.</description><author>Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, Fei Huang</author><pubDate>Wed, 07 Jun 2023 12:52:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04362v1</guid></item><item><title>UCF-Crime Annotation: A Benchmark for Surveillance Video-and-Language Understanding</title><link>http://arxiv.org/abs/2309.13925v1</link><description>Surveillance videos are an essential component of daily life with variouscritical applications, particularly in public security. However, currentsurveillance video tasks mainly focus on classifying and localizing anomalousevents. Existing methods are limited to detecting and classifying thepredefined events with unsatisfactory generalization ability and semanticunderstanding, although they have obtained considerable performance. To addressthis issue, we propose constructing the first multimodal surveillance videodataset by manually annotating the real-world surveillance dataset UCF-Crimewith fine-grained event content and timing. Our newly annotated dataset, UCA(UCF-Crime Annotation), provides a novel benchmark for multimodal surveillancevideo analysis. It not only describes events in detailed descriptions but alsoprovides precise temporal grounding of the events in 0.1-second intervals. UCAcontains 20,822 sentences, with an average length of 23 words, and itsannotated videos are as long as 102 hours. Furthermore, we benchmark thestate-of-the-art models of multiple multimodal tasks on this newly createddataset, including temporal sentence grounding in videos, video captioning, anddense video captioning. Through our experiments, we found that mainstreammodels used in previously publicly available datasets perform poorly onmultimodal surveillance video scenarios, which highlights the necessity ofconstructing this dataset. The link to our dataset and code is provided at:https://github.com/Xuange923/UCA-dataset.</description><author>Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Jian Jin, Zhenzhen Jiao</author><pubDate>Mon, 25 Sep 2023 08:46:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.13925v1</guid></item><item><title>Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation</title><link>http://arxiv.org/abs/2303.12112v3</link><description>The CLIP model has been recently proven to be very effective for a variety ofcross-modal tasks, including the evaluation of captions generated fromvision-and-language architectures. In this paper, we propose a new recipe for acontrastive-based evaluation metric for image captioning, namelyPositive-Augmented Contrastive learning Score (PAC-S), that in a novel wayunifies the learning of a contrastive visual-semantic space with the additionof generated images and text on curated data. Experiments spanning severaldatasets demonstrate that our new metric achieves the highest correlation withhuman judgments on both images and videos, outperforming existingreference-based metrics like CIDEr and SPICE and reference-free metrics likeCLIP-Score. Finally, we test the system-level correlation of the proposedmetric when considering popular image captioning approaches, and assess theimpact of employing different cross-modal features. Our source code and trainedmodels are publicly available at: https://github.com/aimagelab/pacscore.</description><author>Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</author><pubDate>Thu, 20 Jul 2023 09:16:09 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12112v3</guid></item><item><title>Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation</title><link>http://arxiv.org/abs/2303.12112v2</link><description>The CLIP model has been recently proven to be very effective for a variety ofcross-modal tasks, including the evaluation of captions generated fromvision-and-language architectures. In this paper, we propose a new recipe for acontrastive-based evaluation metric for image captioning, namelyPositive-Augmented Contrastive learning Score (PAC-S), that in a novel wayunifies the learning of a contrastive visual-semantic space with the additionof generated images and text on curated data. Experiments spanning severaldatasets demonstrate that our new metric achieves the highest correlation withhuman judgments on both images and videos, outperforming existingreference-based metrics like CIDEr and SPICE and reference-free metrics likeCLIP-Score. Finally, we test the system-level correlation of the proposedmetric when considering popular image captioning approaches, and assess theimpact of employing different cross-modal features. Our source code and trainedmodels are publicly available at: https://github.com/aimagelab/pacscore.</description><author>Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</author><pubDate>Tue, 16 May 2023 19:40:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.12112v2</guid></item><item><title>Dense Video Object Captioning from Disjoint Supervision</title><link>http://arxiv.org/abs/2306.11729v1</link><description>We propose a new task and model for dense video object captioning --detecting, tracking, and captioning trajectories of all objects in a video.This task unifies spatial and temporal understanding of the video, and requiresfine-grained language description. Our model for dense video object captioningis trained end-to-end and consists of different modules for spatiallocalization, tracking, and captioning. As such, we can train our model with amixture of disjoint tasks, and leverage diverse, large-scale datasets whichsupervise different parts of our model. This results in noteworthy zero-shotperformance. Moreover, by finetuning a model from this initialization, we canfurther improve our performance, surpassing strong image-based baselines by asignificant margin. Although we are not aware of other work performing thistask, we are able to repurpose existing video grounding datasets for our task,namely VidSTG and VLN. We show our task is more general than grounding, andmodels trained on our task can directly be applied to grounding by finding thebounding box with the maximum likelihood of generating the query sentence. Ourmodel outperforms dedicated, state-of-the-art models for spatial grounding onboth VidSTG and VLN.</description><author>Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid</author><pubDate>Tue, 20 Jun 2023 18:57:23 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11729v1</guid></item><item><title>Traffic-Domain Video Question Answering with Automatic Captioning</title><link>http://arxiv.org/abs/2307.09636v1</link><description>Video Question Answering (VidQA) exhibits remarkable potential infacilitating advanced machine reasoning capabilities within the domains ofIntelligent Traffic Monitoring and Intelligent Transportation Systems.Nevertheless, the integration of urban traffic scene knowledge into VidQAsystems has received limited attention in previous research endeavors. In thiswork, we present a novel approach termed Traffic-domain Video QuestionAnswering with Automatic Captioning (TRIVIA), which serves as aweak-supervision technique for infusing traffic-domain knowledge into largevideo-language models. Empirical findings obtained from the SUTD-TrafficQA taskhighlight the substantial enhancements achieved by TRIVIA, elevating theaccuracy of representative video-language models by a remarkable 6.5 points(19.88%) compared to baseline settings. This pioneering methodology holds greatpromise for driving advancements in the field, inspiring researchers andpractitioners alike to unlock the full potential of emerging video-languagemodels in traffic-related applications.</description><author>Ehsan Qasemi, Jonathan M. Francis, Alessandro Oltramari</author><pubDate>Tue, 18 Jul 2023 21:56:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.09636v1</guid></item><item><title>ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation</title><link>http://arxiv.org/abs/2303.06458v2</link><description>Natural Language Generation (NLG) accepts input data in the form of images,videos, or text and generates corresponding natural language text as output.Existing NLG methods mainly adopt a supervised approach and rely heavily oncoupled data-to-text pairs. However, for many targeted scenarios and fornon-English languages, sufficient quantities of labeled data are often notavailable. To relax the dependency on labeled data of downstream tasks, wepropose an intuitive and effective zero-shot learning framework, ZeroNLG, whichcan deal with multiple NLG tasks, including image-to-text (image captioning),video-to-text (video captioning), and text-to-text (neural machinetranslation), across English, Chinese, German, and French within a unifiedframework. ZeroNLG does not require any labeled downstream pairs for training.During training, ZeroNLG (i) projects different domains (across modalities andlanguages) to corresponding coordinates in a shared common latent space; (ii)bridges different domains by aligning their corresponding coordinates in thisspace; and (iii) builds an unsupervised multilingual auto-encoder to learn togenerate text by reconstructing the input text given its coordinate in sharedlatent space. Consequently, during inference, based on the data-to-textpipeline, ZeroNLG can generate target sentences across different languagesgiven the coordinate of input data in the common space. Within this unifiedframework, given visual (imaging or video) data as input, ZeroNLG can performzero-shot visual captioning; given textual sentences as input, ZeroNLG canperform zero-shot machine translation. We present the results of extensiveexperiments on twelve NLG tasks, showing that, without using any labeleddownstream pairs for training, ZeroNLG generates high-quality and believableoutputs and significantly outperforms existing zero-shot methods.</description><author>Bang Yang, Fenglin Liu, Yuexian Zou, Xian Wu, Yaowei Wang, David A. Clifton</author><pubDate>Thu, 07 Dec 2023 04:04:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06458v2</guid></item><item><title>Human-centric Behavior Description in Videos: New Benchmark and Model</title><link>http://arxiv.org/abs/2310.02894v1</link><description>In the domain of video surveillance, describing the behavior of eachindividual within the video is becoming increasingly essential, especially incomplex scenarios with multiple individuals present. This is because describingeach individual's behavior provides more detailed situational analysis,enabling accurate assessment and response to potential risks, ensuring thesafety and harmony of public places. Currently, video-level captioning datasetscannot provide fine-grained descriptions for each individual's specificbehavior. However, mere descriptions at the video-level fail to provide anin-depth interpretation of individual behaviors, making it challenging toaccurately determine the specific identity of each individual. To address thischallenge, we construct a human-centric video surveillance captioning dataset,which provides detailed descriptions of the dynamic behaviors of 7,820individuals. Specifically, we have labeled several aspects of each person, suchas location, clothing, and interactions with other elements in the scene, andthese people are distributed across 1,012 videos. Based on this dataset, we canlink individuals to their respective behaviors, allowing for further analysisof each person's behavior in surveillance videos. Besides the dataset, wepropose a novel video captioning approach that can describe individual behaviorin detail on a person-level basis, achieving state-of-the-art results. Tofacilitate further research in this field, we intend to release our dataset andcode.</description><author>Lingru Zhou, Yiqi Gao, Manqing Zhang, Peng Wu, Peng Wang, Yanning Zhang</author><pubDate>Wed, 04 Oct 2023 16:31:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.02894v1</guid></item><item><title>MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in Indonesian</title><link>http://arxiv.org/abs/2306.11341v1</link><description>Multimodal learning on video and text data has been receiving growingattention from many researchers in various research tasks, includingtext-to-video retrieval, video-to-text retrieval, and video captioning.Although many algorithms have been proposed for those challenging tasks, mostof them are developed on English language datasets. Despite Indonesian beingone of the most spoken languages in the world, the research progress on themultimodal video-text with Indonesian sentences is still under-explored, likelydue to the absence of the public benchmark dataset. To address this issue, weconstruct the first public Indonesian video-text dataset by translating Englishsentences from the MSVD dataset to Indonesian sentences. Using our dataset, wethen train neural network models which were developed for the Englishvideo-text dataset on three tasks, i.e., text-to-video retrieval, video-to-textretrieval, and video captioning. The recent neural network-based approaches tovideo-text tasks often utilized a feature extractor that is primarilypretrained on an English vision-language dataset. Since the availability of thepretraining resources with Indonesian sentences is relatively limited, theapplicability of those approaches to our dataset is still questionable. Toovercome the lack of pretraining resources, we apply cross-lingual transferlearning by utilizing the feature extractors pretrained on the English dataset,and we then fine-tune the models on our Indonesian dataset. Our experimentalresults show that this approach can help to improve the performance for thethree tasks on all metrics. Finally, we discuss potential future works usingour dataset, inspiring further research in the Indonesian multimodal video-texttasks. We believe that our dataset and our experimental results could providevaluable contributions to the community. Our dataset is available on GitHub.</description><author>Willy Fitra Hendria</author><pubDate>Tue, 20 Jun 2023 08:19:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.11341v1</guid></item><item><title>Edit As You Wish: Video Description Editing with Multi-grained Commands</title><link>http://arxiv.org/abs/2305.08389v1</link><description>Automatically narrating a video with natural language can assist people ingrasping and managing massive videos on the Internet. From the perspective ofvideo uploaders, they may have varied preferences for writing the desired videodescription to attract more potential followers, e.g. catching customers'attention for product videos. The Controllable Video Captioning task istherefore proposed to generate a description conditioned on the user demand andvideo content. However, existing works suffer from two shortcomings: 1) thecontrol signal is fixed and can only express single-grained control; 2) thevideo description can not be further edited to meet dynamic user demands. Inthis paper, we propose a novel Video Description Editing (VDEdit) task toautomatically revise an existing video description guided by flexible userrequests. Inspired by human writing-revision habits, we design the user commandas a {operation, position, attribute} triplet to cover multi-grained userequirements, which can express coarse-grained control (e.g. expand thedescription) as well as fine-grained control (e.g. add specified details inspecified position) in a unified format. To facilitate the VDEdit task, wefirst automatically construct a large-scale benchmark dataset namely VATEX-EDITin the open domain describing diverse human activities. Considering thereal-life application scenario, we further manually collect an e-commercebenchmark dataset called EMMAD-EDIT. We propose a unified framework to convertthe {operation, position, attribute} triplet into a textual control sequence tohandle multi-grained editing commands. For VDEdit evaluation, we adoptcomprehensive metrics to measure three aspects of model performance, includingcaption quality, caption-command consistency, and caption-video alignment.</description><author>Linli Yao, Yuanmeng Zhang, Ziheng Wang, Xinglin Hou, Tiezheng Ge, Yuning Jiang, Qin Jin</author><pubDate>Mon, 15 May 2023 08:12:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.08389v1</guid></item><item><title>LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning</title><link>http://arxiv.org/abs/2306.10354v1</link><description>Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)competition is detailed in this paper. Unlike conventional video captioningtasks, GEBC demands that the captioning model possess an understanding ofimmediate changes in status around the designated video boundary, making it adifficult task. This paper proposes an effective model LLMVA-GEBC (LargeLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)We utilize a pretrained LLM for generating human-like captions with highquality. (2) To adapt the model to the GEBC task, we take the video Q-former asan adapter and train it with the frozen visual feature extractors and LLM. Ourproposed method achieved a 76.14 score on the test set and won the first placein the challenge. Our code is available athttps://github.com/zjr2000/LLMVA-GEBC .</description><author>Yunlong Tang, Jinrui Zhang, Xiangchen Wang, Teng Wang, Feng Zheng</author><pubDate>Sat, 17 Jun 2023 14:55:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.10354v1</guid></item><item><title>HowToCaption: Prompting LLMs to Transform Video Annotations at Scale</title><link>http://arxiv.org/abs/2310.04900v1</link><description>Instructional videos are an excellent source for learning multimodalrepresentations by leveraging video-subtitle pairs extracted with automaticspeech recognition systems (ASR) from the audio signal in the videos. However,in contrast to human-annotated captions, both speech and subtitles naturallydiffer from the visual content of the videos and thus provide only noisysupervision for multimodal learning. As a result, large-scale annotation-freeweb video training data remains sub-optimal for training text-video models. Inthis work, we propose to leverage the capability of large language models(LLMs) to obtain fine-grained video descriptions aligned with videos.Specifically, we prompt an LLM to create plausible video descriptions based onASR narrations of the video for a large-scale instructional video dataset. Tothis end, we introduce a prompting method that is able to take into account alonger text of subtitles, allowing us to capture context beyond a singlesentence. To align the captions to the video temporally, we prompt the LLM togenerate timestamps for each produced caption based on the subtitles. In thisway, we obtain human-style video captions at scale without human supervision.We apply our method to the subtitles of the HowTo100M dataset, creating a newlarge-scale dataset, HowToCaption. Our evaluation shows that the resultingcaptions not only significantly improve the performance over many differentbenchmark datasets for text-video retrieval but also lead to a disentangling oftextual narration from the audio, boosting performance in text-video-audiotasks.</description><author>Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, Hilde Kuehne</author><pubDate>Sat, 07 Oct 2023 20:32:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04900v1</guid></item><item><title>Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions</title><link>http://arxiv.org/abs/2304.04227v3</link><description>Video captioning aims to convey dynamic scenes from videos using naturallanguage, facilitating the understanding of spatiotemporal information withinour environment. Although there have been recent advances, generating detailedand enriched video descriptions continues to be a substantial challenge. Inthis work, we introduce Video ChatCaptioner, an innovative approach forcreating more comprehensive spatiotemporal video descriptions. Our methodemploys a ChatGPT model as a controller, specifically designed to select framesfor posing video content-driven questions. Subsequently, a robust algorithm isutilized to answer these visual queries. This question-answer frameworkeffectively uncovers intricate video details and shows promise as a method forenhancing video content. Following multiple conversational rounds, ChatGPT cansummarize enriched video content based on previous conversations. Wequalitatively demonstrate that our Video ChatCaptioner can generate captionscontaining more visual details about the videos. The code is publicly availableat https://github.com/Vision-CAIR/ChatCaptioner</description><author>Jun Chen, Deyao Zhu, Kilichbek Haydarov, Xiang Li, Mohamed Elhoseiny</author><pubDate>Wed, 24 May 2023 15:01:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.04227v3</guid></item><item><title>VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation</title><link>http://arxiv.org/abs/2305.03204v1</link><description>We propose a new two-stage pre-training framework for video-to-textgeneration tasks such as video captioning and video question answering: Agenerative encoder-decoder model is first jointly pre-trained on massiveimage-text data to learn fundamental vision-language concepts, and then adaptedto video data in an intermediate video-text pre-training stage to learnvideo-specific skills such as spatio-temporal reasoning. As a result, ourVideoOFA model achieves new state-of-the-art performance on four VideoCaptioning benchmarks, beating prior art by an average of 9.7 points in CIDErscore. It also outperforms existing models on two open-ended Video QuestionAnswering datasets, showcasing its generalization capability as a universalvideo-to-text model.</description><author>Xilun Chen, Lili Yu, Wenhan Xiong, Barlas OÄuz, Yashar Mehdad, Wen-tau Yih</author><pubDate>Fri, 05 May 2023 00:27:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.03204v1</guid></item><item><title>VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending</title><link>http://arxiv.org/abs/2305.13167v1</link><description>Large-scale image-text contrastive pre-training models, such as CLIP, havebeen demonstrated to effectively learn high-quality multimodal representations.However, there is limited research on learning video-text representations forgeneral video multimodal tasks based on these powerful features. Towards thisgoal, we propose a novel video-text pre-training method dubbed VLAB: VideoLanguage pre-training by feature Adapting and Blending, which transfers CLIPrepresentations to video pre-training tasks and develops unified videomultimodal models for a wide range of video-text tasks. Specifically, VLAB isfounded on two key strategies: feature adapting and feature blending. In theformer, we introduce a new video adapter module to address CLIP's deficiency inmodeling temporal information and extend the model's capability to encompassboth contrastive and generative tasks. In the latter, we propose an end-to-endtraining method that further enhances the model's performance by exploiting thecomplementarity of image and video features. We validate the effectiveness andversatility of VLAB through extensive experiments on highly competitive videomultimodal tasks, including video text retrieval, video captioning, and videoquestion answering. Remarkably, VLAB outperforms competing methodssignificantly and sets new records in video question answering on MSRVTT, MSVD,and TGIF datasets. It achieves an accuracy of 49.6, 61.0, and 79.0,respectively. Codes and models will be released.</description><author>Xingjian He, Sihan Chen, Fan Ma, Zhicheng Huang, Xiaojie Jin, Zikang Liu, Dongmei Fu, Yi Yang, Jing Liu, Jiashi Feng</author><pubDate>Mon, 22 May 2023 16:54:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13167v1</guid></item><item><title>From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping</title><link>http://arxiv.org/abs/2304.13273v2</link><description>With the development of Vision-Language Pre-training Models (VLPMs)represented by CLIP and ALIGN, significant breakthroughs have been achieved forassociation-based visual tasks such as image classification and image-textretrieval by the zero-shot capability of CLIP without fine-tuning. However,CLIP is hard to apply to generation-based tasks. This is due to the lack ofdecoder architecture and pre-training tasks for generation. Although previousworks have created generation capacity for CLIP through additional languagemodels, a modality gap between the CLIP representations of different modalitiesand the inability of CLIP to model the offset of this gap, which fails theconcept to transfer across modalities. To solve the problem, we try to mapimages/videos to the language modality and generate captions from the languagemodality. In this paper, we propose the K-nearest-neighbor Cross-modalityMapping (Knight), a zero-shot method from association to generation. Withtext-only unsupervised training, Knight achieves state-of-the-art performancein zero-shot methods for image captioning and video captioning. Our code isavailable at https://github.com/junyangwang0410/Knight.</description><author>Junyang Wang, Ming Yan, Yi Zhang, Jitao Sang</author><pubDate>Thu, 27 Apr 2023 05:54:05 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13273v2</guid></item><item><title>From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping</title><link>http://arxiv.org/abs/2304.13273v3</link><description>With the development of Vision-Language Pre-training Models (VLPMs)represented by CLIP and ALIGN, significant breakthroughs have been achieved forassociation-based visual tasks such as image classification and image-textretrieval by the zero-shot capability of CLIP without fine-tuning. However,CLIP is hard to apply to generation-based tasks. This is due to the lack ofdecoder architecture and pre-training tasks for generation. Although previousworks have created generation capacity for CLIP through additional languagemodels, a modality gap between the CLIP representations of different modalitiesand the inability of CLIP to model the offset of this gap, which fails theconcept to transfer across modalities. To solve the problem, we try to mapimages/videos to the language modality and generate captions from the languagemodality. In this paper, we propose the K-nearest-neighbor Cross-modalityMapping (Knight), a zero-shot method from association to generation. Withtext-only unsupervised training, Knight achieves State-of-the-Art performancein zero-shot methods for image captioning and video captioning. Our code isavailable at https://github.com/junyangwang0410/Knight.</description><author>Junyang Wang, Ming Yan, Yi Zhang, Jitao Sang</author><pubDate>Mon, 08 May 2023 03:29:28 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.13273v3</guid></item><item><title>StoryBench: A Multifaceted Benchmark for Continuous Story Visualization</title><link>http://arxiv.org/abs/2308.11606v2</link><description>Generating video stories from text prompts is a complex task. In addition tohaving high visual quality, videos need to realistically adhere to a sequenceof text prompts whilst being consistent throughout the frames. Creating abenchmark for video generation requires data annotated over time, whichcontrasts with the single caption used often in video datasets. To fill thisgap, we collect comprehensive human annotations on three existing datasets, andintroduce StoryBench: a new, challenging multi-task benchmark to reliablyevaluate forthcoming text-to-video models. Our benchmark includes three videogeneration tasks of increasing difficulty: action execution, where the nextaction must be generated starting from a conditioning video; storycontinuation, where a sequence of actions must be executed starting from aconditioning video; and story generation, where a video must be generated fromonly text prompts. We evaluate small yet strong text-to-video baselines, andshow the benefits of training on story-like data algorithmically generated fromexisting video captions. Finally, we establish guidelines for human evaluationof video stories, and reaffirm the need of better automatic metrics for videogeneration. StoryBench aims at encouraging future research efforts in thisexciting new area.</description><author>Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, Paul Voigtlaender</author><pubDate>Thu, 12 Oct 2023 18:50:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11606v2</guid></item><item><title>StoryBench: A Multifaceted Benchmark for Continuous Story Visualization</title><link>http://arxiv.org/abs/2308.11606v1</link><description>Generating video stories from text prompts is a complex task. In addition tohaving high visual quality, videos need to realistically adhere to a sequenceof text prompts whilst being consistent throughout the frames. Creating abenchmark for video generation requires data annotated over time, whichcontrasts with the single caption used often in video datasets. To fill thisgap, we collect comprehensive human annotations on three existing datasets, andintroduce StoryBench: a new, challenging multi-task benchmark to reliablyevaluate forthcoming text-to-video models. Our benchmark includes three videogeneration tasks of increasing difficulty: action execution, where the nextaction must be generated starting from a conditioning video; storycontinuation, where a sequence of actions must be executed starting from aconditioning video; and story generation, where a video must be generated fromonly text prompts. We evaluate small yet strong text-to-video baselines, andshow the benefits of training on story-like data algorithmically generated fromexisting video captions. Finally, we establish guidelines for human evaluationof video stories, and reaffirm the need of better automatic metrics for videogeneration. StoryBench aims at encouraging future research efforts in thisexciting new area.</description><author>Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, Paul Voigtlaender</author><pubDate>Tue, 22 Aug 2023 18:53:55 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11606v1</guid></item><item><title>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset</title><link>http://arxiv.org/abs/2305.18500v2</link><description>Vision and text have been fully explored in contemporary video-textfoundational models, while other modalities such as audio and subtitles invideos have not received sufficient attention. In this paper, we resort toestablish connections between multi-modality video tracks, including Vision,Audio, and Subtitle, and Text by exploring an automatically generatedlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,we first collect 27 million open-domain video clips and separately train avision and an audio captioner to generate vision and audio captions. Then, weemploy an off-the-shelf Large Language Model (LLM) to integrate the generatedcaptions, together with subtitles and instructional prompts into omni-modalitycaptions. Based on the proposed VAST-27M dataset, we train an omni-modalityvideo-text foundational model named VAST, which can perceive and processvision, audio, and subtitle modalities from video, and better support varioustasks including vision-text, audio-text, and multi-modal video-text tasks(retrieval, captioning and QA). Extensive experiments have been conducted todemonstrate the effectiveness of our proposed VAST-27M corpus and VASTfoundation model. VAST achieves 22 new state-of-the-art results on variouscross-modality benchmarks. Code, model and dataset will be released athttps://github.com/TXH-mercury/VAST.</description><author>Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu</author><pubDate>Sat, 07 Oct 2023 13:58:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18500v2</guid></item><item><title>VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset</title><link>http://arxiv.org/abs/2305.18500v1</link><description>Vision and text have been fully explored in contemporary video-textfoundational models, while other modalities such as audio and subtitles invideos have not received sufficient attention. In this paper, we resort toestablish connections between multi-modality video tracks, including Vision,Audio, and Subtitle, and Text by exploring an automatically generatedlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,we first collect 27 million open-domain video clips and separately train avision and an audio captioner to generate vision and audio captions. Then, weemploy an off-the-shelf Large Language Model (LLM) to integrate the generatedcaptions, together with subtitles and instructional prompts into omni-modalitycaptions. Based on the proposed VAST-27M dataset, we train an omni-modalityvideo-text foundational model named VAST, which can perceive and processvision, audio, and subtitle modalities from video, and better support varioustasks including vision-text, audio-text, and multi-modal video-text tasks(retrieval, captioning and QA). Extensive experiments have been conducted todemonstrate the effectiveness of our proposed VAST-27M corpus and VASTfoundation model. VAST achieves 22 new state-of-the-art results on variouscross-modality benchmarks. Code, model and dataset will be released athttps://github.com/TXH-mercury/VAST.</description><author>Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu</author><pubDate>Mon, 29 May 2023 15:34:50 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18500v1</guid></item><item><title>Video-CSR: Complex Video Digest Creation for Visual-Language Models</title><link>http://arxiv.org/abs/2310.05060v1</link><description>We present a novel task and human annotated dataset for evaluating theability for visual-language models to generate captions and summaries forreal-world video clips, which we call Video-CSR (Captioning, Summarization andRetrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds induration and covers a wide range of topics and interests. Each video clipcorresponds to 5 independently annotated captions (1 sentence) and summaries(3-10 sentences). Given any video selected from the dataset and itscorresponding ASR information, we evaluate visual-language models on eithercaption or summary generation that is grounded in both the visual and auditorycontent of the video. Additionally, models are also evaluated on caption- andsummary-based retrieval tasks, where the summary-based retrieval task requiresthe identification of a target video given excerpts of a corresponding summary.Given the novel nature of the paragraph-length video summarization task, weperform extensive comparative analyses of different existing evaluation metricsand their alignment with human preferences. Finally, we propose a foundationmodel with competitive generation and retrieval capabilities that serves as abaseline for the Video-CSR task. We aim for Video-CSR to serve as a usefulevaluation set in the age of large language models and complex multi-modaltasks.</description><author>Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang</author><pubDate>Sun, 08 Oct 2023 09:02:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05060v1</guid></item><item><title>SPOT! Revisiting Video-Language Models for Event Understanding</title><link>http://arxiv.org/abs/2311.12919v1</link><description>Understanding videos is an important research topic for multimodal learning.Leveraging large-scale datasets of web-crawled video-text pairs as weaksupervision has become a pre-training paradigm for learning jointrepresentations and showcased remarkable potential in video understandingtasks. However, videos can be multi-event and multi-grained, while thesevideo-text pairs usually contain only broad-level video captions. This raises aquestion: with such weak supervision, can video representation invideo-language models gain the ability to distinguish even factualdiscrepancies in textual description and understand fine-grained events? Toaddress this, we introduce SPOT Prober, to benchmark existing video-languagemodels's capacities of distinguishing event-level discrepancies as an indicatorof models' event understanding ability. Our approach involves extracting eventsas tuples (&lt;Subject, Predicate, Object, Attribute, Timestamps&gt;) from videos andgenerating false event tuples by manipulating tuple components systematically.We reevaluate the existing video-language models with these positive andnegative captions and find they fail to distinguish most of the manipulatedevents. Based on our findings, we propose to plug in these manipulated eventcaptions as hard negative samples and find them effective in enhancing modelsfor event understanding.</description><author>Gengyuan Zhang, Jinhe Bi, Jindong Gu, Volker Tresp</author><pubDate>Tue, 21 Nov 2023 18:43:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.12919v1</guid></item><item><title>VTimeLLM: Empower LLM to Grasp Video Moments</title><link>http://arxiv.org/abs/2311.18445v1</link><description>Large language models (LLMs) have shown remarkable text understandingcapabilities, which have been extended as Video LLMs to handle video data forcomprehending visual details. However, existing Video LLMs can only provide acoarse description of the entire video, failing to capture the precise startand end time boundary of specific events. In this paper, we solve this issuevia proposing VTimeLLM, a novel Video LLM designed for fine-grained videomoment understanding and reasoning with respect to time boundary. Specifically,our VTimeLLM adopts a boundary-aware three-stage training strategy, whichrespectively utilizes image-text pairs for feature alignment, multiple-eventvideos to increase temporal-boundary awareness, and high-qualityvideo-instruction tuning to further improve temporal understanding ability aswell as align with human intents. Extensive experiments demonstrate that infine-grained time-related comprehension tasks for videos such as Temporal VideoGrounding and Dense Video Captioning, VTimeLLM significantly outperformsexisting Video LLMs. Besides, benefits from the fine-grained temporalunderstanding of the videos further enable VTimeLLM to beat existing Video LLMsin video dialogue benchmark, showing its superior cross-modal understanding andreasoning abilities.</description><author>Bin Huang, Xin Wang, Hong Chen, Zihan Song, Wenwu Zhu</author><pubDate>Thu, 30 Nov 2023 10:49:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.18445v1</guid></item><item><title>Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks</title><link>http://arxiv.org/abs/2310.04914v1</link><description>Foundational multimodal models pre-trained on large scale image-text pairs orvideo-text pairs or both have shown strong generalization abilities ondownstream tasks. However unlike image-text models, pretraining video-textmodels is always not feasible due to the difficulty in collecting large-scaleclean and aligned data, and exponential computational costs involved in thepretraining phase. Therefore, the pertinent question to ask is: Can image-textmodels be adapted to video tasks and is there any benefit to using these modelsover pretraining directly on videos? In this work, we focus on this question byproposing a detailed study on the generalization abilities of image-text modelswhen evaluated on video understanding tasks in a zero-shot setting. Weinvestigate 9 foundational image-text models on a diverse set of video tasksthat include video action recognition (video AR), video retrieval (video RT),video question answering (video QA), video multiple choice (video MC) and videocaptioning (video CP). Our experiments show that image-text models exhibitimpressive performance on video AR, video RT and video MC. Furthermore, theyperform moderately on video captioning and poorly on video QA. These findingsshed a light on the benefits of adapting foundational image-text models to anarray of video tasks while avoiding the costly pretraining step.</description><author>Avinash Madasu, Anahita Bhiwandiwalla, Vasudev Lal</author><pubDate>Sat, 07 Oct 2023 21:57:54 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.04914v1</guid></item><item><title>An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling</title><link>http://arxiv.org/abs/2209.01540v4</link><description>Masked visual modeling (MVM) has been recently proven effective for visualpre-training. While similar reconstructive objectives on video inputs (e.g.,masked frame modeling) have been explored in video-language (VidL)pre-training, previous studies fail to find a truly effective MVM strategy thatcan largely benefit the downstream performance. In this work, we systematicallyexamine the potential of MVM in the context of VidL learning. Specifically, webase our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), wherethe supervision from MVM training can be backpropagated to the video pixelspace. In total, eight different reconstructive targets of MVM are explored,from low-level pixel values and oriented gradients to high-level depth maps,optical flow, discrete visual tokens, and latent visual features. We conductcomprehensive experiments and provide insights into the factors leading toeffective MVM training, resulting in an enhanced model VIOLETv2. Empirically,we show VIOLETv2 pre-trained with MVM objective achieves notable improvementson 13 VidL benchmarks, ranging from video question answering, video captioning,to text-to-video retrieval.</description><author>Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, Zicheng Liu</author><pubDate>Tue, 30 May 2023 07:56:20 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2209.01540v4</guid></item><item><title>OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation</title><link>http://arxiv.org/abs/2308.04126v1</link><description>This paper presents OmniDataComposer, an innovative approach for multimodaldata fusion and unlimited data generation with an intent to refine anduncomplicate interplay among diverse data modalities. Coming to the corebreakthrough, it introduces a cohesive data structure proficient in processingand merging multimodal data inputs, which include video, audio, and text. Ourcrafted algorithm leverages advancements across multiple operations such asvideo/image caption extraction, dense caption extraction, Automatic SpeechRecognition (ASR), Optical Character Recognition (OCR), Recognize AnythingModel(RAM), and object tracking. OmniDataComposer is capable of identifyingover 6400 categories of objects, substantially broadening the spectrum ofvisual information. It amalgamates these diverse modalities, promotingreciprocal enhancement among modalities and facilitating cross-modal datacorrection. \textbf{The final output metamorphoses each video input into anelaborate sequential document}, virtually transmuting videos into thoroughnarratives, making them easier to be processed by large language models. Futureprospects include optimizing datasets for each modality to encourage unlimiteddata generation. This robust base will offer priceless insights to models likeChatGPT, enabling them to create higher quality datasets for video captioningand easing question-answering tasks based on video content. OmniDataComposerinaugurates a new stage in multimodal learning, imparting enormous potentialfor augmenting AI's understanding and generation of complex, real-world data.</description><author>Dongyang Yu, Shihao Wang, Yuan Fang, Wangpeng An</author><pubDate>Tue, 08 Aug 2023 09:30:16 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04126v1</guid></item><item><title>MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning</title><link>http://arxiv.org/abs/2308.13218v1</link><description>Supervised visual captioning models typically require a large scale of imagesor videos paired with descriptions in a specific language (i.e., thevision-caption pairs) for training. However, collecting and labelinglarge-scale datasets is time-consuming and expensive for many scenarios andlanguages. Therefore, sufficient labeled pairs are usually not available. Todeal with the label shortage problem, we present a simple yet effectivezero-shot approach MultiCapCLIP that can generate visual captions for differentscenarios and languages without any labeled vision-caption pairs of downstreamdatasets. In the training stage, MultiCapCLIP only requires text data forinput. Then it conducts two main steps: 1) retrieving concept prompts thatpreserve the corresponding domain knowledge of new scenarios; 2) auto-encodingthe prompts to learn writing styles to output captions in a desired language.In the testing stage, MultiCapCLIP instead takes visual data as input directlyto retrieve the concept prompts to generate the final visual descriptions. Theextensive experiments on image and video captioning across four benchmarks andfour languages (i.e., English, Chinese, German, and French) confirm theeffectiveness of our approach. Compared with state-of-the-art zero-shot andweakly-supervised methods, our method achieves 4.8% and 21.5% absoluteimprovements in terms of BLEU@4 and CIDEr metrics. Our code is available athttps://github.com/yangbang18/MultiCapCLIP.</description><author>Bang Yang, Fenglin Liu, Xian Wu, Yaowei Wang, Xu Sun, Yuexian Zou</author><pubDate>Fri, 25 Aug 2023 08:32:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.13218v1</guid></item><item><title>Neutral Editing Framework for Diffusion-based Video Editing</title><link>http://arxiv.org/abs/2312.06708v1</link><description>Text-conditioned image editing has succeeded in various types of editingbased on a diffusion framework. Unfortunately, this success did not carry overto a video, which continues to be challenging. Existing video editing systemsare still limited to rigid-type editing such as style transfer and objectoverlay. To this end, this paper proposes Neutral Editing (NeuEdit) frameworkto enable complex non-rigid editing by changing the motion of a person/objectin a video, which has never been attempted before. NeuEdit introduces a conceptof `neutralization' that enhances a tuning-editing process of diffusion-basedediting systems in a model-agnostic manner by leveraging input video and textwithout any other auxiliary aids (e.g., visual masks, video captions).Extensive experiments on numerous videos demonstrate adaptability andeffectiveness of the NeuEdit framework. The website of our work is availablehere: https://neuedit.github.io</description><author>Sunjae Yoon, Gwanhyeong Koo, Ji Woo Hong, Chang D. Yoo</author><pubDate>Sun, 10 Dec 2023 16:28:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.06708v1</guid></item><item><title>Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval</title><link>http://arxiv.org/abs/2308.07648v1</link><description>In text-video retrieval, recent works have benefited from the powerfullearning capabilities of pre-trained text-image foundation models (e.g., CLIP)by adapting them to the video domain. A critical problem for them is how toeffectively capture the rich semantics inside the video using the image encoderof CLIP. To tackle this, state-of-the-art methods adopt complex cross-modalmodeling techniques to fuse the text information into video framerepresentations, which, however, incurs severe efficiency issues in large-scaleretrieval systems as the video representations must be recomputed online forevery text query. In this paper, we discard this problematic cross-modal fusionprocess and aim to learn semantically-enhanced representations purely from thevideo, so that the video representations can be computed offline and reused fordifferent texts. Concretely, we first introduce a spatial-temporal "PromptCube" into the CLIP image encoder and iteratively switch it within the encoderlayers to efficiently incorporate the global video semantics into framerepresentations. We then propose to apply an auxiliary video captioningobjective to train the frame representations, which facilitates the learning ofdetailed video semantics by providing fine-grained guidance in the semanticspace. With a naive temporal fusion strategy (i.e., mean-pooling) on theenhanced frame representations, we obtain state-of-the-art performances onthree benchmark datasets, i.e., MSR-VTT, MSVD, and LSMDC.</description><author>Chaorui Deng, Qi Chen, Pengda Qin, Da Chen, Qi Wu</author><pubDate>Tue, 15 Aug 2023 09:54:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.07648v1</guid></item><item><title>A Recipe for Scaling up Text-to-Video Generation with Text-free Videos</title><link>http://arxiv.org/abs/2312.15770v1</link><description>Diffusion-based text-to-video generation has witnessed impressive progress inthe past year yet still falls behind text-to-image generation. One of the keyreasons is the limited scale of publicly available data (e.g., 10M video-textpairs in WebVid10M vs. 5B image-text pairs in LAION), considering the high costof video captioning. Instead, it could be far easier to collect unlabeled clipsfrom video platforms like YouTube. Motivated by this, we come up with a noveltext-to-video generation framework, termed TF-T2V, which can directly learnwith text-free videos. The rationale behind is to separate the process of textdecoding from that of temporal modeling. To this end, we employ a contentbranch and a motion branch, which are jointly optimized with weights shared.Following such a pipeline, we study the effect of doubling the scale oftraining set (i.e., video-only WebVid10M) with some randomly collectedtext-free videos and are encouraged to observe the performance improvement (FIDfrom 9.67 to 8.19 and FVD from 484 to 441), demonstrating the scalability ofour approach. We also find that our model could enjoy sustainable performancegain (FID from 8.19 to 7.64 and FVD from 441 to 366) after reintroducing sometext labels for training. Finally, we validate the effectiveness andgeneralizability of our ideology on both native text-to-video generation andcompositional video synthesis paradigms. Code and models will be publiclyavailable at https://tf-t2v.github.io/.</description><author>Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, Nong Sang</author><pubDate>Mon, 25 Dec 2023 16:37:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.15770v1</guid></item><item><title>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title><link>http://arxiv.org/abs/2306.02858v2</link><description>We present Video-LLaMA, a multi-modal framework that empowers Large LanguageModels (LLMs) with the capability of understanding both visual and auditorycontent in the video. Video-LLaMA bootstraps cross-modal training from thefrozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previousvision-LLMs that focus on static image comprehensions such as MiniGPT-4 andLLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1)capturing the temporal changes in visual scenes, (2) integrating audio-visualsignals. To counter the first challenge, we propose a Video Q-former toassemble the pre-trained image encoder into our video encoder and introduce avideo-to-text generation task to learn video-language correspondence. For thesecond challenge, we leverage ImageBind, a universal embedding model aligningmultiple modalities as the pre-trained audio encoder, and introduce an AudioQ-former on top of ImageBind to learn reasonable auditory query embeddings forthe LLM module. To align the output of both visual &amp; audio encoders with LLM'sembedding space, we train Video-LLaMA on massive video/image-caption pairs aswell as visual-instruction-tuning datasets of moderate amount but higherquality. We found Video-LLaMA showcases the ability to perceive and comprehendvideo content, generating meaningful responses that are grounded in the visualand auditory information presented in the videos. This highlights the potentialof Video-LLaMA as a promising prototype for audio-visual AI assistants.</description><author>Hang Zhang, Xin Li, Lidong Bing</author><pubDate>Tue, 06 Jun 2023 13:28:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02858v2</guid></item><item><title>TCR: Short Video Title Generation and Cover Selection with Attention Refinement</title><link>http://arxiv.org/abs/2304.12561v1</link><description>With the widespread popularity of user-generated short videos, it becomesincreasingly challenging for content creators to promote their content topotential viewers. Automatically generating appealing titles and covers forshort videos can help grab viewers' attention. Existing studies on videocaptioning mostly focus on generating factual descriptions of actions, which donot conform to video titles intended for catching viewer attention.Furthermore, research for cover selection based on multimodal information issparse. These problems motivate the need for tailored methods to specificallysupport the joint task of short video title generation and cover selection(TG-CS) as well as the demand for creating corresponding datasets to supportthe studies. In this paper, we first collect and present a real-world datasetnamed Short Video Title Generation (SVTG) that contains videos with appealingtitles and covers. We then propose a Title generation and Cover selection withattention Refinement (TCR) method for TG-CS. The refinement procedureprogressively selects high-quality samples and highly relevant frames and texttokens within each sample to refine model training. Extensive experiments showthat our TCR method is superior to various existing video captioning methods ingenerating titles and is able to select better covers for noisy real-worldshort videos.</description><author>Yakun Yu, Jiuding Yang, Weidong Guo, Hui Liu, Yu Xu, Di Niu</author><pubDate>Tue, 25 Apr 2023 05:08:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.12561v1</guid></item><item><title>PaLI-X: On Scaling up a Multilingual Vision and Language Model</title><link>http://arxiv.org/abs/2305.18565v1</link><description>We present the training recipe and results of scaling up PaLI-X, amultilingual vision and language model, both in terms of size of the componentsand the breadth of its training task mixture. Our model achieves new levels ofperformance on a wide-range of varied and complex tasks, including multipleimage-based captioning and question-answering tasks, image-based documentunderstanding and few-shot (in-context) learning, as well as object detection,video question answering, and video captioning. PaLI-X advances thestate-of-the-art on most vision-and-language benchmarks considered (25+ ofthem). Finally, we observe emerging capabilities, such as complex counting andmultilingual object detection, tasks that are not explicitly in the trainingmix.</description><author>Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut</author><pubDate>Mon, 29 May 2023 19:58:38 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.18565v1</guid></item><item><title>OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation</title><link>http://arxiv.org/abs/2308.04126v2</link><description>This paper presents OmniDataComposer, an innovative approach for multimodaldata fusion and unlimited data generation with an intent to refine anduncomplicate interplay among diverse data modalities. Coming to the corebreakthrough, it introduces a cohesive data structure proficient in processingand merging multimodal data inputs, which include video, audio, and text. Our crafted algorithm leverages advancements across multiple operations suchas video/image caption extraction, dense caption extraction, Automatic SpeechRecognition (ASR), Optical Character Recognition (OCR), Recognize AnythingModel(RAM), and object tracking. OmniDataComposer is capable of identifyingover 6400 categories of objects, substantially broadening the spectrum ofvisual information. It amalgamates these diverse modalities, promotingreciprocal enhancement among modalities and facilitating cross-modal datacorrection. \textbf{The final output metamorphoses each video input into anelaborate sequential document}, virtually transmuting videos into thoroughnarratives, making them easier to be processed by large language models. Future prospects include optimizing datasets for each modality to encourageunlimited data generation. This robust base will offer priceless insights tomodels like ChatGPT, enabling them to create higher quality datasets for videocaptioning and easing question-answering tasks based on video content.OmniDataComposer inaugurates a new stage in multimodal learning, impartingenormous potential for augmenting AI's understanding and generation of complex,real-world data.</description><author>Dongyang Yu, Shihao Wang, Yuan Fang, Wangpeng An</author><pubDate>Thu, 17 Aug 2023 10:25:22 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.04126v2</guid></item><item><title>LLM4VG: Large Language Models Evaluation for Video Grounding</title><link>http://arxiv.org/abs/2312.14206v1</link><description>Recently, researchers have attempted to investigate the capability of LLMs inhandling videos and proposed several video LLM models. However, the ability ofLLMs to handle video grounding (VG), which is an important time-related videotask requiring the model to precisely locate the start and end timestamps oftemporal moments in videos that match the given textual queries, still remainsunclear and unexplored in literature. To fill the gap, in this paper, wepropose the LLM4VG benchmark, which systematically evaluates the performance ofdifferent LLMs on video grounding tasks. Based on our proposed LLM4VG, wedesign extensive experiments to examine two groups of video LLM models on videogrounding: (i) the video LLMs trained on the text-video pairs (denoted asVidLLM), and (ii) the LLMs combined with pretrained visual description modelssuch as the video/image captioning model. We propose prompt methods tointegrate the instruction of VG and description from different kinds ofgenerators, including caption-based generators for direct visual descriptionand VQA-based generators for information enhancement. We also providecomprehensive comparisons of various VidLLMs and explore the influence ofdifferent choices of visual models, LLMs, prompt designs, etc, as well. Ourexperimental evaluations lead to two conclusions: (i) the existing VidLLMs arestill far away from achieving satisfactory video grounding performance, andmore time-related video tasks should be included to further fine-tune thesemodels, and (ii) the combination of LLMs and visual models shows preliminaryabilities for video grounding with considerable potential for improvement byresorting to more reliable models and further guidance of prompt instructions.</description><author>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Zihan Song, Yuwei Zhou, Wenwu Zhu</author><pubDate>Thu, 21 Dec 2023 08:15:02 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14206v1</guid></item><item><title>LLM4VG: Large Language Models Evaluation for Video Grounding</title><link>http://arxiv.org/abs/2312.14206v2</link><description>Recently, researchers have attempted to investigate the capability of LLMs inhandling videos and proposed several video LLM models. However, the ability ofLLMs to handle video grounding (VG), which is an important time-related videotask requiring the model to precisely locate the start and end timestamps oftemporal moments in videos that match the given textual queries, still remainsunclear and unexplored in literature. To fill the gap, in this paper, wepropose the LLM4VG benchmark, which systematically evaluates the performance ofdifferent LLMs on video grounding tasks. Based on our proposed LLM4VG, wedesign extensive experiments to examine two groups of video LLM models on videogrounding: (i) the video LLMs trained on the text-video pairs (denoted asVidLLM), and (ii) the LLMs combined with pretrained visual description modelssuch as the video/image captioning model. We propose prompt methods tointegrate the instruction of VG and description from different kinds ofgenerators, including caption-based generators for direct visual descriptionand VQA-based generators for information enhancement. We also providecomprehensive comparisons of various VidLLMs and explore the influence ofdifferent choices of visual models, LLMs, prompt designs, etc, as well. Ourexperimental evaluations lead to two conclusions: (i) the existing VidLLMs arestill far away from achieving satisfactory video grounding performance, andmore time-related video tasks should be included to further fine-tune thesemodels, and (ii) the combination of LLMs and visual models shows preliminaryabilities for video grounding with considerable potential for improvement byresorting to more reliable models and further guidance of prompt instructions.</description><author>Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Zihan Song, Yuwei Zhou, Wenwu Zhu</author><pubDate>Thu, 28 Dec 2023 13:02:31 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.14206v2</guid></item><item><title>Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures</title><link>http://arxiv.org/abs/2307.15220v1</link><description>Recent advancements in surgical computer vision applications have been drivenby fully-supervised methods, primarily using only visual data. These methodsrely on manually annotated surgical videos to predict a fixed set of objectcategories, limiting their generalizability to unseen surgical procedures anddownstream tasks. In this work, we put forward the idea that the surgical videolectures available through open surgical e-learning platforms can provideeffective supervisory signals for multi-modal representation learning withoutrelying on manual annotations. We address the surgery-specific linguisticchallenges present in surgical video lectures by employing multiplecomplementary automatic speech recognition systems to generate texttranscriptions. We then present a novel method, SurgVLP - Surgical VisionLanguage Pre-training, for multi-modal representation learning. SurgVLPconstructs a new contrastive learning objective to align video clip embeddingswith the corresponding multiple text embeddings by bringing them togetherwithin a joint latent space. To effectively show the representation capabilityof the learned joint latent space, we introduce several vision-and-languagetasks for surgery, such as text-based video retrieval, temporal activitygrounding, and video captioning, as benchmarks for evaluation. We furtherdemonstrate that without using any labeled ground truth, our approach can beemployed for traditional vision-only surgical downstream tasks, such assurgical tool, phase, and triplet recognition. The code will be made availableat https://github.com/CAMMA-public/SurgVLP</description><author>Kun Yuan, Vinkle Srivastav, Tong Yu, Joel Lavanchy, Pietro Mascagni, Nassir Navab, Nicolas Padoy</author><pubDate>Thu, 27 Jul 2023 23:38:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.15220v1</guid></item><item><title>Visual Transformation Telling</title><link>http://arxiv.org/abs/2305.01928v1</link><description>In this paper, we propose a new visual reasoning task, called VisualTransformation Telling (VTT). This task requires a machine to describe thetransformation that occurred between every two adjacent states (i.e. images) ina series. Unlike most existing visual reasoning tasks that focus on statereasoning, VTT emphasizes transformation reasoning. We collected 13,547 samplesfrom two instructional video datasets, CrossTask and COIN, and extracteddesired states and transformation descriptions to create a suitable VTTbenchmark dataset. Humans can naturally reason from superficial statesdifferences (e.g. ground wetness) to transformations descriptions (e.g.raining) according to their life experience but how to model this process tobridge this semantic gap is challenging. We designed TTNet on top of existingvisual storytelling models by enhancing the model's state-differencesensitivity and transformation-context awareness. TTNet significantlyoutperforms other baseline models adapted from similar tasks, such as visualstorytelling and dense video captioning, demonstrating the effectiveness of ourmodeling on transformations. Through comprehensive diagnostic analyses, wefound TTNet has strong context utilization abilities, but even with somestate-of-the-art techniques such as CLIP, there remain challenges ingeneralization that need to be further explored.</description><author>Xin Hong, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng</author><pubDate>Wed, 03 May 2023 08:02:57 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01928v1</guid></item><item><title>Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos</title><link>http://arxiv.org/abs/2303.06378v2</link><description>Joint video-language learning has received increasing attention in recentyears. However, existing works mainly focus on single or multiple trimmed videoclips (events), which makes human-annotated event boundaries necessary duringinference. To break away from the ties, we propose a grounded vision-languagelearning framework for untrimmed videos, which automatically detectsinformative events and effectively excavates the alignments betweenmulti-sentence descriptions and corresponding event segments. Instead ofcoarse-level video-language alignments, we present two dual pretext tasks toencourage fine-grained segment-level alignments, i.e., text-to-event grounding(TEG) and event-to-text generation (ETG). TEG learns to adaptively ground thepossible event proposals given a set of sentences by estimating the cross-modaldistance in a joint semantic space. Meanwhile, ETG aims to reconstruct(generate) the matched texts given event proposals, encouraging the eventrepresentation to retain meaningful semantic information. To encourage accuratelabel assignment between the event set and the text set, we propose a novelsemantic-aware cost to mitigate the sub-optimal matching results caused byambiguous boundary annotations. Our framework is easily extensible to taskscovering visually-grounded language understanding and generation. We achievestate-of-the-art dense video captioning performance on ActivityNet Captions,YouCook2 and YouMakeup, and competitive performance on several other languagegeneration and understanding tasks. Our method also achieved 1st place in boththe MTVG and MDVC tasks of the PIC 4th Challenge. Our code is publiclyavailable at https://github.com/zjr2000/GVL.</description><author>Teng Wang, Jinrui Zhang, Feng Zheng, Wenhao Jiang, Ran Cheng, Ping Luo</author><pubDate>Wed, 17 May 2023 10:47:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.06378v2</guid></item><item><title>On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective</title><link>http://arxiv.org/abs/2212.12669v2</link><description>The pervasive uncertainty and dynamic nature of real-world environmentspresent significant challenges for the widespread implementation ofmachine-driven Intelligent Decision-Making (IDM) systems. Consequently, IDMshould possess the ability to continuously acquire new skills and effectivelygeneralize across a broad range of applications. The advancement of ArtificialGeneral Intelligence (AGI) that transcends task and application boundaries iscritical for enhancing IDM. Recent studies have extensively investigated theTransformer neural architecture as a foundational model for various tasks,including computer vision, natural language processing, and reinforcementlearning. We propose that a Foundation Decision Model (FDM) can be developed byformulating diverse decision-making tasks as sequence decoding tasks using theTransformer architecture, offering a promising solution for expanding IDMapplications in complex real-world situations. In this paper, we discuss theefficiency and generalization improvements offered by a foundation decisionmodel for IDM and explore its potential applications in multi-agent game AI,production scheduling, and robotics tasks. Lastly, we present a case studydemonstrating our FDM implementation, DigitalBrain (DB1) with 1.3 billionparameters, achieving human-level performance in 870 tasks, such as textgeneration, image captioning, video game playing, robotic control, andtraveling salesman problems. As a foundation decision model, DB1 represents aninitial step toward more autonomous and efficient real-world IDM applications.</description><author>Ying Wen, Ziyu Wan, Ming Zhou, Shufang Hou, Zhe Cao, Chenyang Le, Jingxiao Chen, Zheng Tian, Weinan Zhang, Jun Wang</author><pubDate>Tue, 16 May 2023 08:03:19 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2212.12669v2</guid></item><item><title>MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks</title><link>http://arxiv.org/abs/2303.16839v3</link><description>The development of language models have moved from encoder-decoder todecoder-only designs. In addition, we observe that the two most popularmultimodal tasks, the generative and contrastive tasks, are nontrivial toaccommodate in one architecture, and further need adaptations for downstreamtasks. We propose a novel paradigm of training with a decoder-only model formultimodal tasks, which is surprisingly effective in jointly learning of thesedisparate vision-language tasks. This is done with a simple model, calledMaMMUT. It consists of a single vision encoder and a text decoder, and is ableto accommodate contrastive and generative learning by a novel two-pass approachon the text decoder. We demonstrate that joint learning of these diverseobjectives is simple, effective, and maximizes the weight-sharing of the modelacross these tasks. Furthermore, the same architecture enables straightforwardextensions to open-vocabulary object detection and video-language tasks. Themodel tackles a diverse range of tasks, while being modest in capacity. Ourmodel achieves the state of the art on image-text and text-image retrieval,video question answering and open-vocabulary detection tasks, outperformingmuch larger and more extensively trained foundational models. It shows verycompetitive results on VQA and Video Captioning, especially considering itscapacity. Ablations confirm the flexibility and advantages of our approach.</description><author>Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, Anelia Angelova</author><pubDate>Wed, 09 Aug 2023 06:39:34 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.16839v3</guid></item><item><title>Factorized Contrastive Learning: Going Beyond Multi-view Redundancy</title><link>http://arxiv.org/abs/2306.05268v1</link><description>In a wide range of multimodal tasks, contrastive learning has become aparticularly appealing approach since it can successfully learn representationsfrom abundant unlabeled data with only pairing information (e.g., image-captionor video-audio pairs). Underpinning these approaches is the assumption ofmulti-view redundancy - that shared information between modalities is necessaryand sufficient for downstream tasks. However, in many real-world settings,task-relevant information is also contained in modality-unique regions:information that is only present in one modality but still relevant to thetask. How can we learn self-supervised multimodal representations to captureboth shared and unique information relevant to downstream tasks? This paperproposes FactorCL, a new multimodal representation learning method to go beyondmulti-view redundancy. FactorCL is built from three new contributions: (1)factorizing task-relevant information into shared and unique representations,(2) capturing task-relevant information via maximizing MI lower bounds andremoving task-irrelevant information via minimizing MI upper bounds, and (3)multimodal data augmentations to approximate task relevance without labels. Onlarge-scale real-world datasets, FactorCL captures both shared and uniqueinformation and achieves state-of-the-art results on six benchmarks.</description><author>Paul Pu Liang, Zihao Deng, Martin Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov</author><pubDate>Thu, 08 Jun 2023 16:17:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.05268v1</guid></item><item><title>Movie101: A New Movie Understanding Benchmark</title><link>http://arxiv.org/abs/2305.12140v2</link><description>To help the visually impaired enjoy movies, automatic movie narrating systemsare expected to narrate accurate, coherent, and role-aware plots when there areno speaking lines of actors. Existing works benchmark this challenge as anormal video captioning task via some simplifications, such as removing rolenames and evaluating narrations with ngram-based metrics, which makes itdifficult for automatic systems to meet the needs of real applicationscenarios. To narrow this gap, we construct a large-scale Chinese moviebenchmark, named Movie101. Closer to real scenarios, the Movie Clip Narrating(MCN) task in our benchmark asks models to generate role-aware narrationparagraphs for complete movie clips where no actors are speaking. Externalknowledge, such as role information and movie genres, is also provided forbetter movie understanding. Besides, we propose a new metric called MovieNarration Score (MNScore) for movie narrating evaluation, which achieves thebest correlation with human evaluation. Our benchmark also supports theTemporal Narration Grounding (TNG) task to investigate clip localization giventext descriptions. For both two tasks, our proposed methods well leverageexternal knowledge and outperform carefully designed baselines. The dataset andcodes are released at https://github.com/yuezih/Movie101.</description><author>Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, Qin Jin</author><pubDate>Tue, 27 Jun 2023 12:42:44 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.12140v2</guid></item><item><title>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</title><link>http://arxiv.org/abs/2306.04539v1</link><description>In many machine learning systems that jointly learn from multiple modalities,a core research question is to understand the nature of multimodalinteractions: the emergence of new task-relevant information during learningfrom both modalities that was not present in either alone. We study thischallenge of interaction quantification in a semi-supervised setting with onlylabeled unimodal data and naturally co-occurring multimodal data (e.g.,unlabeled images and captions, video and corresponding audio) but when labelingthem is time-consuming. Using a precise information-theoretic definition ofinteractions, our key contributions are the derivations of lower and upperbounds to quantify the amount of multimodal interactions in thissemi-supervised setting. We propose two lower bounds based on the amount ofshared information between modalities and the disagreement between separatelytrained unimodal classifiers, and derive an upper bound through connections toapproximate algorithms for min-entropy couplings. We validate these estimatedbounds and show how they accurately track true interactions. Finally, twosemi-supervised multimodal applications are explored based on these theoreticalresults: (1) analyzing the relationship between multimodal performance andestimated interactions, and (2) self-supervised learning that embracesdisagreement between modalities beyond agreement as is typically done.</description><author>Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov</author><pubDate>Wed, 07 Jun 2023 16:44:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.04539v1</guid></item><item><title>Visual Captioning at Will: Describing Images and Videos Guided by a Few Stylized Sentences</title><link>http://arxiv.org/abs/2307.16399v1</link><description>Stylized visual captioning aims to generate image or video descriptions withspecific styles, making them more attractive and emotionally appropriate. Onemajor challenge with this task is the lack of paired stylized captions forvisual content, so most existing works focus on unsupervised methods that donot rely on parallel datasets. However, these approaches still require trainingwith sufficient examples that have style labels, and the generated captions arelimited to predefined styles. To address these limitations, we explore theproblem of Few-Shot Stylized Visual Captioning, which aims to generate captionsin any desired style, using only a few examples as guidance during inference,without requiring further training. We propose a framework called FS-StyleCapfor this task, which utilizes a conditional encoder-decoder language model anda visual projection module. Our two-step training scheme proceeds as follows:first, we train a style extractor to generate style representations on anunlabeled text-only corpus. Then, we freeze the extractor and enable ourdecoder to generate stylized descriptions based on the extracted style vectorand projected visual content vectors. During inference, our model can generatedesired stylized captions by deriving the style representation fromuser-supplied examples. Our automatic evaluation results for few-shotsentimental visual captioning outperform state-of-the-art approaches and arecomparable to models that are fully trained on labeled style corpora. Humanevaluations further confirm our model s ability to handle multiple styles.</description><author>Dingyi Yang, Hongyu Chen, Xinglin Hou, Tiezheng Ge, Yuning Jiang, Qin Jin</author><pubDate>Mon, 31 Jul 2023 05:26:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.16399v1</guid></item><item><title>TVPR: Text-to-Video Person Retrieval and a New Benchmark</title><link>http://arxiv.org/abs/2307.07184v1</link><description>Most existing methods for text-based person retrieval focus on text-to-imageperson retrieval. Nevertheless, due to the lack of dynamic information providedby isolated frames, the performance is hampered when the person is obscured inisolated frames or variable motion details are given in the textualdescription. In this paper, we propose a new task called Text-to-Video PersonRetrieval(TVPR) which aims to effectively overcome the limitations of isolatedframes. Since there is no dataset or benchmark that describes person videoswith natural language, we construct a large-scale cross-modal person videodataset containing detailed natural language annotations, such as person'sappearance, actions and interactions with environment, etc., termed asText-to-Video Person Re-identification (TVPReid) dataset, which will bepublicly available. To this end, a Text-to-Video Person Retrieval Network(TVPRN) is proposed. Specifically, TVPRN acquires video representations byfusing visual and motion representations of person videos, which can deal withtemporal occlusion and the absence of variable motion details in isolatedframes. Meanwhile, we employ the pre-trained BERT to obtain captionrepresentations and the relationship between caption and video representationsto reveal the most relevant person videos. To evaluate the effectiveness of theproposed TVPRN, extensive experiments have been conducted on TVPReid dataset.To the best of our knowledge, TVPRN is the first successful attempt to usevideo for text-based person retrieval task and has achieved state-of-the-artperformance on TVPReid dataset. The TVPReid dataset will be publicly availableto benefit future research.</description><author>Fan Ni, Xu Zhang, Jianhui Wu, Guan-Nan Dong, Aichun Zhu, Hui Liu, Yue Zhang</author><pubDate>Fri, 14 Jul 2023 07:34:00 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.07184v1</guid></item><item><title>Valley: Video Assistant with Large Language model Enhanced abilitY</title><link>http://arxiv.org/abs/2306.07207v2</link><description>Large language models (LLMs), with their remarkable conversationalcapabilities, have demonstrated impressive performance across variousapplications and have emerged as formidable AI assistants. In view of this, itraises an intuitive question: Can we harness the power of LLMs to buildmultimodal AI assistants for visual applications? Recently, several multi-modalmodels have been developed for this purpose. They typically pre-train anadaptation module to align the semantics of the vision encoder and languagemodel, followed by fine-tuning on instruction-following data. However, despitethe success of this pipeline in image and language understanding, itseffectiveness in joint video and language understanding has not been widelyexplored. In this paper, we aim to develop a novel multi-modal foundation modelcapable of comprehending video, image, and language within a general framework.To achieve this goal, we introduce Valley, a Video Assistant with LargeLanguage model Enhanced abilitY. The Valley consists of a LLM, a temporalmodeling module, a visual encoder, and a simple projection module designed tobridge visual and textual modes. To empower Valley with video comprehension andinstruction-following capabilities, we construct a video instruction datasetand adopt a two-stage tuning procedure to train it. Specifically, we employChatGPT to facilitate the construction of task-oriented conversation dataencompassing various tasks, including multi-shot captions, long videodescriptions, action recognition, causal relationship inference, etc.Subsequently, we adopt a pre-training-then-instructions-tuned pipeline to alignvisual and textual modalities and improve the instruction-following capabilityof Valley. Qualitative experiments demonstrate that Valley has the potential tofunction as a highly effective video assistant that can make complex videounderstanding scenarios easy.</description><author>Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, Zhongyu Wei</author><pubDate>Sun, 08 Oct 2023 10:49:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.07207v2</guid></item><item><title>COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training</title><link>http://arxiv.org/abs/2401.00849v1</link><description>In the evolution of Vision-Language Pre-training, shifting from short-textcomprehension to encompassing extended textual contexts is pivotal. Recentautoregressive vision-language models like \cite{flamingo, palme}, leveragingthe long-context capability of Large Language Models, have excelled in few-shottext generation tasks but face challenges in alignment tasks. Addressing thisgap, we introduce the contrastive loss into text generation models, presentingthe COntrastive-Streamlined MultimOdal framework (\ModelName), strategicallypartitioning the language model into dedicated unimodal text processing andadept multimodal data handling components. \ModelName, our unified framework,merges unimodal and multimodal elements, enhancing model performance for tasksinvolving textual and visual data while notably reducing learnable parameters.However, these models demand extensive long-text datasets, yet the availabilityof high-quality long-text video datasets remains limited. To bridge this gap,this work introduces \VideoDatasetName, an inaugural interleaved video-textdataset featuring comprehensive captions, marking a significant step forward.Demonstrating its impact, we illustrate how \VideoDatasetName{} enhances modelperformance in image-text tasks. With 34% learnable parameters and utilizing72\% of the available data, our model demonstrates significant superiority overOpenFlamingo~\cite{openflamingo}. For instance, in the 4-shot flickr captioningtask, performance notably improves from 57.2% to 65.\%. The contributions of\ModelName{} and \VideoDatasetName{} are underscored by notable performancegains across 14 diverse downstream datasets encompassing both image-text andvideo-text tasks.</description><author>Alex Jinpeng Wang, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou</author><pubDate>Mon, 01 Jan 2024 18:58:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2401.00849v1</guid></item><item><title>Video-Helpful Multimodal Machine Translation</title><link>http://arxiv.org/abs/2310.20201v1</link><description>Existing multimodal machine translation (MMT) datasets consist of images andvideo captions or instructional video subtitles, which rarely containlinguistic ambiguity, making visual information ineffective in generatingappropriate translations. Recent work has constructed an ambiguous subtitlesdataset to alleviate this problem but is still limited to the problem thatvideos do not necessarily contribute to disambiguation. We introduce EVA(Extensive training set and Video-helpful evaluation set for Ambiguoussubtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En)parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs,and corresponding video clips collected from movies and TV episodes. Inaddition to the extensive training set, EVA contains a video-helpful evaluationset in which subtitles are ambiguous, and videos are guaranteed helpful fordisambiguation. Furthermore, we propose SAFA, an MMT model based on theSelective Attention model with two novel methods: Frame attention loss andAmbiguity augmentation, aiming to use videos in EVA for disambiguation fully.Experiments on EVA show that visual information and the proposed methods canboost translation performance, and our model performs significantly better thanexisting MMT models. The EVA dataset and the SAFA model are available at:https://github.com/ku-nlp/video-helpful-MMT.git.</description><author>Yihang Li, Shuichiro Shimizu, Chenhui Chu, Sadao Kurohashi, Wei Li</author><pubDate>Tue, 31 Oct 2023 06:51:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.20201v1</guid></item><item><title>C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval</title><link>http://arxiv.org/abs/2210.03625v2</link><description>Multilingual text-video retrieval methods have improved significantly inrecent years, but the performance for other languages lags behind English. Wepropose a Cross-Lingual Cross-Modal Knowledge Distillation method to improvemultilingual text-video retrieval. Inspired by the fact that English text-videoretrieval outperforms other languages, we train a student model using inputtext in different languages to match the cross-modal predictions from teachermodels using input text in English. We propose a cross entropy based objectivewhich forces the distribution over the student's text-video similarity scoresto be similar to those of the teacher models. We introduce a new multilingualvideo dataset, Multi-YouCook2, by translating the English captions in theYouCook2 video dataset to 8 other languages. Our method improves multilingualtext-video retrieval performance on Multi-YouCook2 and several other datasetssuch as Multi-MSRVTT and VATEX. We also conducted an analysis on theeffectiveness of different multilingual text models as teachers. The code,models, and dataset are available at https://github.com/roudimit/c2kd.</description><author>Andrew Rouditchenko, Yung-Sung Chuang, Nina Shvetsova, Samuel Thomas, Rogerio Feris, Brian Kingsbury, Leonid Karlinsky, David Harwath, Hilde Kuehne, James Glass</author><pubDate>Tue, 09 May 2023 20:58:59 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.03625v2</guid></item><item><title>YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus</title><link>http://arxiv.org/abs/2306.15162v1</link><description>Machine learning for sign languages is bottlenecked by data. In this paper,we present YouTube-ASL, a large-scale, open-domain corpus of American SignLanguage (ASL) videos and accompanying English captions drawn from YouTube.With ~1000 hours of videos and &gt;2500 unique signers, YouTube-ASL is ~3x aslarge and has ~10x as many unique signers as the largest prior ASL dataset. Wetrain baseline models for ASL to English translation on YouTube-ASL andevaluate them on How2Sign, where we achieve a new finetuned state of the art of12.39 BLEU and, for the first time, report zero-shot results.</description><author>David Uthus, Garrett Tanzer, Manfred Georg</author><pubDate>Tue, 27 Jun 2023 03:44:07 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15162v1</guid></item><item><title>VLM-Eval: A General Evaluation on Video Large Language Models</title><link>http://arxiv.org/abs/2311.11865v1</link><description>Despite the rapid development of video Large Language Models (LLMs), acomprehensive evaluation is still absent. In this paper, we introduce a unifiedevaluation that encompasses multiple video tasks, including captioning,question and answering, retrieval, and action recognition. In addition toconventional metrics, we showcase how GPT-based evaluation can match human-likeperformance in assessing response quality across multiple aspects. We propose asimple baseline: Video-LLaVA, which uses a single linear projection andoutperforms existing video LLMs. Finally, we evaluate video LLMs beyondacademic datasets, which show encouraging recognition and reasoningcapabilities in driving scenarios with only hundreds of video-instruction pairsfor fine-tuning. We hope our work can serve as a unified evaluation for videoLLMs, and help expand more practical scenarios. The evaluation code will beavailable soon.</description><author>Shuailin Li, Yuang Zhang, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, Tiancai Wang</author><pubDate>Mon, 20 Nov 2023 16:02:10 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.11865v1</guid></item><item><title>A Simple LLM Framework for Long-Range Video Question-Answering</title><link>http://arxiv.org/abs/2312.17235v1</link><description>We present LLoVi, a language-based framework for long-range videoquestion-answering (LVQA). Unlike prior long-range video understanding methods,which are often costly and require specialized long-range video modeling design(e.g., memory queues, state-space layers, etc.), our approach uses aframe/clip-level visual captioner (e.g., BLIP2, LaViLa, LLaVA) coupled with aLarge Language Model (GPT-3.5, GPT-4) leading to a simple yet surprisinglyeffective LVQA framework. Specifically, we decompose short and long-rangemodeling aspects of LVQA into two stages. First, we use a short-term visualcaptioner to generate textual descriptions of short video clips (0.5-8s inlength) densely sampled from a long input video. Afterward, an LLM aggregatesthe densely extracted short-term captions to perform long-range temporalreasoning needed to understand the whole video and answer a question. Toanalyze what makes our simple framework so effective, we thoroughly evaluatevarious components of our system. Our empirical analysis reveals that thechoice of the visual captioner and LLM is critical for good LVQA performance.Furthermore, we show that a specialized prompt that asks the LLM first tosummarize the noisy short-term visual captions and then answer a given inputquestion leads to a significant LVQA performance boost. On EgoSchema, which isbest known as a very long-form video question-answering benchmark, our methodachieves 50.3% accuracy, outperforming the previous best-performing approach by18.1% (absolute gain). In addition, our approach outperforms the previousstate-of-the-art by 4.1% and 3.1% on NeXT-QA and IntentQA. We also extend LLoVito grounded LVQA and show that it outperforms all prior methods on the NeXT-GQAdataset. We will release our code at https://github.com/CeeZh/LLoVi.</description><author>Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, Gedas Bertasius</author><pubDate>Thu, 28 Dec 2023 18:58:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.17235v1</guid></item><item><title>eP-ALM: Efficient Perceptual Augmentation of Language Models</title><link>http://arxiv.org/abs/2303.11403v4</link><description>Large Language Models (LLMs) have so far impressed the world, withunprecedented capabilities that emerge in models at large scales. On the visionside, transformer models (i.e., ViT) are following the same trend, achievingthe best performance on challenging benchmarks. With the abundance of suchunimodal models, a natural question arises; do we need also to follow thistrend to tackle multimodal tasks? In this work, we propose to rather directeffort to efficient adaptations of existing models, and propose to augmentLanguage Models with perception. Existing approaches for adapting pretrainedmodels for vision-language tasks still rely on several key components thathinder their efficiency. In particular, they still train a large number ofparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)trained on huge image-text datasets, and add significant inference overhead. Inaddition, most of these approaches have focused on Zero-Shot and In ContextLearning, with little to no effort on direct finetuning. We investigate theminimal computational effort needed to adapt unimodal models for multimodaltasks and propose a new challenging setup, alongside different approaches, thatefficiently adapts unimodal pretrained models. We show that by freezing morethan 99% of total parameters, training only one linear projection layer, andprepending only one trainable token, our approach (dubbed eP-ALM) significantlyoutperforms other baselines on VQA and Captioning across Image, Video, andAudio modalities, following the proposed setup. The code is available here:https://github.com/mshukor/eP-ALM.</description><author>Mustafa Shukor, Corentin Dancette, Matthieu Cord</author><pubDate>Fri, 27 Oct 2023 17:38:40 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11403v4</guid></item><item><title>eP-ALM: Efficient Perceptual Augmentation of Language Models</title><link>http://arxiv.org/abs/2303.11403v3</link><description>Large Language Models (LLMs) have so far impressed the world, withunprecedented capabilities that emerge in models at large scales. On the visionside, transformer models (i.e., ViT) are following the same trend, achievingthe best performance on challenging benchmarks. With the abundance of suchunimodal models, a natural question arises; do we need also to follow thistrend to tackle multimodal tasks? In this work, we propose to rather directeffort to efficient adaptations of existing models, and propose to augmentLanguage Models with perception. Existing approaches for adapting pretrainedmodels for vision-language tasks still rely on several key components thathinder their efficiency. In particular, they still train a large number ofparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)trained on huge image-text datasets, and add significant inference overhead. Inaddition, most of these approaches have focused on Zero-Shot and In ContextLearning, with little to no effort on direct finetuning. We investigate theminimal computational effort needed to adapt unimodal models for multimodaltasks and propose a new challenging setup, alongside different approaches, thatefficiently adapts unimodal pretrained models. We show that by freezing morethan 99% of total parameters, training only one linear projection layer, andprepending only one trainable token, our approach (dubbed eP-ALM) significantlyoutperforms other baselines on VQA and Captioning across Image, Video, andAudio modalities, following the proposed setup. The code is available here:https://github.com/mshukor/eP-ALM.</description><author>Mustafa Shukor, Corentin Dancette, Matthieu Cord</author><pubDate>Sat, 05 Aug 2023 09:25:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11403v3</guid></item><item><title>eP-ALM: Efficient Perceptual Augmentation of Language Models</title><link>http://arxiv.org/abs/2303.11403v2</link><description>Large Language Models (LLMs) have so far impressed the world, withunprecedented capabilities that emerge in models at large scales. On the visionside, transformer models (i.e., ViT) are following the same trend, achievingthe best performance on challenging benchmarks. With the abundance of suchunimodal models, a natural question arises; do we need also to follow thistrend to tackle multimodal tasks? In this work, we propose to rather directeffort to efficient adaptations of existing models, and propose to augmentLanguage Models with perception. Existing approaches for adapting pretrainedmodels for vision-language tasks still rely on several key components thathinder their efficiency. In particular, they still train a large number ofparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)trained on huge image-text datasets, and add significant inference overhead. Inaddition, most of these approaches have focused on Zero-Shot and In ContextLearning, with little to no effort on direct finetuning. We investigate theminimal computational effort needed to adapt unimodal models for multimodaltasks and propose a new challenging setup, alongside different approaches, thatefficiently adapts unimodal pretrained models. We show that by freezing morethan 99\% of total parameters, training only one linear projection layer, andprepending only one trainable token, our approach (dubbed eP-ALM) significantlyoutperforms other baselines on VQA and Captioning across Image, Video, andAudio modalities, following the proposed setup. The code will be availablehere: https://github.com/mshukor/eP-ALM.</description><author>Mustafa Shukor, Corentin Dancette, Matthieu Cord</author><pubDate>Mon, 12 Jun 2023 21:52:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2303.11403v2</guid></item><item><title>Generative Pretraining in Multimodality</title><link>http://arxiv.org/abs/2307.05222v1</link><description>We present Emu, a Transformer-based multimodal foundation model, which canseamlessly generate images and texts in multimodal context. This omnivore modelcan take in any single-modality or multimodal data input indiscriminately(e.g., interleaved image, text and video) through a one-model-for-allautoregressive training process. First, visual signals are encoded intoembeddings, and together with text tokens form an interleaved input sequence.Emu is then end-to-end trained with a unified objective of classifying the nexttext token or regressing the next visual embedding in the multimodal sequence.This versatile multimodality empowers the exploration of diverse pretrainingdata sources at scale, such as videos with interleaved frames and text,webpages with interleaved images and text, as well as web-scale image-textpairs and video-text pairs. Emu can serve as a generalist multimodal interfacefor both image-to-text and text-to-image tasks, and supports in-context imageand text generation. Across a broad range of zero-shot/few-shot tasks includingimage captioning, visual question answering, video question answering andtext-to-image generation, Emu demonstrates superb performance compared tostate-of-the-art large multimodal models. Extended capabilities such asmultimodal assistants via instruction tuning are also demonstrated withimpressive performance.</description><author>Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang</author><pubDate>Tue, 11 Jul 2023 13:45:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2307.05222v1</guid></item><item><title>Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos</title><link>http://arxiv.org/abs/2312.04746v1</link><description>The gigapixel scale of whole slide images (WSIs) poses a challenge forhistopathology multi-modal chatbots, requiring a global WSI analysis fordiagnosis, compounding evidence from different WSI patches. Current visualinstruction datasets, generated through large language models, focus oncreating question/answer pairs for individual image patches, which may lackdiagnostic capacity on their own in histopathology, further complicated by theabsence of spatial grounding in histopathology image captions. To bridge thisgap, we introduce Quilt-Instruct, a large-scale dataset of 107,131histopathology-specific instruction question/answer pairs, that is collected byleveraging educational histopathology videos from YouTube, which providesspatial localization of captions by automatically extracting narrators' cursormovements. In addition, we provide contextual reasoning by extracting diagnosisand supporting facts from the entire video content to guide the extrapolativereasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which canreason beyond the given single image patch, enabling diagnostic reasoning andthe capability of spatial awareness. To evaluate Quilt-LLaVA, we propose acomprehensive evaluation dataset created from 985 images and 1283human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA usingpublic histopathology datasets, where Quilt-LLaVA significantly outperformsSOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed setVQA. Our code, data, and model are publicly available at quilt-llava.github.io.</description><author>Mehmet Saygin Seyfioglu, Wisdom O. Ikezogwo, Fatemeh Ghezloo, Ranjay Krishna, Linda Shapiro</author><pubDate>Thu, 07 Dec 2023 23:16:37 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.04746v1</guid></item><item><title>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding</title><link>http://arxiv.org/abs/2306.02858v1</link><description>We present Video-LLaMA, a multi-modal framework that empowers Large LanguageModels (LLMs) with the capability of understanding both visual and auditorycontent in the video. Video-LLaMA bootstraps cross-modal training from thefrozen pre-trained visual \&amp; audio encoders and the frozen LLMs. Unlikeprevious vision- LLMs that focus on static image comprehensions such asMiniGPT-4~\citep{zhu2023minigpt} and LLaVA~\citep{liu2023visualit}, Video-LLaMAtackles two challenges in video understanding: (1) capturing the temporalchanges in visual scenes, (2) integrating audio-visual signals. For the firstchallenge, we propose Video Q-former to extend the pre-trained image encoder toa video encoder and introduce a video-to-text generation task to learnvideo-language correspondence. For the second challenge, we leverageImageBind~\citep{girdhar2023imagebind} as the pre-trained audio encoder whichperforms exceptionally well in aligning different modalities to a commonembedding space. And then introduce an Audio Q-former to learn auditory querytokens. To align the output of both visual \&amp; audio encoder with LLM'sembedding space, we train Video-LLaMA on a large-scale vision caption datasetand a hign-quantity vision-instruction-tuning dataset. We found Video-LLaMAshowcases the ability to perceive and comprehend video content, generatingmeaningful responses that are grounded in the visual and auditory informationpresent in the videos. This highlights the potential of Video-LLaMA as apromising prototype for audio-visual AI assistants. Our code, pre-trainedmodel, and demo are available at\url{https://github.com/DAMO-NLP-SG/Video-LLaMA}.</description><author>Hang Zhang, Xin Li, Lidong Bing</author><pubDate>Mon, 05 Jun 2023 14:17:27 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.02858v1</guid></item><item><title>LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos</title><link>http://arxiv.org/abs/2312.05269v1</link><description>The egocentric video natural language query (NLQ) task involves localizing atemporal window in an egocentric video that provides an answer to a posedquery, which has wide applications in building personalized AI assistants.Prior methods for this task have focused on improvements of networkarchitecture and leveraging pre-training for enhanced image and video features,but have struggled with capturing long-range temporal dependencies in lengthyvideos, and cumbersome end-to-end training. Motivated by recent advancements inLarge Language Models (LLMs) and vision language models, we introduceLifelongMemory, a novel framework that utilizes multiple pre-trained models toanswer queries from extensive egocentric video content. We address the uniquechallenge by employing a pre-trained captioning model to create detailednarratives of the videos. These narratives are then used to prompt a frozen LLMto generate coarse-grained temporal window predictions, which are subsequentlyrefined using a pre-trained NLQ model. Empirical results demonstrate that ourmethod achieves competitive performance against existing supervised end-to-endlearning methods, underlining the potential of integrating multiple pre-trainedmultimodal large language models in complex vision-language tasks. We provide acomprehensive analysis of key design decisions and hyperparameters in ourpipeline, offering insights and practical guidelines.</description><author>Ying Wang, Yanlai Yang, Mengye Ren</author><pubDate>Thu, 07 Dec 2023 19:19:25 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2312.05269v1</guid></item><item><title>Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos</title><link>http://arxiv.org/abs/2304.01186v2</link><description>Generating text-editable and pose-controllable character videos have animperious demand in creating various digital human. Nevertheless, this task hasbeen restricted by the absence of a comprehensive dataset featuring pairedvideo-pose captions and the generative prior models for videos. In this work,we design a novel two-stage training scheme that can utilize easily obtaineddatasets (i.e.,image pose pair and pose-free video) and the pre-trainedtext-to-image (T2I) model to obtain the pose-controllable character videos.Specifically, in the first stage, only the keypoint-image pairs are used onlyfor a controllable text-to-image generation. We learn a zero-initializedconvolutional encoder to encode the pose information. In the second stage, wefinetune the motion of the above network via a pose-free video dataset byadding the learnable temporal self-attention and reformed cross-frameself-attention blocks. Powered by our new designs, our method successfullygenerates continuously pose-controllable character videos while keeps theediting and concept composition ability of the pre-trained T2I model. The codeand models will be made publicly available.</description><author>Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen</author><pubDate>Wed, 03 Jan 2024 09:10:12 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2304.01186v2</guid></item><item><title>Scalable Mask Annotation for Video Text Spotting</title><link>http://arxiv.org/abs/2305.01443v1</link><description>Video text spotting refers to localizing, recognizing, and tracking textualelements such as captions, logos, license plates, signs, and other forms oftext within consecutive video frames. However, current datasets available forthis task rely on quadrilateral ground truth annotations, which may result inincluding excessive background content and inaccurate text boundaries.Furthermore, methods trained on these datasets often produce prediction resultsin the form of quadrilateral boxes, which limits their ability to handlecomplex scenarios such as dense or curved text. To address these issues, wepropose a scalable mask annotation pipeline called SAMText for video textspotting. SAMText leverages the SAM model to generate mask annotations forscene text images or video frames at scale. Using SAMText, we have created alarge-scale dataset, SAMText-9M, that contains over 2,400 video clips sourcedfrom existing datasets and over 9 million mask annotations. We have alsoconducted a thorough statistical analysis of the generated masks and theirquality, identifying several research topics that could be further exploredbased on this dataset. The code and dataset will be released at\url{https://github.com/ViTAE-Transformer/SAMText}.</description><author>Haibin He, Jing Zhang, Mengyang Xu, Juhua Liu, Bo Du, Dacheng Tao</author><pubDate>Tue, 02 May 2023 15:18:45 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.01443v1</guid></item><item><title>GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient Partially Relevant Video Retrieval</title><link>http://arxiv.org/abs/2310.05195v2</link><description>Given a text query, partially relevant video retrieval (PRVR) seeks to finduntrimmed videos containing pertinent moments in a database. For PRVR, clipmodeling is essential to capture the partial relationship between texts andvideos. Current PRVR methods adopt scanning-based clip construction to achieveexplicit clip modeling, which is information-redundant and requires a largestorage overhead. To solve the efficiency problem of PRVR methods, this paperproposes GMMFormer, a Gaussian-Mixture-Model based Transformer which modelsclip representations implicitly. During frame interactions, we incorporateGaussian-Mixture-Model constraints to focus each frame on its adjacent framesinstead of the whole video. Then generated representations will containmulti-scale clip information, achieving implicit clip modeling. In addition,PRVR methods ignore semantic differences between text queries relevant to thesame video, leading to a sparse embedding space. We propose a query diverseloss to distinguish these text queries, making the embedding space moreintensive and contain more semantic information. Extensive experiments on threelarge-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)demonstrate the superiority and efficiency of GMMFormer. Code is available at\url{https://github.com/huangmozhi9527/GMMFormer}.</description><author>Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia</author><pubDate>Wed, 03 Jan 2024 07:40:15 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.05195v2</guid></item><item><title>ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models</title><link>http://arxiv.org/abs/2306.16533v1</link><description>Video retrieval (VR) involves retrieving the ground truth video from thevideo database given a text caption or vice-versa. The two important componentsof compositionality: objects \&amp; attributes and actions are joined using correctsemantics to form a proper text query. These components (objects \&amp; attributes,actions and semantics) each play an important role to help distinguish amongvideos and retrieve the correct ground truth video. However, it is unclear whatis the effect of these components on the video retrieval performance. Wetherefore, conduct a systematic study to evaluate the compositional andsemantic understanding of video retrieval models on standard benchmarks such asMSRVTT, MSVD and DIDEMO. The study is performed on two categories of videoretrieval models: (i) which are pre-trained on video-text pairs and fine-tunedon downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.)(ii) which adapt pre-trained image-text representations like CLIP for videoretrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal thatactions and semantics play a minor role compared to objects \&amp; attributes invideo understanding. Moreover, video retrieval models that use pre-trainedimage-text representations (CLIP) have better semantic and compositionalunderstanding as compared to models pre-trained on video-text data.</description><author>Avinash Madasu, Vasudev Lal</author><pubDate>Wed, 28 Jun 2023 21:06:36 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16533v1</guid></item><item><title>Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention</title><link>http://arxiv.org/abs/2210.16428v3</link><description>Audio captioning aims to generate text descriptions of audio clips. In thereal world, many objects produce similar sounds. How to accurately recognizeambiguous sounds is a major challenge for audio captioning. In this work,inspired by inherent human multimodal perception, we propose visually-awareaudio captioning, which makes use of visual information to help the descriptionof ambiguous sounding objects. Specifically, we introduce an off-the-shelfvisual encoder to extract video features and incorporate the visual featuresinto an audio captioning system. Furthermore, to better exploit complementaryaudio-visual contexts, we propose an audio-visual attention mechanism thatadaptively integrates audio and visual context and removes the redundantinformation in the latent space. Experimental results on AudioCaps, the largestaudio captioning dataset, show that our proposed method achievesstate-of-the-art results on machine translation metrics.</description><author>Xubo Liu, Qiushi Huang, Xinhao Mei, Haohe Liu, Qiuqiang Kong, Jianyuan Sun, Shengchen Li, Tom Ko, Yu Zhang, Lilian H. Tang, Mark D. Plumbley, Volkan KÄ±lÄ±Ã§, Wenwu Wang</author><pubDate>Mon, 29 May 2023 04:53:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16428v3</guid></item><item><title>Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention</title><link>http://arxiv.org/abs/2210.16428v2</link><description>Audio captioning aims to generate text descriptions of audio clips. In thereal world, many objects produce similar sounds. How to accurately recognizeambiguous sounds is a major challenge for audio captioning. In this work,inspired by inherent human multimodal perception, we propose visually-awareaudio captioning, which makes use of visual information to help the descriptionof ambiguous sounding objects. Specifically, we introduce an off-the-shelfvisual encoder to extract video features and incorporate the visual featuresinto an audio captioning system. Furthermore, to better exploit complementaryaudio-visual contexts, we propose an audio-visual attention mechanism thatadaptively integrates audio and visual context and removes the redundantinformation in the latent space. Experimental results on AudioCaps, the largestaudio captioning dataset, show that our proposed method achievesstate-of-the-art results on machine translation metrics.</description><author>Xubo Liu, Qiushi Huang, Xinhao Mei, Haohe Liu, Qiuqiang Kong, Jianyuan Sun, Shengchen Li, Tom Ko, Yu Zhang, Lilian H. Tang, Mark D. Plumbley, Volkan KÄ±lÄ±Ã§, Wenwu Wang</author><pubDate>Wed, 24 May 2023 06:59:04 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2210.16428v2</guid></item><item><title>Towards Contrastive Learning in Music Video Domain</title><link>http://arxiv.org/abs/2309.00347v1</link><description>Contrastive learning is a powerful way of learning multimodal representationsacross various domains such as image-caption retrieval and audio-visualrepresentation learning. In this work, we investigate if these findingsgeneralize to the domain of music videos. Specifically, we create a dualen-coder for the audio and video modalities and train it using a bidirectionalcontrastive loss. For the experiments, we use an industry dataset containing550 000 music videos as well as the public Million Song Dataset, and evaluatethe quality of learned representations on the downstream tasks of music taggingand genre classification. Our results indicate that pre-trained networkswithout contrastive fine-tuning outperform our contrastive learning approachwhen evaluated on both tasks. To gain a better understanding of the reasonscontrastive learning was not successful for music videos, we perform aqualitative analysis of the learned representations, revealing why contrastivelearning might have difficulties uniting embeddings from two modalities. Basedon these findings, we outline possible directions for future work. Tofacilitate the reproducibility of our results, we share our code and thepre-trained model.</description><author>Karel Veldkamp, Mariya Hendriksen, ZoltÃ¡n SzlÃ¡vik, Alexander Keijser</author><pubDate>Fri, 01 Sep 2023 10:08:21 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.00347v1</guid></item><item><title>Style-transfer based Speech and Audio-visual Scene Understanding for Robot Action Sequence Acquisition from Videos</title><link>http://arxiv.org/abs/2306.15644v1</link><description>To realize human-robot collaboration, robots need to execute actions for newtasks according to human instructions given finite prior knowledge. Humanexperts can share their knowledge of how to perform a task with a robot throughmulti-modal instructions in their demonstrations, showing a sequence ofshort-horizon steps to achieve a long-horizon goal. This paper introduces amethod for robot action sequence generation from instruction videos using (1)an audio-visual Transformer that converts audio-visual features and instructionspeech to a sequence of robot actions called dynamic movement primitives (DMPs)and (2) style-transfer-based training that employs multi-task learning withvideo captioning and weakly-supervised learning with a semantic classifier toexploit unpaired video-action data. We built a system that accomplishes variouscooking actions, where an arm robot executes a DMP sequence acquired from acooking video using the audio-visual Transformer. Experiments withEpic-Kitchen-100, YouCookII, QuerYD, and in-house instruction video datasetsshow that the proposed method improves the quality of DMP sequences by 2.3times the METEOR score obtained with a baseline video-to-action Transformer.The model achieved 32% of the task success rate with the task knowledge of theobject.</description><author>Chiori Hori, Puyuan Peng, David Harwath, Xinyu Liu, Kei Ota, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, Jonathan Le Roux</author><pubDate>Tue, 27 Jun 2023 18:37:53 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.15644v1</guid></item><item><title>Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction</title><link>http://arxiv.org/abs/2301.09209v3</link><description>We study object interaction anticipation in egocentric videos. This taskrequires an understanding of the spatiotemporal context formed by past actionson objects, coined action context. We propose TransFusion, a multimodaltransformer-based architecture. It exploits the representational power oflanguage by summarising the action context. TransFusion leverages pre-trainedimage captioning and vision-language models to extract the action context frompast video frames. This action context together with the next video frame isprocessed by the multimodal fusion module to forecast the next objectinteraction. Our model enables more efficient end-to-end learning. The largepre-trained language models add common sense and a generalisation capability.Experiments on Ego4D and EPIC-KITCHENS-100 show the effectiveness of ourmultimodal fusion model. They also highlight the benefits of usinglanguage-based context summaries in a task where vision seems to suffice. Ourmethod outperforms state-of-the-art approaches by 40.4% in relative terms inoverall mAP on the Ego4D test set. We validate the effectiveness of TransFusionvia experiments on EPIC-KITCHENS-100. Video and code are available athttps://eth-ait.github.io/transfusion-proj/.</description><author>Razvan-George Pasca, Alexey Gavryushin, Yen-Ling Kuo, Luc Van Gool, Otmar Hilliges, Xi Wang</author><pubDate>Fri, 23 Jun 2023 11:43:14 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2301.09209v3</guid></item><item><title>COSA: Concatenated Sample Pretrained Vision-Language Foundation Model</title><link>http://arxiv.org/abs/2306.09085v1</link><description>Due to the limited scale and quality of video-text training corpus, mostvision-language foundation models employ image-text datasets for pretrainingand primarily focus on modeling visually semantic representations whiledisregarding temporal semantic representations and correlations. To addressthis issue, we propose COSA, a COncatenated SAmple pretrained vision-languagefoundation model. COSA jointly models visual contents and event-level temporalcues using only image-text corpora. We achieve this by sequentiallyconcatenating multiple image-text pairs as inputs for pretraining. Thistransformation effectively converts existing image-text corpora into a pseudolong-form video-paragraph corpus, enabling richer scene transformations andexplicit event-description correspondence. Extensive experiments demonstratethat COSA consistently improves performance across a broad range of downstreamtasks, including long-form/short-form video-text tasks and image-text taskssuch as retrieval, captioning, and question answering. Notably, COSA achievesstate-of-the-art results on various competitive benchmarks. Code and model arereleased at https://github.com/TXH-mercury/COSA.</description><author>Sihan Chen, Xingjian He, Handong Li, Xiaojie Jin, Jiashi Feng, Jing Liu</author><pubDate>Thu, 15 Jun 2023 13:29:42 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.09085v1</guid></item><item><title>Type-to-Track: Retrieve Any Object via Prompt-based Tracking</title><link>http://arxiv.org/abs/2305.13495v2</link><description>One of the recent trends in vision problems is to use natural languagecaptions to describe the objects of interest. This approach can overcome somelimitations of traditional methods that rely on bounding boxes or categoryannotations. This paper introduces a novel paradigm for Multiple ObjectTracking called Type-to-Track, which allows users to track objects in videos bytyping natural language descriptions. We present a new dataset for thatGrounded Multiple Object Tracking task, called GroOT, that contains videos withvarious types of objects and their corresponding textual captions describingtheir appearance and action in detail. Additionally, we introduce two newevaluation protocols and formulate evaluation metrics specifically for thistask. We develop a new efficient method that models a transformer-basedeMbed-ENcoDE-extRact framework (MENDER) using the third-order tensordecomposition. The experiments in five scenarios show that our MENDER approachoutperforms another two-stage design in terms of accuracy and efficiency, up to14.7% accuracy and 4$\times$ speed faster.</description><author>Pha Nguyen, Kha Gia Quach, Kris Kitani, Khoa Luu</author><pubDate>Tue, 22 Aug 2023 17:49:32 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13495v2</guid></item><item><title>Type-to-Track: Retrieve Any Object via Prompt-based Tracking</title><link>http://arxiv.org/abs/2305.13495v3</link><description>One of the recent trends in vision problems is to use natural languagecaptions to describe the objects of interest. This approach can overcome somelimitations of traditional methods that rely on bounding boxes or categoryannotations. This paper introduces a novel paradigm for Multiple ObjectTracking called Type-to-Track, which allows users to track objects in videos bytyping natural language descriptions. We present a new dataset for thatGrounded Multiple Object Tracking task, called GroOT, that contains videos withvarious types of objects and their corresponding textual captions describingtheir appearance and action in detail. Additionally, we introduce two newevaluation protocols and formulate evaluation metrics specifically for thistask. We develop a new efficient method that models a transformer-basedeMbed-ENcoDE-extRact framework (MENDER) using the third-order tensordecomposition. The experiments in five scenarios show that our MENDER approachoutperforms another two-stage design in terms of accuracy and efficiency, up to14.7% accuracy and 4$\times$ speed faster.</description><author>Pha Nguyen, Kha Gia Quach, Kris Kitani, Khoa Luu</author><pubDate>Sat, 30 Sep 2023 19:58:41 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13495v3</guid></item><item><title>Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought</title><link>http://arxiv.org/abs/2305.13903v3</link><description>Despite exciting recent results showing vision-language systems' capacity toreason about images using natural language, their capacity for video reasoningremains under-explored. We motivate framing video reasoning as the sequentialunderstanding of a small number of keyframes, thereby leveraging the power androbustness of vision-language while alleviating the computational complexitiesof processing videos. To evaluate this novel application, we introduce VIP, aninference-time challenge dataset designed to explore models' reasoningcapabilities through video chain-of-thought. Inspired by visually descriptivescene plays, we propose two formats for keyframe description: unstructureddense captions and structured scene descriptions that identify the focus,action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate videoreasoning, we propose two tasks: Video Infilling and Video Prediction, whichtest abilities to generate multiple intermediate keyframes and predict futurekeyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP,demonstrate the performance gap in these complex video reasoning tasks, andencourage future work to prioritize language models for efficient andgeneralized video reasoning.</description><author>Vaishnavi Himakunthala, Andy Ouyang, Daniel Rose, Ryan He, Alex Mei, Yujie Lu, Chinmay Sonar, Michael Saxon, William Yang Wang</author><pubDate>Thu, 09 Nov 2023 06:50:26 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2305.13903v3</guid></item><item><title>BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation</title><link>http://arxiv.org/abs/2310.10586v1</link><description>Building models that generate textual responses to user instructions forvideos is a practical and challenging topic, as it requires both visionunderstanding and knowledge reasoning. Compared to language and imagemodalities, training efficiency remains a serious problem as existing studiestrain models on massive sparse videos aligned with brief descriptions. In thispaper, we introduce BiLL-VTG, a fast adaptive framework that leverages largelanguage models (LLMs) to reasoning on videos based on essential lightweightvisual tools. Specifically, we reveal the key to response specific instructionsis the concentration on relevant video events, and utilize two visual tools ofstructured scene graph generation and descriptive image caption generation togather and represent the events information. Thus, a LLM equipped with worldknowledge is adopted as the reasoning agent to achieve the response byperforming multiple reasoning steps on specified video events.To address thedifficulty of specifying events from agent, we further propose anInstruction-oriented Video Events Recognition (InsOVER) algorithm based on theefficient Hungarian matching to localize corresponding video events usinglinguistic instructions, enabling LLMs to interact with long videos. Extensiveexperiments on two typical video-based texts generations tasks show that ourtuning-free framework outperforms the pre-trained models includingFlamingo-80B, to achieve the state-of-the-art performance.</description><author>Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li</author><pubDate>Mon, 16 Oct 2023 18:05:56 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10586v1</guid></item><item><title>Does Video Summarization Require Videos? Quantifying the Effectiveness of Language in Video Summarization</title><link>http://arxiv.org/abs/2309.09405v1</link><description>Video summarization remains a huge challenge in computer vision due to thesize of the input videos to be summarized. We propose an efficient,language-only video summarizer that achieves competitive accuracy with highdata efficiency. Using only textual captions obtained via a zero-shot approach,we train a language transformer model and forego image representations. Thismethod allows us to perform filtration amongst the representative text vectorsand condense the sequence. With our approach, we gain explainability withnatural language that comes easily for human interpretation and textualsummaries of the videos. An ablation study that focuses on modality and datacompression shows that leveraging text modality only effectively reduces inputdata processing while retaining comparable results.</description><author>Yoonsoo Nam, Adam Lehavi, Daniel Yang, Digbalay Bose, Swabha Swayamdipta, Shrikanth Narayanan</author><pubDate>Mon, 18 Sep 2023 01:08:49 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2309.09405v1</guid></item><item><title>LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</title><link>http://arxiv.org/abs/2311.17043v1</link><description>In this work, we present a novel method to tackle the token generationchallenge in Vision Language Models (VLMs) for video and image understanding,called LLaMA-VID. Current VLMs, while proficient in tasks like image captioningand visual question answering, face computational burdens when processing longvideos due to the excessive visual tokens. LLaMA-VID addresses this issue byrepresenting each frame with two distinct tokens, namely context token andcontent token. The context token encodes the overall image context based onuser input, whereas the content token encapsulates visual cues in each frame.This dual-token strategy significantly reduces the overload of long videoswhile preserving critical information. Generally, LLaMA-VID empowers existingframeworks to support hour-long videos and pushes their upper limit with anextra context token. It is proved to surpass previous methods on most of video-or image-based benchmarks. Code is availablehttps://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID</description><author>Yanwei Li, Chengyao Wang, Jiaya Jia</author><pubDate>Tue, 28 Nov 2023 18:53:43 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2311.17043v1</guid></item><item><title>ViCo: Engaging Video Comment Generation with Human Preference Rewards</title><link>http://arxiv.org/abs/2308.11171v1</link><description>Engaging video comments play an important role in video social media, as theyare the carrier of feelings, thoughts, or humor of the audience. Preliminaryworks have made initial exploration for video comment generation by adoptingcaption-style encoder-decoder models. However, comment generation presents someunique challenges distinct from caption generation, which makes these methodssomewhat less effective at generating engaging comments. In contrast to theobjective and descriptive nature of captions, comments tend to be inherentlysubjective, making it hard to quantify and evaluate the engagement of comments.Furthermore, the scarcity of truly engaging comments brings difficulty tocollecting enough high-quality training examples. In this paper, we proposeViCo with three novel designs to tackle the above challenges for generatingengaging Video Comments. Firstly, to quantify the engagement of comments, weutilize the number of "likes" each comment receives as a proxy of humanpreference after an appropriate debiasing procedure. Secondly, to automaticallyevaluate the engagement of comments, we train a reward model to align itsjudgment to the above proxy. Our user studies indicate that this reward modeleffectively aligns with human judgments. Lastly, to alleviate the scarcity ofhigh-quality comments, an initial generator is trained on readily available butnoisy data to generate comments. Then the reward model is employed to offerfeedback on the generated comments, thus optimizing the initial generator. Tofacilitate the research of video commenting, we collect a large videocomment-dataset (ViCo-20k) with rich metadata from a popular video website.Experiments on ViCo-20k show that the comments generated by our ViCo modelexhibit the best performance in terms of both quantitative and qualitativeresults, particularly when engagement is considered.</description><author>Yuchong Sun, Bei Liu, Xu Chen, Ruihua Song, Jianlong Fu</author><pubDate>Tue, 22 Aug 2023 05:01:01 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2308.11171v1</guid></item><item><title>Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023</title><link>http://arxiv.org/abs/2306.16545v1</link><description>We present Palm, a solution to the Long-Term Action Anticipation (LTA) taskutilizing vision-language and large language models. Given an input video withannotated action periods, the LTA task aims to predict possible future actions.We hypothesize that an optimal solution should capture the interdependencybetween past and future actions, and be able to infer future actions based onthe structure and dependency encoded in the past actions. Large language modelshave demonstrated remarkable commonsense-based reasoning ability. Inspired bythat, Palm chains an image captioning model and a large language model. Itpredicts future actions based on frame descriptions and action labels extractedfrom the input videos. Our method outperforms other participants in the EGO4DLTA challenge and achieves the best performance in terms of action prediction.Our code is available at https://github.com/DanDoge/Palm</description><author>Daoji Huang, Otmar Hilliges, Luc Van Gool, Xi Wang</author><pubDate>Wed, 28 Jun 2023 21:33:52 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2306.16545v1</guid></item><item><title>Few-shot Action Recognition with Captioning Foundation Models</title><link>http://arxiv.org/abs/2310.10125v1</link><description>Transferring vision-language knowledge from pretrained multimodal foundationmodels to various downstream tasks is a promising direction. However, mostcurrent few-shot action recognition methods are still limited to a singlevisual modality input due to the high cost of annotating additional textualdescriptions. In this paper, we develop an effective plug-and-play frameworkcalled CapFSAR to exploit the knowledge of multimodal models without manuallyannotating text. To be specific, we first utilize a captioning foundation model(i.e., BLIP) to extract visual features and automatically generate associatedcaptions for input videos. Then, we apply a text encoder to the syntheticcaptions to obtain representative text embeddings. Finally, a visual-textaggregation module based on Transformer is further designed to incorporatecross-modal spatio-temporal complementary information for reliable few-shotmatching. In this way, CapFSAR can benefit from powerful multimodal knowledgeof pretrained foundation models, yielding more comprehensive classification inthe low-shot regime. Extensive experiments on multiple standard few-shotbenchmarks demonstrate that the proposed CapFSAR performs favorably againstexisting methods and achieves state-of-the-art performance. The code will bemade publicly available.</description><author>Xiang Wang, Shiwei Zhang, Hangjie Yuan, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang</author><pubDate>Mon, 16 Oct 2023 08:08:39 GMT</pubDate><guid isPermaLink="true">http://arxiv.org/abs/2310.10125v1</guid></item></channel></rss>